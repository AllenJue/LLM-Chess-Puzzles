{
  "result": "gpt-3.5-turbo-instruct wins by disqualification (meta-llama/llama-3.3-70b-instruct failed to produce a legal move)",
  "white_player": "meta-llama/llama-3.3-70b-instruct",
  "black_player": "gpt-3.5-turbo-instruct",
  "white_score": 0.0,
  "black_score": 1.0,
  "total_moves": 2,
  "moves": [
    "e2e4",
    "e7e5"
  ],
  "game_history": [
    {
      "move_number": 1,
      "player": "meta-llama/llama-3.3-70b-instruct",
      "color": "white",
      "move_uci": "e2e4",
      "move_san": "e4",
      "fen_after": "rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "move_number": 2,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "e7e5",
      "move_san": "e5",
      "fen_after": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    }
  ],
  "final_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2",
  "white_tokens": {
    "prompt_tokens": 19901,
    "completion_tokens": 4443,
    "total_tokens": 24344
  },
  "black_tokens": {
    "prompt_tokens": 17155,
    "completion_tokens": 62774,
    "total_tokens": 79929
  },
  "white_token_log": [
    {
      "turn": 2,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 14,
          "total_tokens": 162,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 446,
        "total_completion_tokens": 28,
        "total_tokens": 474
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 14,
          "total_tokens": 162,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 4,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 21,
          "total_tokens": 177,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 18,
          "total_tokens": 174,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 18,
          "total_tokens": 173,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 467,
        "total_completion_tokens": 57,
        "total_tokens": 524
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 21,
          "total_tokens": 177,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 18,
          "total_tokens": 174,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 18,
          "total_tokens": 173,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 6,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 165,
          "completion_tokens": 28,
          "total_tokens": 193,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 165,
          "completion_tokens": 30,
          "total_tokens": 195,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 164,
          "completion_tokens": 27,
          "total_tokens": 191,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 494,
        "total_completion_tokens": 85,
        "total_tokens": 579
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 165,
          "completion_tokens": 28,
          "total_tokens": 193,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 165,
          "completion_tokens": 30,
          "total_tokens": 195,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 164,
          "completion_tokens": 27,
          "total_tokens": 191,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 8,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 174,
          "completion_tokens": 36,
          "total_tokens": 210,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 174,
          "completion_tokens": 36,
          "total_tokens": 210,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 173,
          "completion_tokens": 36,
          "total_tokens": 209,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 521,
        "total_completion_tokens": 108,
        "total_tokens": 629
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 174,
          "completion_tokens": 36,
          "total_tokens": 210,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 174,
          "completion_tokens": 36,
          "total_tokens": 210,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 173,
          "completion_tokens": 36,
          "total_tokens": 209,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 10,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 182,
          "completion_tokens": 47,
          "total_tokens": 229,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 182,
          "completion_tokens": 48,
          "total_tokens": 230,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 181,
          "completion_tokens": 48,
          "total_tokens": 229,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 545,
        "total_completion_tokens": 143,
        "total_tokens": 688
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 182,
          "completion_tokens": 47,
          "total_tokens": 229,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 182,
          "completion_tokens": 48,
          "total_tokens": 230,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 181,
          "completion_tokens": 48,
          "total_tokens": 229,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 12,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 190,
          "completion_tokens": 53,
          "total_tokens": 243,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 190,
          "completion_tokens": 53,
          "total_tokens": 243,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 189,
          "completion_tokens": 53,
          "total_tokens": 242,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 569,
        "total_completion_tokens": 159,
        "total_tokens": 728
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 190,
          "completion_tokens": 53,
          "total_tokens": 243,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 190,
          "completion_tokens": 53,
          "total_tokens": 243,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 189,
          "completion_tokens": 53,
          "total_tokens": 242,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 14,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 199,
          "completion_tokens": 65,
          "total_tokens": 264,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 199,
          "completion_tokens": 63,
          "total_tokens": 262,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 198,
          "completion_tokens": 65,
          "total_tokens": 263,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 596,
        "total_completion_tokens": 193,
        "total_tokens": 789
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 199,
          "completion_tokens": 65,
          "total_tokens": 264,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 199,
          "completion_tokens": 63,
          "total_tokens": 262,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 198,
          "completion_tokens": 65,
          "total_tokens": 263,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 16,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 208,
          "completion_tokens": 72,
          "total_tokens": 280,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 208,
          "completion_tokens": 72,
          "total_tokens": 280,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 207,
          "completion_tokens": 72,
          "total_tokens": 279,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 623,
        "total_completion_tokens": 216,
        "total_tokens": 839
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 208,
          "completion_tokens": 72,
          "total_tokens": 280,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 208,
          "completion_tokens": 72,
          "total_tokens": 280,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 207,
          "completion_tokens": 72,
          "total_tokens": 279,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 184,
          "completion_tokens": 16,
          "total_tokens": 200,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 554,
        "total_completion_tokens": 48,
        "total_tokens": 602
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 184,
          "completion_tokens": 16,
          "total_tokens": 200,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 2,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 14,
          "total_tokens": 162,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 446,
        "total_completion_tokens": 28,
        "total_tokens": 474
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 14,
          "total_tokens": 162,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 4,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 21,
          "total_tokens": 177,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 18,
          "total_tokens": 174,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 18,
          "total_tokens": 173,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 467,
        "total_completion_tokens": 57,
        "total_tokens": 524
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 21,
          "total_tokens": 177,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 18,
          "total_tokens": 174,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 18,
          "total_tokens": 173,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 6,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 165,
          "completion_tokens": 28,
          "total_tokens": 193,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 165,
          "completion_tokens": 30,
          "total_tokens": 195,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 164,
          "completion_tokens": 27,
          "total_tokens": 191,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 494,
        "total_completion_tokens": 85,
        "total_tokens": 579
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 165,
          "completion_tokens": 28,
          "total_tokens": 193,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 165,
          "completion_tokens": 30,
          "total_tokens": 195,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 164,
          "completion_tokens": 27,
          "total_tokens": 191,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 8,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 174,
          "completion_tokens": 36,
          "total_tokens": 210,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 174,
          "completion_tokens": 38,
          "total_tokens": 212,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 173,
          "completion_tokens": 36,
          "total_tokens": 209,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 521,
        "total_completion_tokens": 110,
        "total_tokens": 631
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 174,
          "completion_tokens": 36,
          "total_tokens": 210,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 174,
          "completion_tokens": 38,
          "total_tokens": 212,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 173,
          "completion_tokens": 36,
          "total_tokens": 209,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 10,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 182,
          "completion_tokens": 47,
          "total_tokens": 229,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 182,
          "completion_tokens": 48,
          "total_tokens": 230,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 181,
          "completion_tokens": 48,
          "total_tokens": 229,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 545,
        "total_completion_tokens": 143,
        "total_tokens": 688
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 182,
          "completion_tokens": 47,
          "total_tokens": 229,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 182,
          "completion_tokens": 48,
          "total_tokens": 230,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 181,
          "completion_tokens": 48,
          "total_tokens": 229,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 12,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 190,
          "completion_tokens": 54,
          "total_tokens": 244,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 190,
          "completion_tokens": 53,
          "total_tokens": 243,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 189,
          "completion_tokens": 53,
          "total_tokens": 242,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 569,
        "total_completion_tokens": 160,
        "total_tokens": 729
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 190,
          "completion_tokens": 54,
          "total_tokens": 244,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 190,
          "completion_tokens": 53,
          "total_tokens": 243,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 189,
          "completion_tokens": 53,
          "total_tokens": 242,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 14,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 199,
          "completion_tokens": 65,
          "total_tokens": 264,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 199,
          "completion_tokens": 63,
          "total_tokens": 262,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 198,
          "completion_tokens": 65,
          "total_tokens": 263,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 596,
        "total_completion_tokens": 193,
        "total_tokens": 789
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 199,
          "completion_tokens": 65,
          "total_tokens": 264,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 199,
          "completion_tokens": 63,
          "total_tokens": 262,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 198,
          "completion_tokens": 65,
          "total_tokens": 263,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 16,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 208,
          "completion_tokens": 72,
          "total_tokens": 280,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 208,
          "completion_tokens": 72,
          "total_tokens": 280,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 207,
          "completion_tokens": 72,
          "total_tokens": 279,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 623,
        "total_completion_tokens": 216,
        "total_tokens": 839
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 208,
          "completion_tokens": 72,
          "total_tokens": 280,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 208,
          "completion_tokens": 72,
          "total_tokens": 280,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 207,
          "completion_tokens": 72,
          "total_tokens": 279,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 184,
          "completion_tokens": 16,
          "total_tokens": 200,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 554,
        "total_completion_tokens": 48,
        "total_tokens": 602
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 184,
          "completion_tokens": 16,
          "total_tokens": 200,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 2,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 25,
          "total_tokens": 174,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 25,
          "total_tokens": 174,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 25,
          "total_tokens": 173,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 446,
        "total_completion_tokens": 75,
        "total_tokens": 521
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 25,
          "total_tokens": 174,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 25,
          "total_tokens": 174,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 25,
          "total_tokens": 173,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 4,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 157,
          "completion_tokens": 20,
          "total_tokens": 177,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 157,
          "completion_tokens": 20,
          "total_tokens": 177,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 156,
          "completion_tokens": 20,
          "total_tokens": 176,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 470,
        "total_completion_tokens": 60,
        "total_tokens": 530
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 157,
          "completion_tokens": 20,
          "total_tokens": 177,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 157,
          "completion_tokens": 20,
          "total_tokens": 177,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 156,
          "completion_tokens": 20,
          "total_tokens": 176,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 6,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 166,
          "completion_tokens": 27,
          "total_tokens": 193,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 166,
          "completion_tokens": 29,
          "total_tokens": 195,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 165,
          "completion_tokens": 33,
          "total_tokens": 198,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 497,
        "total_completion_tokens": 89,
        "total_tokens": 586
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 166,
          "completion_tokens": 27,
          "total_tokens": 193,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 166,
          "completion_tokens": 29,
          "total_tokens": 195,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 165,
          "completion_tokens": 33,
          "total_tokens": 198,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 8,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 174,
          "completion_tokens": 43,
          "total_tokens": 217,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 174,
          "completion_tokens": 34,
          "total_tokens": 208,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 173,
          "completion_tokens": 35,
          "total_tokens": 208,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 521,
        "total_completion_tokens": 112,
        "total_tokens": 633
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 174,
          "completion_tokens": 43,
          "total_tokens": 217,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 174,
          "completion_tokens": 34,
          "total_tokens": 208,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 173,
          "completion_tokens": 35,
          "total_tokens": 208,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 10,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 183,
          "completion_tokens": 47,
          "total_tokens": 230,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 183,
          "completion_tokens": 50,
          "total_tokens": 233,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 182,
          "completion_tokens": 47,
          "total_tokens": 229,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 548,
        "total_completion_tokens": 144,
        "total_tokens": 692
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 183,
          "completion_tokens": 47,
          "total_tokens": 230,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 183,
          "completion_tokens": 50,
          "total_tokens": 233,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 182,
          "completion_tokens": 47,
          "total_tokens": 229,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 12,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 191,
          "completion_tokens": 56,
          "total_tokens": 247,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 191,
          "completion_tokens": 54,
          "total_tokens": 245,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 190,
          "completion_tokens": 57,
          "total_tokens": 247,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 572,
        "total_completion_tokens": 167,
        "total_tokens": 739
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 191,
          "completion_tokens": 56,
          "total_tokens": 247,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 191,
          "completion_tokens": 54,
          "total_tokens": 245,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 190,
          "completion_tokens": 57,
          "total_tokens": 247,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 14,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 199,
          "completion_tokens": 64,
          "total_tokens": 263,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 199,
          "completion_tokens": 61,
          "total_tokens": 260,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 198,
          "completion_tokens": 61,
          "total_tokens": 259,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 596,
        "total_completion_tokens": 186,
        "total_tokens": 782
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 199,
          "completion_tokens": 64,
          "total_tokens": 263,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 199,
          "completion_tokens": 61,
          "total_tokens": 260,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 198,
          "completion_tokens": 61,
          "total_tokens": 259,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 16,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 208,
          "completion_tokens": 73,
          "total_tokens": 281,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 208,
          "completion_tokens": 68,
          "total_tokens": 276,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 207,
          "completion_tokens": 73,
          "total_tokens": 280,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 623,
        "total_completion_tokens": 214,
        "total_tokens": 837
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 208,
          "completion_tokens": 73,
          "total_tokens": 281,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 208,
          "completion_tokens": 68,
          "total_tokens": 276,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 207,
          "completion_tokens": 73,
          "total_tokens": 280,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 18,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 216,
          "completion_tokens": 79,
          "total_tokens": 295,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 216,
          "completion_tokens": 79,
          "total_tokens": 295,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 215,
          "completion_tokens": 79,
          "total_tokens": 294,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 647,
        "total_completion_tokens": 237,
        "total_tokens": 884
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 216,
          "completion_tokens": 79,
          "total_tokens": 295,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 216,
          "completion_tokens": 79,
          "total_tokens": 295,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 215,
          "completion_tokens": 79,
          "total_tokens": 294,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 185,
          "completion_tokens": 28,
          "total_tokens": 213,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 184,
          "completion_tokens": 16,
          "total_tokens": 200,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 554,
        "total_completion_tokens": 60,
        "total_tokens": 614
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 185,
          "completion_tokens": 28,
          "total_tokens": 213,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 184,
          "completion_tokens": 16,
          "total_tokens": 200,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 2,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 14,
          "total_tokens": 162,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 446,
        "total_completion_tokens": 28,
        "total_tokens": 474
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 14,
          "total_tokens": 162,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 4,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 21,
          "total_tokens": 177,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 18,
          "total_tokens": 174,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 18,
          "total_tokens": 173,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 467,
        "total_completion_tokens": 57,
        "total_tokens": 524
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 21,
          "total_tokens": 177,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 18,
          "total_tokens": 174,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 18,
          "total_tokens": 173,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 6,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 165,
          "completion_tokens": 28,
          "total_tokens": 193,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 165,
          "completion_tokens": 30,
          "total_tokens": 195,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 164,
          "completion_tokens": 27,
          "total_tokens": 191,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 494,
        "total_completion_tokens": 85,
        "total_tokens": 579
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 165,
          "completion_tokens": 28,
          "total_tokens": 193,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 165,
          "completion_tokens": 30,
          "total_tokens": 195,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 164,
          "completion_tokens": 27,
          "total_tokens": 191,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 8,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 173,
          "completion_tokens": 35,
          "total_tokens": 208,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 173,
          "completion_tokens": 35,
          "total_tokens": 208,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 172,
          "completion_tokens": 35,
          "total_tokens": 207,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 518,
        "total_completion_tokens": 105,
        "total_tokens": 623
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 173,
          "completion_tokens": 35,
          "total_tokens": 208,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 173,
          "completion_tokens": 35,
          "total_tokens": 208,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 172,
          "completion_tokens": 35,
          "total_tokens": 207,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 10,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 182,
          "completion_tokens": 53,
          "total_tokens": 235,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 182,
          "completion_tokens": 49,
          "total_tokens": 231,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 181,
          "completion_tokens": 49,
          "total_tokens": 230,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 545,
        "total_completion_tokens": 151,
        "total_tokens": 696
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 182,
          "completion_tokens": 53,
          "total_tokens": 235,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 182,
          "completion_tokens": 49,
          "total_tokens": 231,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 181,
          "completion_tokens": 49,
          "total_tokens": 230,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 12,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 189,
          "completion_tokens": 51,
          "total_tokens": 240,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 189,
          "completion_tokens": 50,
          "total_tokens": 239,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 188,
          "completion_tokens": 52,
          "total_tokens": 240,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 566,
        "total_completion_tokens": 153,
        "total_tokens": 719
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 189,
          "completion_tokens": 51,
          "total_tokens": 240,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 189,
          "completion_tokens": 50,
          "total_tokens": 239,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 188,
          "completion_tokens": 52,
          "total_tokens": 240,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 14,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 197,
          "completion_tokens": 59,
          "total_tokens": 256,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 197,
          "completion_tokens": 58,
          "total_tokens": 255,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 196,
          "completion_tokens": 63,
          "total_tokens": 259,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 590,
        "total_completion_tokens": 180,
        "total_tokens": 770
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 197,
          "completion_tokens": 59,
          "total_tokens": 256,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 197,
          "completion_tokens": 58,
          "total_tokens": 255,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 196,
          "completion_tokens": 63,
          "total_tokens": 259,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 16,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 206,
          "completion_tokens": 69,
          "total_tokens": 275,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 206,
          "completion_tokens": 67,
          "total_tokens": 273,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 205,
          "completion_tokens": 67,
          "total_tokens": 272,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 617,
        "total_completion_tokens": 203,
        "total_tokens": 820
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 206,
          "completion_tokens": 69,
          "total_tokens": 275,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 206,
          "completion_tokens": 67,
          "total_tokens": 273,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 205,
          "completion_tokens": 67,
          "total_tokens": 272,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 185,
          "completion_tokens": 28,
          "total_tokens": 213,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 184,
          "completion_tokens": 16,
          "total_tokens": 200,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 554,
        "total_completion_tokens": 60,
        "total_tokens": 614
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 185,
          "completion_tokens": 28,
          "total_tokens": 213,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 184,
          "completion_tokens": 16,
          "total_tokens": 200,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    }
  ],
  "black_token_log": [
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 547,
          "total_tokens": 657,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 640,
          "total_tokens": 749,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 329,
        "total_completion_tokens": 1827,
        "total_tokens": 2156
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 547,
          "total_tokens": 657,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 640,
          "total_tokens": 749,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 3,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 116,
          "completion_tokens": 640,
          "total_tokens": 756,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 116,
          "completion_tokens": 640,
          "total_tokens": 756,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 115,
          "completion_tokens": 640,
          "total_tokens": 755,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 347,
        "total_completion_tokens": 1920,
        "total_tokens": 2267
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 116,
          "completion_tokens": 640,
          "total_tokens": 756,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 116,
          "completion_tokens": 640,
          "total_tokens": 756,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 115,
          "completion_tokens": 640,
          "total_tokens": 755,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 5,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 124,
          "completion_tokens": 640,
          "total_tokens": 764,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 124,
          "completion_tokens": 640,
          "total_tokens": 764,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 123,
          "completion_tokens": 640,
          "total_tokens": 763,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 371,
        "total_completion_tokens": 1920,
        "total_tokens": 2291
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 124,
          "completion_tokens": 640,
          "total_tokens": 764,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 124,
          "completion_tokens": 640,
          "total_tokens": 764,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 123,
          "completion_tokens": 640,
          "total_tokens": 763,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 7,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 133,
          "completion_tokens": 519,
          "total_tokens": 652,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 133,
          "completion_tokens": 640,
          "total_tokens": 773,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 132,
          "completion_tokens": 254,
          "total_tokens": 386,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 398,
        "total_completion_tokens": 1413,
        "total_tokens": 1811
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 133,
          "completion_tokens": 519,
          "total_tokens": 652,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 133,
          "completion_tokens": 640,
          "total_tokens": 773,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 132,
          "completion_tokens": 254,
          "total_tokens": 386,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 9,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 142,
          "completion_tokens": 640,
          "total_tokens": 782,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 142,
          "completion_tokens": 640,
          "total_tokens": 782,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 141,
          "completion_tokens": 180,
          "total_tokens": 321,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 425,
        "total_completion_tokens": 1460,
        "total_tokens": 1885
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 142,
          "completion_tokens": 640,
          "total_tokens": 782,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 142,
          "completion_tokens": 640,
          "total_tokens": 782,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 141,
          "completion_tokens": 180,
          "total_tokens": 321,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 11,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 150,
          "completion_tokens": 291,
          "total_tokens": 441,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 150,
          "completion_tokens": 640,
          "total_tokens": 790,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 149,
          "completion_tokens": 640,
          "total_tokens": 789,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 449,
        "total_completion_tokens": 1571,
        "total_tokens": 2020
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 150,
          "completion_tokens": 291,
          "total_tokens": 441,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 150,
          "completion_tokens": 640,
          "total_tokens": 790,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 149,
          "completion_tokens": 640,
          "total_tokens": 789,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 13,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 157,
          "completion_tokens": 446,
          "total_tokens": 603,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 157,
          "completion_tokens": 640,
          "total_tokens": 797,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 156,
          "completion_tokens": 460,
          "total_tokens": 616,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 470,
        "total_completion_tokens": 1546,
        "total_tokens": 2016
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 157,
          "completion_tokens": 446,
          "total_tokens": 603,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 157,
          "completion_tokens": 640,
          "total_tokens": 797,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 156,
          "completion_tokens": 460,
          "total_tokens": 616,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 15,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 167,
          "completion_tokens": 640,
          "total_tokens": 807,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 167,
          "completion_tokens": 640,
          "total_tokens": 807,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 166,
          "completion_tokens": 409,
          "total_tokens": 575,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 500,
        "total_completion_tokens": 1689,
        "total_tokens": 2189
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 167,
          "completion_tokens": 640,
          "total_tokens": 807,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 167,
          "completion_tokens": 640,
          "total_tokens": 807,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 166,
          "completion_tokens": 409,
          "total_tokens": 575,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 17,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 175,
          "completion_tokens": 288,
          "total_tokens": 463,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 175,
          "completion_tokens": 640,
          "total_tokens": 815,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 174,
          "completion_tokens": 640,
          "total_tokens": 814,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 524,
        "total_completion_tokens": 1568,
        "total_tokens": 2092
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 175,
          "completion_tokens": 288,
          "total_tokens": 463,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 175,
          "completion_tokens": 640,
          "total_tokens": 815,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 174,
          "completion_tokens": 640,
          "total_tokens": 814,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 2,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 236,
          "total_tokens": 350,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 622,
          "total_tokens": 736,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 230,
          "total_tokens": 343,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 341,
        "total_completion_tokens": 1088,
        "total_tokens": 1429
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 236,
          "total_tokens": 350,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 622,
          "total_tokens": 736,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 230,
          "total_tokens": 343,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 492,
          "total_tokens": 602,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 568,
          "total_tokens": 677,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 329,
        "total_completion_tokens": 1700,
        "total_tokens": 2029
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 492,
          "total_tokens": 602,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 568,
          "total_tokens": 677,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 3,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 116,
          "completion_tokens": 525,
          "total_tokens": 641,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 116,
          "completion_tokens": 636,
          "total_tokens": 752,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 115,
          "completion_tokens": 640,
          "total_tokens": 755,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 347,
        "total_completion_tokens": 1801,
        "total_tokens": 2148
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 116,
          "completion_tokens": 525,
          "total_tokens": 641,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 116,
          "completion_tokens": 636,
          "total_tokens": 752,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 115,
          "completion_tokens": 640,
          "total_tokens": 755,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 5,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 124,
          "completion_tokens": 601,
          "total_tokens": 725,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 124,
          "completion_tokens": 508,
          "total_tokens": 632,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 123,
          "completion_tokens": 640,
          "total_tokens": 763,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 371,
        "total_completion_tokens": 1749,
        "total_tokens": 2120
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 124,
          "completion_tokens": 601,
          "total_tokens": 725,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 124,
          "completion_tokens": 508,
          "total_tokens": 632,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 123,
          "completion_tokens": 640,
          "total_tokens": 763,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 7,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 133,
          "completion_tokens": 291,
          "total_tokens": 424,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 133,
          "completion_tokens": 595,
          "total_tokens": 728,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 132,
          "completion_tokens": 323,
          "total_tokens": 455,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 398,
        "total_completion_tokens": 1209,
        "total_tokens": 1607
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 133,
          "completion_tokens": 291,
          "total_tokens": 424,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 133,
          "completion_tokens": 595,
          "total_tokens": 728,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 132,
          "completion_tokens": 323,
          "total_tokens": 455,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 9,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 142,
          "completion_tokens": 640,
          "total_tokens": 782,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 142,
          "completion_tokens": 640,
          "total_tokens": 782,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 141,
          "completion_tokens": 365,
          "total_tokens": 506,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 425,
        "total_completion_tokens": 1645,
        "total_tokens": 2070
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 142,
          "completion_tokens": 640,
          "total_tokens": 782,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 142,
          "completion_tokens": 640,
          "total_tokens": 782,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 141,
          "completion_tokens": 365,
          "total_tokens": 506,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 11,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 150,
          "completion_tokens": 338,
          "total_tokens": 488,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 150,
          "completion_tokens": 640,
          "total_tokens": 790,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 149,
          "completion_tokens": 522,
          "total_tokens": 671,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 449,
        "total_completion_tokens": 1500,
        "total_tokens": 1949
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 150,
          "completion_tokens": 338,
          "total_tokens": 488,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 150,
          "completion_tokens": 640,
          "total_tokens": 790,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 149,
          "completion_tokens": 522,
          "total_tokens": 671,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 13,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 157,
          "completion_tokens": 640,
          "total_tokens": 797,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 157,
          "completion_tokens": 640,
          "total_tokens": 797,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 156,
          "completion_tokens": 640,
          "total_tokens": 796,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 470,
        "total_completion_tokens": 1920,
        "total_tokens": 2390
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 157,
          "completion_tokens": 640,
          "total_tokens": 797,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 157,
          "completion_tokens": 640,
          "total_tokens": 797,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 156,
          "completion_tokens": 640,
          "total_tokens": 796,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 15,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 167,
          "completion_tokens": 114,
          "total_tokens": 281,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 167,
          "completion_tokens": 426,
          "total_tokens": 593,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 166,
          "completion_tokens": 640,
          "total_tokens": 806,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 500,
        "total_completion_tokens": 1180,
        "total_tokens": 1680
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 167,
          "completion_tokens": 114,
          "total_tokens": 281,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 167,
          "completion_tokens": 426,
          "total_tokens": 593,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 166,
          "completion_tokens": 640,
          "total_tokens": 806,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 17,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 175,
          "completion_tokens": 416,
          "total_tokens": 591,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 175,
          "completion_tokens": 572,
          "total_tokens": 747,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 174,
          "completion_tokens": 545,
          "total_tokens": 719,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 524,
        "total_completion_tokens": 1533,
        "total_tokens": 2057
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 175,
          "completion_tokens": 416,
          "total_tokens": 591,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 175,
          "completion_tokens": 572,
          "total_tokens": 747,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 174,
          "completion_tokens": 545,
          "total_tokens": 719,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 2,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 590,
          "total_tokens": 704,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 230,
          "total_tokens": 343,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 341,
        "total_completion_tokens": 1460,
        "total_tokens": 1801
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 590,
          "total_tokens": 704,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 230,
          "total_tokens": 343,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 317,
          "total_tokens": 427,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 563,
          "total_tokens": 673,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 640,
          "total_tokens": 749,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 329,
        "total_completion_tokens": 1520,
        "total_tokens": 1849
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 317,
          "total_tokens": 427,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 563,
          "total_tokens": 673,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 640,
          "total_tokens": 749,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 3,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 116,
          "completion_tokens": 640,
          "total_tokens": 756,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 116,
          "completion_tokens": 400,
          "total_tokens": 516,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 115,
          "completion_tokens": 640,
          "total_tokens": 755,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 347,
        "total_completion_tokens": 1680,
        "total_tokens": 2027
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 116,
          "completion_tokens": 640,
          "total_tokens": 756,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 116,
          "completion_tokens": 400,
          "total_tokens": 516,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 115,
          "completion_tokens": 640,
          "total_tokens": 755,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 5,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 125,
          "completion_tokens": 640,
          "total_tokens": 765,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 125,
          "completion_tokens": 640,
          "total_tokens": 765,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 124,
          "completion_tokens": 640,
          "total_tokens": 764,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 374,
        "total_completion_tokens": 1920,
        "total_tokens": 2294
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 125,
          "completion_tokens": 640,
          "total_tokens": 765,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 125,
          "completion_tokens": 640,
          "total_tokens": 765,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 124,
          "completion_tokens": 640,
          "total_tokens": 764,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 7,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 133,
          "completion_tokens": 207,
          "total_tokens": 340,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 133,
          "completion_tokens": 640,
          "total_tokens": 773,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 132,
          "completion_tokens": 327,
          "total_tokens": 459,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 398,
        "total_completion_tokens": 1174,
        "total_tokens": 1572
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 133,
          "completion_tokens": 207,
          "total_tokens": 340,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 133,
          "completion_tokens": 640,
          "total_tokens": 773,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 132,
          "completion_tokens": 327,
          "total_tokens": 459,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 9,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 142,
          "completion_tokens": 566,
          "total_tokens": 708,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 142,
          "completion_tokens": 640,
          "total_tokens": 782,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 141,
          "completion_tokens": 574,
          "total_tokens": 715,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 425,
        "total_completion_tokens": 1780,
        "total_tokens": 2205
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 142,
          "completion_tokens": 566,
          "total_tokens": 708,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 142,
          "completion_tokens": 640,
          "total_tokens": 782,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 141,
          "completion_tokens": 574,
          "total_tokens": 715,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 11,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 151,
          "completion_tokens": 233,
          "total_tokens": 384,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 151,
          "completion_tokens": 640,
          "total_tokens": 791,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 150,
          "completion_tokens": 422,
          "total_tokens": 572,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 452,
        "total_completion_tokens": 1295,
        "total_tokens": 1747
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 151,
          "completion_tokens": 233,
          "total_tokens": 384,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 151,
          "completion_tokens": 640,
          "total_tokens": 791,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 150,
          "completion_tokens": 422,
          "total_tokens": 572,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 13,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 158,
          "completion_tokens": 507,
          "total_tokens": 665,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 158,
          "completion_tokens": 640,
          "total_tokens": 798,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 157,
          "completion_tokens": 406,
          "total_tokens": 563,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 473,
        "total_completion_tokens": 1553,
        "total_tokens": 2026
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 158,
          "completion_tokens": 507,
          "total_tokens": 665,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 158,
          "completion_tokens": 640,
          "total_tokens": 798,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 157,
          "completion_tokens": 406,
          "total_tokens": 563,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 15,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 167,
          "completion_tokens": 260,
          "total_tokens": 427,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 167,
          "completion_tokens": 256,
          "total_tokens": 423,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 166,
          "completion_tokens": 250,
          "total_tokens": 416,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 500,
        "total_completion_tokens": 766,
        "total_tokens": 1266
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 167,
          "completion_tokens": 260,
          "total_tokens": 427,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 167,
          "completion_tokens": 256,
          "total_tokens": 423,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 166,
          "completion_tokens": 250,
          "total_tokens": 416,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 17,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 176,
          "completion_tokens": 136,
          "total_tokens": 312,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 176,
          "completion_tokens": 227,
          "total_tokens": 403,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 175,
          "completion_tokens": 640,
          "total_tokens": 815,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 527,
        "total_completion_tokens": 1003,
        "total_tokens": 1530
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 176,
          "completion_tokens": 136,
          "total_tokens": 312,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 176,
          "completion_tokens": 227,
          "total_tokens": 403,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 175,
          "completion_tokens": 640,
          "total_tokens": 815,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 19,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 183,
          "completion_tokens": 550,
          "total_tokens": 733,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 183,
          "completion_tokens": 640,
          "total_tokens": 823,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 182,
          "completion_tokens": 185,
          "total_tokens": 367,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 548,
        "total_completion_tokens": 1375,
        "total_tokens": 1923
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 183,
          "completion_tokens": 550,
          "total_tokens": 733,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 183,
          "completion_tokens": 640,
          "total_tokens": 823,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 182,
          "completion_tokens": 185,
          "total_tokens": 367,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 2,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 256,
          "total_tokens": 370,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 528,
          "total_tokens": 642,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 640,
          "total_tokens": 753,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 341,
        "total_completion_tokens": 1424,
        "total_tokens": 1765
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 256,
          "total_tokens": 370,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 528,
          "total_tokens": 642,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 640,
          "total_tokens": 753,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 345,
          "total_tokens": 455,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 640,
          "total_tokens": 749,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 329,
        "total_completion_tokens": 1625,
        "total_tokens": 1954
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 345,
          "total_tokens": 455,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 640,
          "total_tokens": 749,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 3,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 116,
          "completion_tokens": 640,
          "total_tokens": 756,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 116,
          "completion_tokens": 640,
          "total_tokens": 756,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 115,
          "completion_tokens": 575,
          "total_tokens": 690,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 347,
        "total_completion_tokens": 1855,
        "total_tokens": 2202
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 116,
          "completion_tokens": 640,
          "total_tokens": 756,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 116,
          "completion_tokens": 640,
          "total_tokens": 756,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 115,
          "completion_tokens": 575,
          "total_tokens": 690,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 5,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 124,
          "completion_tokens": 585,
          "total_tokens": 709,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 124,
          "completion_tokens": 640,
          "total_tokens": 764,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 123,
          "completion_tokens": 640,
          "total_tokens": 763,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 371,
        "total_completion_tokens": 1865,
        "total_tokens": 2236
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 124,
          "completion_tokens": 585,
          "total_tokens": 709,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 124,
          "completion_tokens": 640,
          "total_tokens": 764,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 123,
          "completion_tokens": 640,
          "total_tokens": 763,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 7,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 133,
          "completion_tokens": 640,
          "total_tokens": 773,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 133,
          "completion_tokens": 640,
          "total_tokens": 773,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 132,
          "completion_tokens": 285,
          "total_tokens": 417,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 398,
        "total_completion_tokens": 1565,
        "total_tokens": 1963
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 133,
          "completion_tokens": 640,
          "total_tokens": 773,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 133,
          "completion_tokens": 640,
          "total_tokens": 773,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 132,
          "completion_tokens": 285,
          "total_tokens": 417,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 9,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 141,
          "completion_tokens": 640,
          "total_tokens": 781,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 141,
          "completion_tokens": 597,
          "total_tokens": 738,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 140,
          "completion_tokens": 640,
          "total_tokens": 780,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 422,
        "total_completion_tokens": 1877,
        "total_tokens": 2299
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 141,
          "completion_tokens": 640,
          "total_tokens": 781,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 141,
          "completion_tokens": 597,
          "total_tokens": 738,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 140,
          "completion_tokens": 640,
          "total_tokens": 780,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 11,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 408,
          "total_tokens": 557,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 640,
          "total_tokens": 789,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 403,
          "total_tokens": 551,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 446,
        "total_completion_tokens": 1451,
        "total_tokens": 1897
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 408,
          "total_tokens": 557,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 640,
          "total_tokens": 789,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 403,
          "total_tokens": 551,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 13,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 640,
          "total_tokens": 796,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 640,
          "total_tokens": 796,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 245,
          "total_tokens": 400,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 467,
        "total_completion_tokens": 1525,
        "total_tokens": 1992
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 640,
          "total_tokens": 796,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 640,
          "total_tokens": 796,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 245,
          "total_tokens": 400,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 15,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 164,
          "completion_tokens": 429,
          "total_tokens": 593,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 164,
          "completion_tokens": 296,
          "total_tokens": 460,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 163,
          "completion_tokens": 316,
          "total_tokens": 479,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 491,
        "total_completion_tokens": 1041,
        "total_tokens": 1532
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 164,
          "completion_tokens": 429,
          "total_tokens": 593,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 164,
          "completion_tokens": 296,
          "total_tokens": 460,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 163,
          "completion_tokens": 316,
          "total_tokens": 479,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 17,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 174,
          "completion_tokens": 359,
          "total_tokens": 533,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 174,
          "completion_tokens": 640,
          "total_tokens": 814,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 173,
          "completion_tokens": 449,
          "total_tokens": 622,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 521,
        "total_completion_tokens": 1448,
        "total_tokens": 1969
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 174,
          "completion_tokens": 359,
          "total_tokens": 533,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 174,
          "completion_tokens": 640,
          "total_tokens": 814,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 173,
          "completion_tokens": 449,
          "total_tokens": 622,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 2,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 462,
          "total_tokens": 576,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 231,
          "total_tokens": 344,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 341,
        "total_completion_tokens": 1333,
        "total_tokens": 1674
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 462,
          "total_tokens": 576,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 231,
          "total_tokens": 344,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    }
  ],
  "white_responses": [
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 \nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4 \nWhite",
          "positional_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d5",
          "neutral_uci": "d7d5",
          "neutral_response": "1. d4 \nd5 Nf3 Nc6",
          "neutral_tokens": {
            "prompt_tokens": 148,
            "completion_tokens": 14,
            "total_tokens": 162,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": "d7d5",
          "consensus_move": "d7d5",
          "source_agents": [
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf3",
              "Nc6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 148,
            "completion_tokens": 14,
            "total_tokens": 162,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 446,
          "total_completion_tokens": 28,
          "total_tokens": 474
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 148,
            "completion_tokens": 14,
            "total_tokens": 162,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 4,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d6",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 \nd6 Nf3 Nc6",
          "aggressive_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 21,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "dxc4",
          "positional_uci": "d5c4",
          "positional_response": "1. d4 d5 2. c4 \ndxc4 b4",
          "positional_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 18,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "dxc4",
          "neutral_uci": "d5c4",
          "neutral_response": "1. d4 d5 2. c4 \ndxc4 b3",
          "neutral_tokens": {
            "prompt_tokens": 155,
            "completion_tokens": 18,
            "total_tokens": 173,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": "d5c4",
          "neutral_uci": "d5c4",
          "consensus_move": "d5c4",
          "source_agents": [
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf3",
              "Nc6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "b4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "b3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 156,
            "completion_tokens": 21,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 156,
            "completion_tokens": 18,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 155,
            "completion_tokens": 18,
            "total_tokens": 173,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 467,
          "total_completion_tokens": 57,
          "total_tokens": 524
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 156,
            "completion_tokens": 21,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 156,
            "completion_tokens": 18,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 155,
            "completion_tokens": 18,
            "total_tokens": 173,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/3p4/2PP4/8/PP2PPPP/RNBQKBNR b KQkq - 0 2"
    },
    {
      "turn": 6,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc6",
          "aggressive_uci": "b8c6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 \nNc6 Bg5",
          "aggressive_tokens": {
            "prompt_tokens": 165,
            "completion_tokens": 28,
            "total_tokens": 193,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bg5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nc6",
          "positional_uci": "b8c6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 \nNc6 e6 Nbd2",
          "positional_tokens": {
            "prompt_tokens": 165,
            "completion_tokens": 30,
            "total_tokens": 195,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e6",
            "Nbd2"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc6",
          "neutral_uci": "b8c6",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 \nNc6 e6",
          "neutral_tokens": {
            "prompt_tokens": 164,
            "completion_tokens": 27,
            "total_tokens": 191,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b8c6",
          "positional_uci": "b8c6",
          "neutral_uci": "b8c6",
          "consensus_move": "b8c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bg5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e6",
              "Nbd2"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 165,
            "completion_tokens": 28,
            "total_tokens": 193,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 165,
            "completion_tokens": 30,
            "total_tokens": 195,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 164,
            "completion_tokens": 27,
            "total_tokens": 191,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 494,
          "total_completion_tokens": 85,
          "total_tokens": 579
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 165,
            "completion_tokens": 28,
            "total_tokens": 193,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 165,
            "completion_tokens": 30,
            "total_tokens": 195,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 164,
            "completion_tokens": 27,
            "total_tokens": 191,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/8/2pP4/5N2/PP2PPPP/RNBQKB1R b KQkq - 1 3"
    },
    {
      "turn": 8,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf6",
          "aggressive_uci": "g8f6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 \nNf6 e3",
          "aggressive_tokens": {
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf6",
          "positional_uci": "g8f6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 \nNf6 e3",
          "positional_tokens": {
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf6",
          "neutral_uci": "g8f6",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 \nNf6 e3",
          "neutral_tokens": {
            "prompt_tokens": 173,
            "completion_tokens": 36,
            "total_tokens": 209,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g8f6",
          "positional_uci": "g8f6",
          "neutral_uci": "g8f6",
          "consensus_move": "g8f6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "e3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 173,
            "completion_tokens": 36,
            "total_tokens": 209,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 521,
          "total_completion_tokens": 108,
          "total_tokens": 629
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 173,
            "completion_tokens": 36,
            "total_tokens": 209,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkbnr/ppp1pppp/2n5/8/2pP4/2N2N2/PP2PPPP/R1BQKB1R b KQkq - 3 4"
    },
    {
      "turn": 10,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e5",
          "aggressive_uci": "e7e5",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 \ne5 Nxe4 Nxd4",
          "aggressive_tokens": {
            "prompt_tokens": 182,
            "completion_tokens": 47,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxe4",
            "Nxd4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bg4",
          "positional_uci": "c8g4",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 Nc2 a6 b3",
          "positional_tokens": {
            "prompt_tokens": 182,
            "completion_tokens": 48,
            "total_tokens": 230,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc2",
            "a6",
            "b3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bg4",
          "neutral_uci": "c8g4",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 Nc2 a6 b3",
          "neutral_tokens": {
            "prompt_tokens": 181,
            "completion_tokens": 48,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc2",
            "a6",
            "b3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e7e5",
          "positional_uci": "c8g4",
          "neutral_uci": "c8g4",
          "consensus_move": "c8g4",
          "source_agents": [
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nxe4",
              "Nxd4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nc2",
              "a6",
              "b3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc2",
              "a6",
              "b3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 182,
            "completion_tokens": 47,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 182,
            "completion_tokens": 48,
            "total_tokens": 230,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 181,
            "completion_tokens": 48,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 545,
          "total_completion_tokens": 143,
          "total_tokens": 688
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 182,
            "completion_tokens": 47,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 182,
            "completion_tokens": 48,
            "total_tokens": 230,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 181,
            "completion_tokens": 48,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp1pppp/2n2n2/8/2pPP3/2N2N2/PP3PPP/R1BQKB1R b KQkq - 0 5"
    },
    {
      "turn": 12,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Na5",
          "aggressive_uci": "c6a5",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 Nc2 e6",
          "aggressive_tokens": {
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc2",
            "e6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Na5",
          "positional_uci": "c6a5",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 Nc2 h6",
          "positional_tokens": {
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc2",
            "h6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Na5",
          "neutral_uci": "c6a5",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 Nc2 e6",
          "neutral_tokens": {
            "prompt_tokens": 189,
            "completion_tokens": 53,
            "total_tokens": 242,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc2",
            "e6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "c6a5",
          "positional_uci": "c6a5",
          "neutral_uci": "c6a5",
          "consensus_move": "c6a5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nc2",
              "e6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nc2",
              "h6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc2",
              "e6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 189,
            "completion_tokens": 53,
            "total_tokens": 242,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 569,
          "total_completion_tokens": 159,
          "total_tokens": 728
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 189,
            "completion_tokens": 53,
            "total_tokens": 242,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/2n2n2/3P4/2p1P1b1/2N2N2/PP3PPP/R1BQKB1R b KQkq - 0 6"
    },
    {
      "turn": 14,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc6",
          "aggressive_uci": "a5c6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 b5 Qxb5 a6",
          "aggressive_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 65,
            "total_tokens": 264,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b5",
            "Qxb5",
            "a6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nc6",
          "positional_uci": "a5c6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 b5 Qxb5",
          "positional_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 63,
            "total_tokens": 262,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b5",
            "Qxb5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc6",
          "neutral_uci": "a5c6",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 b5 Qxb5 a6",
          "neutral_tokens": {
            "prompt_tokens": 198,
            "completion_tokens": 65,
            "total_tokens": 263,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b5",
            "Qxb5",
            "a6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "a5c6",
          "positional_uci": "a5c6",
          "neutral_uci": "a5c6",
          "consensus_move": "a5c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "b5",
              "Qxb5",
              "a6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "b5",
              "Qxb5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "b5",
              "Qxb5",
              "a6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 199,
            "completion_tokens": 65,
            "total_tokens": 264,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 199,
            "completion_tokens": 63,
            "total_tokens": 262,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 198,
            "completion_tokens": 65,
            "total_tokens": 263,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 596,
          "total_completion_tokens": 193,
          "total_tokens": 789
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 199,
            "completion_tokens": 65,
            "total_tokens": 264,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 199,
            "completion_tokens": 63,
            "total_tokens": 262,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 198,
            "completion_tokens": 65,
            "total_tokens": 263,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/5n2/n2P4/Q1p1P1b1/2N2N2/PP3PPP/R1B1KB1R b KQkq - 2 7"
    },
    {
      "turn": 16,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "b6",
          "aggressive_uci": "b7b6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 Bb4 Qe7",
          "aggressive_tokens": {
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bb4",
            "Qe7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "b6",
          "positional_uci": "b7b6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 Bc5 Qe7",
          "positional_tokens": {
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "Qe7"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "b6",
          "neutral_uci": "b7b6",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 Bb4+ Bd7",
          "neutral_tokens": {
            "prompt_tokens": 207,
            "completion_tokens": 72,
            "total_tokens": 279,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bb4+",
            "Bd7"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b7b6",
          "positional_uci": "b7b6",
          "neutral_uci": "b7b6",
          "consensus_move": "b7b6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bb4",
              "Qe7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bc5",
              "Qe7"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bb4+",
              "Bd7"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 207,
            "completion_tokens": 72,
            "total_tokens": 279,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 623,
          "total_completion_tokens": 216,
          "total_tokens": 839
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 207,
            "completion_tokens": 72,
            "total_tokens": 279,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/2P2n2/8/Q1p1P1b1/2N2N2/PP3PPP/R1B1KB1R b KQkq - 0 8"
    },
    {
      "turn": 18,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qe7",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6 b4",
          "aggressive_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6",
            "b4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qe7",
          "positional_uci": null,
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 Nd7 Bb5",
          "positional_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "Nd7",
            "Bb5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qe7",
          "neutral_uci": null,
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6",
          "neutral_tokens": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qe2",
              "a6",
              "b4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qe2",
              "Nd7",
              "Bb5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qe2",
              "a6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 647,
          "total_completion_tokens": 245,
          "total_tokens": 892
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/p1p1pppp/1pP2n2/8/Q1B1P1b1/2N2N2/PP3PPP/R1B1K2R b KQkq - 0 9"
    },
    {
      "turn": 18,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qe7",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 Nd7 b4",
          "aggressive_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "Nd7",
            "b4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qe7",
          "positional_uci": null,
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 Nd7 Bb5",
          "positional_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "Nd7",
            "Bb5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qe7",
          "neutral_uci": null,
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6",
          "neutral_tokens": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qe2",
              "Nd7",
              "b4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qe2",
              "Nd7",
              "Bb5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qe2",
              "a6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 647,
          "total_completion_tokens": 245,
          "total_tokens": 892
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/p1p1pppp/1pP2n2/8/Q1B1P1b1/2N2N2/PP3PPP/R1B1K2R b KQkq - 0 9"
    },
    {
      "turn": 18,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qe7",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6 b4",
          "aggressive_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6",
            "b4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qe7",
          "positional_uci": null,
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 Nd7 Bb5",
          "positional_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "Nd7",
            "Bb5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qe7",
          "neutral_uci": null,
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6",
          "neutral_tokens": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qe2",
              "a6",
              "b4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qe2",
              "Nd7",
              "Bb5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qe2",
              "a6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 647,
          "total_completion_tokens": 245,
          "total_tokens": 892
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/p1p1pppp/1pP2n2/8/Q1B1P1b1/2N2N2/PP3PPP/R1B1K2R b KQkq - 0 9"
    },
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": "e2e4",
          "aggressive_response": "Move: e4\nPlan: e5 Nf3 Nc6",
          "aggressive_tokens": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "Move: e4 \nPlan: e5 Nf3 Nc6",
          "positional_tokens": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": "e2e4",
          "neutral_response": "Move: e4\nPlan: e5 Nf3 Nc6",
          "neutral_tokens": {
            "prompt_tokens": 184,
            "completion_tokens": 16,
            "total_tokens": 200,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e2e4",
          "positional_uci": "e2e4",
          "neutral_uci": "e2e4",
          "consensus_move": "e2e4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5",
              "Nf3",
              "Nc6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Nf3",
              "Nc6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5",
              "Nf3",
              "Nc6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 184,
            "completion_tokens": 16,
            "total_tokens": 200,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 554,
          "total_completion_tokens": 48,
          "total_tokens": 602
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 184,
            "completion_tokens": 16,
            "total_tokens": 200,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 3,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": null,
          "positional_response": "1. e4 e5\nWhite",
          "positional_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\nWhite",
          "neutral_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 452,
          "total_completion_tokens": 27,
          "total_tokens": 479
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 3,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": null,
          "positional_response": "1. e4 e5\nWhite",
          "positional_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\nWhite",
          "neutral_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 452,
          "total_completion_tokens": 27,
          "total_tokens": 479
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 3,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": null,
          "positional_response": "1. e4 e5\nWhite",
          "positional_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\nWhite",
          "neutral_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 452,
          "total_completion_tokens": 27,
          "total_tokens": 479
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 \nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4 \nWhite",
          "positional_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d5",
          "neutral_uci": "d7d5",
          "neutral_response": "1. d4 \nd5 Nf3 Nc6",
          "neutral_tokens": {
            "prompt_tokens": 148,
            "completion_tokens": 14,
            "total_tokens": 162,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": "d7d5",
          "consensus_move": "d7d5",
          "source_agents": [
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf3",
              "Nc6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 148,
            "completion_tokens": 14,
            "total_tokens": 162,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 446,
          "total_completion_tokens": 28,
          "total_tokens": 474
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 148,
            "completion_tokens": 14,
            "total_tokens": 162,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 4,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d6",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 \nd6 Nf3 Nc6",
          "aggressive_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 21,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "dxc4",
          "positional_uci": "d5c4",
          "positional_response": "1. d4 d5 2. c4 \ndxc4 b4",
          "positional_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 18,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "dxc4",
          "neutral_uci": "d5c4",
          "neutral_response": "1. d4 d5 2. c4 \ndxc4 b3",
          "neutral_tokens": {
            "prompt_tokens": 155,
            "completion_tokens": 18,
            "total_tokens": 173,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": "d5c4",
          "neutral_uci": "d5c4",
          "consensus_move": "d5c4",
          "source_agents": [
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf3",
              "Nc6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "b4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "b3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 156,
            "completion_tokens": 21,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 156,
            "completion_tokens": 18,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 155,
            "completion_tokens": 18,
            "total_tokens": 173,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 467,
          "total_completion_tokens": 57,
          "total_tokens": 524
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 156,
            "completion_tokens": 21,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 156,
            "completion_tokens": 18,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 155,
            "completion_tokens": 18,
            "total_tokens": 173,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/3p4/2PP4/8/PP2PPPP/RNBQKBNR b KQkq - 0 2"
    },
    {
      "turn": 6,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc6",
          "aggressive_uci": "b8c6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 \nNc6 Bg5",
          "aggressive_tokens": {
            "prompt_tokens": 165,
            "completion_tokens": 28,
            "total_tokens": 193,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bg5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nc6",
          "positional_uci": "b8c6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 \nNc6 e6 Nbd2",
          "positional_tokens": {
            "prompt_tokens": 165,
            "completion_tokens": 30,
            "total_tokens": 195,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e6",
            "Nbd2"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc6",
          "neutral_uci": "b8c6",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 \nNc6 e6",
          "neutral_tokens": {
            "prompt_tokens": 164,
            "completion_tokens": 27,
            "total_tokens": 191,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b8c6",
          "positional_uci": "b8c6",
          "neutral_uci": "b8c6",
          "consensus_move": "b8c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bg5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e6",
              "Nbd2"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 165,
            "completion_tokens": 28,
            "total_tokens": 193,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 165,
            "completion_tokens": 30,
            "total_tokens": 195,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 164,
            "completion_tokens": 27,
            "total_tokens": 191,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 494,
          "total_completion_tokens": 85,
          "total_tokens": 579
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 165,
            "completion_tokens": 28,
            "total_tokens": 193,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 165,
            "completion_tokens": 30,
            "total_tokens": 195,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 164,
            "completion_tokens": 27,
            "total_tokens": 191,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/8/2pP4/5N2/PP2PPPP/RNBQKB1R b KQkq - 1 3"
    },
    {
      "turn": 8,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf6",
          "aggressive_uci": "g8f6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 \nNf6 e3",
          "aggressive_tokens": {
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e6",
          "positional_uci": "e7e6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 e6 Nf6 Bg4",
          "positional_tokens": {
            "prompt_tokens": 174,
            "completion_tokens": 38,
            "total_tokens": 212,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "Bg4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf6",
          "neutral_uci": "g8f6",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 \nNf6 e3",
          "neutral_tokens": {
            "prompt_tokens": 173,
            "completion_tokens": 36,
            "total_tokens": 209,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g8f6",
          "positional_uci": "e7e6",
          "neutral_uci": "g8f6",
          "consensus_move": "g8f6",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "e3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf6",
              "Bg4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 174,
            "completion_tokens": 38,
            "total_tokens": 212,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 173,
            "completion_tokens": 36,
            "total_tokens": 209,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 521,
          "total_completion_tokens": 110,
          "total_tokens": 631
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 174,
            "completion_tokens": 38,
            "total_tokens": 212,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 173,
            "completion_tokens": 36,
            "total_tokens": 209,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkbnr/ppp1pppp/2n5/8/2pP4/2N2N2/PP2PPPP/R1BQKB1R b KQkq - 3 4"
    },
    {
      "turn": 10,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e5",
          "aggressive_uci": "e7e5",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 \ne5 Nxe4 Nxd4",
          "aggressive_tokens": {
            "prompt_tokens": 182,
            "completion_tokens": 47,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxe4",
            "Nxd4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bg4",
          "positional_uci": "c8g4",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 Nc2 a6 b3",
          "positional_tokens": {
            "prompt_tokens": 182,
            "completion_tokens": 48,
            "total_tokens": 230,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc2",
            "a6",
            "b3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bg4",
          "neutral_uci": "c8g4",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 Nc2 a6 b3",
          "neutral_tokens": {
            "prompt_tokens": 181,
            "completion_tokens": 48,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc2",
            "a6",
            "b3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e7e5",
          "positional_uci": "c8g4",
          "neutral_uci": "c8g4",
          "consensus_move": "c8g4",
          "source_agents": [
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nxe4",
              "Nxd4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nc2",
              "a6",
              "b3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc2",
              "a6",
              "b3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 182,
            "completion_tokens": 47,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 182,
            "completion_tokens": 48,
            "total_tokens": 230,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 181,
            "completion_tokens": 48,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 545,
          "total_completion_tokens": 143,
          "total_tokens": 688
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 182,
            "completion_tokens": 47,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 182,
            "completion_tokens": 48,
            "total_tokens": 230,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 181,
            "completion_tokens": 48,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp1pppp/2n2n2/8/2pPP3/2N2N2/PP3PPP/R1BQKB1R b KQkq - 0 5"
    },
    {
      "turn": 12,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Na5",
          "aggressive_uci": "c6a5",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 Nxd5 exd5",
          "aggressive_tokens": {
            "prompt_tokens": 190,
            "completion_tokens": 54,
            "total_tokens": 244,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxd5",
            "exd5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Na5",
          "positional_uci": "c6a5",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 Nc2 h6",
          "positional_tokens": {
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc2",
            "h6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Na5",
          "neutral_uci": "c6a5",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 Nc2 e6",
          "neutral_tokens": {
            "prompt_tokens": 189,
            "completion_tokens": 53,
            "total_tokens": 242,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc2",
            "e6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "c6a5",
          "positional_uci": "c6a5",
          "neutral_uci": "c6a5",
          "consensus_move": "c6a5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nxd5",
              "exd5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nc2",
              "h6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc2",
              "e6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 190,
            "completion_tokens": 54,
            "total_tokens": 244,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 189,
            "completion_tokens": 53,
            "total_tokens": 242,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 569,
          "total_completion_tokens": 160,
          "total_tokens": 729
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 190,
            "completion_tokens": 54,
            "total_tokens": 244,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 189,
            "completion_tokens": 53,
            "total_tokens": 242,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/2n2n2/3P4/2p1P1b1/2N2N2/PP3PPP/R1BQKB1R b KQkq - 0 6"
    },
    {
      "turn": 14,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc6",
          "aggressive_uci": "a5c6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 b5 Qxb5 a6",
          "aggressive_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 65,
            "total_tokens": 264,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b5",
            "Qxb5",
            "a6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nc6",
          "positional_uci": "a5c6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 b5 Qxb5",
          "positional_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 63,
            "total_tokens": 262,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b5",
            "Qxb5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc6",
          "neutral_uci": "a5c6",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 b5 Qxb5 a6",
          "neutral_tokens": {
            "prompt_tokens": 198,
            "completion_tokens": 65,
            "total_tokens": 263,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b5",
            "Qxb5",
            "a6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "a5c6",
          "positional_uci": "a5c6",
          "neutral_uci": "a5c6",
          "consensus_move": "a5c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "b5",
              "Qxb5",
              "a6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "b5",
              "Qxb5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "b5",
              "Qxb5",
              "a6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 199,
            "completion_tokens": 65,
            "total_tokens": 264,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 199,
            "completion_tokens": 63,
            "total_tokens": 262,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 198,
            "completion_tokens": 65,
            "total_tokens": 263,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 596,
          "total_completion_tokens": 193,
          "total_tokens": 789
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 199,
            "completion_tokens": 65,
            "total_tokens": 264,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 199,
            "completion_tokens": 63,
            "total_tokens": 262,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 198,
            "completion_tokens": 65,
            "total_tokens": 263,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/5n2/n2P4/Q1p1P1b1/2N2N2/PP3PPP/R1B1KB1R b KQkq - 2 7"
    },
    {
      "turn": 16,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "b6",
          "aggressive_uci": "b7b6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 Bb4 Qe7",
          "aggressive_tokens": {
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bb4",
            "Qe7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "b6",
          "positional_uci": "b7b6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 Bc5 Qe7",
          "positional_tokens": {
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "Qe7"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "b6",
          "neutral_uci": "b7b6",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 Bc5 Qe7",
          "neutral_tokens": {
            "prompt_tokens": 207,
            "completion_tokens": 72,
            "total_tokens": 279,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "Qe7"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b7b6",
          "positional_uci": "b7b6",
          "neutral_uci": "b7b6",
          "consensus_move": "b7b6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bb4",
              "Qe7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bc5",
              "Qe7"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bc5",
              "Qe7"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 207,
            "completion_tokens": 72,
            "total_tokens": 279,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 623,
          "total_completion_tokens": 216,
          "total_tokens": 839
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 207,
            "completion_tokens": 72,
            "total_tokens": 279,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/2P2n2/8/Q1p1P1b1/2N2N2/PP3PPP/R1B1KB1R b KQkq - 0 8"
    },
    {
      "turn": 18,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qe7",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6 b4",
          "aggressive_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6",
            "b4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qe7",
          "positional_uci": null,
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 Nd7 Bb5",
          "positional_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "Nd7",
            "Bb5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qe7",
          "neutral_uci": null,
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6",
          "neutral_tokens": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qe2",
              "a6",
              "b4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qe2",
              "Nd7",
              "Bb5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qe2",
              "a6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 647,
          "total_completion_tokens": 245,
          "total_tokens": 892
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/p1p1pppp/1pP2n2/8/Q1B1P1b1/2N2N2/PP3PPP/R1B1K2R b KQkq - 0 9"
    },
    {
      "turn": 18,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qe7",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6 b4",
          "aggressive_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6",
            "b4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qe7",
          "positional_uci": null,
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 Nd7 Bb5",
          "positional_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "Nd7",
            "Bb5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qe7",
          "neutral_uci": null,
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6",
          "neutral_tokens": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qe2",
              "a6",
              "b4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qe2",
              "Nd7",
              "Bb5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qe2",
              "a6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 647,
          "total_completion_tokens": 245,
          "total_tokens": 892
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/p1p1pppp/1pP2n2/8/Q1B1P1b1/2N2N2/PP3PPP/R1B1K2R b KQkq - 0 9"
    },
    {
      "turn": 18,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qe7",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6 b4",
          "aggressive_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6",
            "b4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qe7",
          "positional_uci": null,
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 Nd7 Bb5",
          "positional_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "Nd7",
            "Bb5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qe7",
          "neutral_uci": null,
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6",
          "neutral_tokens": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qe2",
              "a6",
              "b4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qe2",
              "Nd7",
              "Bb5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qe2",
              "a6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 647,
          "total_completion_tokens": 245,
          "total_tokens": 892
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/p1p1pppp/1pP2n2/8/Q1B1P1b1/2N2N2/PP3PPP/R1B1K2R b KQkq - 0 9"
    },
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": "e2e4",
          "aggressive_response": "Move: e4\nPlan: e5 Nf3 Nc6",
          "aggressive_tokens": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "Move: e4 \nPlan: e5 Nf3 Nc6",
          "positional_tokens": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": "e2e4",
          "neutral_response": "Move: e4\nPlan: e5 Nf3 Nc6",
          "neutral_tokens": {
            "prompt_tokens": 184,
            "completion_tokens": 16,
            "total_tokens": 200,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e2e4",
          "positional_uci": "e2e4",
          "neutral_uci": "e2e4",
          "consensus_move": "e2e4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5",
              "Nf3",
              "Nc6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Nf3",
              "Nc6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5",
              "Nf3",
              "Nc6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 184,
            "completion_tokens": 16,
            "total_tokens": 200,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 554,
          "total_completion_tokens": 48,
          "total_tokens": 602
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 184,
            "completion_tokens": 16,
            "total_tokens": 200,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 3,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": null,
          "positional_response": "1. e4 e5\nWhite",
          "positional_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\nWhite",
          "neutral_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 452,
          "total_completion_tokens": 27,
          "total_tokens": 479
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 3,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": null,
          "positional_response": "1. e4 e5\nWhite",
          "positional_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\nWhite",
          "neutral_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 452,
          "total_completion_tokens": 27,
          "total_tokens": 479
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 3,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": null,
          "positional_response": "1. e4 e5\nWhite",
          "positional_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\nWhite",
          "neutral_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 452,
          "total_completion_tokens": 27,
          "total_tokens": 479
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e5",
          "aggressive_uci": "e7e5",
          "aggressive_response": "1. e4 \ne5 Nf3 Nc6 Bc4 Bc5 d3 Qe7",
          "aggressive_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 25,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bc4",
            "Bc5",
            "d3",
            "Qe7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e5",
          "positional_uci": "e7e5",
          "positional_response": "1. e4 \ne5 Nf3 Nc6 Bc4 Bc5 d3 Qe7",
          "positional_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 25,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bc4",
            "Bc5",
            "d3",
            "Qe7"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e5",
          "neutral_uci": "e7e5",
          "neutral_response": "1. e4 \ne5 Nf3 Nc6 Bc4 Bc5 d3 Qe7",
          "neutral_tokens": {
            "prompt_tokens": 148,
            "completion_tokens": 25,
            "total_tokens": 173,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bc4",
            "Bc5",
            "d3",
            "Qe7"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e7e5",
          "positional_uci": "e7e5",
          "neutral_uci": "e7e5",
          "consensus_move": "e7e5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf3",
              "Nc6",
              "Bc4",
              "Bc5",
              "d3",
              "Qe7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf3",
              "Nc6",
              "Bc4",
              "Bc5",
              "d3",
              "Qe7"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf3",
              "Nc6",
              "Bc4",
              "Bc5",
              "d3",
              "Qe7"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 149,
            "completion_tokens": 25,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 149,
            "completion_tokens": 25,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 148,
            "completion_tokens": 25,
            "total_tokens": 173,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 446,
          "total_completion_tokens": 75,
          "total_tokens": 521
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 149,
            "completion_tokens": 25,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 149,
            "completion_tokens": 25,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 148,
            "completion_tokens": 25,
            "total_tokens": 173,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 4,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc6",
          "aggressive_uci": "b8c6",
          "aggressive_response": "1. e4 e5 2. Nf3 \nNc6 Bc4",
          "aggressive_tokens": {
            "prompt_tokens": 157,
            "completion_tokens": 20,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nc6",
          "positional_uci": "b8c6",
          "positional_response": "1. e4 e5 2. Nf3 \nNc6 Nc3",
          "positional_tokens": {
            "prompt_tokens": 157,
            "completion_tokens": 20,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc6",
          "neutral_uci": "b8c6",
          "neutral_response": "1. e4 e5 2. Nf3 \nNc6 Nf6",
          "neutral_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 20,
            "total_tokens": 176,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b8c6",
          "positional_uci": "b8c6",
          "neutral_uci": "b8c6",
          "consensus_move": "b8c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bc4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nc3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 157,
            "completion_tokens": 20,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 157,
            "completion_tokens": 20,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 156,
            "completion_tokens": 20,
            "total_tokens": 176,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 470,
          "total_completion_tokens": 60,
          "total_tokens": 530
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 157,
            "completion_tokens": 20,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 157,
            "completion_tokens": 20,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 156,
            "completion_tokens": 20,
            "total_tokens": 176,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R b KQkq - 1 2"
    },
    {
      "turn": 6,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf3",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 \nWhite Nf3",
          "aggressive_tokens": {
            "prompt_tokens": 166,
            "completion_tokens": 27,
            "total_tokens": 193,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf6",
          "positional_uci": "g8f6",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 \nNf6 Bc4",
          "positional_tokens": {
            "prompt_tokens": 166,
            "completion_tokens": 29,
            "total_tokens": 195,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "a6",
          "neutral_uci": "a7a6",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 \nBc4 Nf6 d3",
          "neutral_tokens": {
            "prompt_tokens": 165,
            "completion_tokens": 33,
            "total_tokens": 198,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc4",
            "Nf6",
            "d3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": "g8f6",
          "neutral_uci": "a7a6",
          "consensus_move": "a7a6",
          "source_agents": [
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [
              "Bc4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bc4",
              "Nf6",
              "d3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 166,
            "completion_tokens": 27,
            "total_tokens": 193,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 166,
            "completion_tokens": 29,
            "total_tokens": 195,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 165,
            "completion_tokens": 33,
            "total_tokens": 198,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 497,
          "total_completion_tokens": 89,
          "total_tokens": 586
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 166,
            "completion_tokens": 27,
            "total_tokens": 193,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 166,
            "completion_tokens": 29,
            "total_tokens": 195,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 165,
            "completion_tokens": 33,
            "total_tokens": 198,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkbnr/pppp1ppp/2n5/1B2p3/4P3/5N2/PPPP1PPP/RNBQK2R b KQkq - 3 3"
    },
    {
      "turn": 8,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "bxc6",
          "aggressive_uci": "b7c6",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 \nWhite Nc3 Nf6 d3",
          "aggressive_tokens": {
            "prompt_tokens": 174,
            "completion_tokens": 43,
            "total_tokens": 217,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc3",
            "Nf6",
            "d3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "bxc6",
          "positional_uci": "b7c6",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 White",
          "positional_tokens": {
            "prompt_tokens": 174,
            "completion_tokens": 34,
            "total_tokens": 208,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "bxc6",
          "neutral_uci": "b7c6",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 \nWhite",
          "neutral_tokens": {
            "prompt_tokens": 173,
            "completion_tokens": 35,
            "total_tokens": 208,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b7c6",
          "positional_uci": "b7c6",
          "neutral_uci": "b7c6",
          "consensus_move": "b7c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nc3",
              "Nf6",
              "d3"
            ],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 174,
            "completion_tokens": 43,
            "total_tokens": 217,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 174,
            "completion_tokens": 34,
            "total_tokens": 208,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 173,
            "completion_tokens": 35,
            "total_tokens": 208,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 521,
          "total_completion_tokens": 112,
          "total_tokens": 633
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 174,
            "completion_tokens": 43,
            "total_tokens": 217,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 174,
            "completion_tokens": 34,
            "total_tokens": 208,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 173,
            "completion_tokens": 35,
            "total_tokens": 208,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkbnr/1ppp1ppp/p1B5/4p3/4P3/5N2/PPPP1PPP/RNBQK2R b KQkq - 0 4"
    },
    {
      "turn": 10,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf6",
          "aggressive_uci": "g8f6",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 \nNf6 d3 d6",
          "aggressive_tokens": {
            "prompt_tokens": 183,
            "completion_tokens": 47,
            "total_tokens": 230,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d3",
            "d6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qe7",
          "positional_uci": "d8e7",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 Qe7 Nf3 d6 Bg4",
          "positional_tokens": {
            "prompt_tokens": 183,
            "completion_tokens": 50,
            "total_tokens": 233,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "d6",
            "Bg4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf6",
          "neutral_uci": "g8f6",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 \nNf6 d3 d6",
          "neutral_tokens": {
            "prompt_tokens": 182,
            "completion_tokens": 47,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d3",
            "d6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g8f6",
          "positional_uci": "d8e7",
          "neutral_uci": "g8f6",
          "consensus_move": "g8f6",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d3",
              "d6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf3",
              "d6",
              "Bg4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "d3",
              "d6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 183,
            "completion_tokens": 47,
            "total_tokens": 230,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 183,
            "completion_tokens": 50,
            "total_tokens": 233,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 182,
            "completion_tokens": 47,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 548,
          "total_completion_tokens": 144,
          "total_tokens": 692
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 183,
            "completion_tokens": 47,
            "total_tokens": 230,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 183,
            "completion_tokens": 50,
            "total_tokens": 233,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 182,
            "completion_tokens": 47,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkbnr/2pp1ppp/p1p5/4N3/4P3/8/PPPP1PPP/RNBQK2R b KQkq - 0 5"
    },
    {
      "turn": 12,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d6",
          "aggressive_uci": "d7d6",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 Nf6 6. O-O \nd6 Qe2 Qe7",
          "aggressive_tokens": {
            "prompt_tokens": 191,
            "completion_tokens": 56,
            "total_tokens": 247,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "Qe7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d6",
          "positional_uci": "d7d6",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 Nf6 6. O-O d6 Qe2 Re8",
          "positional_tokens": {
            "prompt_tokens": 191,
            "completion_tokens": 54,
            "total_tokens": 245,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "Re8"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d6",
          "neutral_uci": "d7d6",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 Nf6 6. O-O d6 Qe2 Bd7 Nxd7",
          "neutral_tokens": {
            "prompt_tokens": 190,
            "completion_tokens": 57,
            "total_tokens": 247,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "Bd7",
            "Nxd7"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d7d6",
          "positional_uci": "d7d6",
          "neutral_uci": "d7d6",
          "consensus_move": "d7d6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qe2",
              "Qe7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qe2",
              "Re8"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qe2",
              "Bd7",
              "Nxd7"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 191,
            "completion_tokens": 56,
            "total_tokens": 247,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 191,
            "completion_tokens": 54,
            "total_tokens": 245,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 190,
            "completion_tokens": 57,
            "total_tokens": 247,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 572,
          "total_completion_tokens": 167,
          "total_tokens": 739
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 191,
            "completion_tokens": 56,
            "total_tokens": 247,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 191,
            "completion_tokens": 54,
            "total_tokens": 245,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 190,
            "completion_tokens": 57,
            "total_tokens": 247,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/2pp1ppp/p1p2n2/4N3/4P3/8/PPPP1PPP/RNBQ1RK1 b kq - 2 6"
    },
    {
      "turn": 14,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qe7",
          "aggressive_uci": "d8e7",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 Nf6 6. O-O d6 7. Nxc6 Qe7 Nc3 Bb7",
          "aggressive_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 64,
            "total_tokens": 263,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc3",
            "Bb7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qe7",
          "positional_uci": "d8e7",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 Nf6 6. O-O d6 7. Nxc6 Qe7 Qd5",
          "positional_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 61,
            "total_tokens": 260,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qd5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qe7",
          "neutral_uci": "d8e7",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 Nf6 6. O-O d6 7. Nxc6 Qe7 Qd5",
          "neutral_tokens": {
            "prompt_tokens": 198,
            "completion_tokens": 61,
            "total_tokens": 259,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qd5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d8e7",
          "positional_uci": "d8e7",
          "neutral_uci": "d8e7",
          "consensus_move": "d8e7",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nc3",
              "Bb7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qd5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qd5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 199,
            "completion_tokens": 64,
            "total_tokens": 263,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 199,
            "completion_tokens": 61,
            "total_tokens": 260,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 198,
            "completion_tokens": 61,
            "total_tokens": 259,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 596,
          "total_completion_tokens": 186,
          "total_tokens": 782
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 199,
            "completion_tokens": 64,
            "total_tokens": 263,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 199,
            "completion_tokens": 61,
            "total_tokens": 260,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 198,
            "completion_tokens": 61,
            "total_tokens": 259,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/2p2ppp/p1Np1n2/8/4P3/8/PPPP1PPP/RNBQ1RK1 b kq - 0 7"
    },
    {
      "turn": 16,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Kxe7",
          "aggressive_uci": "e8e7",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 Nf6 6. O-O d6 7. Nxc6 Qe7 8. Nxe7 \nKxe7 Nf3 d5",
          "aggressive_tokens": {
            "prompt_tokens": 208,
            "completion_tokens": 73,
            "total_tokens": 281,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "d5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Kxe7",
          "positional_uci": "e8e7",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 Nf6 6. O-O d6 7. Nxc6 Qe7 8. Nxe7 Kxe7 White",
          "positional_tokens": {
            "prompt_tokens": 208,
            "completion_tokens": 68,
            "total_tokens": 276,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Kxe7",
          "neutral_uci": "e8e7",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 Nf6 6. O-O d6 7. Nxc6 Qe7 8. Nxe7 \nKxe7 Nf3 d5",
          "neutral_tokens": {
            "prompt_tokens": 207,
            "completion_tokens": 73,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "d5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e8e7",
          "positional_uci": "e8e7",
          "neutral_uci": "e8e7",
          "consensus_move": "e8e7",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf3",
              "d5"
            ],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf3",
              "d5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 208,
            "completion_tokens": 73,
            "total_tokens": 281,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 208,
            "completion_tokens": 68,
            "total_tokens": 276,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 207,
            "completion_tokens": 73,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 623,
          "total_completion_tokens": 214,
          "total_tokens": 837
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 208,
            "completion_tokens": 73,
            "total_tokens": 281,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 208,
            "completion_tokens": 68,
            "total_tokens": 276,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 207,
            "completion_tokens": 73,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1b1kb1r/2p1Nppp/p2p1n2/8/4P3/8/PPPP1PPP/RNBQ1RK1 b kq - 0 8"
    },
    {
      "turn": 18,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d5",
          "aggressive_uci": "d6d5",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 Nf6 6. O-O d6 7. Nxc6 Qe7 8. Nxe7 Kxe7 9. d4 \nd5 Nd2 Bd6",
          "aggressive_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 79,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nd2",
            "Bd6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d5",
          "positional_uci": "d6d5",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 Nf6 6. O-O d6 7. Nxc6 Qe7 8. Nxe7 Kxe7 9. d4 \nd5 Re1 Be6",
          "positional_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 79,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Re1",
            "Be6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d5",
          "neutral_uci": "d6d5",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 Nf6 6. O-O d6 7. Nxc6 Qe7 8. Nxe7 Kxe7 9. d4 \nd5 Re1 Be6",
          "neutral_tokens": {
            "prompt_tokens": 215,
            "completion_tokens": 79,
            "total_tokens": 294,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Re1",
            "Be6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d6d5",
          "positional_uci": "d6d5",
          "neutral_uci": "d6d5",
          "consensus_move": "d6d5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nd2",
              "Bd6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Re1",
              "Be6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Re1",
              "Be6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 216,
            "completion_tokens": 79,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 216,
            "completion_tokens": 79,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 215,
            "completion_tokens": 79,
            "total_tokens": 294,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 647,
          "total_completion_tokens": 237,
          "total_tokens": 884
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 216,
            "completion_tokens": 79,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 216,
            "completion_tokens": 79,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 215,
            "completion_tokens": 79,
            "total_tokens": 294,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1b2b1r/2p1kppp/p2p1n2/8/3PP3/8/PPP2PPP/RNBQ1RK1 b - - 0 9"
    },
    {
      "turn": 20,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qe7+",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 Nf6 6. O-O d6 7. Nxc6 Qe7 8. Nxe7 Kxe7 9. d4 d5 10. exd5 Qe7+ Kf7 Re1+",
          "aggressive_tokens": {
            "prompt_tokens": 224,
            "completion_tokens": 90,
            "total_tokens": 314,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kf7",
            "Re1+"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Kf8",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 Nf6 6. O-O d6 7. Nxc6 Qe7 8. Nxe7 Kxe7 9. d4 d5 10. exd5 Kf8 Rd1 Ke7",
          "positional_tokens": {
            "prompt_tokens": 224,
            "completion_tokens": 87,
            "total_tokens": 311,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Rd1",
            "Ke7"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qe7+",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 Nf6 6. O-O d6 7. Nxc6 Qe7 8. Nxe7 Kxe7 9. d4 d5 10. exd5 Qe7+ Kf7 Rd1",
          "neutral_tokens": {
            "prompt_tokens": 223,
            "completion_tokens": 89,
            "total_tokens": 312,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kf7",
            "Rd1"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Kf7",
              "Re1+"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Rd1",
              "Ke7"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Kf7",
              "Rd1"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 224,
            "completion_tokens": 90,
            "total_tokens": 314,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 224,
            "completion_tokens": 87,
            "total_tokens": 311,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 223,
            "completion_tokens": 89,
            "total_tokens": 312,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 671,
          "total_completion_tokens": 266,
          "total_tokens": 937
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 19,
            "turn_number": 10,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 224,
            "completion_tokens": 90,
            "total_tokens": 314,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 19,
            "turn_number": 10,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 224,
            "completion_tokens": 87,
            "total_tokens": 311,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 19,
            "turn_number": 10,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 223,
            "completion_tokens": 89,
            "total_tokens": 312,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1b2b1r/2p1kppp/p4n2/3P4/3P4/8/PPP2PPP/RNBQ1RK1 b - - 0 10"
    },
    {
      "turn": 20,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qe7+",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 Nf6 6. O-O d6 7. Nxc6 Qe7 8. Nxe7 Kxe7 9. d4 d5 10. exd5 Qe7+ Kf7 Re1+",
          "aggressive_tokens": {
            "prompt_tokens": 224,
            "completion_tokens": 90,
            "total_tokens": 314,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kf7",
            "Re1+"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Kf6",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 Nf6 6. O-O d6 7. Nxc6 Qe7 8. Nxe7 Kxe7 9. d4 d5 10. exd5 Kf6 Rd1 Ke7",
          "positional_tokens": {
            "prompt_tokens": 224,
            "completion_tokens": 87,
            "total_tokens": 311,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Rd1",
            "Ke7"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qe7+",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 Nf6 6. O-O d6 7. Nxc6 Qe7 8. Nxe7 Kxe7 9. d4 d5 10. exd5 Qe7+ Kf7 Rd1",
          "neutral_tokens": {
            "prompt_tokens": 223,
            "completion_tokens": 89,
            "total_tokens": 312,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kf7",
            "Rd1"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Kf7",
              "Re1+"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Rd1",
              "Ke7"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Kf7",
              "Rd1"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 224,
            "completion_tokens": 90,
            "total_tokens": 314,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 224,
            "completion_tokens": 87,
            "total_tokens": 311,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 223,
            "completion_tokens": 89,
            "total_tokens": 312,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 671,
          "total_completion_tokens": 266,
          "total_tokens": 937
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 19,
            "turn_number": 10,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 224,
            "completion_tokens": 90,
            "total_tokens": 314,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 19,
            "turn_number": 10,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 224,
            "completion_tokens": 87,
            "total_tokens": 311,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 19,
            "turn_number": 10,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 223,
            "completion_tokens": 89,
            "total_tokens": 312,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1b2b1r/2p1kppp/p4n2/3P4/3P4/8/PPP2PPP/RNBQ1RK1 b - - 0 10"
    },
    {
      "turn": 20,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qe7+",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 Nf6 6. O-O d6 7. Nxc6 Qe7 8. Nxe7 Kxe7 9. d4 d5 10. exd5 Qe7+ Kf7 Re1+",
          "aggressive_tokens": {
            "prompt_tokens": 224,
            "completion_tokens": 90,
            "total_tokens": 314,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kf7",
            "Re1+"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Kf6",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 Nf6 6. O-O d6 7. Nxc6 Qe7 8. Nxe7 Kxe7 9. d4 d5 10. exd5 Kf6 Rd1 Ke7",
          "positional_tokens": {
            "prompt_tokens": 224,
            "completion_tokens": 87,
            "total_tokens": 311,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Rd1",
            "Ke7"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qe7+",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Bxc6 bxc6 5. Nxe5 Nf6 6. O-O d6 7. Nxc6 Qe7 8. Nxe7 Kxe7 9. d4 d5 10. exd5 Qe7+ Kf7 Rd1",
          "neutral_tokens": {
            "prompt_tokens": 223,
            "completion_tokens": 89,
            "total_tokens": 312,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kf7",
            "Rd1"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Kf7",
              "Re1+"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Rd1",
              "Ke7"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Kf7",
              "Rd1"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 224,
            "completion_tokens": 90,
            "total_tokens": 314,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 224,
            "completion_tokens": 87,
            "total_tokens": 311,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 223,
            "completion_tokens": 89,
            "total_tokens": 312,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 671,
          "total_completion_tokens": 266,
          "total_tokens": 937
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 19,
            "turn_number": 10,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 224,
            "completion_tokens": 90,
            "total_tokens": 314,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 19,
            "turn_number": 10,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 224,
            "completion_tokens": 87,
            "total_tokens": 311,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 19,
            "turn_number": 10,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 223,
            "completion_tokens": 89,
            "total_tokens": 312,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1b2b1r/2p1kppp/p4n2/3P4/3P4/8/PPP2PPP/RNBQ1RK1 b - - 0 10"
    },
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": "e2e4",
          "aggressive_response": "There is no game to repeat. \nMove: e4\nPlan: e5, Nb1-c3, Nb8-c6",
          "aggressive_tokens": {
            "prompt_tokens": 185,
            "completion_tokens": 28,
            "total_tokens": 213,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nb1",
            "c3",
            "Nb8",
            "c6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "Move: e4 \nPlan: e5 Nf3 Nc6",
          "positional_tokens": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": "e2e4",
          "neutral_response": "Move: e4\nPlan: e5 Nf3 Nc6",
          "neutral_tokens": {
            "prompt_tokens": 184,
            "completion_tokens": 16,
            "total_tokens": 200,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e2e4",
          "positional_uci": "e2e4",
          "neutral_uci": "e2e4",
          "consensus_move": "e2e4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5",
              "Nb1",
              "c3",
              "Nb8",
              "c6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Nf3",
              "Nc6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5",
              "Nf3",
              "Nc6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 185,
            "completion_tokens": 28,
            "total_tokens": 213,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 184,
            "completion_tokens": 16,
            "total_tokens": 200,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 554,
          "total_completion_tokens": 60,
          "total_tokens": 614
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 185,
            "completion_tokens": 28,
            "total_tokens": 213,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 184,
            "completion_tokens": 16,
            "total_tokens": 200,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 3,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": null,
          "positional_response": "1. e4 e5\nWhite",
          "positional_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\nWhite",
          "neutral_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 452,
          "total_completion_tokens": 27,
          "total_tokens": 479
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 3,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": null,
          "positional_response": "1. e4 e5\nWhite",
          "positional_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\nWhite",
          "neutral_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 452,
          "total_completion_tokens": 27,
          "total_tokens": 479
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 3,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": null,
          "positional_response": "1. e4 e5\nWhite",
          "positional_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\nWhite",
          "neutral_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 452,
          "total_completion_tokens": 27,
          "total_tokens": 479
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 \nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4 \nWhite",
          "positional_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d5",
          "neutral_uci": "d7d5",
          "neutral_response": "1. d4 \nd5 Nf3 Nc6",
          "neutral_tokens": {
            "prompt_tokens": 148,
            "completion_tokens": 14,
            "total_tokens": 162,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": "d7d5",
          "consensus_move": "d7d5",
          "source_agents": [
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf3",
              "Nc6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 148,
            "completion_tokens": 14,
            "total_tokens": 162,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 446,
          "total_completion_tokens": 28,
          "total_tokens": 474
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 148,
            "completion_tokens": 14,
            "total_tokens": 162,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 4,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d6",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 \nd6 Nf3 Nc6",
          "aggressive_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 21,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "dxc4",
          "positional_uci": "d5c4",
          "positional_response": "1. d4 d5 2. c4 \ndxc4 b4",
          "positional_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 18,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "dxc4",
          "neutral_uci": "d5c4",
          "neutral_response": "1. d4 d5 2. c4 \ndxc4 b3",
          "neutral_tokens": {
            "prompt_tokens": 155,
            "completion_tokens": 18,
            "total_tokens": 173,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": "d5c4",
          "neutral_uci": "d5c4",
          "consensus_move": "d5c4",
          "source_agents": [
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf3",
              "Nc6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "b4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "b3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 156,
            "completion_tokens": 21,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 156,
            "completion_tokens": 18,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 155,
            "completion_tokens": 18,
            "total_tokens": 173,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 467,
          "total_completion_tokens": 57,
          "total_tokens": 524
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 156,
            "completion_tokens": 21,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 156,
            "completion_tokens": 18,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 155,
            "completion_tokens": 18,
            "total_tokens": 173,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/3p4/2PP4/8/PP2PPPP/RNBQKBNR b KQkq - 0 2"
    },
    {
      "turn": 6,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc6",
          "aggressive_uci": "b8c6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 \nNc6 Bg5",
          "aggressive_tokens": {
            "prompt_tokens": 165,
            "completion_tokens": 28,
            "total_tokens": 193,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bg5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nc6",
          "positional_uci": "b8c6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 \nNc6 e6 Nbd2",
          "positional_tokens": {
            "prompt_tokens": 165,
            "completion_tokens": 30,
            "total_tokens": 195,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e6",
            "Nbd2"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc6",
          "neutral_uci": "b8c6",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 \nNc6 e6",
          "neutral_tokens": {
            "prompt_tokens": 164,
            "completion_tokens": 27,
            "total_tokens": 191,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b8c6",
          "positional_uci": "b8c6",
          "neutral_uci": "b8c6",
          "consensus_move": "b8c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bg5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e6",
              "Nbd2"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 165,
            "completion_tokens": 28,
            "total_tokens": 193,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 165,
            "completion_tokens": 30,
            "total_tokens": 195,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 164,
            "completion_tokens": 27,
            "total_tokens": 191,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 494,
          "total_completion_tokens": 85,
          "total_tokens": 579
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 165,
            "completion_tokens": 28,
            "total_tokens": 193,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 165,
            "completion_tokens": 30,
            "total_tokens": 195,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 164,
            "completion_tokens": 27,
            "total_tokens": 191,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/8/2pP4/5N2/PP2PPPP/RNBQKB1R b KQkq - 1 3"
    },
    {
      "turn": 8,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e6",
          "aggressive_uci": "e7e6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 \ne6 Nc3",
          "aggressive_tokens": {
            "prompt_tokens": 173,
            "completion_tokens": 35,
            "total_tokens": 208,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf6",
          "positional_uci": "g8f6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 \nNf6 e5",
          "positional_tokens": {
            "prompt_tokens": 173,
            "completion_tokens": 35,
            "total_tokens": 208,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf6",
          "neutral_uci": "g8f6",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 \nNf6 e5",
          "neutral_tokens": {
            "prompt_tokens": 172,
            "completion_tokens": 35,
            "total_tokens": 207,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e7e6",
          "positional_uci": "g8f6",
          "neutral_uci": "g8f6",
          "consensus_move": "g8f6",
          "source_agents": [
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nc3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 173,
            "completion_tokens": 35,
            "total_tokens": 208,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 173,
            "completion_tokens": 35,
            "total_tokens": 208,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 172,
            "completion_tokens": 35,
            "total_tokens": 207,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 518,
          "total_completion_tokens": 105,
          "total_tokens": 623
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 173,
            "completion_tokens": 35,
            "total_tokens": 208,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 173,
            "completion_tokens": 35,
            "total_tokens": 208,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 172,
            "completion_tokens": 35,
            "total_tokens": 207,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkbnr/ppp1pppp/2n5/8/2pP4/4PN2/PP3PPP/RNBQKB1R b KQkq - 0 4"
    },
    {
      "turn": 10,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e6",
          "aggressive_uci": "e7e6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 Nf6 5. Bxc4 e6 6. O-O Bd6 7. Nc3",
          "aggressive_tokens": {
            "prompt_tokens": 182,
            "completion_tokens": 53,
            "total_tokens": 235,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "O-O",
            "Bd6",
            "Nc3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e6",
          "positional_uci": "e7e6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 Nf6 5. Bxc4 e6 \nBd3 b6 Nbd2",
          "positional_tokens": {
            "prompt_tokens": 182,
            "completion_tokens": 49,
            "total_tokens": 231,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bd3",
            "b6",
            "Nbd2"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e6",
          "neutral_uci": "e7e6",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 Nf6 5. Bxc4 e6 \nBb5 Bd6 Nc3",
          "neutral_tokens": {
            "prompt_tokens": 181,
            "completion_tokens": 49,
            "total_tokens": 230,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bb5",
            "Bd6",
            "Nc3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e7e6",
          "positional_uci": "e7e6",
          "neutral_uci": "e7e6",
          "consensus_move": "e7e6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "O-O",
              "Bd6",
              "Nc3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bd3",
              "b6",
              "Nbd2"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bb5",
              "Bd6",
              "Nc3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 182,
            "completion_tokens": 53,
            "total_tokens": 235,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 182,
            "completion_tokens": 49,
            "total_tokens": 231,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 181,
            "completion_tokens": 49,
            "total_tokens": 230,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 545,
          "total_completion_tokens": 151,
          "total_tokens": 696
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 182,
            "completion_tokens": 53,
            "total_tokens": 235,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 182,
            "completion_tokens": 49,
            "total_tokens": 231,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 181,
            "completion_tokens": 49,
            "total_tokens": 230,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp1pppp/2n2n2/8/2BP4/4PN2/PP3PPP/RNBQK2R b KQkq - 0 5"
    },
    {
      "turn": 12,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bd6",
          "aggressive_uci": "f8d6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 Nf6 5. Bxc4 e6 6. O-O \nBd6 b6",
          "aggressive_tokens": {
            "prompt_tokens": 189,
            "completion_tokens": 51,
            "total_tokens": 240,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Be7",
          "positional_uci": "f8e7",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 Nf6 5. Bxc4 e6 6. O-O \nBe7 b3",
          "positional_tokens": {
            "prompt_tokens": 189,
            "completion_tokens": 50,
            "total_tokens": 239,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Be7",
          "neutral_uci": "f8e7",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 Nf6 5. Bxc4 e6 6. O-O \nBe7 b3 a6",
          "neutral_tokens": {
            "prompt_tokens": 188,
            "completion_tokens": 52,
            "total_tokens": 240,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b3",
            "a6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f8d6",
          "positional_uci": "f8e7",
          "neutral_uci": "f8e7",
          "consensus_move": "f8e7",
          "source_agents": [
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "b6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "b3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "b3",
              "a6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 189,
            "completion_tokens": 51,
            "total_tokens": 240,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 189,
            "completion_tokens": 50,
            "total_tokens": 239,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 188,
            "completion_tokens": 52,
            "total_tokens": 240,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 566,
          "total_completion_tokens": 153,
          "total_tokens": 719
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 189,
            "completion_tokens": 51,
            "total_tokens": 240,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 189,
            "completion_tokens": 50,
            "total_tokens": 239,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 188,
            "completion_tokens": 52,
            "total_tokens": 240,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n1pn2/8/2BP4/4PN2/PP3PPP/RNBQ1RK1 b kq - 1 6"
    },
    {
      "turn": 14,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "b6",
          "aggressive_uci": "b7b6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 Nf6 5. Bxc4 e6 6. O-O Be7 7. Nc3 \nb6 Nbd2",
          "aggressive_tokens": {
            "prompt_tokens": 197,
            "completion_tokens": 59,
            "total_tokens": 256,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nbd2"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "b6",
          "positional_uci": "b7b6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 Nf6 5. Bxc4 e6 6. O-O Be7 7. Nc3 \nb6 Bd2",
          "positional_tokens": {
            "prompt_tokens": 197,
            "completion_tokens": 58,
            "total_tokens": 255,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bd2"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf6",
          "neutral_uci": null,
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 Nf6 5. Bxc4 e6 6. O-O Be7 7. Nc3 \nNf6  b3  a6",
          "neutral_tokens": {
            "prompt_tokens": 196,
            "completion_tokens": 63,
            "total_tokens": 259,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b3",
            "a6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b7b6",
          "positional_uci": "b7b6",
          "neutral_uci": null,
          "consensus_move": "b7b6",
          "source_agents": [
            "aggressive",
            "positional"
          ],
          "selected_agent": "aggressive"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nbd2"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bd2"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "b3",
              "a6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 197,
            "completion_tokens": 59,
            "total_tokens": 256,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 197,
            "completion_tokens": 58,
            "total_tokens": 255,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 196,
            "completion_tokens": 63,
            "total_tokens": 259,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 590,
          "total_completion_tokens": 180,
          "total_tokens": 770
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 197,
            "completion_tokens": 59,
            "total_tokens": 256,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 197,
            "completion_tokens": 58,
            "total_tokens": 255,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 196,
            "completion_tokens": 63,
            "total_tokens": 259,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqk2r/ppp1bppp/2n1pn2/8/2BP4/2N1PN2/PP3PPP/R1BQ1RK1 b kq - 3 7"
    },
    {
      "turn": 16,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bb7",
          "aggressive_uci": "c8b7",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 Nf6 5. Bxc4 e6 6. O-O Be7 7. Nc3 b6 8. Bb5 \nBb7 Na4 a6",
          "aggressive_tokens": {
            "prompt_tokens": 206,
            "completion_tokens": 69,
            "total_tokens": 275,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Na4",
            "a6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "a6",
          "positional_uci": "a7a6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 Nf6 5. Bxc4 e6 6. O-O Be7 7. Nc3 b6 8. Bb5 a6 Ba4 Nd5",
          "positional_tokens": {
            "prompt_tokens": 206,
            "completion_tokens": 67,
            "total_tokens": 273,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Ba4",
            "Nd5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bb7",
          "neutral_uci": "c8b7",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 Nf6 5. Bxc4 e6 6. O-O Be7 7. Nc3 b6 8. Bb5 \nBb7 Na4",
          "neutral_tokens": {
            "prompt_tokens": 205,
            "completion_tokens": 67,
            "total_tokens": 272,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Na4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "c8b7",
          "positional_uci": "a7a6",
          "neutral_uci": "c8b7",
          "consensus_move": "c8b7",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Na4",
              "a6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Ba4",
              "Nd5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Na4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 206,
            "completion_tokens": 69,
            "total_tokens": 275,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 206,
            "completion_tokens": 67,
            "total_tokens": 273,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 205,
            "completion_tokens": 67,
            "total_tokens": 272,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 617,
          "total_completion_tokens": 203,
          "total_tokens": 820
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 206,
            "completion_tokens": 69,
            "total_tokens": 275,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 206,
            "completion_tokens": 67,
            "total_tokens": 273,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 205,
            "completion_tokens": 67,
            "total_tokens": 272,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqk2r/p1p1bppp/1pn1pn2/1B6/3P4/2N1PN2/PP3PPP/R1BQ1RK1 b kq - 1 8"
    },
    {
      "turn": 18,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Na5",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 Nf6 5. Bxc4 e6 6. O-O Be7 7. Nc3 b6 8. Bb5 Bb7 9. Ne5 \nNa5 Nc4 Qe7",
          "aggressive_tokens": {
            "prompt_tokens": 214,
            "completion_tokens": 78,
            "total_tokens": 292,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc4",
            "Qe7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Na5",
          "positional_uci": null,
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 Nf6 5. Bxc4 e6 6. O-O Be7 7. Nc3 b6 8. Bb5 Bb7 9. Ne5 \nNa5 Bd6",
          "positional_tokens": {
            "prompt_tokens": 214,
            "completion_tokens": 74,
            "total_tokens": 288,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bd6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Na5",
          "neutral_uci": null,
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 Nf6 5. Bxc4 e6 6. O-O Be7 7. Nc3 b6 8. Bb5 Bb7 9. Ne5 Na5 Bd2 Qe7",
          "neutral_tokens": {
            "prompt_tokens": 213,
            "completion_tokens": 76,
            "total_tokens": 289,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bd2",
            "Qe7"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nc4",
              "Qe7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bd6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bd2",
              "Qe7"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 214,
            "completion_tokens": 78,
            "total_tokens": 292,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 214,
            "completion_tokens": 74,
            "total_tokens": 288,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 213,
            "completion_tokens": 76,
            "total_tokens": 289,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 641,
          "total_completion_tokens": 228,
          "total_tokens": 869
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 214,
            "completion_tokens": 78,
            "total_tokens": 292,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 214,
            "completion_tokens": 74,
            "total_tokens": 288,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 213,
            "completion_tokens": 76,
            "total_tokens": 289,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qk2r/pbp1bppp/1pn1pn2/1B2N3/3P4/2N1P3/PP3PPP/R1BQ1RK1 b kq - 3 9"
    },
    {
      "turn": 18,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "Na5",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 Nf6 5. Bxc4 e6 6. O-O Be7 7. Nc3 b6 8. Bb5 Bb7 9. Ne5 \nNa5 Nc4 Qe7",
          "aggressive_tokens": {
            "prompt_tokens": 214,
            "completion_tokens": 78,
            "total_tokens": 292,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc4",
            "Qe7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Na5",
          "positional_uci": null,
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 Nf6 5. Bxc4 e6 6. O-O Be7 7. Nc3 b6 8. Bb5 Bb7 9. Ne5 Na5 Bd2",
          "positional_tokens": {
            "prompt_tokens": 214,
            "completion_tokens": 73,
            "total_tokens": 287,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bd2"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Na5",
          "neutral_uci": null,
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 Nf6 5. Bxc4 e6 6. O-O Be7 7. Nc3 b6 8. Bb5 Bb7 9. Ne5 \nNa5 Nc4 Qe7",
          "neutral_tokens": {
            "prompt_tokens": 213,
            "completion_tokens": 78,
            "total_tokens": 291,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc4",
            "Qe7"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nc4",
              "Qe7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bd2"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc4",
              "Qe7"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 214,
            "completion_tokens": 78,
            "total_tokens": 292,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 214,
            "completion_tokens": 73,
            "total_tokens": 287,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 213,
            "completion_tokens": 78,
            "total_tokens": 291,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 641,
          "total_completion_tokens": 229,
          "total_tokens": 870
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 214,
            "completion_tokens": 78,
            "total_tokens": 292,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 214,
            "completion_tokens": 73,
            "total_tokens": 287,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 213,
            "completion_tokens": 78,
            "total_tokens": 291,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qk2r/pbp1bppp/1pn1pn2/1B2N3/3P4/2N1P3/PP3PPP/R1BQ1RK1 b kq - 3 9"
    },
    {
      "turn": 18,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "Na5",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 Nf6 5. Bxc4 e6 6. O-O Be7 7. Nc3 b6 8. Bb5 Bb7 9. Ne5 \nNa5 Bd6",
          "aggressive_tokens": {
            "prompt_tokens": 214,
            "completion_tokens": 74,
            "total_tokens": 288,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bd6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Na5",
          "positional_uci": null,
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 Nf6 5. Bxc4 e6 6. O-O Be7 7. Nc3 b6 8. Bb5 Bb7 9. Ne5 Na5 Bd2",
          "positional_tokens": {
            "prompt_tokens": 214,
            "completion_tokens": 73,
            "total_tokens": 287,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bd2"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Na5",
          "neutral_uci": null,
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. e3 Nf6 5. Bxc4 e6 6. O-O Be7 7. Nc3 b6 8. Bb5 Bb7 9. Ne5 \nNa5 Bd6",
          "neutral_tokens": {
            "prompt_tokens": 213,
            "completion_tokens": 74,
            "total_tokens": 287,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bd6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bd6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bd2"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bd6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 214,
            "completion_tokens": 74,
            "total_tokens": 288,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 214,
            "completion_tokens": 73,
            "total_tokens": 287,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 213,
            "completion_tokens": 74,
            "total_tokens": 287,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 641,
          "total_completion_tokens": 221,
          "total_tokens": 862
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 214,
            "completion_tokens": 74,
            "total_tokens": 288,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 214,
            "completion_tokens": 73,
            "total_tokens": 287,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 213,
            "completion_tokens": 74,
            "total_tokens": 287,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qk2r/pbp1bppp/1pn1pn2/1B2N3/3P4/2N1P3/PP3PPP/R1BQ1RK1 b kq - 3 9"
    },
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": "e2e4",
          "aggressive_response": "There is no game to repeat. \nMove: e4\nPlan: e5, Nb1-c3, Nb8-c6",
          "aggressive_tokens": {
            "prompt_tokens": 185,
            "completion_tokens": 28,
            "total_tokens": 213,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nb1",
            "c3",
            "Nb8",
            "c6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "Move: e4 \nPlan: e5 Nf3 Nc6",
          "positional_tokens": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": "e2e4",
          "neutral_response": "Move: e4\nPlan: e5 Nf3 Nc6",
          "neutral_tokens": {
            "prompt_tokens": 184,
            "completion_tokens": 16,
            "total_tokens": 200,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e2e4",
          "positional_uci": "e2e4",
          "neutral_uci": "e2e4",
          "consensus_move": "e2e4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5",
              "Nb1",
              "c3",
              "Nb8",
              "c6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Nf3",
              "Nc6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5",
              "Nf3",
              "Nc6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 185,
            "completion_tokens": 28,
            "total_tokens": 213,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 184,
            "completion_tokens": 16,
            "total_tokens": 200,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 554,
          "total_completion_tokens": 60,
          "total_tokens": 614
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 185,
            "completion_tokens": 28,
            "total_tokens": 213,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 184,
            "completion_tokens": 16,
            "total_tokens": 200,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 3,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": null,
          "positional_response": "1. e4 e5\nWhite",
          "positional_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\nWhite",
          "neutral_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 452,
          "total_completion_tokens": 27,
          "total_tokens": 479
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 3,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": null,
          "positional_response": "1. e4 e5\nWhite",
          "positional_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\nWhite",
          "neutral_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 452,
          "total_completion_tokens": 27,
          "total_tokens": 479
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 3,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": null,
          "positional_response": "1. e4 e5\nWhite",
          "positional_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\nWhite",
          "neutral_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 452,
          "total_completion_tokens": 27,
          "total_tokens": 479
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    }
  ],
  "black_responses": [
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": "d2d4",
          "aggressive_response": "d4 d5 2. Nf3 Nf6 3. Bg5 Bf5 4. e3 e6 5. Bd3 Bxd3 6. Qxd3 Be7 7. O-O O-O 8. Nbd2 Nbd7 9. c4 c6 10. Rac1 Rc8 11. cxd5 exd5 12. a3 Re8 13. b4 a6 14. Nb3 Ne4 15. Bxe7 Qxe7 16. Nc5 Ndxc5 17. bxc5 f5 18. Rb1 Rc7 19. Rb2 Qf6 20. Rfb1 Ree7 21. Ne5 Nxc5 22. dxc5 Qxe5 23. Rb4 Qf6 24. Rf4 g6 25. g4 Re4 26. Rxe4 fxe4 27. Qd4 Qxd4 28. exd4 Kf7 29. Kg2 Kf6 30. Kg3 Kg5 31. h4+ Kf6 32. Kf4 h6 33. g5+ hxg5+ 34. hxg5+ Ke6 35. Rh1 Rf7+ 36. Ke3 Kf5 37. Rg1 Rh7 38. f3 Rh3 39. Rf1 Rxf3+ 40. Rxf3+ exf3 41. Kxf3 Kxg5 42. Kg3 Kf5 43. Kf3 g5 44. a4 g4+ 45. Ke3 g3 46. Kf3 g2 47. Kxg2 Ke4 48. Kf2 Kxd4 49. Ke2 Kxc5 50. Kd3 Kb4 51. Kd4 Kxa4 52. Kc5 Kb3 53. Kb6 d4 54. Kxb7 d3 55. Kxc6 d2 56. Kb6 d1=Q 57. Kxa6 Qd5 58. Kb6 Kb4 59. Kc7 Qe6 60. Kb7 Kb5 61. Kc7 Qe7+ 62. Kc8 Kb6 63. Kb8 Qb7# d4 d5 2. Nf3 Nf6 3. Bg5 Bf5 4. e3 e6 5. Bd3 Bxd3 6. Qxd3 Be7 7. O-O O-O 8. Nbd2 Nbd7 9. c4 c6 10. Rac1 Rc8 11. cxd5 exd5 12. a3 Re8 13. b4 a6 14. Nb3 Ne4 15. Bxe7 Qxe7 16. Nc5 Ndxc",
          "aggressive_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Nf3",
            "Nf6",
            "Bg5",
            "Bf5",
            "e3",
            "e6",
            "Bd3",
            "Bxd3",
            "Qxd3",
            "Be7",
            "O-O",
            "O-O",
            "Nbd2",
            "Nbd7",
            "c4",
            "c6",
            "Rac1",
            "Rc8",
            "cxd5",
            "exd5",
            "a3",
            "Re8",
            "b4",
            "a6",
            "Nb3",
            "Ne4",
            "Bxe7",
            "Qxe7",
            "Nc5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 Bc5 4. O-O Nge7 5. Nxe5 Nxe5 6. d4 Bxd4 7. Qxd4 N7c6 8. Bxc6 Nxc6 9. Qxg7 Rf8 10. Bh6 Qe7 11. Nc3 d6 12. Nd5 Qe5 13. Qxf8+ Kd7 14. Qxf7+ Kd8 15. Qxc7+ Ke8 16. Qxh7 Be6 17. Nc7+ Kd8 18. Nxa8 Nd4 19. Qc7+ Ke8 20. Qxb7 Ne2+ 21. Kh1 Qh5 22. Nc7+ Kf7 23. Nxe6+ Kxe6 24. Qd5+ Qxd5 25. exd5+ Kxd5 26. Rad1+ Kc4 27. Rfe1 Nd4 28. Re4 d5 29. Rexd4+ Kc5 30. Rxd5+ Kc6 31. Rd6+ Kc5 32. R1d5+ Kc4 33. b3+ Kc3 34. Rc6+ Kb2 35. Bg7+ Kxa2 36. Ra5+ Kb1 37. Ra1# 1-0 e4 Nf6 d4 Nxd4 Nxe5 Nxe5 d4 Bxd4 Qxd4 N7c6 Bxc6 Nxc6 Qxg7 Rf8 Bh6 Qe7 Nc3 d6 Nd5 Qe5 Qxf8+ Kd7 Qxf7+ Kd8 Qxc7+ Ke8 Qxh7 Be6 Nc7+ Kd8 Nxa8 Nd4 Qc7+ Ke8 Qxb7 Ne2+ Kh1 Qh5 Nc7+ Kf7 Nxe6+ Kxe6 Qd5+ Qxd5 exd5+ Kxd5 Rad1+ Kc4 Rfe1 Nd4 Re4 d5 Rexd4+ Kc5 Rxd5+ Kc6 Rd6+ Kc5 R1d5+ Kc4 b3+ Kc3 Rc5+ Kb2 Rc2+ Kxc2 a5 Rc6+ Kb4 Rd4+ Ka3 Ra4#",
          "positional_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 547,
            "total_tokens": 657,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "d4",
            "Nxd4",
            "Nxe5",
            "Nxe5",
            "d4",
            "Bxd4",
            "Qxd4",
            "N7c6",
            "Bxc6",
            "Nxc6",
            "Qxg7",
            "Rf8",
            "Bh6",
            "Qe7",
            "Nc3",
            "d6",
            "Nd5",
            "Qe5",
            "Qxf8+",
            "Kd7",
            "Qxf7+",
            "Kd8",
            "Qxc7+",
            "Ke8",
            "Qxh7",
            "Be6",
            "Nc7+",
            "Kd8",
            "Nxa8",
            "Nd4",
            "Qc7+",
            "Ke8",
            "Qxb7",
            "Ne2+",
            "Kh1",
            "Qh5",
            "Nc7+",
            "Kf7",
            "Nxe6+",
            "Kxe6",
            "Qd5+",
            "Qxd5",
            "exd5+",
            "Kxd5",
            "Rad1+",
            "Kc4",
            "Rfe1",
            "Nd4",
            "Re4",
            "d5",
            "Rexd4+",
            "Kc5",
            "Rxd5+",
            "Kc6",
            "Rd6+",
            "Kc5",
            "R1d5+",
            "Kc4",
            "b3+",
            "Kc3",
            "Rc5+",
            "Kb2",
            "Rc2+",
            "Kxc2",
            "a5",
            "Rc6+",
            "Kb4",
            "Rd4+",
            "Ka3",
            "Ra4#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": "d2d4",
          "neutral_response": "d4 Nf6\nd4 Nf6 Nc3 d5 Bg5 e6 e4 dxe4 Nxe4 Be7 Nxf6+ Bxf6 Bxf6 Qxf6 Nf3 O-O Bd3 Nc6 O-O Rd8 c3 e5 dxe5 Nxe5 Nxe5 Qxe5 Re1 Qd6 Re3 Bf5 Qe2 Bxd3 Qe1 Bb5 a4 Bc6 b4 Re8 b5 Rxe3 Qxe3 Bd7 Re1 h6 c4 Re8 Qc1 Rxe1+ Qxe1 Qe6 Qc1 Qe4 h3 Be6 c5 Qxa4 b6 axb6 cxb6 cxb6 Qxb6 Qd1+ Kh2 Qd6+ g3 Qd5 Qb8+ Kh7 Qf4 b5 h4 Qc4 Qe3 b4 f4 b3 f5 b2 fxe6 b1=Q exf7 Qbc2+ Kh3 Qc8+ g4 Q8c3+ Qg3 Qxg3+ Kxg3 Qd3+ Kf4 Qd4+ Kg3 Qe3+ Kg2 Qe2+ Kg3 Qe1+ Kg2 Qxh4 f8=Q Qxg4+ Kf2 Qf5+ Qxf5+ Qxf5+ Kg3 Kg6 Kh4 h5 Kg3 Kg5 Kh3 h4 Kg2 Kg4 Kh2 h3 Kg1 Kg3 Kh1 h2+ Kg2 Kh4 Kxh2 g5 Kg2 g4 Kh1 g3 Kg2 Kg4 Kg1 Kf3 Kh2 Kf2 Kh3 g2 Kh4 g1=Q Kh5 Kf4 Kh6 Kf3 Kh4 Qg4#\nd4 Nf6 Nc3 d5 Bg5 e6 e4 dxe4 Nxe4 Be7 Nxf6+ Bxf6 Bxf6 Qxf6 Nf3 O-O Bd3 Nc6 O-O Rd8 c3 e5 dxe5 Nxe5 Nxe5 Qxe5 Re1 Qd6 Re3 Bf5 Qe2 Bxd3 Qe1 Bb5 a4 Bc6 b4 Re8 b5 Rxe3 Qxe3 Bd7 Re1 h6 c4 Re8 Qc1 Rxe1+ Qxe1 Qe6 Qc1 Qe4 h3 Be6 c5 Qxa4 b6 axb6 cxb6 cxb6 Qxb6 Qd1+ Kh2 Qd6+ g3 Qd5 Qb8+ Kh7 Qf4 b5 h4 Qc4 Qe3 b4 f4 b3 f5 Bxf5 Qf3 Qc2+ Kg1 b2 g4 b1=Q+ Qf1 Qxf1+ Kxf1 Bxg4 Kf2 Kg6",
          "neutral_tokens": {
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf6",
            "Nc3",
            "d5",
            "Bg5",
            "e6",
            "e4",
            "dxe4",
            "Nxe4",
            "Be7",
            "Nxf6+",
            "Bxf6",
            "Bxf6",
            "Qxf6",
            "Nf3",
            "O-O",
            "Bd3",
            "Nc6",
            "O-O",
            "Rd8",
            "c3",
            "e5",
            "dxe5",
            "Nxe5",
            "Nxe5",
            "Qxe5",
            "Re1",
            "Qd6",
            "Re3",
            "Bf5",
            "Qe2",
            "Bxd3",
            "Qe1",
            "Bb5",
            "a4",
            "Bc6",
            "b4",
            "Re8",
            "b5",
            "Rxe3",
            "Qxe3",
            "Bd7",
            "Re1",
            "h6",
            "c4",
            "Re8",
            "Qc1",
            "Rxe1+",
            "Qxe1",
            "Qe6",
            "Qc1",
            "Qe4",
            "h3",
            "Be6",
            "c5",
            "Qxa4",
            "b6",
            "axb6",
            "cxb6",
            "cxb6",
            "Qxb6",
            "Qd1+",
            "Kh2",
            "Qd6+",
            "g3",
            "Qd5",
            "Qb8+",
            "Kh7",
            "Qf4",
            "b5",
            "h4",
            "Qc4",
            "Qe3",
            "b4",
            "f4",
            "b3",
            "f5",
            "Bxf5",
            "Qf3",
            "Qc2+",
            "Kg1",
            "b2",
            "g4",
            "b1=Q+",
            "Qf1",
            "Qxf1+",
            "Kxf1",
            "Bxg4",
            "Kf2",
            "Kg6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d2d4",
          "positional_uci": "e2e4",
          "neutral_uci": "d2d4",
          "consensus_move": "d2d4",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d5",
              "Nf3",
              "Nf6",
              "Bg5",
              "Bf5",
              "e3",
              "e6",
              "Bd3",
              "Bxd3",
              "Qxd3",
              "Be7",
              "O-O",
              "O-O",
              "Nbd2",
              "Nbd7",
              "c4",
              "c6",
              "Rac1",
              "Rc8",
              "cxd5",
              "exd5",
              "a3",
              "Re8",
              "b4",
              "a6",
              "Nb3",
              "Ne4",
              "Bxe7",
              "Qxe7",
              "Nc5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf6",
              "d4",
              "Nxd4",
              "Nxe5",
              "Nxe5",
              "d4",
              "Bxd4",
              "Qxd4",
              "N7c6",
              "Bxc6",
              "Nxc6",
              "Qxg7",
              "Rf8",
              "Bh6",
              "Qe7",
              "Nc3",
              "d6",
              "Nd5",
              "Qe5",
              "Qxf8+",
              "Kd7",
              "Qxf7+",
              "Kd8",
              "Qxc7+",
              "Ke8",
              "Qxh7",
              "Be6",
              "Nc7+",
              "Kd8",
              "Nxa8",
              "Nd4",
              "Qc7+",
              "Ke8",
              "Qxb7",
              "Ne2+",
              "Kh1",
              "Qh5",
              "Nc7+",
              "Kf7",
              "Nxe6+",
              "Kxe6",
              "Qd5+",
              "Qxd5",
              "exd5+",
              "Kxd5",
              "Rad1+",
              "Kc4",
              "Rfe1",
              "Nd4",
              "Re4",
              "d5",
              "Rexd4+",
              "Kc5",
              "Rxd5+",
              "Kc6",
              "Rd6+",
              "Kc5",
              "R1d5+",
              "Kc4",
              "b3+",
              "Kc3",
              "Rc5+",
              "Kb2",
              "Rc2+",
              "Kxc2",
              "a5",
              "Rc6+",
              "Kb4",
              "Rd4+",
              "Ka3",
              "Ra4#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf6",
              "Nc3",
              "d5",
              "Bg5",
              "e6",
              "e4",
              "dxe4",
              "Nxe4",
              "Be7",
              "Nxf6+",
              "Bxf6",
              "Bxf6",
              "Qxf6",
              "Nf3",
              "O-O",
              "Bd3",
              "Nc6",
              "O-O",
              "Rd8",
              "c3",
              "e5",
              "dxe5",
              "Nxe5",
              "Nxe5",
              "Qxe5",
              "Re1",
              "Qd6",
              "Re3",
              "Bf5",
              "Qe2",
              "Bxd3",
              "Qe1",
              "Bb5",
              "a4",
              "Bc6",
              "b4",
              "Re8",
              "b5",
              "Rxe3",
              "Qxe3",
              "Bd7",
              "Re1",
              "h6",
              "c4",
              "Re8",
              "Qc1",
              "Rxe1+",
              "Qxe1",
              "Qe6",
              "Qc1",
              "Qe4",
              "h3",
              "Be6",
              "c5",
              "Qxa4",
              "b6",
              "axb6",
              "cxb6",
              "cxb6",
              "Qxb6",
              "Qd1+",
              "Kh2",
              "Qd6+",
              "g3",
              "Qd5",
              "Qb8+",
              "Kh7",
              "Qf4",
              "b5",
              "h4",
              "Qc4",
              "Qe3",
              "b4",
              "f4",
              "b3",
              "f5",
              "Bxf5",
              "Qf3",
              "Qc2+",
              "Kg1",
              "b2",
              "g4",
              "b1=Q+",
              "Qf1",
              "Qxf1+",
              "Kxf1",
              "Bxg4",
              "Kf2",
              "Kg6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 110,
            "completion_tokens": 547,
            "total_tokens": 657,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 329,
          "total_completion_tokens": 1827,
          "total_tokens": 2156
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 110,
            "completion_tokens": 547,
            "total_tokens": 657,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 3,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "c4",
          "aggressive_uci": "c2c4",
          "aggressive_response": "2. c4 c6 3. Nc3 Nf6 4. Nf3 dxc4 5. a4 Bf5 6. e3 e6 7. Bxc4 Bb4 8. O-O O-O 9. Qe2 Nbd7 10. e4 Bg6 11. Bd3 Bh5 12. e5 Nd5 13. Nxd5 cxd5 14. Qe3 Bg6 15. Bxg6 hxg6 16. Ng5 Re8 17. Qh3 Nf8 18. Rd1 Rc8 19. Qb3 Qb6 20. Qf3 Re7 21. Qd3 Rec7 22. Be3 Be7 23. Nf3 Qxb2 24. Rab1 Qc2 25. Qxc2 Rxc2 26. Rxb7 R2c7 27. Rdb1 Nd7 28. g3 Nb6 29. Rxc7 Rxc7 30. a5 Nc4 31. Rb8+ Kh7 32. Bg5 f6 33. Bf4 g5 34. exf6 gxf4 35. fxe7 Rxe7 36. a6 fxg3 37. hxg3 Kg6 38. Rb7 Kf6 39. Ne5 Nxe5 40. dxe5+ Kf7 41. f4 Ke8 42. Kf2 Kd8 43. Rb8+ Kc7 44. Rb7+ Kd8 45. Rb8+ Kc7 46. Rb7+ Kd8 47. Rb8+ Kc7 48. Rb7+ Kd8 49. Rb8+ Kc7 50. Rb7+ Kd8 51. Rb8+ Kc7 52. Rb7+ Kd8 53. Rb8+ Kc7 54. Rb7+ Kd8 55. Rb8+ Kc7 56. Rb7+ Kd8 57. Rb8+ Kc7 58. Rb7+ Kd8 59. Rb8+ Kc7 60. Rb7+ Kd8 61. Rb8+ Kc7 62. Rb7+ Kd8 63. Rb8+ Kc7 64. Rb7+ Kd8 65. Rb8+ Kc7 66. Rb7+ Kd8 67. Rb8+ Kc7 68. Rb7+ Kd8 69. Rb8+ Kc7 70. Rb7+ Kd8 71. Rb8+ Kc7 72. Rb7+ Kd8",
          "aggressive_tokens": {
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "c6",
            "Nc3",
            "Nf6",
            "Nf3",
            "dxc4",
            "a4",
            "Bf5",
            "e3",
            "e6",
            "Bxc4",
            "Bb4",
            "O-O",
            "O-O",
            "Qe2",
            "Nbd7",
            "e4",
            "Bg6",
            "Bd3",
            "Bh5",
            "e5",
            "Nd5",
            "Nxd5",
            "cxd5",
            "Qe3",
            "Bg6",
            "Bxg6",
            "hxg6",
            "Ng5",
            "Re8",
            "Qh3",
            "Nf8",
            "Rd1",
            "Rc8",
            "Qb3",
            "Qb6",
            "Qf3",
            "Re7",
            "Qd3",
            "Rec7",
            "Be3",
            "Be7",
            "Nf3",
            "Qxb2",
            "Rab1",
            "Qc2",
            "Qxc2",
            "Rxc2",
            "Rxb7",
            "R2c7",
            "Rdb1",
            "Nd7",
            "g3",
            "Nb6",
            "Rxc7",
            "Rxc7",
            "a5",
            "Nc4",
            "Rb8+",
            "Kh7",
            "Bg5",
            "f6",
            "Bf4",
            "g5",
            "exf6",
            "gxf4",
            "fxe7",
            "Rxe7",
            "a6",
            "fxg3",
            "hxg3",
            "Kg6",
            "Rb7",
            "Kf6",
            "Ne5",
            "Nxe5",
            "dxe5+",
            "Kf7",
            "f4",
            "Ke8",
            "Kf2",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf3",
          "positional_uci": "g1f3",
          "positional_response": "2. Nf3 Nf6 3. c4 c6 4. Nc3 dxc4 5. a4 Bf5 6. e3 e6 7. Bxc4 Bb4 8. O-O O-O 9. Qe2 Bg6 10. Ne5 Nbd7 11. Nxg6 hxg6 12. Rd1 Qa5 13. Bd2 e5 14. dxe5 Nxe5 15. Bb3 Rad8 16. Be1 Rxd1 17. Rxd1 Rd8 18. Rxd8+ Qxd8 19. Qd1 Qxd1 20. Bxd1 Nd3 21. Kf1 Nxb2 22. Bb3 Nd3 23. Bd2 Nc5 24. Bc2 Bxc3 25. Bxc3 Nd5 26. Bd4 b6 27. a5 Kf8 28. axb6 axb6 29. Ke2 Ne6 30. Bb2 Ke7 31. g3 Kd6 32. e4 Nb4 33. Bb3 c5 34. f4 b5 35. f5 gxf5 36. exf5 Nd4+ 37. Bxd4 cxd4 38. Bxf7 Ke5 39. Be8 Kxf5 40. Bxb5 Kg4 41. Bd7+ Kg5 42. h4+ Kf6 43. g4 Nd5 44. Kd3 Ke5 45. Bf5 Nf4+ 46. Kd2 Ng2 47. h5 Ne3 48. Bc8 Kf4 49. Kd3 Nxg4 50. Kxd4 Kg5 51. Ke4 Kxh5 52. Kf4 Nf6 53. Bb7 g5+ 54. Kg3 Kg6 55. Bc6 Kf5 56. Bb7 Ne4+ 57. Kg2 Kf4 58. Bc6 Nf6 59. Bb7 Ng4 60. Bc6 Ne3+ 61. Kg1 Kg3 62. Bb7 Nf5 63. Bc6 Nd4 64. Bb7 Nf3+ 65. Kf1 g4 66. Bc6 Kh2 67. Kf2 Ne5 68. Bd5 g3+ 69. Kf1 Ng4 70. Ke2 Nf6 71. Bc6 Nh5 72. Kf1 Nf4 73. Bf3 Nd3 74. Be4 Nf2 75. Bc6 Ng4 76. Ke2 Ne5 77. Be4 Kg1 78. Ke3 Kf1 79. Kf4 Kf",
          "positional_tokens": {
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Kf1",
            "g4",
            "Bc6",
            "Kh2",
            "Kf2",
            "Ne5",
            "Bd5",
            "g3+",
            "Kf1",
            "Ng4",
            "Ke2",
            "Nf6",
            "Bc6",
            "Nh5",
            "Kf1",
            "Nf4",
            "Bf3",
            "Nd3",
            "Be4",
            "Nf2",
            "Bc6",
            "Ng4",
            "Ke2",
            "Ne5",
            "Be4",
            "Kg1",
            "Ke3",
            "Kf1",
            "Kf4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "c4",
          "neutral_uci": "c2c4",
          "neutral_response": "2. c4 c6 *\n3. Nc3 Nf6 *\n4. Nf3 e6 *\n5. Bg5 Nbd7 *\n6. e3 Qa5 *\n7. Nd2 Bb4 *\n8. Qc2 O-O *\n9. Be2 dxc4 *\n10. Bxf6 Nxf6 *\n11. Nxc4 Qc7 *\n12. O-O b6 *\n13. Rac1 Bb7 *\n14. a3 Be7 *\n15. b4 Rac8 *\n16. Qb3 Qb8 *\n17. Rfd1 Rfd8 *\n18. Ne5 c5 *\n19. bxc5 bxc5 *\n20. Rb1 cxd4 *\n21. exd4 Ba8 *\n22. Qxb8 Rxb8 *\n23. Rxb8 Rxb8 *\n24. Nb5 a6 *\n25. Nc7 Bb7 *\n26. Rb1 Bd6 *\n27. Nxa6 Bxa6 *\n28. Rxb8+ Bxb8 *\n29. Bxa6 Bxe5 *\n30. dxe5 Nd7 *\n31. f4 Nc5 *\n32. Bb5 Kf8 *\n33. Kf2 Ke7 *\n34. Ke3 Kd8 *\n35. Kd4 Nb7 *\n36. a4 Kc7 *\n37. Be8 Nd8 *\n38. Kc5 Nb7+ *\n39. Kb5 Nd8 *\n40. a5 Kb7 *\n41. a6+ Ka7 *\n42. g4 h6 *\n43. h4 g6 *\n44. g5 hxg5 *\n45. hxg5 Ka8 *\n46. Kb6 Kb8 *\n47. a7+ Ka8 *\n48. Ka6 Nb7 *\n49. Bc6 f5 *\n50. Bxb7# f5 *\n51. exf6 e5 *\n52. f7 exf4 *\n53. f8=Q# f3 *\n54. Qc8# f2 *\n55. Qa8# f1=Q *\n56. Bxf1# f4 *\n57. Qb7# f3 *\n58. Qc8# f2 *\n59. Qb7# f1=Q *\n60. Bxf1# f4 *\n61. Qc8# f3 *\n62. Qb7# f2 *\n63. Qc8# f1=Q *\n64. Bxf1# f4 *\n65. Qc8# f3 *\n66. Qb7# f2 *\n67. Qc8# f1=Q *\n68. Bxf1# f4 *\n69. Qc8#",
          "neutral_tokens": {
            "prompt_tokens": 115,
            "completion_tokens": 640,
            "total_tokens": 755,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "c6",
            "Nc3",
            "Nf6",
            "Nf3",
            "e6",
            "Bg5",
            "Nbd7",
            "e3",
            "Qa5",
            "Nd2",
            "Bb4",
            "Qc2",
            "O-O",
            "Be2",
            "dxc4",
            "Bxf6",
            "Nxf6",
            "Nxc4",
            "Qc7",
            "O-O",
            "b6",
            "Rac1",
            "Bb7",
            "a3",
            "Be7",
            "b4",
            "Rac8",
            "Qb3",
            "Qb8",
            "Rfd1",
            "Rfd8",
            "Ne5",
            "c5",
            "bxc5",
            "bxc5",
            "Rb1",
            "cxd4",
            "exd4",
            "Ba8",
            "Qxb8",
            "Rxb8",
            "Rxb8",
            "Rxb8",
            "Nb5",
            "a6",
            "Nc7",
            "Bb7",
            "Rb1",
            "Bd6",
            "Nxa6",
            "Bxa6",
            "Rxb8+",
            "Bxb8",
            "Bxa6",
            "Bxe5",
            "dxe5",
            "Nd7",
            "f4",
            "Nc5",
            "Bb5",
            "Kf8",
            "Kf2",
            "Ke7",
            "Ke3",
            "Kd8",
            "Kd4",
            "Nb7",
            "a4",
            "Kc7",
            "Be8",
            "Nd8",
            "Kc5",
            "Nb7+",
            "Kb5",
            "Nd8",
            "a5",
            "Kb7",
            "a6+",
            "Ka7",
            "g4",
            "h6",
            "h4",
            "g6",
            "g5",
            "hxg5",
            "hxg5",
            "Ka8",
            "Kb6",
            "Kb8",
            "a7+",
            "Ka8",
            "Ka6",
            "Nb7",
            "Bc6",
            "f5",
            "Bxb7#",
            "f5",
            "exf6",
            "e5",
            "f7",
            "exf4",
            "f8=Q#",
            "f3",
            "Qc8#",
            "f2",
            "Qa8#",
            "f1=Q",
            "Bxf1#",
            "f4",
            "Qb7#",
            "f3",
            "Qc8#",
            "f2",
            "Qb7#",
            "f1=Q",
            "Bxf1#",
            "f4",
            "Qc8#",
            "f3",
            "Qb7#",
            "f2",
            "Qc8#",
            "f1=Q",
            "Bxf1#",
            "f4",
            "Qc8#",
            "f3",
            "Qb7#",
            "f2",
            "Qc8#",
            "f1=Q",
            "Bxf1#",
            "f4",
            "Qc8#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "c2c4",
          "positional_uci": "g1f3",
          "neutral_uci": "c2c4",
          "consensus_move": "c2c4",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "c6",
              "Nc3",
              "Nf6",
              "Nf3",
              "dxc4",
              "a4",
              "Bf5",
              "e3",
              "e6",
              "Bxc4",
              "Bb4",
              "O-O",
              "O-O",
              "Qe2",
              "Nbd7",
              "e4",
              "Bg6",
              "Bd3",
              "Bh5",
              "e5",
              "Nd5",
              "Nxd5",
              "cxd5",
              "Qe3",
              "Bg6",
              "Bxg6",
              "hxg6",
              "Ng5",
              "Re8",
              "Qh3",
              "Nf8",
              "Rd1",
              "Rc8",
              "Qb3",
              "Qb6",
              "Qf3",
              "Re7",
              "Qd3",
              "Rec7",
              "Be3",
              "Be7",
              "Nf3",
              "Qxb2",
              "Rab1",
              "Qc2",
              "Qxc2",
              "Rxc2",
              "Rxb7",
              "R2c7",
              "Rdb1",
              "Nd7",
              "g3",
              "Nb6",
              "Rxc7",
              "Rxc7",
              "a5",
              "Nc4",
              "Rb8+",
              "Kh7",
              "Bg5",
              "f6",
              "Bf4",
              "g5",
              "exf6",
              "gxf4",
              "fxe7",
              "Rxe7",
              "a6",
              "fxg3",
              "hxg3",
              "Kg6",
              "Rb7",
              "Kf6",
              "Ne5",
              "Nxe5",
              "dxe5+",
              "Kf7",
              "f4",
              "Ke8",
              "Kf2",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Kf1",
              "g4",
              "Bc6",
              "Kh2",
              "Kf2",
              "Ne5",
              "Bd5",
              "g3+",
              "Kf1",
              "Ng4",
              "Ke2",
              "Nf6",
              "Bc6",
              "Nh5",
              "Kf1",
              "Nf4",
              "Bf3",
              "Nd3",
              "Be4",
              "Nf2",
              "Bc6",
              "Ng4",
              "Ke2",
              "Ne5",
              "Be4",
              "Kg1",
              "Ke3",
              "Kf1",
              "Kf4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "c6",
              "Nc3",
              "Nf6",
              "Nf3",
              "e6",
              "Bg5",
              "Nbd7",
              "e3",
              "Qa5",
              "Nd2",
              "Bb4",
              "Qc2",
              "O-O",
              "Be2",
              "dxc4",
              "Bxf6",
              "Nxf6",
              "Nxc4",
              "Qc7",
              "O-O",
              "b6",
              "Rac1",
              "Bb7",
              "a3",
              "Be7",
              "b4",
              "Rac8",
              "Qb3",
              "Qb8",
              "Rfd1",
              "Rfd8",
              "Ne5",
              "c5",
              "bxc5",
              "bxc5",
              "Rb1",
              "cxd4",
              "exd4",
              "Ba8",
              "Qxb8",
              "Rxb8",
              "Rxb8",
              "Rxb8",
              "Nb5",
              "a6",
              "Nc7",
              "Bb7",
              "Rb1",
              "Bd6",
              "Nxa6",
              "Bxa6",
              "Rxb8+",
              "Bxb8",
              "Bxa6",
              "Bxe5",
              "dxe5",
              "Nd7",
              "f4",
              "Nc5",
              "Bb5",
              "Kf8",
              "Kf2",
              "Ke7",
              "Ke3",
              "Kd8",
              "Kd4",
              "Nb7",
              "a4",
              "Kc7",
              "Be8",
              "Nd8",
              "Kc5",
              "Nb7+",
              "Kb5",
              "Nd8",
              "a5",
              "Kb7",
              "a6+",
              "Ka7",
              "g4",
              "h6",
              "h4",
              "g6",
              "g5",
              "hxg5",
              "hxg5",
              "Ka8",
              "Kb6",
              "Kb8",
              "a7+",
              "Ka8",
              "Ka6",
              "Nb7",
              "Bc6",
              "f5",
              "Bxb7#",
              "f5",
              "exf6",
              "e5",
              "f7",
              "exf4",
              "f8=Q#",
              "f3",
              "Qc8#",
              "f2",
              "Qa8#",
              "f1=Q",
              "Bxf1#",
              "f4",
              "Qb7#",
              "f3",
              "Qc8#",
              "f2",
              "Qb7#",
              "f1=Q",
              "Bxf1#",
              "f4",
              "Qc8#",
              "f3",
              "Qb7#",
              "f2",
              "Qc8#",
              "f1=Q",
              "Bxf1#",
              "f4",
              "Qc8#",
              "f3",
              "Qb7#",
              "f2",
              "Qc8#",
              "f1=Q",
              "Bxf1#",
              "f4",
              "Qc8#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 115,
            "completion_tokens": 640,
            "total_tokens": 755,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 347,
          "total_completion_tokens": 1920,
          "total_tokens": 2267
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 115,
            "completion_tokens": 640,
            "total_tokens": 755,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/3p4/3P4/8/PPP1PPPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 5,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf3",
          "aggressive_uci": "g1f3",
          "aggressive_response": "3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. dxc5 Qxd1 8. Rxd1 Bxc5 9. Nbd2 O-O 10. Nb3 Be7 11. Be2 Nc6 12. Bd2 e5 13. Bc3 e4 14. Nfd4 Nxd4 15. Bxd4 Be6 16. Nc5 Bxc5 17. Bxc5 Rfd8 18. Bd4 Rac8 19. Bc3 Nd5 20. Ba5 b6 21. Be1 a5 22. Rac1 Rxc1 23. Rxc1 Rc8 24. Rxc8+ Bxc8 25. Bc4 Be6 26. Bxd5 Bxd5 27. a3 f5 28. Bc3 Kf7 29. Bd4 b5 30. Bb6 a4 31. Bd4 g6 32. Kf1 Ke6 33. Ke1 Bc4 34. Kd2 Kd5 35. Kc3 Bf1 36. g3 Bc4 37. Kb4 Bf1 38. Ka5 Kc4 39. Kb6 Kb3 40. Kc6 Kc2 41. Kd6 Kd2 42. Ke6 Ke2 43. Kf6 Kxf2 44. Kg7 Kg2 45. Kxh7 Kxh2 46. Kxg6 Bh3 47. Be5 Kg2 48. Kf6 Kf3 49. Bf4 Ke2 50. Ke5 Kd3 51. Kd5 Kc2 52. Be5 Kd3 53. Bd4 Kc2 54. Kc5 Bf1 55. Kd5 Kd3 56. Ke5 Bh3 57. Kf4 Kc2 58. Kg5 Kd3 59. Kh4 Bg4 60. Kg5 Kc2 61. Kf4 Kd3 62. Ke5 Kc2 63. Kd5 Kd3 64. Kc5 Bh3 65. Kxb5 Bf1 66. Kxa4 Kc4 67. b3+ Kd5 68. Kb4 Kc6 69. a4 Bd3 70. Kc3 Be2 71. b4 Bd1 72. b5+ Kb7 73. Kb4 Be2 74. a5 Bd3 75. a6+ Ka8 76. Ka5 Bc4 77. b6 Bd5 78. b7+ Bxb7 79. axb7+ Kxb7 80. Kb5 Kc7 81. K",
          "aggressive_tokens": {
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf6",
            "e3",
            "e6",
            "Bxc4",
            "c5",
            "O-O",
            "a6",
            "dxc5",
            "Qxd1",
            "Rxd1",
            "Bxc5",
            "Nbd2",
            "O-O",
            "Nb3",
            "Be7",
            "Be2",
            "Nc6",
            "Bd2",
            "e5",
            "Bc3",
            "e4",
            "Nfd4",
            "Nxd4",
            "Bxd4",
            "Be6",
            "Nc5",
            "Bxc5",
            "Bxc5",
            "Rfd8",
            "Bd4",
            "Rac8",
            "Bc3",
            "Nd5",
            "Ba5",
            "b6",
            "Be1",
            "a5",
            "Rac1",
            "Rxc1",
            "Rxc1",
            "Rc8",
            "Rxc8+",
            "Bxc8",
            "Bc4",
            "Be6",
            "Bxd5",
            "Bxd5",
            "a3",
            "f5",
            "Bc3",
            "Kf7",
            "Bd4",
            "b5",
            "Bb6",
            "a4",
            "Bd4",
            "g6",
            "Kf1",
            "Ke6",
            "Ke1",
            "Bc4",
            "Kd2",
            "Kd5",
            "Kc3",
            "Bf1",
            "g3",
            "Bc4",
            "Kb4",
            "Bf1",
            "Ka5",
            "Kc4",
            "Kb6",
            "Kb3",
            "Kc6",
            "Kc2",
            "Kd6",
            "Kd2",
            "Ke6",
            "Ke2",
            "Kf6",
            "Kxf2",
            "Kg7",
            "Kg2",
            "Kxh7",
            "Kxh2",
            "Kxg6",
            "Bh3",
            "Be5",
            "Kg2",
            "Kf6",
            "Kf3",
            "Bf4",
            "Ke2",
            "Ke5",
            "Kd3",
            "Kd5",
            "Kc2",
            "Be5",
            "Kd3",
            "Bd4",
            "Kc2",
            "Kc5",
            "Bf1",
            "Kd5",
            "Kd3",
            "Ke5",
            "Bh3",
            "Kf4",
            "Kc2",
            "Kg5",
            "Kd3",
            "Kh4",
            "Bg4",
            "Kg5",
            "Kc2",
            "Kf4",
            "Kd3",
            "Ke5",
            "Kc2",
            "Kd5",
            "Kd3",
            "Kc5",
            "Bh3",
            "Kxb5",
            "Bf1",
            "Kxa4",
            "Kc4",
            "b3+",
            "Kd5",
            "Kb4",
            "Kc6",
            "a4",
            "Bd3",
            "Kc3",
            "Be2",
            "b4",
            "Bd1",
            "b5+",
            "Kb7",
            "Kb4",
            "Be2",
            "a5",
            "Bd3",
            "a6+",
            "Ka8",
            "Ka5",
            "Bc4",
            "b6",
            "Bd5",
            "b7+",
            "Bxb7",
            "axb7+",
            "Kxb7",
            "Kb5",
            "Kc7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf3",
          "positional_uci": "g1f3",
          "positional_response": "3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. dxc5 Qxd1 8. Rxd1 Bxc5 9. b3 O-O 10. Bb2 b5 11. Be2 Bb7 12. Nbd2 Nbd7 13. Rac1 Rac8 14. a4 bxa4 15. bxa4 Nb6 16. a5 Na4 17. Bxf6 gxf6 18. Nb3 Bb4 19. Rxc8 Rxc8 20. Rd7 Bd5 21. Nfd4 Nc5 22. Nxc5 Rxc5 23. h3 Rxa5 24. Bh5 e5 25. Nf5 Be6 26. Rd8+ Bf8 27. Ne7+ Kg7 28. Nc6 Ra1+ 29. Kh2 a5 30. Ra8 a4 31. Nd8 Bd5 32. Ra7 a3 33. Nxf7 Bxf7 34. Bxf7 Kh6 35. Bg8 Bg7 36. g4 Kg6 37. h4 h6 38. f4 exf4 39. exf4 f5 40. h5+ Kf6 41. Rf7+ Ke6 42. Rxg7+ Kd6 43. gxf5 a2 44. Ra7 Rf1 45. Rxa2 Rxf4 46. Be6 Ke5 47. Kg3 Rb4 48. Ra5+ Kf6 49. Ra8 Kg5 50. Rg8+ Kxh5 51. Bf7# 1-0 1. d4 d5 2. c4 dxc4 3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. dxc5 Qxd1 8. Rxd1 Bxc5 9. b3 O-O 10. Bb2 b5 11. Be2 Bb7 12. Nbd2 Nbd7 13. Rac1 Rac8 14. a4 bxa4 15. bxa4 Nb6 16. a5 Na4 17. Bxf6 gxf6 18. Nb3 Bb4 19. Rxc8 Rxc8 20. Rd7 Bd5 21. Nfd4 Nc5 22. Nxc5 Rxc5 23. h3 Rxa5 24. Bh5 e5 25. Nf5 Be6 26. Rd8+ Bf8 27. Ne7+ Kg7 28. Nc6 Ra1+ 29. Kh2 a5 30. Ra8 a",
          "positional_tokens": {
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf6",
            "e3",
            "e6",
            "Bxc4",
            "c5",
            "O-O",
            "a6",
            "dxc5",
            "Qxd1",
            "Rxd1",
            "Bxc5",
            "b3",
            "O-O",
            "Bb2",
            "b5",
            "Be2",
            "Bb7",
            "Nbd2",
            "Nbd7",
            "Rac1",
            "Rac8",
            "a4",
            "bxa4",
            "bxa4",
            "Nb6",
            "a5",
            "Na4",
            "Bxf6",
            "gxf6",
            "Nb3",
            "Bb4",
            "Rxc8",
            "Rxc8",
            "Rd7",
            "Bd5",
            "Nfd4",
            "Nc5",
            "Nxc5",
            "Rxc5",
            "h3",
            "Rxa5",
            "Bh5",
            "e5",
            "Nf5",
            "Be6",
            "Rd8+",
            "Bf8",
            "Ne7+",
            "Kg7",
            "Nc6",
            "Ra1+",
            "Kh2",
            "a5",
            "Ra8"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf3",
          "neutral_uci": "g1f3",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. Qe2 b5 8. Bb3 Bb7 9. Rd1 Nbd7 10. Nc3 Qb8 11. d5 c4 12. dxe6 fxe6 13. Bc2 Bd6 14. h3 O-O 15. Ng5 Nc5 16. b4 cxb3 17. axb3 h6 18. b4 hxg5 19. bxc5 Bxc5 20. Bb3 Qe5 21. Bb2 Bd6 22. g3 Qf5 23. Rxd6 Qxh3 24. Bxe6+ Qxe6 25. Rxe6 Kf7 26. Rxf6+ gxf6 27. Qh5+ Kg7 28. Rd1 Rf7 29. Nd5 Rh8 30. Qxg5+ Kf8 31. Ba3+ Ke8 32. Nxf6+ Rxf6 33. Qe5+ Kf7 34. Rd7+ Kg6 35. Rxb7 Rh5 36. Qe4+ Rhf5 37. g4 1-0 3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. Qe2 b5 8. Bb3 Bb7 9. Rd1 Nbd7 10. Nc3 Qb8 11. d5 c4 12. dxe6 fxe6 13. Bc2 Bd6 14. h3 O-O 15. Ng5 Nc5 16. b4 cxb3 17. axb3 h6 18. b4 hxg5 19. bxc5 Bxc5 20. Bb3 Qe5 21. Bb2 Bd6 22. g3 Qf5 23. Rxd6 Qxh3 24. Bxe6+ Qxe6 25. Rxe6 Kf7 26. Rxf6+ gxf6 27. Qh5+ Kg7 28. Rd1 Rf7 29. Nd5 Rh8 30. Qxg5+ Kf8 31. Ba3+ Ke8 32. Nxf6+ Rxf6 33. Qe5+ Kf7 34. Rd7+ Kg6 35. Rxb7 Rh5 36. Qe4+ Rhf5 37. g4 1-0 3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6.",
          "neutral_tokens": {
            "prompt_tokens": 123,
            "completion_tokens": 640,
            "total_tokens": 763,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf6",
            "e3",
            "e6",
            "Bxc4",
            "c5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g1f3",
          "positional_uci": "g1f3",
          "neutral_uci": "g1f3",
          "consensus_move": "g1f3",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf6",
              "e3",
              "e6",
              "Bxc4",
              "c5",
              "O-O",
              "a6",
              "dxc5",
              "Qxd1",
              "Rxd1",
              "Bxc5",
              "Nbd2",
              "O-O",
              "Nb3",
              "Be7",
              "Be2",
              "Nc6",
              "Bd2",
              "e5",
              "Bc3",
              "e4",
              "Nfd4",
              "Nxd4",
              "Bxd4",
              "Be6",
              "Nc5",
              "Bxc5",
              "Bxc5",
              "Rfd8",
              "Bd4",
              "Rac8",
              "Bc3",
              "Nd5",
              "Ba5",
              "b6",
              "Be1",
              "a5",
              "Rac1",
              "Rxc1",
              "Rxc1",
              "Rc8",
              "Rxc8+",
              "Bxc8",
              "Bc4",
              "Be6",
              "Bxd5",
              "Bxd5",
              "a3",
              "f5",
              "Bc3",
              "Kf7",
              "Bd4",
              "b5",
              "Bb6",
              "a4",
              "Bd4",
              "g6",
              "Kf1",
              "Ke6",
              "Ke1",
              "Bc4",
              "Kd2",
              "Kd5",
              "Kc3",
              "Bf1",
              "g3",
              "Bc4",
              "Kb4",
              "Bf1",
              "Ka5",
              "Kc4",
              "Kb6",
              "Kb3",
              "Kc6",
              "Kc2",
              "Kd6",
              "Kd2",
              "Ke6",
              "Ke2",
              "Kf6",
              "Kxf2",
              "Kg7",
              "Kg2",
              "Kxh7",
              "Kxh2",
              "Kxg6",
              "Bh3",
              "Be5",
              "Kg2",
              "Kf6",
              "Kf3",
              "Bf4",
              "Ke2",
              "Ke5",
              "Kd3",
              "Kd5",
              "Kc2",
              "Be5",
              "Kd3",
              "Bd4",
              "Kc2",
              "Kc5",
              "Bf1",
              "Kd5",
              "Kd3",
              "Ke5",
              "Bh3",
              "Kf4",
              "Kc2",
              "Kg5",
              "Kd3",
              "Kh4",
              "Bg4",
              "Kg5",
              "Kc2",
              "Kf4",
              "Kd3",
              "Ke5",
              "Kc2",
              "Kd5",
              "Kd3",
              "Kc5",
              "Bh3",
              "Kxb5",
              "Bf1",
              "Kxa4",
              "Kc4",
              "b3+",
              "Kd5",
              "Kb4",
              "Kc6",
              "a4",
              "Bd3",
              "Kc3",
              "Be2",
              "b4",
              "Bd1",
              "b5+",
              "Kb7",
              "Kb4",
              "Be2",
              "a5",
              "Bd3",
              "a6+",
              "Ka8",
              "Ka5",
              "Bc4",
              "b6",
              "Bd5",
              "b7+",
              "Bxb7",
              "axb7+",
              "Kxb7",
              "Kb5",
              "Kc7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf6",
              "e3",
              "e6",
              "Bxc4",
              "c5",
              "O-O",
              "a6",
              "dxc5",
              "Qxd1",
              "Rxd1",
              "Bxc5",
              "b3",
              "O-O",
              "Bb2",
              "b5",
              "Be2",
              "Bb7",
              "Nbd2",
              "Nbd7",
              "Rac1",
              "Rac8",
              "a4",
              "bxa4",
              "bxa4",
              "Nb6",
              "a5",
              "Na4",
              "Bxf6",
              "gxf6",
              "Nb3",
              "Bb4",
              "Rxc8",
              "Rxc8",
              "Rd7",
              "Bd5",
              "Nfd4",
              "Nc5",
              "Nxc5",
              "Rxc5",
              "h3",
              "Rxa5",
              "Bh5",
              "e5",
              "Nf5",
              "Be6",
              "Rd8+",
              "Bf8",
              "Ne7+",
              "Kg7",
              "Nc6",
              "Ra1+",
              "Kh2",
              "a5",
              "Ra8"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf6",
              "e3",
              "e6",
              "Bxc4",
              "c5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 123,
            "completion_tokens": 640,
            "total_tokens": 763,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 371,
          "total_completion_tokens": 1920,
          "total_tokens": 2291
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 123,
            "completion_tokens": 640,
            "total_tokens": 763,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/8/2pP4/8/PP2PPPP/RNBQKBNR w KQkq - 0 3"
    },
    {
      "turn": 7,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc3",
          "aggressive_uci": "b1c3",
          "aggressive_response": "4. Nc3 Nf6 5. e4 Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Ne5 14. Qe2 Nc6 15. e5 Nd5 16. Nxd5 exd5 17. f4 f5 18. Nf3 Na5 19. Bc2 Nc4 20. Ng5 Qe7 21. h4 h6 22. h5 Be8 23. gxf5 hxg5 24. fxg5 Nxe3 25. Qxe3 c5 26. f6 Qc7 27. Qd3 gxf6 28. gxf6 Kh8 29. Rdg1 c4 30. Qg3 Qf7 31. Qg7+ Qxg7 32. fxg7+ Kg8 33. Bh7+ Kxh7 34. gxf8=Q Bxf8 35. Rf1 Bh6+ 36. Kc2 Ra7 37. Rf6 Bg7 38. Rd6 Bf7 39. h6 Bf8 40. Rf6 Bg6+ 41. Kc3 b4+ 42. Kd2 Be7 43. Re6 Bg5+ 44. Ke2 Be4 45. Rg1 Bxh6 46. Rg3 Rg7 47. Rh3 Rg6 48. Re7+ Kg8 49. Re8+ Kf7 50. Rh8 Bg7 51. R8h7 Rg2+ 52. Kf1 Rxb2 53. Rg3 Bxh7 54. Rf3+ Ke6 55. Rh3 Bd3+ 56. Ke1 Rxa2 57. Rg3 Bh6 58. Rh3 Bd2+ 59. Kd1 Bc3 60. Rh6+ Kf5 61. Rf6+ Ke4 62. Rf4+ Kxf4 63. e6 Ra1# e4 d5 Nc6",
          "aggressive_tokens": {
            "prompt_tokens": 133,
            "completion_tokens": 519,
            "total_tokens": 652,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "e4",
            "Bg4",
            "Be3",
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O",
            "O-O-O",
            "a6",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Nd2",
            "b5",
            "Bb3",
            "Ne5",
            "Qe2",
            "Nc6",
            "e5",
            "Nd5",
            "Nxd5",
            "exd5",
            "f4",
            "f5",
            "Nf3",
            "Na5",
            "Bc2",
            "Nc4",
            "Ng5",
            "Qe7",
            "h4",
            "h6",
            "h5",
            "Be8",
            "gxf5",
            "hxg5",
            "fxg5",
            "Nxe3",
            "Qxe3",
            "c5",
            "f6",
            "Qc7",
            "Qd3",
            "gxf6",
            "gxf6",
            "Kh8",
            "Rdg1",
            "c4",
            "Qg3",
            "Qf7",
            "Qg7+",
            "Qxg7",
            "fxg7+",
            "Kg8",
            "Bh7+",
            "Kxh7",
            "gxf8=Q",
            "Bxf8",
            "Rf1",
            "Bh6+",
            "Kc2",
            "Ra7",
            "Rf6",
            "Bg7",
            "Rd6",
            "Bf7",
            "h6",
            "Bf8",
            "Rf6",
            "Bg6+",
            "Kc3",
            "b4+",
            "Kd2",
            "Be7",
            "Re6",
            "Bg5+",
            "Ke2",
            "Be4",
            "Rg1",
            "Bxh6",
            "Rg3",
            "Rg7",
            "Rh3",
            "Rg6",
            "Re7+",
            "Kg8",
            "Re8+",
            "Kf7",
            "Rh8",
            "Bg7",
            "R8h7",
            "Rg2+",
            "Kf1",
            "Rxb2",
            "Rg3",
            "Bxh7",
            "Rf3+",
            "Ke6",
            "Rh3",
            "Bd3+",
            "Ke1",
            "Rxa2",
            "Rg3",
            "Bh6",
            "Rh3",
            "Bd2+",
            "Kd1",
            "Bc3",
            "Rh6+",
            "Kf5",
            "Rf6+",
            "Ke4",
            "Rf4+",
            "Kxf4",
            "e6",
            "Ra1#",
            "e4",
            "d5",
            "Nc6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e3",
          "positional_uci": "e2e3",
          "positional_response": "4. e3 e5 5. Bxc4 exd4 6. exd4 Bb4+ 7. Nc3 Nf6 8. O-O O-O 9. Bg5 Bg4 10. Nd5 Be7 11. Nxe7+ Qxe7 12. Re1 Qd6 13. d5 Ne5 14. Be2 Bxf3 15. Bxf3 Nxf3+ 16. Qxf3 Qxd5 17. Qg3 Ne4 18. Rxe4 Qxe4 19. Bh6 Qg6 20. Qxg6 hxg6 21. Bf4 c6 22. Rd1 Rad8 23. Re1 Rfe8 24. Be3 f5 25. g3 g5 26. Rc1 g4 27. Bxa7 Ra8 28. Bd4 Rxa2 29. Rc5 g6 30. Rc3 Rxb2 31. Ra3 Rd2 32. Be3 Rd1+ 33. Kg2 b5 34. Ra6 Rc8 35. Ra7 c5 36. Bh6 c4 37. Rg7+ Kh8 38. Rxg6 c3 39. Rb6 c2 40. Rxb5 c1=Q 41. Bxc1 Rcxc1 42. Rxf5 Rg1# 0-1 d4 d5 c4 dxc4 Nf3 Nc6 e3 e5 Bxc4 exd4 exd4 Bb4+ Nc3 Nf6 O-O O-O Bg5 Bg4 Nd5 Be7 Nxe7+ Qxe7 Re1 Qd6 d5 Ne5 Be2 Bxf3 Bxf3 Nxf3+ Qxf3 Qxd5 Qg3 Ne4 Bf4 Nxg3 hxg3 Qd4 Be3 Qxb2 Rab1 Qxa2 Rxb7 Rab8 Rxc7 Rb1 Rxb1 Qxb1+ Kh2 Qf5 Bxa7 Qh5+ Kg1 Rd8 f3 Rd1+ Kf2 Qa5 Be3 Qe1# 0-1 d4 d5 c4 dxc4 Nf3 Nc6 e3 e5 Bxc4 exd4 exd4 Bb4+ Nc3 Nf6 O-O O-O Bg5 Bg4 Nd5 Be7 Nxe7+ Qxe7 Re1 Qd6 d5 Ne5 Be2 Bxf3 Bxf3 Nxf3+ Qxf3 Qxd5 Qg3 Ne4 Bf4 Nxg3 hxg3 Qd4 Be3 Qxb2 Rab1 Qxa2 Rxb7 Rab8 Rxc7 Rb1 Rxb1 Qxb1+ Kh2 Qf5 Bxa7 Qh5+ Kg1 Rd8 f3 Rd1",
          "positional_tokens": {
            "prompt_tokens": 133,
            "completion_tokens": 640,
            "total_tokens": 773,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "e5",
            "Bxc4",
            "exd4",
            "exd4",
            "Bb4+",
            "Nc3",
            "Nf6",
            "O-O",
            "O-O",
            "Bg5",
            "Bg4",
            "Nd5",
            "Be7",
            "Nxe7+",
            "Qxe7",
            "Re1",
            "Qd6",
            "d5",
            "Ne5",
            "Be2",
            "Bxf3",
            "Bxf3",
            "Nxf3+",
            "Qxf3",
            "Qxd5",
            "Qg3",
            "Ne4",
            "Bf4",
            "Nxg3",
            "hxg3",
            "Qd4",
            "Be3",
            "Qxb2",
            "Rab1",
            "Qxa2",
            "Rxb7",
            "Rab8",
            "Rxc7",
            "Rb1",
            "Rxb1",
            "Qxb1+",
            "Kh2",
            "Qf5",
            "Bxa7",
            "Qh5+",
            "Kg1",
            "Rd8",
            "f3",
            "Rd1"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc3",
          "neutral_uci": "b1c3",
          "neutral_response": "4. Nc3 Nf6 5. e4 Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Ne5 14. Qc2 Nc6 15. f4 Nxd4 16. Bxd4 Qxd4 17. f5 exf5 18. exf5 Bxf5 19. gxf5 c5 20. Nde4 Qe5 21. Nxf6+ Qxf6 22. Nd5 Qh6+ 23. Kb1 c4 24. Nxb4 cxb3 25. Qxb3 a5 26. Nd5 a4 27. Qxb5 Rfb8 28. Qc5 a3 29. b3 Qh4 30. Qd4 Qg3 31. Rhg1 Qxh3 32. Qxg7# 1-0 Nf6 Nxe4 Qxe4",
          "neutral_tokens": {
            "prompt_tokens": 132,
            "completion_tokens": 254,
            "total_tokens": 386,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "e4",
            "Bg4",
            "Be3",
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O",
            "O-O-O",
            "a6",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Nd2",
            "b5",
            "Bb3",
            "Ne5",
            "Qc2",
            "Nc6",
            "f4",
            "Nxd4",
            "Bxd4",
            "Qxd4",
            "f5",
            "exf5",
            "exf5",
            "Bxf5",
            "gxf5",
            "c5",
            "Nde4",
            "Qe5",
            "Nxf6+",
            "Qxf6",
            "Nd5",
            "Qh6+",
            "Kb1",
            "c4",
            "Nxb4",
            "cxb3",
            "Qxb3",
            "a5",
            "Nd5",
            "a4",
            "Qxb5",
            "Rfb8",
            "Qc5",
            "a3",
            "b3",
            "Qh4",
            "Qd4",
            "Qg3",
            "Rhg1",
            "Qxh3",
            "Qxg7#",
            "Nf6",
            "Nxe4",
            "Qxe4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b1c3",
          "positional_uci": "e2e3",
          "neutral_uci": "b1c3",
          "consensus_move": "b1c3",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf6",
              "e4",
              "Bg4",
              "Be3",
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O",
              "O-O-O",
              "a6",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Nd2",
              "b5",
              "Bb3",
              "Ne5",
              "Qe2",
              "Nc6",
              "e5",
              "Nd5",
              "Nxd5",
              "exd5",
              "f4",
              "f5",
              "Nf3",
              "Na5",
              "Bc2",
              "Nc4",
              "Ng5",
              "Qe7",
              "h4",
              "h6",
              "h5",
              "Be8",
              "gxf5",
              "hxg5",
              "fxg5",
              "Nxe3",
              "Qxe3",
              "c5",
              "f6",
              "Qc7",
              "Qd3",
              "gxf6",
              "gxf6",
              "Kh8",
              "Rdg1",
              "c4",
              "Qg3",
              "Qf7",
              "Qg7+",
              "Qxg7",
              "fxg7+",
              "Kg8",
              "Bh7+",
              "Kxh7",
              "gxf8=Q",
              "Bxf8",
              "Rf1",
              "Bh6+",
              "Kc2",
              "Ra7",
              "Rf6",
              "Bg7",
              "Rd6",
              "Bf7",
              "h6",
              "Bf8",
              "Rf6",
              "Bg6+",
              "Kc3",
              "b4+",
              "Kd2",
              "Be7",
              "Re6",
              "Bg5+",
              "Ke2",
              "Be4",
              "Rg1",
              "Bxh6",
              "Rg3",
              "Rg7",
              "Rh3",
              "Rg6",
              "Re7+",
              "Kg8",
              "Re8+",
              "Kf7",
              "Rh8",
              "Bg7",
              "R8h7",
              "Rg2+",
              "Kf1",
              "Rxb2",
              "Rg3",
              "Bxh7",
              "Rf3+",
              "Ke6",
              "Rh3",
              "Bd3+",
              "Ke1",
              "Rxa2",
              "Rg3",
              "Bh6",
              "Rh3",
              "Bd2+",
              "Kd1",
              "Bc3",
              "Rh6+",
              "Kf5",
              "Rf6+",
              "Ke4",
              "Rf4+",
              "Kxf4",
              "e6",
              "Ra1#",
              "e4",
              "d5",
              "Nc6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Bxc4",
              "exd4",
              "exd4",
              "Bb4+",
              "Nc3",
              "Nf6",
              "O-O",
              "O-O",
              "Bg5",
              "Bg4",
              "Nd5",
              "Be7",
              "Nxe7+",
              "Qxe7",
              "Re1",
              "Qd6",
              "d5",
              "Ne5",
              "Be2",
              "Bxf3",
              "Bxf3",
              "Nxf3+",
              "Qxf3",
              "Qxd5",
              "Qg3",
              "Ne4",
              "Bf4",
              "Nxg3",
              "hxg3",
              "Qd4",
              "Be3",
              "Qxb2",
              "Rab1",
              "Qxa2",
              "Rxb7",
              "Rab8",
              "Rxc7",
              "Rb1",
              "Rxb1",
              "Qxb1+",
              "Kh2",
              "Qf5",
              "Bxa7",
              "Qh5+",
              "Kg1",
              "Rd8",
              "f3",
              "Rd1"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf6",
              "e4",
              "Bg4",
              "Be3",
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O",
              "O-O-O",
              "a6",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Nd2",
              "b5",
              "Bb3",
              "Ne5",
              "Qc2",
              "Nc6",
              "f4",
              "Nxd4",
              "Bxd4",
              "Qxd4",
              "f5",
              "exf5",
              "exf5",
              "Bxf5",
              "gxf5",
              "c5",
              "Nde4",
              "Qe5",
              "Nxf6+",
              "Qxf6",
              "Nd5",
              "Qh6+",
              "Kb1",
              "c4",
              "Nxb4",
              "cxb3",
              "Qxb3",
              "a5",
              "Nd5",
              "a4",
              "Qxb5",
              "Rfb8",
              "Qc5",
              "a3",
              "b3",
              "Qh4",
              "Qd4",
              "Qg3",
              "Rhg1",
              "Qxh3",
              "Qxg7#",
              "Nf6",
              "Nxe4",
              "Qxe4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 133,
            "completion_tokens": 519,
            "total_tokens": 652,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 133,
            "completion_tokens": 640,
            "total_tokens": 773,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 132,
            "completion_tokens": 254,
            "total_tokens": 386,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 398,
          "total_completion_tokens": 1413,
          "total_tokens": 1811
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 133,
            "completion_tokens": 519,
            "total_tokens": 652,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 133,
            "completion_tokens": 640,
            "total_tokens": 773,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 132,
            "completion_tokens": 254,
            "total_tokens": 386,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkbnr/ppp1pppp/2n5/8/2pP4/5N2/PP2PPPP/RNBQKB1R w KQkq - 2 4"
    },
    {
      "turn": 9,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": "e2e4",
          "aggressive_response": "5. e4 Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Na5 14. Bc2 c5 15. dxc5 Qxd3 16. Bxd3 Rfd8 17. Bc2 Bxc3 18. bxc3 Rac8 19. f3 Nd7 20. Nb3 Nc4 21. Bf2 Nde5 22. Rxd8+ Rxd8 23. Rd1 Rxd1+ 24. Bxd1 Nd3+ 25. Kc2 Nxf2 26. c6 Nd6 27. c7 Nc8 28. Nc5 Nxd1 29. Kxd1 Kf8 30. Nxa6 Ke7 31. Kd2 Kd6 32. Kd3 f5 33. gxf5 exf5 34. Kd4 fxe4 35. fxe4 Bf7 36. e5+ Kc6 37. Nb4+ Kxc7 38. Kc5 Bc4 39. a4 Bf1 40. axb5 Bxh3 41. Nd5+ Kb7 42. c4 Be6 43. Nf4 Bf5 44. Kd5 g5 45. Ne2 h5 46. c5 h4 47. Nd4 Ne7+ 48. Kd6 h3 49. Nf3 g4 50. Nh2 g3 51. Nf3 h2 52. Kxe7 h1=Q 53. Nd4 Qh7+ 54. Kd6 g2 55. c6+ Kb6 56. e6 g1=Q 57. Nxf5 Qxf5 58. e7 Qd4# e4 d5 c4 dxc4 Nf3 Nc6 Nc3 Nf6 e4 Bg4 Be3 e6 Bxc4 Bb4 Qd3 O-O-O h3 Bg6 g4 Bg6 Nd2 b5 Bb3 Na5 Bc2 c5 dxc5 Qxd3 Bxd3 Rfd8 Bc2 Bxc3 bxc3 Rac8 f3 Nd7 Nb3 Nc4 Bf2 Nde5 Rd1 Rxd1+ Bxd1 Nd3+ Kc2 Nxf2 c6 Nd6 Nxa6 Kd6 c7 Kxc7 Kd3 Kb6 Nb4 Kc5 a3 Nc4 Nc2 Ne5+ Ke3 Nc4+ Kd3 Ne5+ Ke3 Nc4+ Kd3 Ne5+ Ke3 Nc4+ K",
          "aggressive_tokens": {
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bg4",
            "Be3",
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O-O",
            "h3",
            "Bg6",
            "g4",
            "Bg6",
            "Nd2",
            "b5",
            "Bb3",
            "Na5",
            "Bc2",
            "c5",
            "dxc5",
            "Qxd3",
            "Bxd3",
            "Rfd8",
            "Bc2",
            "Bxc3",
            "bxc3",
            "Rac8",
            "f3",
            "Nd7",
            "Nb3",
            "Nc4",
            "Bf2",
            "Nde5",
            "Rd1",
            "Rxd1+",
            "Bxd1",
            "Nd3+",
            "Kc2",
            "Nxf2",
            "c6",
            "Nd6",
            "Nxa6",
            "Kd6",
            "c7",
            "Kxc7",
            "Kd3",
            "Kb6",
            "Nb4",
            "Kc5",
            "a3",
            "Nc4",
            "Nc2",
            "Ne5+",
            "Ke3",
            "Nc4+",
            "Kd3",
            "Ne5+",
            "Ke3",
            "Nc4+",
            "Kd3",
            "Ne5+",
            "Ke3",
            "Nc4+"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "5. e4 Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Na5 14. Bc2 c5 15. f4 c4 16. Qe2 Bxc3 17. bxc3 Qe7 18. f5 Qa3+ 19. Kb1 Qxc3 20. fxg6 Qb4+ 21. Ka1 Qc3+ 22. Kb1 Qb4+ 23. Ka1 Qc3+ 24. Kb1 Qb4+ 25. Ka1 Qc3+ 26. Kb1 Qb4+ 27. Ka1 Qc3+ 28. Kb1 Qb4+ 29. Ka1 Qc3+ 30. Kb1 Qb4+ 31. Ka1 Qc3+ 32. Kb1 Qb4+ 33. Ka1 Qc3+ 34. Kb1 Qb4+ 35. Ka1 Qc3+ 36. Kb1 Qb4+ 37. Ka1 Qc3+ 38. Kb1 Qb4+ 39. Ka1 Qc3+ 40. Kb1 Qb4+ 41. Ka1 Qc3+ 42. Kb1 Qb4+ 43. Ka1 Qc3+ 44. Kb1 Qb4+ 45. Ka1 Qc3+ 46. Kb1 Qb4+ 47. Ka1 Qc3+ 48. Kb1 Qb4+ 49. Ka1 Qc3+ 50. Kb1 Qb4+ 51. Ka1 Qc3+ 52. Kb1 Qb4+ 53. Ka1 Qc3+ 54. Kb1 Qb4+ 55. Ka1 Qc3+ 56. Kb1 Qb4+ 57. Ka1 Qc3+ 58. Kb1 Qb4+ 59. Ka1 Qc3+ 60. Kb1 Qb4+ 61. Ka1 Qc3+ 62. Kb1 Qb4+ 63. Ka1 Qc3+ 64. Kb1 Qb4+ 65. Ka1 Qc3+ 66. Kb1 Qb4+ 67. Ka1 Qc3+ 68. Kb1 Qb4+ 69. Ka1 Qc3+ 70. Kb1 Qb4+ 71. Ka1 Qc3+ 72. Kb1 Qb4+ 73. Ka1 Qc3+ 74. Kb1 Qb4+ 75. Ka1 Qc3+ 76. Kb1 Qb4+ 77. Ka1 Q",
          "positional_tokens": {
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bg4",
            "Be3",
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O",
            "O-O-O",
            "a6",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Nd2",
            "b5",
            "Bb3",
            "Na5",
            "Bc2",
            "c5",
            "f4",
            "c4",
            "Qe2",
            "Bxc3",
            "bxc3",
            "Qe7",
            "f5",
            "Qa3+",
            "Kb1",
            "Qxc3",
            "fxg6",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": "e2e4",
          "neutral_response": "5. e4 e6 6. Bxc4 Bb4 7. Qd3 O-O 8. O-O Re8 9. Bg5 h6 10. Bh4 g5 11. Bg3 g4 12. Ne5 Nxd4 13. Rad1 c5 14. Bh4 Kg7 15. Nxg4 Nxg4 16. Bxd8 Rxd8 17. Qg3 h5 18. h3 f5 19. hxg4 hxg4 20. Qe5+ Kg6 21. exf5+ exf5 22. Nd5 Be6 23. Qf6+ Kh5 24. Nf4# 1-0\n25. Qg6+ Kh4 26. Qh6#",
          "neutral_tokens": {
            "prompt_tokens": 141,
            "completion_tokens": 180,
            "total_tokens": 321,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O",
            "O-O",
            "Re8",
            "Bg5",
            "h6",
            "Bh4",
            "g5",
            "Bg3",
            "g4",
            "Ne5",
            "Nxd4",
            "Rad1",
            "c5",
            "Bh4",
            "Kg7",
            "Nxg4",
            "Nxg4",
            "Bxd8",
            "Rxd8",
            "Qg3",
            "h5",
            "h3",
            "f5",
            "hxg4",
            "hxg4",
            "Qe5+",
            "Kg6",
            "exf5+",
            "exf5",
            "Nd5",
            "Be6",
            "Qf6+",
            "Kh5",
            "Nf4#",
            "Qg6+",
            "Kh4",
            "Qh6#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e2e4",
          "positional_uci": "e2e4",
          "neutral_uci": "e2e4",
          "consensus_move": "e2e4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bg4",
              "Be3",
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O-O",
              "h3",
              "Bg6",
              "g4",
              "Bg6",
              "Nd2",
              "b5",
              "Bb3",
              "Na5",
              "Bc2",
              "c5",
              "dxc5",
              "Qxd3",
              "Bxd3",
              "Rfd8",
              "Bc2",
              "Bxc3",
              "bxc3",
              "Rac8",
              "f3",
              "Nd7",
              "Nb3",
              "Nc4",
              "Bf2",
              "Nde5",
              "Rd1",
              "Rxd1+",
              "Bxd1",
              "Nd3+",
              "Kc2",
              "Nxf2",
              "c6",
              "Nd6",
              "Nxa6",
              "Kd6",
              "c7",
              "Kxc7",
              "Kd3",
              "Kb6",
              "Nb4",
              "Kc5",
              "a3",
              "Nc4",
              "Nc2",
              "Ne5+",
              "Ke3",
              "Nc4+",
              "Kd3",
              "Ne5+",
              "Ke3",
              "Nc4+",
              "Kd3",
              "Ne5+",
              "Ke3",
              "Nc4+"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bg4",
              "Be3",
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O",
              "O-O-O",
              "a6",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Nd2",
              "b5",
              "Bb3",
              "Na5",
              "Bc2",
              "c5",
              "f4",
              "c4",
              "Qe2",
              "Bxc3",
              "bxc3",
              "Qe7",
              "f5",
              "Qa3+",
              "Kb1",
              "Qxc3",
              "fxg6",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O",
              "O-O",
              "Re8",
              "Bg5",
              "h6",
              "Bh4",
              "g5",
              "Bg3",
              "g4",
              "Ne5",
              "Nxd4",
              "Rad1",
              "c5",
              "Bh4",
              "Kg7",
              "Nxg4",
              "Nxg4",
              "Bxd8",
              "Rxd8",
              "Qg3",
              "h5",
              "h3",
              "f5",
              "hxg4",
              "hxg4",
              "Qe5+",
              "Kg6",
              "exf5+",
              "exf5",
              "Nd5",
              "Be6",
              "Qf6+",
              "Kh5",
              "Nf4#",
              "Qg6+",
              "Kh4",
              "Qh6#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 141,
            "completion_tokens": 180,
            "total_tokens": 321,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 425,
          "total_completion_tokens": 1460,
          "total_tokens": 1885
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 141,
            "completion_tokens": 180,
            "total_tokens": 321,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp1pppp/2n2n2/8/2pP4/2N2N2/PP2PPPP/R1BQKB1R w KQkq - 4 5"
    },
    {
      "turn": 11,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d5",
          "aggressive_uci": "d4d5",
          "aggressive_response": "6. d5 Ne5 7. Bf4 Nfd7 8. Bxe5 Nxe5 9. Bxc4 Nxc4 10. Qa4+ Qd7 11. Qxc4 Bxf3 12. gxf3 g6 13. Qd4 f6 14. O-O-O Bg7 15. f4 O-O 16. h4 f5 17. e5 c6 18. h5 cxd5 19. hxg6 hxg6 20. Nxd5 Rac8+ 21. Kb1 Qe6 22. Qxa7 Ra8 23. Qxb7 Rfb8 24. Qxe7 Qa6 25. Nb4 Qc4 26. Rd7 Qe4+ 27. Nc2 Qxh1+ 28. Ne1 Qxe1+ 29. Kc2 Qxf2+ 30. Kd3 Qf3+ 31. Kc2 Qe2+ 32. Kc3 Qxb2+ 33. Kd3 Ra3+ 34. Kc4 Qb5+ 35. Kd4 Rd3# Rb1 Qd3+ Ke1 Qe3+ Kd1 Qe2+ Kc1 Qc2#",
          "aggressive_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 291,
            "total_tokens": 441,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Ne5",
            "Bf4",
            "Nfd7",
            "Bxe5",
            "Nxe5",
            "Bxc4",
            "Nxc4",
            "Qa4+",
            "Qd7",
            "Qxc4",
            "Bxf3",
            "gxf3",
            "g6",
            "Qd4",
            "f6",
            "O-O-O",
            "Bg7",
            "f4",
            "O-O",
            "h4",
            "f5",
            "e5",
            "c6",
            "h5",
            "cxd5",
            "hxg6",
            "hxg6",
            "Nxd5",
            "Rac8+",
            "Kb1",
            "Qe6",
            "Qxa7",
            "Ra8",
            "Qxb7",
            "Rfb8",
            "Qxe7",
            "Qa6",
            "Nb4",
            "Qc4",
            "Rd7",
            "Qe4+",
            "Nc2",
            "Qxh1+",
            "Ne1",
            "Qxe1+",
            "Kc2",
            "Qxf2+",
            "Kd3",
            "Qf3+",
            "Kc2",
            "Qe2+",
            "Kc3",
            "Qxb2+",
            "Kd3",
            "Ra3+",
            "Kc4",
            "Qb5+",
            "Kd4",
            "Rd3#",
            "Rb1",
            "Qd3+",
            "Ke1",
            "Qe3+",
            "Kd1",
            "Qe2+",
            "Kc1",
            "Qc2#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d5",
          "positional_uci": "d4d5",
          "positional_response": "6. d5 Ne5 7. Bf4 Nfd7 8. Bxe5 Nxe5 9. Qa4+ Bd7 10. Qd1 Nxf3+ 11. Qxf3 b5 12. a4 a6 13. Be2 e6 14. O-O Bd6 15. dxe6 fxe6 16. axb5 axb5 17. Rxa8 Qxa8 18. Qh5+ g6 19. Qh6 Bf8 20. Qf4 Bd6 21. Qh6 Bf8 22. Qf4 Bd6 23. Qh6 Bf8 24. Qf4 Bd6 25. Qh6 Bf8 26. Qf4 Bd6 27. Qh6 Bf8 28. Qf4 Bd6 29. Qh6 Bf8 30. Qf4 Bd6 31. Qh6 Bf8 32. Qf4 Bd6 33. Qh6 Bf8 34. Qf4 Bd6 35. Qh6 Bf8 36. Qf4 Bd6 37. Qh6 Bf8 38. Qf4 Bd6 39. Qh6 Bf8 40. Qf4 Bd6 41. Qh6 Bf8 42. Qf4 Bd6 43. Qh6 Bf8 44. Qf4 Bd6 45. Qh6 Bf8 46. Qf4 Bd6 47. Qh6 Bf8 48. Qf4 Bd6 49. Qh6 Bf8 50. Qf4 Bd6 51. Qh6 Bf8 52. Qf4 Bd6 53. Qh6 Bf8 54. Qf4 Bd6 55. Qh6 Bf8 56. Qf4 Bd6 57. Qh6 Bf8 58. Qf4 Bd6 59. Qh6 Bf8 60. Qf4 Bd6 61. Qh6 Bf8 62. Qf4 Bd6 63. Qh6 Bf8 64. Qf4 Bd6 65. Qh6 Bf8 66. Qf4 Bd6 67. Qh6 Bf8 68. Qf4 Bd6 69. Qh6 Bf8 70. Qf4 Bd6 71. Qh6 Bf8 72. Qf4 Bd6 73. Qh6 Bf8 74. Qf4 Bd6 75. Qh6 Bf8 76. Qf4 Bd6 77. Qh6 Bf8 78. Qf4 Bd6 79. Qh6 Bf8 80. Qf4 Bd6 81. Q",
          "positional_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 640,
            "total_tokens": 790,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Ne5",
            "Bf4",
            "Nfd7",
            "Bxe5",
            "Nxe5",
            "Qa4+",
            "Bd7",
            "Qd1",
            "Nxf3+",
            "Qxf3",
            "b5",
            "a4",
            "a6",
            "Be2",
            "e6",
            "O-O",
            "Bd6",
            "dxe6",
            "fxe6",
            "axb5",
            "axb5",
            "Rxa8",
            "Qxa8",
            "Qh5+",
            "g6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d5",
          "neutral_uci": "d4d5",
          "neutral_response": "6. d5 Ne5 7. Bf4 Nfd7 8. Bxe5 Nxe5 9. Qa4+ Bd7 10. Qd1 Nxf3+ 11. Qxf3 b5 12. a4 a6 13. Be2 e6 14. O-O Bc5 15. dxe6 fxe6 16. Qh5+ g6 17. Qxc5 Qe7 18. Qxe7+ Kxe7 19. axb5 axb5 20. Rxa8 Rxa8 21. f4 c6 22. e5 c5 23. Ne4 Ra2 24. Rb1 Bc6 25. Nxc5 Bd5 26. Bf3 Bxf3 27. gxf3 b4 28. Ne4 Kd7 29. Nd6 c3 30. bxc3 bxc3 31. Rc1 c2 32. Kf2 Kc6 33. Ke2 Kd5 34. Kd2 Ra3 35. Rxc2 Rxf3 36. Rc4 Rf2+ 37. Ke3 Rxh2 38. Rd4+ Kc5 39. Ne4+ Kc6 40. Rd6+ Kc7 41. Rxe6 Rh3+ 42. Kd4 Rf3 43. Rf6 Kd7 44. Rf7+ Ke6 45. Ng5# Kd7 46. Nxf3 Ke6 47. Ng5# Kd7 48. Nxh7 Ke6 49. Ng5# Kd7 50. Rf6 Ke7 51. Rxg6 Kd7 52. Rd6+ Ke7 53. f5 Ke8 54. f6 Kf8 55. Rd8# Kxd8 56. e6 Ke8 57. Kd5 Kf8 58. Kd6 Ke8 59. f7+ Kf8 60. e7+ Kg7 61. f8=Q+ Kg6 62. e8=Q+ Kxg5 63. Qe5+ Kg4 64. Qff4+ Kh3 65. Qg3# Kxg3 66. Qf5 Kg2 67. Qf4 Kh3 68. Qg5 Kh2 69. Qg4 Kh1 70. Ke5 Kh2 71. Kf4 Kh1 72. Kf3 Kh2 73. Qg2# Kh1 74. Qg3# Kh1 75. Qg2# Kh1 76. Qg4# Kh2 77. Qg3+ Kh1 78. Qg2# Kh1 79. Qg4# Kh2 80. Qg3+ Kh1",
          "neutral_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Ne5",
            "Bf4",
            "Nfd7",
            "Bxe5",
            "Nxe5",
            "Qa4+",
            "Bd7",
            "Qd1",
            "Nxf3+",
            "Qxf3",
            "b5",
            "a4",
            "a6",
            "Be2",
            "e6",
            "O-O",
            "Bc5",
            "dxe6",
            "fxe6",
            "Qh5+",
            "g6",
            "Qxc5",
            "Qe7",
            "Qxe7+",
            "Kxe7",
            "axb5",
            "axb5",
            "Rxa8",
            "Rxa8",
            "f4",
            "c6",
            "e5",
            "c5",
            "Ne4",
            "Ra2",
            "Rb1",
            "Bc6",
            "Nxc5",
            "Bd5",
            "Bf3",
            "Bxf3",
            "gxf3",
            "b4",
            "Ne4",
            "Kd7",
            "Nd6",
            "c3",
            "bxc3",
            "bxc3",
            "Rc1",
            "c2",
            "Kf2",
            "Kc6",
            "Ke2",
            "Kd5",
            "Kd2",
            "Ra3",
            "Rxc2",
            "Rxf3",
            "Rc4",
            "Rf2+",
            "Ke3",
            "Rxh2",
            "Rd4+",
            "Kc5",
            "Ne4+",
            "Kc6",
            "Rd6+",
            "Kc7",
            "Rxe6",
            "Rh3+",
            "Kd4",
            "Rf3",
            "Rf6",
            "Kd7",
            "Rf7+",
            "Ke6",
            "Ng5#",
            "Kd7",
            "Nxf3",
            "Ke6",
            "Ng5#",
            "Kd7",
            "Nxh7",
            "Ke6",
            "Ng5#",
            "Kd7",
            "Rf6",
            "Ke7",
            "Rxg6",
            "Kd7",
            "Rd6+",
            "Ke7",
            "f5",
            "Ke8",
            "f6",
            "Kf8",
            "Rd8#",
            "Kxd8",
            "e6",
            "Ke8",
            "Kd5",
            "Kf8",
            "Kd6",
            "Ke8",
            "f7+",
            "Kf8",
            "e7+",
            "Kg7",
            "f8=Q+",
            "Kg6",
            "e8=Q+",
            "Kxg5",
            "Qe5+",
            "Kg4",
            "Qff4+",
            "Kh3",
            "Qg3#",
            "Kxg3",
            "Qf5",
            "Kg2",
            "Qf4",
            "Kh3",
            "Qg5",
            "Kh2",
            "Qg4",
            "Kh1",
            "Ke5",
            "Kh2",
            "Kf4",
            "Kh1",
            "Kf3",
            "Kh2",
            "Qg2#",
            "Kh1",
            "Qg3#",
            "Kh1",
            "Qg2#",
            "Kh1",
            "Qg4#",
            "Kh2",
            "Qg3+",
            "Kh1",
            "Qg2#",
            "Kh1",
            "Qg4#",
            "Kh2",
            "Qg3+",
            "Kh1"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d4d5",
          "positional_uci": "d4d5",
          "neutral_uci": "d4d5",
          "consensus_move": "d4d5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Ne5",
              "Bf4",
              "Nfd7",
              "Bxe5",
              "Nxe5",
              "Bxc4",
              "Nxc4",
              "Qa4+",
              "Qd7",
              "Qxc4",
              "Bxf3",
              "gxf3",
              "g6",
              "Qd4",
              "f6",
              "O-O-O",
              "Bg7",
              "f4",
              "O-O",
              "h4",
              "f5",
              "e5",
              "c6",
              "h5",
              "cxd5",
              "hxg6",
              "hxg6",
              "Nxd5",
              "Rac8+",
              "Kb1",
              "Qe6",
              "Qxa7",
              "Ra8",
              "Qxb7",
              "Rfb8",
              "Qxe7",
              "Qa6",
              "Nb4",
              "Qc4",
              "Rd7",
              "Qe4+",
              "Nc2",
              "Qxh1+",
              "Ne1",
              "Qxe1+",
              "Kc2",
              "Qxf2+",
              "Kd3",
              "Qf3+",
              "Kc2",
              "Qe2+",
              "Kc3",
              "Qxb2+",
              "Kd3",
              "Ra3+",
              "Kc4",
              "Qb5+",
              "Kd4",
              "Rd3#",
              "Rb1",
              "Qd3+",
              "Ke1",
              "Qe3+",
              "Kd1",
              "Qe2+",
              "Kc1",
              "Qc2#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Ne5",
              "Bf4",
              "Nfd7",
              "Bxe5",
              "Nxe5",
              "Qa4+",
              "Bd7",
              "Qd1",
              "Nxf3+",
              "Qxf3",
              "b5",
              "a4",
              "a6",
              "Be2",
              "e6",
              "O-O",
              "Bd6",
              "dxe6",
              "fxe6",
              "axb5",
              "axb5",
              "Rxa8",
              "Qxa8",
              "Qh5+",
              "g6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Ne5",
              "Bf4",
              "Nfd7",
              "Bxe5",
              "Nxe5",
              "Qa4+",
              "Bd7",
              "Qd1",
              "Nxf3+",
              "Qxf3",
              "b5",
              "a4",
              "a6",
              "Be2",
              "e6",
              "O-O",
              "Bc5",
              "dxe6",
              "fxe6",
              "Qh5+",
              "g6",
              "Qxc5",
              "Qe7",
              "Qxe7+",
              "Kxe7",
              "axb5",
              "axb5",
              "Rxa8",
              "Rxa8",
              "f4",
              "c6",
              "e5",
              "c5",
              "Ne4",
              "Ra2",
              "Rb1",
              "Bc6",
              "Nxc5",
              "Bd5",
              "Bf3",
              "Bxf3",
              "gxf3",
              "b4",
              "Ne4",
              "Kd7",
              "Nd6",
              "c3",
              "bxc3",
              "bxc3",
              "Rc1",
              "c2",
              "Kf2",
              "Kc6",
              "Ke2",
              "Kd5",
              "Kd2",
              "Ra3",
              "Rxc2",
              "Rxf3",
              "Rc4",
              "Rf2+",
              "Ke3",
              "Rxh2",
              "Rd4+",
              "Kc5",
              "Ne4+",
              "Kc6",
              "Rd6+",
              "Kc7",
              "Rxe6",
              "Rh3+",
              "Kd4",
              "Rf3",
              "Rf6",
              "Kd7",
              "Rf7+",
              "Ke6",
              "Ng5#",
              "Kd7",
              "Nxf3",
              "Ke6",
              "Ng5#",
              "Kd7",
              "Nxh7",
              "Ke6",
              "Ng5#",
              "Kd7",
              "Rf6",
              "Ke7",
              "Rxg6",
              "Kd7",
              "Rd6+",
              "Ke7",
              "f5",
              "Ke8",
              "f6",
              "Kf8",
              "Rd8#",
              "Kxd8",
              "e6",
              "Ke8",
              "Kd5",
              "Kf8",
              "Kd6",
              "Ke8",
              "f7+",
              "Kf8",
              "e7+",
              "Kg7",
              "f8=Q+",
              "Kg6",
              "e8=Q+",
              "Kxg5",
              "Qe5+",
              "Kg4",
              "Qff4+",
              "Kh3",
              "Qg3#",
              "Kxg3",
              "Qf5",
              "Kg2",
              "Qf4",
              "Kh3",
              "Qg5",
              "Kh2",
              "Qg4",
              "Kh1",
              "Ke5",
              "Kh2",
              "Kf4",
              "Kh1",
              "Kf3",
              "Kh2",
              "Qg2#",
              "Kh1",
              "Qg3#",
              "Kh1",
              "Qg2#",
              "Kh1",
              "Qg4#",
              "Kh2",
              "Qg3+",
              "Kh1",
              "Qg2#",
              "Kh1",
              "Qg4#",
              "Kh2",
              "Qg3+",
              "Kh1"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 150,
            "completion_tokens": 291,
            "total_tokens": 441,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 150,
            "completion_tokens": 640,
            "total_tokens": 790,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 449,
          "total_completion_tokens": 1571,
          "total_tokens": 2020
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 150,
            "completion_tokens": 291,
            "total_tokens": 441,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 150,
            "completion_tokens": 640,
            "total_tokens": 790,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/2n2n2/8/2pPP1b1/2N2N2/PP3PPP/R1BQKB1R w KQkq - 1 6"
    },
    {
      "turn": 13,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qa4+",
          "aggressive_uci": "d1a4",
          "aggressive_response": "7. Qa4+ c6 8. b4 cxb3 9. axb3 b6 10. dxc6 Qc7 11. Nb5 Qxc6 12. Nc7+ Kd8 13. Nxa8 Qxa4 14. Rxa4 Bd7 15. Rd4 Nxb3 16. Rd1 Nxc1 17. Rxc1 Nxe4 18. Ne5 Be6 19. Nc7 Nc5 20. Nxe6+ Nxe6 21. Nxf7+ Ke8 22. Nxh8 g6 23. Bb5+ Kd8 24. Nf7# c5 bxc5 bxc5 *\n25. Rxc5 Nxc5 26. O-O a6 27. Rd1+ Kc7 28. Rc1 Kb6 29. Bc4 e6 30. Ng5 Bh6 31. Rb1+ Kc6 32. Nxe6 Nxe6 33. Bxe6 a5 34. Bg8 a4 35. Bxh7 a3 36. Bxg6 Bg7 37. Bf7 Kc5 38. h4 Kd4 39. g4 Ke4 40. Kg2 Kf4 41. Rb4+ Ke5 42. f4+ Kf6 43. Ba2 Bf8 44. Rb6+ Kg7 45. h5 Bc5 46. Rb7+ Kh6 47. Kh3 Be3 48. Kh4 Bxf4 49. Bb1 Bg5+ 50. Kh3 Bf6 51. Rh7+ Kg5 52. Rf7 Bb2 53. Rf5+ Kh6 54. Kh4 Bc1 55. g5+ Kg7 56. h6+ Kh8 57. Rf8# g5+ Kh7 Kh5 *",
          "aggressive_tokens": {
            "prompt_tokens": 157,
            "completion_tokens": 446,
            "total_tokens": 603,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "c6",
            "b4",
            "cxb3",
            "axb3",
            "b6",
            "dxc6",
            "Qc7",
            "Nb5",
            "Qxc6",
            "Nc7+",
            "Kd8",
            "Nxa8",
            "Qxa4",
            "Rxa4",
            "Bd7",
            "Rd4",
            "Nxb3",
            "Rd1",
            "Nxc1",
            "Rxc1",
            "Nxe4",
            "Ne5",
            "Be6",
            "Nc7",
            "Nc5",
            "Nxe6+",
            "Nxe6",
            "Nxf7+",
            "Ke8",
            "Nxh8",
            "g6",
            "Bb5+",
            "Kd8",
            "Nf7#",
            "c5",
            "bxc5",
            "bxc5",
            "Rxc5",
            "Nxc5",
            "O-O",
            "a6",
            "Rd1+",
            "Kc7",
            "Rc1",
            "Kb6",
            "Bc4",
            "e6",
            "Ng5",
            "Bh6",
            "Rb1+",
            "Kc6",
            "Nxe6",
            "Nxe6",
            "Bxe6",
            "a5",
            "Bg8",
            "a4",
            "Bxh7",
            "a3",
            "Bxg6",
            "Bg7",
            "Bf7",
            "Kc5",
            "h4",
            "Kd4",
            "g4",
            "Ke4",
            "Kg2",
            "Kf4",
            "Rb4+",
            "Ke5",
            "f4+",
            "Kf6",
            "Ba2",
            "Bf8",
            "Rb6+",
            "Kg7",
            "h5",
            "Bc5",
            "Rb7+",
            "Kh6",
            "Kh3",
            "Be3",
            "Kh4",
            "Bxf4",
            "Bb1",
            "Bg5+",
            "Kh3",
            "Bf6",
            "Rh7+",
            "Kg5",
            "Rf7",
            "Bb2",
            "Rf5+",
            "Kh6",
            "Kh4",
            "Bc1",
            "g5+",
            "Kg7",
            "h6+",
            "Kh8",
            "Rf8#",
            "g5+",
            "Kh7",
            "Kh5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qa4+",
          "positional_uci": "d1a4",
          "positional_response": "7. Qa4+ c6 8. b4 cxb3 9. axb3 Bxf3 10. gxf3 e6 11. Qxa5 Qxa5 12. Rxa5 Bb4 13. Ra4 Bxc3+ 14. Bd2 Bxd2+ 15. Kxd2 exd5 16. exd5 Nxd5 17. Bc4 O-O-O 18. Rxa7 Nb6+ 19. Kc3 Nxc4 20. bxc4 Kc7 21. Rb1 Rb8 22. c5 Rhd8 23. Re1 Rd7 24. Re4 Rbd8 25. Rb4 Rd3+ 26. Kc2 Rd2+ 27. Kc1 Rd1+ 28. Kc2 R8d2+ 29. Kc3 Rd3+ 30. Kc2 R1d2+ 31. Kc1 Rd1+ 32. Kc2 R3d2+ 33. Kc3 Rd3+ 34. Kc2 R1d2+ 35. Kc1 Rd1+ 36. Kc2 R3d2+ 37. Kc3 Rd3+ 38. Kc2 R1d2+ 39. Kc1 Rd1+ 40. Kc2 R3d2+ 41. Kc3 Rd3+ 42. Kc2 R1d2+ 43. Kc1 Rd1+ 44. Kc2 R3d2+ 45. Kc3 Rd3+ 46. Kc2 R1d2+ 47. Kc1 Rd1+ 48. Kc2 R3d2+ 49. Kc3 Rd3+ 50. Kc2 R1d2+ 51. Kc1 Rd1+ 52. Kc2 R3d2+ 53. Kc3 Rd3+ 54. Kc2 R1d2+ 55. Kc1 Rd1+ 56. Kc2 R3d2+ 57. Kc3 Rd3+ 58. Kc2 R1d2+ 59. Kc1 Rd1+ 60. Kc2 R3d2+ 61. Kc3 Rd3+ 62. Kc2 R1d2+ 63. Kc1 Rd1+ 64. Kc2 R3d2+ 65. Kc3 Rd3+ 66. Kc2 R1d2+ 67. Kc1 Rd1+ 68. Kc2 R3d2+ 69. Kc3 Rd3+ 70. Kc2 R1d2+ 71. Kc1 Rd1+ 72. Kc2 R3d2+ 73. Kc3",
          "positional_tokens": {
            "prompt_tokens": 157,
            "completion_tokens": 640,
            "total_tokens": 797,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "c6",
            "b4",
            "cxb3",
            "axb3",
            "Bxf3",
            "gxf3",
            "e6",
            "Qxa5",
            "Qxa5",
            "Rxa5",
            "Bb4",
            "Ra4",
            "Bxc3+",
            "Bd2",
            "Bxd2+",
            "Kxd2",
            "exd5",
            "exd5",
            "Nxd5",
            "Bc4",
            "O-O-O",
            "Rxa7",
            "Nb6+",
            "Kc3",
            "Nxc4",
            "bxc4",
            "Kc7",
            "Rb1",
            "Rb8",
            "c5",
            "Rhd8",
            "Re1",
            "Rd7",
            "Re4",
            "Rbd8",
            "Rb4",
            "Rd3+",
            "Kc2",
            "Rd2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R8d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qa4+",
          "neutral_uci": "d1a4",
          "neutral_response": "7. Qa4+ c6 8. b4 cxb3 9. axb3 Bxf3 10. gxf3 b6 11. dxc6 Qc7 12. Nd5 Nxd5 13. exd5 e6 14. Bf4 Bd6 15. Bxd6 Qxd6 16. c7+ Ke7 17. Rc1 Rhc8 18. Ba6 Rxc7 19. Rxc7+ Qxc7 20. O-O Rd8 21. dxe6 fxe6 22. Qh4+ Kf7 23. Qxh7 Nxb3 24. Qh5+ Kg8 25. Re1 Nc5 26. Bc4 Qf7 27. Qg4 Rd6 28. Kh1 a5 29. Rg1 a4 30. f4 Ne4 31. Qf3 Nd2 32. Qa8+ Kh7 33. Qxa4 Qxf4 34. Qc2+ Kh8 35. Be2 Qe4+ 36. Qxe4 Nxe4 37. Kg2 Rd2 38. Kf3 Nc3 39. Bc4 Nd5 40. Rb1 Rd4 41. Bxd5 Rxd5 42. Rxb6 Re5 43. Kf4 Re2 44. Kf3 Re5 45. Kf4 Re2 46. Kf3 Re1 47. Rb4 Kh7 48. Re4 Rxe4 49. Kxe4 Kg6 50. Ke5 Kf7 51. f4 Ke7 52. h4 Kf7 53. h5 Ke7 54. f5 exf5 55. Kxf5 Kf7 56. Kg5 Kg8 57. Kg6 Kh8 58. h6 gxh6 59. Kxh6 * e5 60. Kg5 e4 61. Kf4 e3 62. Kxe3",
          "neutral_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 460,
            "total_tokens": 616,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "c6",
            "b4",
            "cxb3",
            "axb3",
            "Bxf3",
            "gxf3",
            "b6",
            "dxc6",
            "Qc7",
            "Nd5",
            "Nxd5",
            "exd5",
            "e6",
            "Bf4",
            "Bd6",
            "Bxd6",
            "Qxd6",
            "c7+",
            "Ke7",
            "Rc1",
            "Rhc8",
            "Ba6",
            "Rxc7",
            "Rxc7+",
            "Qxc7",
            "O-O",
            "Rd8",
            "dxe6",
            "fxe6",
            "Qh4+",
            "Kf7",
            "Qxh7",
            "Nxb3",
            "Qh5+",
            "Kg8",
            "Re1",
            "Nc5",
            "Bc4",
            "Qf7",
            "Qg4",
            "Rd6",
            "Kh1",
            "a5",
            "Rg1",
            "a4",
            "f4",
            "Ne4",
            "Qf3",
            "Nd2",
            "Qa8+",
            "Kh7",
            "Qxa4",
            "Qxf4",
            "Qc2+",
            "Kh8",
            "Be2",
            "Qe4+",
            "Qxe4",
            "Nxe4",
            "Kg2",
            "Rd2",
            "Kf3",
            "Nc3",
            "Bc4",
            "Nd5",
            "Rb1",
            "Rd4",
            "Bxd5",
            "Rxd5",
            "Rxb6",
            "Re5",
            "Kf4",
            "Re2",
            "Kf3",
            "Re5",
            "Kf4",
            "Re2",
            "Kf3",
            "Re1",
            "Rb4",
            "Kh7",
            "Re4",
            "Rxe4",
            "Kxe4",
            "Kg6",
            "Ke5",
            "Kf7",
            "f4",
            "Ke7",
            "h4",
            "Kf7",
            "h5",
            "Ke7",
            "f5",
            "exf5",
            "Kxf5",
            "Kf7",
            "Kg5",
            "Kg8",
            "Kg6",
            "Kh8",
            "h6",
            "gxh6",
            "Kxh6",
            "e5",
            "Kg5",
            "e4",
            "Kf4",
            "e3",
            "Kxe3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d1a4",
          "positional_uci": "d1a4",
          "neutral_uci": "d1a4",
          "consensus_move": "d1a4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "c6",
              "b4",
              "cxb3",
              "axb3",
              "b6",
              "dxc6",
              "Qc7",
              "Nb5",
              "Qxc6",
              "Nc7+",
              "Kd8",
              "Nxa8",
              "Qxa4",
              "Rxa4",
              "Bd7",
              "Rd4",
              "Nxb3",
              "Rd1",
              "Nxc1",
              "Rxc1",
              "Nxe4",
              "Ne5",
              "Be6",
              "Nc7",
              "Nc5",
              "Nxe6+",
              "Nxe6",
              "Nxf7+",
              "Ke8",
              "Nxh8",
              "g6",
              "Bb5+",
              "Kd8",
              "Nf7#",
              "c5",
              "bxc5",
              "bxc5",
              "Rxc5",
              "Nxc5",
              "O-O",
              "a6",
              "Rd1+",
              "Kc7",
              "Rc1",
              "Kb6",
              "Bc4",
              "e6",
              "Ng5",
              "Bh6",
              "Rb1+",
              "Kc6",
              "Nxe6",
              "Nxe6",
              "Bxe6",
              "a5",
              "Bg8",
              "a4",
              "Bxh7",
              "a3",
              "Bxg6",
              "Bg7",
              "Bf7",
              "Kc5",
              "h4",
              "Kd4",
              "g4",
              "Ke4",
              "Kg2",
              "Kf4",
              "Rb4+",
              "Ke5",
              "f4+",
              "Kf6",
              "Ba2",
              "Bf8",
              "Rb6+",
              "Kg7",
              "h5",
              "Bc5",
              "Rb7+",
              "Kh6",
              "Kh3",
              "Be3",
              "Kh4",
              "Bxf4",
              "Bb1",
              "Bg5+",
              "Kh3",
              "Bf6",
              "Rh7+",
              "Kg5",
              "Rf7",
              "Bb2",
              "Rf5+",
              "Kh6",
              "Kh4",
              "Bc1",
              "g5+",
              "Kg7",
              "h6+",
              "Kh8",
              "Rf8#",
              "g5+",
              "Kh7",
              "Kh5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "c6",
              "b4",
              "cxb3",
              "axb3",
              "Bxf3",
              "gxf3",
              "e6",
              "Qxa5",
              "Qxa5",
              "Rxa5",
              "Bb4",
              "Ra4",
              "Bxc3+",
              "Bd2",
              "Bxd2+",
              "Kxd2",
              "exd5",
              "exd5",
              "Nxd5",
              "Bc4",
              "O-O-O",
              "Rxa7",
              "Nb6+",
              "Kc3",
              "Nxc4",
              "bxc4",
              "Kc7",
              "Rb1",
              "Rb8",
              "c5",
              "Rhd8",
              "Re1",
              "Rd7",
              "Re4",
              "Rbd8",
              "Rb4",
              "Rd3+",
              "Kc2",
              "Rd2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R8d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "c6",
              "b4",
              "cxb3",
              "axb3",
              "Bxf3",
              "gxf3",
              "b6",
              "dxc6",
              "Qc7",
              "Nd5",
              "Nxd5",
              "exd5",
              "e6",
              "Bf4",
              "Bd6",
              "Bxd6",
              "Qxd6",
              "c7+",
              "Ke7",
              "Rc1",
              "Rhc8",
              "Ba6",
              "Rxc7",
              "Rxc7+",
              "Qxc7",
              "O-O",
              "Rd8",
              "dxe6",
              "fxe6",
              "Qh4+",
              "Kf7",
              "Qxh7",
              "Nxb3",
              "Qh5+",
              "Kg8",
              "Re1",
              "Nc5",
              "Bc4",
              "Qf7",
              "Qg4",
              "Rd6",
              "Kh1",
              "a5",
              "Rg1",
              "a4",
              "f4",
              "Ne4",
              "Qf3",
              "Nd2",
              "Qa8+",
              "Kh7",
              "Qxa4",
              "Qxf4",
              "Qc2+",
              "Kh8",
              "Be2",
              "Qe4+",
              "Qxe4",
              "Nxe4",
              "Kg2",
              "Rd2",
              "Kf3",
              "Nc3",
              "Bc4",
              "Nd5",
              "Rb1",
              "Rd4",
              "Bxd5",
              "Rxd5",
              "Rxb6",
              "Re5",
              "Kf4",
              "Re2",
              "Kf3",
              "Re5",
              "Kf4",
              "Re2",
              "Kf3",
              "Re1",
              "Rb4",
              "Kh7",
              "Re4",
              "Rxe4",
              "Kxe4",
              "Kg6",
              "Ke5",
              "Kf7",
              "f4",
              "Ke7",
              "h4",
              "Kf7",
              "h5",
              "Ke7",
              "f5",
              "exf5",
              "Kxf5",
              "Kf7",
              "Kg5",
              "Kg8",
              "Kg6",
              "Kh8",
              "h6",
              "gxh6",
              "Kxh6",
              "e5",
              "Kg5",
              "e4",
              "Kf4",
              "e3",
              "Kxe3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 157,
            "completion_tokens": 446,
            "total_tokens": 603,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 157,
            "completion_tokens": 640,
            "total_tokens": 797,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 156,
            "completion_tokens": 460,
            "total_tokens": 616,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 470,
          "total_completion_tokens": 1546,
          "total_tokens": 2016
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 157,
            "completion_tokens": 446,
            "total_tokens": 603,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 157,
            "completion_tokens": 640,
            "total_tokens": 797,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 156,
            "completion_tokens": 460,
            "total_tokens": 616,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/5n2/n2P4/2p1P1b1/2N2N2/PP3PPP/R1BQKB1R w KQkq - 1 7"
    },
    {
      "turn": 15,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "dxc6",
          "aggressive_uci": "d5c6",
          "aggressive_response": "8. dxc6 bxc6 9. Qxc6+ Nd7 10. Ne5 Be6 11. Bxc4 Bxc4 12. Nxc4 e6 13. Nb5 Bb4+ 14. Bd2 Bxd2+ 15. Nxd2 O-O 16. Qxc7 Qe7 17. O-O Rfc8 18. Qd6 Qe8 19. Nc7 Rxc7 20. Qxc7 Rc8 21. Qxa7 Rc2 22. Qd4 e5 23. Qd3 Rxb2 24. Rfb1 Rxb1+ 25. Rxb1 Nc5 26. Qb5 Qxb5 27. Rxb5 Nd7 28. Rd5 Nf8 29. Rxe5 f6 30. Re7 Ng6 31. Rb7 Ne5 32. a4 Nc6 33. Nc4 Nd4 34. a5 Nc6 35. a6 Kf8 36. a7 Nxa7 37. Rxa7 h6 38. Nd6 Kg8 39. Nf5 g6 40. Nxh6+ Kf8 41. Ng4 f5 42. exf5 gxf5 43. Nh6 f4 44. g3 fxg3 45. hxg3 Ke8 46. g4 Kf8 47. g5 Ke8 48. g6 Kf8 49. g7+ Ke8 50. g8=Q# e4 f5 g5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fx",
          "aggressive_tokens": {
            "prompt_tokens": 167,
            "completion_tokens": 640,
            "total_tokens": 807,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "bxc6",
            "Qxc6+",
            "Nd7",
            "Ne5",
            "Be6",
            "Bxc4",
            "Bxc4",
            "Nxc4",
            "e6",
            "Nb5",
            "Bb4+",
            "Bd2",
            "Bxd2+",
            "Nxd2",
            "O-O",
            "Qxc7",
            "Qe7",
            "O-O",
            "Rfc8",
            "Qd6",
            "Qe8",
            "Nc7",
            "Rxc7",
            "Qxc7",
            "Rc8",
            "Qxa7",
            "Rc2",
            "Qd4",
            "e5",
            "Qd3",
            "Rxb2",
            "Rfb1",
            "Rxb1+",
            "Rxb1",
            "Nc5",
            "Qb5",
            "Qxb5",
            "Rxb5",
            "Nd7",
            "Rd5",
            "Nf8",
            "Rxe5",
            "f6",
            "Re7",
            "Ng6",
            "Rb7",
            "Ne5",
            "a4",
            "Nc6",
            "Nc4",
            "Nd4",
            "a5",
            "Nc6",
            "a6",
            "Kf8",
            "a7",
            "Nxa7",
            "Rxa7",
            "h6",
            "Nd6",
            "Kg8",
            "Nf5",
            "g6",
            "Nxh6+",
            "Kf8",
            "Ng4",
            "f5",
            "exf5",
            "gxf5",
            "Nh6",
            "f4",
            "g3",
            "fxg3",
            "hxg3",
            "Ke8",
            "g4",
            "Kf8",
            "g5",
            "Ke8",
            "g6",
            "Kf8",
            "g7+",
            "Ke8",
            "g8=Q#",
            "e4",
            "f5",
            "g5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "dxc6",
          "positional_uci": "d5c6",
          "positional_response": "8. dxc6 bxc6 9. Qxc6+ Nd7 10. Ne5 Be6 11. Bxc4 Bxc4 12. Nxc4 e6 13. Nb5 Bb4+ 14. Bd2 Bxd2+ 15. Nxd2 O-O 16. Qxc7 Qxc7 17. Nxc7 Rac8 18. Nb5 Rc2 19. b3 a6 20. Nd4 Rc3 21. O-O Rd3 22. N2f3 e5 23. Nc6 f6 24. Rfd1 Rxd1+ 25. Rxd1 Nc5 26. Nd2 Kf7 27. f3 Rc8 28. Nb4 a5 29. Nd5 Nd3 30. Nc4 Nb2 31. Nxb2 Rc2 32. Nc4 Rxa2 33. Rd2 Ra1+ 34. Kf2 a4 35. bxa4 Rxa4 36. Nd6+ Kg6 37. Ne7+ Kh6 38. Ndf5+ Kg5 39. h4+ Kh5 40. g4# Nxe7 41. Nxe7 Kxh4 42. Nf5+ Kg5 43. Nxg7 Kg6 44. Nf5 h5 45. gxh5+ Kxh5 46. Rd6 Kg5 47. Kg3 Ra1 48. Rd8 Rg1+ 49. Kf2 Ra1 50. Rg8+ Kh5 51. Ng3+ Kh6 52. Rf8 Kg7 53. Rc8 Ra2+ 54. Ke3 Ra3+ 55. Kf2 Ra2+ 56. Ne2 Kg6 57. Rc6 Kg5 58. Ke3 Ra3+ 59. Rc3 Ra1 60. Ng3 Re1+ 61. Kf2 Ra1 62. Rc8 Ra2+ 63. Ne2 Kg6 64. Rg8+ Kf7 65. Rg4 Ke6 66. f4 exf4 67. Rxf4 Ke5 68. Kf3 Ra3+ 69. Kg4 Ra2 70. Ng3 Ra8 71. Rf5+ Ke6 72. Kf4 Ra1 73. Nh5 Rf1+ 74. Kg4 Rg1+ 75. Ng3 Re1 76. Rf4 Ke5 77. Rf5+ Ke6 78. Kf4 Rg1 79. Nh5 Rf1+ 80. Kg4 Rg1+ 81. Ng3 Re1 82. Rf4 Ke5 83. Rf5+ Ke6 84. Kf4 Rg1 85. Nh5 Rf",
          "positional_tokens": {
            "prompt_tokens": 167,
            "completion_tokens": 640,
            "total_tokens": 807,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "bxc6",
            "Qxc6+",
            "Nd7",
            "Ne5",
            "Be6",
            "Bxc4",
            "Bxc4",
            "Nxc4",
            "e6",
            "Nb5",
            "Bb4+",
            "Bd2",
            "Bxd2+",
            "Nxd2",
            "O-O",
            "Qxc7",
            "Qxc7",
            "Nxc7",
            "Rac8",
            "Nb5",
            "Rc2",
            "b3",
            "a6",
            "Nd4",
            "Rc3",
            "O-O",
            "Rd3",
            "N2f3",
            "e5",
            "Nc6",
            "f6",
            "Rfd1",
            "Rxd1+",
            "Rxd1",
            "Nc5",
            "Nd2",
            "Kf7",
            "f3",
            "Rc8",
            "Nb4",
            "a5",
            "Nd5",
            "Nd3",
            "Nc4",
            "Nb2",
            "Nxb2",
            "Rc2",
            "Nc4",
            "Rxa2",
            "Rd2",
            "Ra1+",
            "Kf2",
            "a4",
            "bxa4",
            "Rxa4",
            "Nd6+",
            "Kg6",
            "Ne7+",
            "Kh6",
            "Ndf5+",
            "Kg5",
            "h4+",
            "Kh5",
            "g4#",
            "Nxe7",
            "Nxe7",
            "Kxh4",
            "Nf5+",
            "Kg5",
            "Nxg7",
            "Kg6",
            "Nf5",
            "h5",
            "gxh5+",
            "Kxh5",
            "Rd6",
            "Kg5",
            "Kg3",
            "Ra1",
            "Rd8",
            "Rg1+",
            "Kf2",
            "Ra1",
            "Rg8+",
            "Kh5",
            "Ng3+",
            "Kh6",
            "Rf8",
            "Kg7",
            "Rc8",
            "Ra2+",
            "Ke3",
            "Ra3+",
            "Kf2",
            "Ra2+",
            "Ne2",
            "Kg6",
            "Rc6",
            "Kg5",
            "Ke3",
            "Ra3+",
            "Rc3",
            "Ra1",
            "Ng3",
            "Re1+",
            "Kf2",
            "Ra1",
            "Rc8",
            "Ra2+",
            "Ne2",
            "Kg6",
            "Rg8+",
            "Kf7",
            "Rg4",
            "Ke6",
            "f4",
            "exf4",
            "Rxf4",
            "Ke5",
            "Kf3",
            "Ra3+",
            "Kg4",
            "Ra2",
            "Ng3",
            "Ra8",
            "Rf5+",
            "Ke6",
            "Kf4",
            "Ra1",
            "Nh5",
            "Rf1+",
            "Kg4",
            "Rg1+",
            "Ng3",
            "Re1",
            "Rf4",
            "Ke5",
            "Rf5+",
            "Ke6",
            "Kf4",
            "Rg1",
            "Nh5",
            "Rf1+",
            "Kg4",
            "Rg1+",
            "Ng3",
            "Re1",
            "Rf4",
            "Ke5",
            "Rf5+",
            "Ke6",
            "Kf4",
            "Rg1",
            "Nh5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "dxc6",
          "neutral_uci": "d5c6",
          "neutral_response": "8. dxc6 bxc6 9. Qxc6+ Bd7 10. Qxc4 e6 11. Bg5 Be7 12. Rd1 O-O 13. Ne5 Bd6 14. Nxd7 Qxd7 15. e5 Nd5 16. exd6 cxd6 17. Nxd5 exd5 18. Qxd5 Rfe8+ 19. Be2 Qg4 20. Be3 Qb4+ 21. Qd2 Qb7 22. O-O Rad8 23. Bf3 Qa6 24. a3 Re5 25. Bd4 Rf5 26. Be4 Rh5 27. Qf4 d5 28. Qg4 Qh6 29. Be3 g5 30. Rxd5 Rxd5 31. Bxd5 Rxh2 32. Qxg5+ Qxg5 33. Bxg5 Rh5 34. f4 h6 35. Bf3 hxg5 36. Bxh5 gxf4 37. Rxf4 Kg7 38. Rxf7+ Kh6 39. g4 Kg5 40. Rxa7 Kh4 41. b4 Kg3 42. b5 Kf4 43. b6 Ke5 44. b7 Kd6 45. b8=Q+ Kc6 46. Rc7+ Kd6 47. Qd8+ Ke6 48. Re7+ Kf6 49. Qf8+ Kg5 50. Rg7+ Kh4 51. Qf2+ Kh3 52. Qh2# 1-0 8. dxc6 bxc6 9. Qxc6+ Bd7 10. Qxc4 e6",
          "neutral_tokens": {
            "prompt_tokens": 166,
            "completion_tokens": 409,
            "total_tokens": 575,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "bxc6",
            "Qxc6+",
            "Bd7",
            "Qxc4",
            "e6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d5c6",
          "positional_uci": "d5c6",
          "neutral_uci": "d5c6",
          "consensus_move": "d5c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "bxc6",
              "Qxc6+",
              "Nd7",
              "Ne5",
              "Be6",
              "Bxc4",
              "Bxc4",
              "Nxc4",
              "e6",
              "Nb5",
              "Bb4+",
              "Bd2",
              "Bxd2+",
              "Nxd2",
              "O-O",
              "Qxc7",
              "Qe7",
              "O-O",
              "Rfc8",
              "Qd6",
              "Qe8",
              "Nc7",
              "Rxc7",
              "Qxc7",
              "Rc8",
              "Qxa7",
              "Rc2",
              "Qd4",
              "e5",
              "Qd3",
              "Rxb2",
              "Rfb1",
              "Rxb1+",
              "Rxb1",
              "Nc5",
              "Qb5",
              "Qxb5",
              "Rxb5",
              "Nd7",
              "Rd5",
              "Nf8",
              "Rxe5",
              "f6",
              "Re7",
              "Ng6",
              "Rb7",
              "Ne5",
              "a4",
              "Nc6",
              "Nc4",
              "Nd4",
              "a5",
              "Nc6",
              "a6",
              "Kf8",
              "a7",
              "Nxa7",
              "Rxa7",
              "h6",
              "Nd6",
              "Kg8",
              "Nf5",
              "g6",
              "Nxh6+",
              "Kf8",
              "Ng4",
              "f5",
              "exf5",
              "gxf5",
              "Nh6",
              "f4",
              "g3",
              "fxg3",
              "hxg3",
              "Ke8",
              "g4",
              "Kf8",
              "g5",
              "Ke8",
              "g6",
              "Kf8",
              "g7+",
              "Ke8",
              "g8=Q#",
              "e4",
              "f5",
              "g5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "bxc6",
              "Qxc6+",
              "Nd7",
              "Ne5",
              "Be6",
              "Bxc4",
              "Bxc4",
              "Nxc4",
              "e6",
              "Nb5",
              "Bb4+",
              "Bd2",
              "Bxd2+",
              "Nxd2",
              "O-O",
              "Qxc7",
              "Qxc7",
              "Nxc7",
              "Rac8",
              "Nb5",
              "Rc2",
              "b3",
              "a6",
              "Nd4",
              "Rc3",
              "O-O",
              "Rd3",
              "N2f3",
              "e5",
              "Nc6",
              "f6",
              "Rfd1",
              "Rxd1+",
              "Rxd1",
              "Nc5",
              "Nd2",
              "Kf7",
              "f3",
              "Rc8",
              "Nb4",
              "a5",
              "Nd5",
              "Nd3",
              "Nc4",
              "Nb2",
              "Nxb2",
              "Rc2",
              "Nc4",
              "Rxa2",
              "Rd2",
              "Ra1+",
              "Kf2",
              "a4",
              "bxa4",
              "Rxa4",
              "Nd6+",
              "Kg6",
              "Ne7+",
              "Kh6",
              "Ndf5+",
              "Kg5",
              "h4+",
              "Kh5",
              "g4#",
              "Nxe7",
              "Nxe7",
              "Kxh4",
              "Nf5+",
              "Kg5",
              "Nxg7",
              "Kg6",
              "Nf5",
              "h5",
              "gxh5+",
              "Kxh5",
              "Rd6",
              "Kg5",
              "Kg3",
              "Ra1",
              "Rd8",
              "Rg1+",
              "Kf2",
              "Ra1",
              "Rg8+",
              "Kh5",
              "Ng3+",
              "Kh6",
              "Rf8",
              "Kg7",
              "Rc8",
              "Ra2+",
              "Ke3",
              "Ra3+",
              "Kf2",
              "Ra2+",
              "Ne2",
              "Kg6",
              "Rc6",
              "Kg5",
              "Ke3",
              "Ra3+",
              "Rc3",
              "Ra1",
              "Ng3",
              "Re1+",
              "Kf2",
              "Ra1",
              "Rc8",
              "Ra2+",
              "Ne2",
              "Kg6",
              "Rg8+",
              "Kf7",
              "Rg4",
              "Ke6",
              "f4",
              "exf4",
              "Rxf4",
              "Ke5",
              "Kf3",
              "Ra3+",
              "Kg4",
              "Ra2",
              "Ng3",
              "Ra8",
              "Rf5+",
              "Ke6",
              "Kf4",
              "Ra1",
              "Nh5",
              "Rf1+",
              "Kg4",
              "Rg1+",
              "Ng3",
              "Re1",
              "Rf4",
              "Ke5",
              "Rf5+",
              "Ke6",
              "Kf4",
              "Rg1",
              "Nh5",
              "Rf1+",
              "Kg4",
              "Rg1+",
              "Ng3",
              "Re1",
              "Rf4",
              "Ke5",
              "Rf5+",
              "Ke6",
              "Kf4",
              "Rg1",
              "Nh5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "bxc6",
              "Qxc6+",
              "Bd7",
              "Qxc4",
              "e6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 167,
            "completion_tokens": 640,
            "total_tokens": 807,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 167,
            "completion_tokens": 640,
            "total_tokens": 807,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 166,
            "completion_tokens": 409,
            "total_tokens": 575,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 500,
          "total_completion_tokens": 1689,
          "total_tokens": 2189
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 167,
            "completion_tokens": 640,
            "total_tokens": 807,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 167,
            "completion_tokens": 640,
            "total_tokens": 807,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 166,
            "completion_tokens": 409,
            "total_tokens": 575,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/2n2n2/3P4/Q1p1P1b1/2N2N2/PP3PPP/R1B1KB1R w KQkq - 3 8"
    },
    {
      "turn": 17,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bxc4",
          "aggressive_uci": "f1c4",
          "aggressive_response": "9. Bxc4 Bxf3 10. gxf3 e6 11. Bg5 Be7 12. Rd1 Qc8 13. Ba6 Qb8 14. Bb7 O-O 15.\nBxa8 Qxa8 16. e5 Nd5 17. Nxd5 Bxg5 18. Nxc7 Qb8 19. Rd7 Rd8 20. Na6 Qxe5+ 21.\nKf1 Rxd7 22. cxd7 Qd6 23. Kg2 h6 24. Rd1 Qe7 25. d8=Q+ Qxd8 26. Rxd8+ Bxd8 27.\nQe8+ Kh7 28. Qxd8 Kg6 29. Qd3+ f5 30. Nc7 Kf6 31. Qd6 Kg5 32. Nxe6+ Kh5 33.\nNxg7+ Kg5 34. f4+ Kg4 35. Qg6+ Kxf4 36. Qxf5# e4 Qg4+ Kf3 Qg6+ Ke2 Qg4+ Kd2 Qg6+ Kc3 Qg4+ Kb4 Qg6+ Ka5 Qg4+ b5#",
          "aggressive_tokens": {
            "prompt_tokens": 175,
            "completion_tokens": 288,
            "total_tokens": 463,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bxf3",
            "gxf3",
            "e6",
            "Bg5",
            "Be7",
            "Rd1",
            "Qc8",
            "Ba6",
            "Qb8",
            "Bb7",
            "O-O",
            "Bxa8",
            "Qxa8",
            "e5",
            "Nd5",
            "Nxd5",
            "Bxg5",
            "Nxc7",
            "Qb8",
            "Rd7",
            "Rd8",
            "Na6",
            "Qxe5+",
            "Kf1",
            "Rxd7",
            "cxd7",
            "Qd6",
            "Kg2",
            "h6",
            "Rd1",
            "Qe7",
            "d8=Q+",
            "Qxd8",
            "Rxd8+",
            "Bxd8",
            "Qe8+",
            "Kh7",
            "Qxd8",
            "Kg6",
            "Qd3+",
            "f5",
            "Nc7",
            "Kf6",
            "Qd6",
            "Kg5",
            "Nxe6+",
            "Kh5",
            "Nxg7+",
            "Kg5",
            "f4+",
            "Kg4",
            "Qg6+",
            "Kxf4",
            "Qxf5#",
            "e4",
            "Qg4+",
            "Kf3",
            "Qg6+",
            "Ke2",
            "Qg4+",
            "Kd2",
            "Qg6+",
            "Kc3",
            "Qg4+",
            "Kb4",
            "Qg6+",
            "Ka5",
            "Qg4+",
            "b5#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bxc4",
          "positional_uci": "f1c4",
          "positional_response": "9. Bxc4 Bxf3 10. gxf3 e6 11. Bg5 Be7 12. Rd1 Qc8 13. Ba6 Qb8 14. Bb7 O-O 15.\nBxa8 Qxa8 16. e5 Nd5 17. Nxd5 Bxg5 18. Nxc7 Qb8 19. Rd7 Rd8 20. Na6 Qxe5+ 21.\nKf1 Rc8 22. c7 h6 23. Qc6 Qxb2 24. Qb7 Rf8 25. c8=Q Rxc8 26. Qxc8+ Kh7 27. Rxf7\nQxa2 28. Qb7 Qa1+ 29. Kg2 Qe5 30. Qxa7 Bf6 31. Qxb6 Qg5+ 32. Kf1 Qc1+ 33. Ke2\nQxh1 34. Qxe6 Qa1 35. Qf5+ Kg8 36. Qd5 Kh7 37. Nc5 Qb2+ 38. Kf1 Qc1+ 39. Kg2\nQg5+ 40. Qxg5 hxg5 41. Ne4 Kg6 42. Rb7 Kf5 43. Rb5+ Kg6 44. Kg3 Kh6 45. Kg4 Kg6\n46. Nxg5 Kh6 47. Nf7+ Kg6 48. Nh8+ Kh7 49. Rh5+ Kg8 50. Kf5 Kf8 51. Ng6+ Kf7\n52. Rh8 Be7 53. Nxe7 Kxe7 54. Kg6 Ke6 55. Kxg7 Kf5 56. h4 Kf4 57. h5 Kxf3 58.\nh6 Kxf2 59. h7 Kf3 60. Rf8+ Kg4 61. h8=Q Kg5 62. Qh6+ Kg4 63. Rf4+ Kg3 64.\nQh4+ Kg2 65. Rf2+ Kg1 66. Qh2# Qd4+ 68. Kg6 Qg4+ 69. Kf6 Qd4+ 70. Ke6 Qe4+ 71.\nKd6 Qd4+ 72. Kc6 Qc4+ 73. Kb6 Qb4+ 74. Kc7 Qc5+ 75. Kd7 Qd5+ 76. Qd6 Qb7+\n77. Ke6 Qe4+ 78. Qe5 Qc6+ 79. Kf5 Qd7+ 80. Kg5 Qd8+ 81. Kg4 Qg8+ 82",
          "positional_tokens": {
            "prompt_tokens": 175,
            "completion_tokens": 640,
            "total_tokens": 815,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bxf3",
            "gxf3",
            "e6",
            "Bg5",
            "Be7",
            "Rd1",
            "Qc8",
            "Ba6",
            "Qb8",
            "Bb7",
            "O-O",
            "Bxa8",
            "Qxa8",
            "e5",
            "Nd5",
            "Nxd5",
            "Bxg5",
            "Nxc7",
            "Qb8",
            "Rd7",
            "Rd8",
            "Na6",
            "Qxe5+",
            "Kf1",
            "Rc8",
            "c7",
            "h6",
            "Qc6",
            "Qxb2",
            "Qb7",
            "Rf8",
            "c8=Q",
            "Rxc8",
            "Qxc8+",
            "Kh7",
            "Rxf7",
            "Qxa2",
            "Qb7",
            "Qa1+",
            "Kg2",
            "Qe5",
            "Qxa7",
            "Bf6",
            "Qxb6",
            "Qg5+",
            "Kf1",
            "Qc1+",
            "Ke2",
            "Qxh1",
            "Qxe6",
            "Qa1",
            "Qf5+",
            "Kg8",
            "Qd5",
            "Kh7",
            "Nc5",
            "Qb2+",
            "Kf1",
            "Qc1+",
            "Kg2",
            "Qg5+",
            "Qxg5",
            "hxg5",
            "Ne4",
            "Kg6",
            "Rb7",
            "Kf5",
            "Rb5+",
            "Kg6",
            "Kg3",
            "Kh6",
            "Kg4",
            "Kg6",
            "Nxg5",
            "Kh6",
            "Nf7+",
            "Kg6",
            "Nh8+",
            "Kh7",
            "Rh5+",
            "Kg8",
            "Kf5",
            "Kf8",
            "Ng6+",
            "Kf7",
            "Rh8",
            "Be7",
            "Nxe7",
            "Kxe7",
            "Kg6",
            "Ke6",
            "Kxg7",
            "Kf5",
            "h4",
            "Kf4",
            "h5",
            "Kxf3",
            "h6",
            "Kxf2",
            "h7",
            "Kf3",
            "Rf8+",
            "Kg4",
            "h8=Q",
            "Kg5",
            "Qh6+",
            "Kg4",
            "Rf4+",
            "Kg3",
            "Qh4+",
            "Kg2",
            "Rf2+",
            "Kg1",
            "Qh2#",
            "Qd4+",
            "Kg6",
            "Qg4+",
            "Kf6",
            "Qd4+",
            "Ke6",
            "Qe4+",
            "Kd6",
            "Qd4+",
            "Kc6",
            "Qc4+",
            "Kb6",
            "Qb4+",
            "Kc7",
            "Qc5+",
            "Kd7",
            "Qd5+",
            "Qd6",
            "Qb7+",
            "Ke6",
            "Qe4+",
            "Qe5",
            "Qc6+",
            "Kf5",
            "Qd7+",
            "Kg5",
            "Qd8+",
            "Kg4",
            "Qg8+"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bxc4",
          "neutral_uci": "f1c4",
          "neutral_response": "9. Bxc4 e6 10. Ne5 Bd6 11. Nxg4 Nxg4 12. h3 Ne5 13. O-O O-O 14. Be3 Qh4 15.\nBe2 f5 16. exf5 Qxa4 17. Nxa4 exf5 18. Rac1 f4 19. Bd4 f3 20. Bxe5 fxe2 21.\nRfe1 Bxe5 22. Rxe2 Rae8 23. Rce1 Rf5 24. g4 Rg5 25. Kg2 Kf7 26. f4 Bxf4 27.\nRxe8 Ra5 28. R8e7+ Kf6 29. R1e6+ Kg5 30. Rxg7+ Kh4 31. Rxh7+ Kg5 32. Rh5# Bg5\n33. Nc3 Rc5 34. Ne4+ Kf4 35. Nxc5 bxc5 36. Rg6 Be7 37. Rf5+ Ke4 38. Re6+ Kd4\n39. Rxe7 c4 40. Rxc7 c3 41. bxc3+ Kxc3 42. Rxa7 Kb2 43. c7 Ka1 44. c8=Q Kb2\n45. Rb7+ Ka1 46. Qc1+ Kxa2 47. Ra5# Bb6 48. Rxb6# Bb4 49. Rxb4# Bb6 50.\nQb2# Bb4 51. Qxb4# Bb6 52. Qb2# Bb4 53. Qxb4# Bb6 54. Qb2# Bb4 55. Qxb4# Bb6\n56. Qb2# Bb4 57. Qxb4# Bb6 58. Qb2# Bb4 59. Qxb4# Bb6 60. Qb2# Bb4 61. Qxb4# Bb6\n62. Qb2# Bb4 63. Qxb4# Bb6 64. Qb2# Bb4 65. Qxb4# Bb6 66. Qb2# Bb4 67. Qxb4# Bb6\n68. Qb2# Bb4 69. Qxb4# Bb6 70. Qb2# Bb4 71. Qxb4# Bb6 72. Qb2# Bb4 73. Qxb4# Bb6\n74. Qb2# Bb4 75. Qxb4# Bb6 76. Qb2# Bb4 77. Qxb4# Bb6 78. Qb2# Bb4",
          "neutral_tokens": {
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "e6",
            "Ne5",
            "Bd6",
            "Nxg4",
            "Nxg4",
            "h3",
            "Ne5",
            "O-O",
            "O-O",
            "Be3",
            "Qh4",
            "Be2",
            "f5",
            "exf5",
            "Qxa4",
            "Nxa4",
            "exf5",
            "Rac1",
            "f4",
            "Bd4",
            "f3",
            "Bxe5",
            "fxe2",
            "Rfe1",
            "Bxe5",
            "Rxe2",
            "Rae8",
            "Rce1",
            "Rf5",
            "g4",
            "Rg5",
            "Kg2",
            "Kf7",
            "f4",
            "Bxf4",
            "Rxe8",
            "Ra5",
            "R8e7+",
            "Kf6",
            "R1e6+",
            "Kg5",
            "Rxg7+",
            "Kh4",
            "Rxh7+",
            "Kg5",
            "Rh5#",
            "Bg5",
            "Nc3",
            "Rc5",
            "Ne4+",
            "Kf4",
            "Nxc5",
            "bxc5",
            "Rg6",
            "Be7",
            "Rf5+",
            "Ke4",
            "Re6+",
            "Kd4",
            "Rxe7",
            "c4",
            "Rxc7",
            "c3",
            "bxc3+",
            "Kxc3",
            "Rxa7",
            "Kb2",
            "c7",
            "Ka1",
            "c8=Q",
            "Kb2",
            "Rb7+",
            "Ka1",
            "Qc1+",
            "Kxa2",
            "Ra5#",
            "Bb6",
            "Rxb6#",
            "Bb4",
            "Rxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f1c4",
          "positional_uci": "f1c4",
          "neutral_uci": "f1c4",
          "consensus_move": "f1c4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bxf3",
              "gxf3",
              "e6",
              "Bg5",
              "Be7",
              "Rd1",
              "Qc8",
              "Ba6",
              "Qb8",
              "Bb7",
              "O-O",
              "Bxa8",
              "Qxa8",
              "e5",
              "Nd5",
              "Nxd5",
              "Bxg5",
              "Nxc7",
              "Qb8",
              "Rd7",
              "Rd8",
              "Na6",
              "Qxe5+",
              "Kf1",
              "Rxd7",
              "cxd7",
              "Qd6",
              "Kg2",
              "h6",
              "Rd1",
              "Qe7",
              "d8=Q+",
              "Qxd8",
              "Rxd8+",
              "Bxd8",
              "Qe8+",
              "Kh7",
              "Qxd8",
              "Kg6",
              "Qd3+",
              "f5",
              "Nc7",
              "Kf6",
              "Qd6",
              "Kg5",
              "Nxe6+",
              "Kh5",
              "Nxg7+",
              "Kg5",
              "f4+",
              "Kg4",
              "Qg6+",
              "Kxf4",
              "Qxf5#",
              "e4",
              "Qg4+",
              "Kf3",
              "Qg6+",
              "Ke2",
              "Qg4+",
              "Kd2",
              "Qg6+",
              "Kc3",
              "Qg4+",
              "Kb4",
              "Qg6+",
              "Ka5",
              "Qg4+",
              "b5#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bxf3",
              "gxf3",
              "e6",
              "Bg5",
              "Be7",
              "Rd1",
              "Qc8",
              "Ba6",
              "Qb8",
              "Bb7",
              "O-O",
              "Bxa8",
              "Qxa8",
              "e5",
              "Nd5",
              "Nxd5",
              "Bxg5",
              "Nxc7",
              "Qb8",
              "Rd7",
              "Rd8",
              "Na6",
              "Qxe5+",
              "Kf1",
              "Rc8",
              "c7",
              "h6",
              "Qc6",
              "Qxb2",
              "Qb7",
              "Rf8",
              "c8=Q",
              "Rxc8",
              "Qxc8+",
              "Kh7",
              "Rxf7",
              "Qxa2",
              "Qb7",
              "Qa1+",
              "Kg2",
              "Qe5",
              "Qxa7",
              "Bf6",
              "Qxb6",
              "Qg5+",
              "Kf1",
              "Qc1+",
              "Ke2",
              "Qxh1",
              "Qxe6",
              "Qa1",
              "Qf5+",
              "Kg8",
              "Qd5",
              "Kh7",
              "Nc5",
              "Qb2+",
              "Kf1",
              "Qc1+",
              "Kg2",
              "Qg5+",
              "Qxg5",
              "hxg5",
              "Ne4",
              "Kg6",
              "Rb7",
              "Kf5",
              "Rb5+",
              "Kg6",
              "Kg3",
              "Kh6",
              "Kg4",
              "Kg6",
              "Nxg5",
              "Kh6",
              "Nf7+",
              "Kg6",
              "Nh8+",
              "Kh7",
              "Rh5+",
              "Kg8",
              "Kf5",
              "Kf8",
              "Ng6+",
              "Kf7",
              "Rh8",
              "Be7",
              "Nxe7",
              "Kxe7",
              "Kg6",
              "Ke6",
              "Kxg7",
              "Kf5",
              "h4",
              "Kf4",
              "h5",
              "Kxf3",
              "h6",
              "Kxf2",
              "h7",
              "Kf3",
              "Rf8+",
              "Kg4",
              "h8=Q",
              "Kg5",
              "Qh6+",
              "Kg4",
              "Rf4+",
              "Kg3",
              "Qh4+",
              "Kg2",
              "Rf2+",
              "Kg1",
              "Qh2#",
              "Qd4+",
              "Kg6",
              "Qg4+",
              "Kf6",
              "Qd4+",
              "Ke6",
              "Qe4+",
              "Kd6",
              "Qd4+",
              "Kc6",
              "Qc4+",
              "Kb6",
              "Qb4+",
              "Kc7",
              "Qc5+",
              "Kd7",
              "Qd5+",
              "Qd6",
              "Qb7+",
              "Ke6",
              "Qe4+",
              "Qe5",
              "Qc6+",
              "Kf5",
              "Qd7+",
              "Kg5",
              "Qd8+",
              "Kg4",
              "Qg8+"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e6",
              "Ne5",
              "Bd6",
              "Nxg4",
              "Nxg4",
              "h3",
              "Ne5",
              "O-O",
              "O-O",
              "Be3",
              "Qh4",
              "Be2",
              "f5",
              "exf5",
              "Qxa4",
              "Nxa4",
              "exf5",
              "Rac1",
              "f4",
              "Bd4",
              "f3",
              "Bxe5",
              "fxe2",
              "Rfe1",
              "Bxe5",
              "Rxe2",
              "Rae8",
              "Rce1",
              "Rf5",
              "g4",
              "Rg5",
              "Kg2",
              "Kf7",
              "f4",
              "Bxf4",
              "Rxe8",
              "Ra5",
              "R8e7+",
              "Kf6",
              "R1e6+",
              "Kg5",
              "Rxg7+",
              "Kh4",
              "Rxh7+",
              "Kg5",
              "Rh5#",
              "Bg5",
              "Nc3",
              "Rc5",
              "Ne4+",
              "Kf4",
              "Nxc5",
              "bxc5",
              "Rg6",
              "Be7",
              "Rf5+",
              "Ke4",
              "Re6+",
              "Kd4",
              "Rxe7",
              "c4",
              "Rxc7",
              "c3",
              "bxc3+",
              "Kxc3",
              "Rxa7",
              "Kb2",
              "c7",
              "Ka1",
              "c8=Q",
              "Kb2",
              "Rb7+",
              "Ka1",
              "Qc1+",
              "Kxa2",
              "Ra5#",
              "Bb6",
              "Rxb6#",
              "Bb4",
              "Rxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 175,
            "completion_tokens": 288,
            "total_tokens": 463,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 175,
            "completion_tokens": 640,
            "total_tokens": 815,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 524,
          "total_completion_tokens": 1568,
          "total_tokens": 2092
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 175,
            "completion_tokens": 288,
            "total_tokens": 463,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 175,
            "completion_tokens": 640,
            "total_tokens": 815,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r2qkb1r/p1p1pppp/1pP2n2/8/Q1p1P1b1/2N2N2/PP3PPP/R1B1KB1R w KQkq - 0 9"
    },
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e5",
          "aggressive_uci": "e7e5",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6\n4. Ba4 Nf6 5. O-O Be7 6. Re1 b5\n7. Bb3 d6 8. c3 O-O 9. h3 Na5\n10. Bc2 c5 11. d4 Qc7 12. Nbd2 cxd4\n13. cxd4 Nc6 14. Nb3 a5 15. Be3 a4\n16. Nbd2 Bd7 17. Rc1 Qb7 18. Bb1 Rfc8\n19. Nf1 Bd8 20. Ng3 Bb6 21. Qd2 Ba5\n22. Qe2 Bxe1 23. Rxe1 h6 24. Qd2 Qa6\n25. Bxh6 gxh6 26. Qxh6 Nh7 27. Nh5 1-0\n27. Nh7 Nxd4 28. Nf6+ Kh8",
          "aggressive_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 236,
            "total_tokens": 350,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Qc7",
            "Nbd2",
            "cxd4",
            "cxd4",
            "Nc6",
            "Nb3",
            "a5",
            "Be3",
            "a4",
            "Nbd2",
            "Bd7",
            "Rc1",
            "Qb7",
            "Bb1",
            "Rfc8",
            "Nf1",
            "Bd8",
            "Ng3",
            "Bb6",
            "Qd2",
            "Ba5",
            "Qe2",
            "Bxe1",
            "Rxe1",
            "h6",
            "Qd2",
            "Qa6",
            "Bxh6",
            "gxh6",
            "Qxh6",
            "Nh7",
            "Nh5",
            "Nh7",
            "Nxd4",
            "Nf6+",
            "Kh8"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e5",
          "positional_uci": "e7e5",
          "positional_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7\n6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5\n11. d4 Qc7 12. Nbd2 cxd4 13. cxd4 Nc6 14. Nb3 a5\n15. Be3 a4 16. Nbd2 Bd7 17. Rc1 Qb7 18. Bb1 Rfc8\n19. Nf1 Bd8 20. Ng3 Bb6 21. Qd2 Ba5 22. Qe2 Bxe1\n23. Rxe1 Re8 24. Qd2 Rac8 25. Bg5 Kh8 26. Bxf6 gxf6\n27. Qh6 Rg8 28. Qxf6+ Rg7 29. Nh5 Rg8 30. Ng5 Be8\n31. Nxg7 Rxg7 32. dxe5 dxe5 33. Rd1 Qe7 34. Qxe7 Nxe7\n35. Rd8 Rg8 36. Nf3 f6 37. Bd3 Kg7 38. Rb8 Kf7\n39. Bxb5 Bxb5 40. Rxb5 Rc8 41. Rb4 Rc1+ 42. Kh2 Rc2\n43. Kg3 Ng6 44. Ne1 Re2 45. Nd3 Rd2 46. Nc5 Nf4\n47. Nxa4 Nd3 48. Rb7+ Kg6 49. f3 Nf4 50. Kh2 Rxg2+\n51. Kh1 Rf2 52. Nc3 Rxf3 53. a4 Rxh3+ 54. Kg1 Rd3\n55. a5 Rd6 56. Rb6 Rd7 57. a6 Ra7 58. Nb5 Ra8\n59. a7 Nd3 60. Rb8 Rxa7 61. Nxa7 Nc5 62. b4 Nxe4\n63. b5 Nd6 64. b6 Kf5 65. Rd8 Nb7 66. Rd7 Nc5\n67. Rxh7 Ke6 68. b7 Nxb7 69. Rxb7 Kd5 70. Kf2 f5\n71. Rb5+ Ke4 72. Nc6 Kf4 73. Rxe5 Kg4 74. Nd4 f4\n75. Rf5 Kh4 76. Kf3 Kh3 77. Rh5# e5 d5 Nf6",
          "positional_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 622,
            "total_tokens": 736,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d5",
            "Nf6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e5",
          "neutral_uci": "e7e5",
          "neutral_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6\n4. Ba4 Nf6 5. O-O Be7 6. Re1 b5\n7. Bb3 d6 8. c3 O-O 9. h3 Bb7\n10. d4 Re8 11. Nbd2 Bf8 12. a4 h6\n13. Bc2 exd4 14. cxd4 Nb4 15. Bb1 c5\n16. d5 Nd7 17. Ra3 c4 18. Nd4 Qf6\n19. N2f3 Nc5 20. axb5 Nbd3 21. Bxd3 Nxd3\n22. Re2 axb5 23. Nxb5 Rxa3 24. Nxa3 Ba6\n25. Qa4 Ra8 26. Be3 Nxb2\n27. Qc6 Rc8 28. Qxa6 c3 29. Qxc8 Nd3",
          "neutral_tokens": {
            "prompt_tokens": 113,
            "completion_tokens": 230,
            "total_tokens": 343,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Bb7",
            "d4",
            "Re8",
            "Nbd2",
            "Bf8",
            "a4",
            "h6",
            "Bc2",
            "exd4",
            "cxd4",
            "Nb4",
            "Bb1",
            "c5",
            "d5",
            "Nd7",
            "Ra3",
            "c4",
            "Nd4",
            "Qf6",
            "N2f3",
            "Nc5",
            "axb5",
            "Nbd3",
            "Bxd3",
            "Nxd3",
            "Re2",
            "axb5",
            "Nxb5",
            "Rxa3",
            "Nxa3",
            "Ba6",
            "Qa4",
            "Ra8",
            "Be3",
            "Nxb2",
            "Qc6",
            "Rc8",
            "Qxa6",
            "c3",
            "Qxc8",
            "Nd3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e7e5",
          "positional_uci": "e7e5",
          "neutral_uci": "e7e5",
          "consensus_move": "e7e5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Qc7",
              "Nbd2",
              "cxd4",
              "cxd4",
              "Nc6",
              "Nb3",
              "a5",
              "Be3",
              "a4",
              "Nbd2",
              "Bd7",
              "Rc1",
              "Qb7",
              "Bb1",
              "Rfc8",
              "Nf1",
              "Bd8",
              "Ng3",
              "Bb6",
              "Qd2",
              "Ba5",
              "Qe2",
              "Bxe1",
              "Rxe1",
              "h6",
              "Qd2",
              "Qa6",
              "Bxh6",
              "gxh6",
              "Qxh6",
              "Nh7",
              "Nh5",
              "Nh7",
              "Nxd4",
              "Nf6+",
              "Kh8"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "d5",
              "Nf6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Bb7",
              "d4",
              "Re8",
              "Nbd2",
              "Bf8",
              "a4",
              "h6",
              "Bc2",
              "exd4",
              "cxd4",
              "Nb4",
              "Bb1",
              "c5",
              "d5",
              "Nd7",
              "Ra3",
              "c4",
              "Nd4",
              "Qf6",
              "N2f3",
              "Nc5",
              "axb5",
              "Nbd3",
              "Bxd3",
              "Nxd3",
              "Re2",
              "axb5",
              "Nxb5",
              "Rxa3",
              "Nxa3",
              "Ba6",
              "Qa4",
              "Ra8",
              "Be3",
              "Nxb2",
              "Qc6",
              "Rc8",
              "Qxa6",
              "c3",
              "Qxc8",
              "Nd3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 114,
            "completion_tokens": 236,
            "total_tokens": 350,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 114,
            "completion_tokens": 622,
            "total_tokens": 736,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 113,
            "completion_tokens": 230,
            "total_tokens": 343,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 341,
          "total_completion_tokens": 1088,
          "total_tokens": 1429
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 114,
            "completion_tokens": 236,
            "total_tokens": 350,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 114,
            "completion_tokens": 622,
            "total_tokens": 736,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 113,
            "completion_tokens": 230,
            "total_tokens": 343,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": "d2d4",
          "aggressive_response": "d4 d5 2. Nf3 Nf6 3. Bg5 Bf5 4. e3 e6 5. Bd3 Bxd3 6. Qxd3 Be7 7. O-O O-O 8. Nbd2 Nbd7 9. c4 c6 10. Rac1 Rc8 11. cxd5 exd5 12. a3 Re8 13. b4 a6 14. Nb3 Ne4 15. Bxe7 Qxe7 16. Nc5 Ndxc5 17. bxc5 f5 18. Rb1 Rc7 19. Rb6 Qf6 20. Rfb1 Ree7 21. Ne5 Qh4 22. g3 Qh3 23. Nxc6 bxc6 24. Rb8+ Kf7 25. Qxa6 Nd2 26. Qe2 Nxb1 27. Rxb1 Rb7 28. Ra1 Qg4 29. Qc2 Qe4 30. Qd1 Rb2 31. a4 Reb7 32. Qh5+ Kf8 33. Rf1 Rb1 34. Qe2 Rxf1+ 35. Qxf1 Rb1 36. a5 Rxf1+ 37. Kxf1 Qb1+ 38. Kg2 Qa2 39. Kf3 Qxa5 40. Kf4 g6 41. Ke5 Ke7 42. f4 Qc7# d4 d5 2. Nf3 Nf6 3. Bg5 Bf5 4. e3 e6 5. Bd3 Bxd3 6. Qxd3 Be7 7. O-O O-O 8. Nbd2 Nbd7 9. c4 c6 10. Rac1 Rc8 11. cxd5 exd5 12. a3 Re8 13. b4 a6 14. Nb3 Ne4 15. Bxe7 Qxe7 16. Nc5 Ndxc5 17. bxc5 f5 18. Rb1 Rc7 19. Rb6 Qf6 20. Rfb1 Ree7 21. Ne5 Qh4 22. g3 Qh3 23. Nxc6 bxc6 24. Rb8+ Kf7 25. Qxa6 Nd2 26. Qe2 Nxb1 27. Rxb1 Rb7 28. Ra1 Qg4 29. Qc2 Qe4 30. Qd1 Rb2 31. a4 Reb7 32. Qh5+ Kf8 33. Rf1 Rb1 34. Qe2 Rxf1+ 35. Qxf1 Rb1 36. a",
          "aggressive_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Nf3",
            "Nf6",
            "Bg5",
            "Bf5",
            "e3",
            "e6",
            "Bd3",
            "Bxd3",
            "Qxd3",
            "Be7",
            "O-O",
            "O-O",
            "Nbd2",
            "Nbd7",
            "c4",
            "c6",
            "Rac1",
            "Rc8",
            "cxd5",
            "exd5",
            "a3",
            "Re8",
            "b4",
            "a6",
            "Nb3",
            "Ne4",
            "Bxe7",
            "Qxe7",
            "Nc5",
            "Ndxc5",
            "bxc5",
            "f5",
            "Rb1",
            "Rc7",
            "Rb6",
            "Qf6",
            "Rfb1",
            "Ree7",
            "Ne5",
            "Qh4",
            "g3",
            "Qh3",
            "Nxc6",
            "bxc6",
            "Rb8+",
            "Kf7",
            "Qxa6",
            "Nd2",
            "Qe2",
            "Nxb1",
            "Rxb1",
            "Rb7",
            "Ra1",
            "Qg4",
            "Qc2",
            "Qe4",
            "Qd1",
            "Rb2",
            "a4",
            "Reb7",
            "Qh5+",
            "Kf8",
            "Rf1",
            "Rb1",
            "Qe2",
            "Rxf1+",
            "Qxf1",
            "Rb1"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 Bc5 4. O-O Nge7 5. Nxe5 Nxe5 6. d4 Bxd4 7. Qxd4 N7c6 8. Bxc6 Nxc6 9. Qxg7 Rf8 10. Bh6 Qe7 11. Nc3 d6 12. Nd5 Qe5 13. Qxf8+ Kd7 14. Qxf7+ Kd8 15. Qxc7+ Ke8 16. Qxh7 Kd8 17. Qg8+ Kd7 18. Qf7+ Kd8 19. Bg7 Qe8 20. Bf6+ Ne7 21. Bxe7+ Kd7 22. Nf6+ Kc6 23. Qxe8+ Kb6 24. Nd5+ Ka6 25. Qa4# 1-0 e4 Nc6 Nf3 Nf6 d4 d5 Bg5 Be7 Bxf6 Bxf6 Nxd5 Nxd5 Qxd5 O-O Nxe5 Bxe5 Qxe5 Nc6 Qe7 Qf3 d6 Re1 Be6 Bxc6+ bxc6 Qxc6+ Qd7 Qxa8+ Ke7 Qxh8 Qc6 Qxg7 Qxc2 Bg5+ Kd7 Qf6 Qxb2 Qxb2 Kc6 Qc3+ Kd7 Qd4 Kc6 Rac1+ Kd7 Qa4# 1-0 e4 e5 Nf3 Nc6 Bb5 Bc5 O-O Nge7 Nxe5 Nxe5 d4 Bxd4 Qxd4 N7c6 Bxc6 Nxc6 Qxg7 Rf8 Bh6 Qe7 Nc3 d6 Nd5 Qe5 Qxf8+ Kd7 Qxf7+ Kd8 Qxc7+ Ke8 Qxh7 Kd8 Qg8+ Kd7 Qf7+ Kd8 Bg7 Qe5 Bxe5 Nxe5 Qe7#",
          "positional_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 492,
            "total_tokens": 602,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bb5",
            "Bc5",
            "O-O",
            "Nge7",
            "Nxe5",
            "Nxe5",
            "d4",
            "Bxd4",
            "Qxd4",
            "N7c6",
            "Bxc6",
            "Nxc6",
            "Qxg7",
            "Rf8",
            "Bh6",
            "Qe7",
            "Nc3",
            "d6",
            "Nd5",
            "Qe5",
            "Qxf8+",
            "Kd7",
            "Qxf7+",
            "Kd8",
            "Qxc7+",
            "Ke8",
            "Qxh7",
            "Kd8",
            "Qg8+",
            "Kd7",
            "Qf7+",
            "Kd8",
            "Bg7",
            "Qe5",
            "Bxe5",
            "Nxe5",
            "Qe7#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": "d2d4",
          "neutral_response": "d4 d5 2. Nf3 Nf6 3. Bg5 Bf5 4. e3 e6\n5. Bd3 Bxd3 6. Qxd3 Be7 7. O-O O-O 8. Nbd2 Nbd7\n9. c4 c6 10. Rac1 Rc8 11. cxd5 exd5 12. a3 Re8\n13. b4 Ne4 14. Bxe7 Qxe7 15. Nb3 f5 16. Nc5 Ndxc5\n17. bxc5 g5 18. Ne5 g4 19. f3 gxf3 20. Rxf3 Rf8\n21. Rcf1 Qe6 22. Rf4 Rc7 23. Qe2 Rg7 24. Qh5 Rg5\n25. Qh3 Rf6 26. Nf3 Rh6 27. Nh4 Rgh5 28. g3 Ng5\n29. Qg2 Qxe3+ 30. Kh1 Qe6 31. Rxf5 Qe4 32. Rf8+ Kg7\n33. Qxe4 Nxe4 34. R1f7# 1-0\nd4 d5 2. Nf3 Nf6 3. Bg5 Bf5 4. e3 e6 5. Bd3 Bxd3 6. Qxd3 Be7 7. O-O O-O 8. Nbd2 Nbd7 9. c4 c6 10. Rac1 Rc8 11. cxd5 exd5 12. a3 Re8 13. b4 Ne4 14. Bxe7 Qxe7 15. Nb3 f5 16. Nc5 Ndxc5 17. bxc5 g5 18. Ne5 g4 19. f3 gxf3 20. Rxf3 Rf8 21. Rcf1 Qe6 22. Rf4 Rc7 23. Qe2 Rg7 24. Qh5 Rg5 25. Qh3 Rf6 26. Nf3 Rh6 27. Nh4 Rgh5 28. g3 Ng5 29. Qg2 Qxe3+ 30. Kh1 Qe6 31. Rxf5 Qe4 32. Rf8+ Kg7 33. Qxe4 Nxe4 34. R1f7# 1-0 Nf6 Nxe4 Nxe4",
          "neutral_tokens": {
            "prompt_tokens": 109,
            "completion_tokens": 568,
            "total_tokens": 677,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d5",
            "Nf3",
            "Nf6",
            "Bg5",
            "Bf5",
            "e3",
            "e6",
            "Bd3",
            "Bxd3",
            "Qxd3",
            "Be7",
            "O-O",
            "O-O",
            "Nbd2",
            "Nbd7",
            "c4",
            "c6",
            "Rac1",
            "Rc8",
            "cxd5",
            "exd5",
            "a3",
            "Re8",
            "b4",
            "Ne4",
            "Bxe7",
            "Qxe7",
            "Nb3",
            "f5",
            "Nc5",
            "Ndxc5",
            "bxc5",
            "g5",
            "Ne5",
            "g4",
            "f3",
            "gxf3",
            "Rxf3",
            "Rf8",
            "Rcf1",
            "Qe6",
            "Rf4",
            "Rc7",
            "Qe2",
            "Rg7",
            "Qh5",
            "Rg5",
            "Qh3",
            "Rf6",
            "Nf3",
            "Rh6",
            "Nh4",
            "Rgh5",
            "g3",
            "Ng5",
            "Qg2",
            "Qxe3+",
            "Kh1",
            "Qe6",
            "Rxf5",
            "Qe4",
            "Rf8+",
            "Kg7",
            "Qxe4",
            "Nxe4",
            "R1f7#",
            "Nf6",
            "Nxe4",
            "Nxe4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d2d4",
          "positional_uci": "e2e4",
          "neutral_uci": "d2d4",
          "consensus_move": "d2d4",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d5",
              "Nf3",
              "Nf6",
              "Bg5",
              "Bf5",
              "e3",
              "e6",
              "Bd3",
              "Bxd3",
              "Qxd3",
              "Be7",
              "O-O",
              "O-O",
              "Nbd2",
              "Nbd7",
              "c4",
              "c6",
              "Rac1",
              "Rc8",
              "cxd5",
              "exd5",
              "a3",
              "Re8",
              "b4",
              "a6",
              "Nb3",
              "Ne4",
              "Bxe7",
              "Qxe7",
              "Nc5",
              "Ndxc5",
              "bxc5",
              "f5",
              "Rb1",
              "Rc7",
              "Rb6",
              "Qf6",
              "Rfb1",
              "Ree7",
              "Ne5",
              "Qh4",
              "g3",
              "Qh3",
              "Nxc6",
              "bxc6",
              "Rb8+",
              "Kf7",
              "Qxa6",
              "Nd2",
              "Qe2",
              "Nxb1",
              "Rxb1",
              "Rb7",
              "Ra1",
              "Qg4",
              "Qc2",
              "Qe4",
              "Qd1",
              "Rb2",
              "a4",
              "Reb7",
              "Qh5+",
              "Kf8",
              "Rf1",
              "Rb1",
              "Qe2",
              "Rxf1+",
              "Qxf1",
              "Rb1"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bb5",
              "Bc5",
              "O-O",
              "Nge7",
              "Nxe5",
              "Nxe5",
              "d4",
              "Bxd4",
              "Qxd4",
              "N7c6",
              "Bxc6",
              "Nxc6",
              "Qxg7",
              "Rf8",
              "Bh6",
              "Qe7",
              "Nc3",
              "d6",
              "Nd5",
              "Qe5",
              "Qxf8+",
              "Kd7",
              "Qxf7+",
              "Kd8",
              "Qxc7+",
              "Ke8",
              "Qxh7",
              "Kd8",
              "Qg8+",
              "Kd7",
              "Qf7+",
              "Kd8",
              "Bg7",
              "Qe5",
              "Bxe5",
              "Nxe5",
              "Qe7#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "d5",
              "Nf3",
              "Nf6",
              "Bg5",
              "Bf5",
              "e3",
              "e6",
              "Bd3",
              "Bxd3",
              "Qxd3",
              "Be7",
              "O-O",
              "O-O",
              "Nbd2",
              "Nbd7",
              "c4",
              "c6",
              "Rac1",
              "Rc8",
              "cxd5",
              "exd5",
              "a3",
              "Re8",
              "b4",
              "Ne4",
              "Bxe7",
              "Qxe7",
              "Nb3",
              "f5",
              "Nc5",
              "Ndxc5",
              "bxc5",
              "g5",
              "Ne5",
              "g4",
              "f3",
              "gxf3",
              "Rxf3",
              "Rf8",
              "Rcf1",
              "Qe6",
              "Rf4",
              "Rc7",
              "Qe2",
              "Rg7",
              "Qh5",
              "Rg5",
              "Qh3",
              "Rf6",
              "Nf3",
              "Rh6",
              "Nh4",
              "Rgh5",
              "g3",
              "Ng5",
              "Qg2",
              "Qxe3+",
              "Kh1",
              "Qe6",
              "Rxf5",
              "Qe4",
              "Rf8+",
              "Kg7",
              "Qxe4",
              "Nxe4",
              "R1f7#",
              "Nf6",
              "Nxe4",
              "Nxe4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 110,
            "completion_tokens": 492,
            "total_tokens": 602,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 109,
            "completion_tokens": 568,
            "total_tokens": 677,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 329,
          "total_completion_tokens": 1700,
          "total_tokens": 2029
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 110,
            "completion_tokens": 492,
            "total_tokens": 602,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 109,
            "completion_tokens": 568,
            "total_tokens": 677,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 3,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "c4",
          "aggressive_uci": "c2c4",
          "aggressive_response": "2. c4 c6 3. Nf3 Nf6 4. Nc3 dxc4 5. a4 Bf5 6. e3 e6 7. Bxc4 Bb4 8. O-O O-O 9. Qe2 Bg6 10. Ne5 Nbd7 11. Nxg6 hxg6 12. Rd1 Qa5 13. Bd2 e5 14. dxe5 Nxe5 15. Bb3 Rad8 16. Be1 Rxd1 17. Rxd1 Rd8 18. Rxd8+ Qxd8 19. Qd1 Qxd1 20. Bxd1 Nd3 21. Kf1 Nxb2 22. Bb3 Nd3 23. Bd2 Nc5 24. Bc2 Bxc3 25. Bxc3 Nd5 26. Bd4 b6 27. a5 Kf8 28. axb6 axb6 29. Ke2 Ne6 30. Bb2 Ke7 31. g3 Kd6 32. e4 Nb4 33. Bb3 c5 34. f4 b5 35. f5 gxf5 36. exf5 Nd4+ 37. Bxd4 cxd4 38. Bxf7 Ke5 39. Be8 Kxf5 40. Bxb5 Kg4 41. Bd7+ Kg5 42. h4+ Kh5 43. Kf3 d3 44. Bg4+ Kh6 45. Ke3 Kg6 46. Bf3 Kf5 47. g4+ Ke5 48. Be4 d2 49. Bf3 Nc2+ 50. Kxd2 Nd4 51. Ke3 Nxf3 52. Kxf3 Kf6 53. Kf4 Kg6 54. Ke5 Kh6 55. Kf5 Kh7 56. Kg5 Kh8 57. Kg6 Kg8 58. h5 Kh8 59. Kf7 Kh7 60. g5 Kh8 61. h6 gxh6 62. g6 h5 63. g7+ Kh7 64. g8=Q+ Kh6 65. Qg6# Nf3 Nf4 Nf5",
          "aggressive_tokens": {
            "prompt_tokens": 116,
            "completion_tokens": 525,
            "total_tokens": 641,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "c6",
            "Nf3",
            "Nf6",
            "Nc3",
            "dxc4",
            "a4",
            "Bf5",
            "e3",
            "e6",
            "Bxc4",
            "Bb4",
            "O-O",
            "O-O",
            "Qe2",
            "Bg6",
            "Ne5",
            "Nbd7",
            "Nxg6",
            "hxg6",
            "Rd1",
            "Qa5",
            "Bd2",
            "e5",
            "dxe5",
            "Nxe5",
            "Bb3",
            "Rad8",
            "Be1",
            "Rxd1",
            "Rxd1",
            "Rd8",
            "Rxd8+",
            "Qxd8",
            "Qd1",
            "Qxd1",
            "Bxd1",
            "Nd3",
            "Kf1",
            "Nxb2",
            "Bb3",
            "Nd3",
            "Bd2",
            "Nc5",
            "Bc2",
            "Bxc3",
            "Bxc3",
            "Nd5",
            "Bd4",
            "b6",
            "a5",
            "Kf8",
            "axb6",
            "axb6",
            "Ke2",
            "Ne6",
            "Bb2",
            "Ke7",
            "g3",
            "Kd6",
            "e4",
            "Nb4",
            "Bb3",
            "c5",
            "f4",
            "b5",
            "f5",
            "gxf5",
            "exf5",
            "Nd4+",
            "Bxd4",
            "cxd4",
            "Bxf7",
            "Ke5",
            "Be8",
            "Kxf5",
            "Bxb5",
            "Kg4",
            "Bd7+",
            "Kg5",
            "h4+",
            "Kh5",
            "Kf3",
            "d3",
            "Bg4+",
            "Kh6",
            "Ke3",
            "Kg6",
            "Bf3",
            "Kf5",
            "g4+",
            "Ke5",
            "Be4",
            "d2",
            "Bf3",
            "Nc2+",
            "Kxd2",
            "Nd4",
            "Ke3",
            "Nxf3",
            "Kxf3",
            "Kf6",
            "Kf4",
            "Kg6",
            "Ke5",
            "Kh6",
            "Kf5",
            "Kh7",
            "Kg5",
            "Kh8",
            "Kg6",
            "Kg8",
            "h5",
            "Kh8",
            "Kf7",
            "Kh7",
            "g5",
            "Kh8",
            "h6",
            "gxh6",
            "g6",
            "h5",
            "g7+",
            "Kh7",
            "g8=Q+",
            "Kh6",
            "Qg6#",
            "Nf3",
            "Nf4",
            "Nf5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf3",
          "positional_uci": "g1f3",
          "positional_response": "2. Nf3 Nf6 3. c4 c6 4. Nc3 dxc4 5. a4 Bf5 6. e3 e6 7. Bxc4 Bb4 8. O-O O-O 9. Qe2 Nbd7 10. e4 Bg6 11. Bd3 Bh5 12. e5 Nd5 13. Nxd5 cxd5 14. Qe3 Bg6 15. Bxg6 hxg6 16. Ng5 Re8 17. Qh3 Nf8 18. Qf3 Qd7 19. Qb3 Be7 20. Bd2 Rec8 21. Rfc1 Rxc1+ 22. Rxc1 Rc8 23. Rxc8 Qxc8 24. Nf3 Nd7 25. Qc3 Qxc3 26. Bxc3 Nb6 27. b3 Kf8 28. Kf1 Ke8 29. Ke2 Kd7 30. Kd3 Kc6 31. h4 Nd7 32. g3 b5 33. axb5+ Kxb5 34. Ne1 Nb8 35. Nc2 Nc6 36. Bd2 a5 37. Bc3 a4 38. bxa4+ Kxa4 39. Bd2 Kb3 40. Bc3 Bd8 41. Bd2 Bb6 42. Bc3 Ba5 43. Bxa5 Nxa5 44. Ne3 Nc4 45. Ng2 Nb2+ 46. Ke3 Kc3 47. Nf4 Nc4+ 48. Kf3 Kxd4 49. h5 Nxe5+ 50. Ke2 gxh5 51. Nxh5 g6 52. Nf4 Ke4 53. Ng2 d4 54. Ne1 d3+ 55. Kd2 Nf3+ 56. Nxf3 Kxf3 57. Kxd3 Kxf2 58. g4 Kf3 59. g5 Kf4 60. Ke2 Kxg5 61. Kf3 Kf5 62. Kg3 e5 63. Kf3 e4+ 64. Ke3 Ke5 65. Kf2 Kf4 66. Ke2 e3 67. Kf1 Kf3 68. Ke1 e2 69. Kd2 Kf2 70. Kd3 e1=Q 71. Kd4 Qe6 72. Kc5 Ke3 73. Kb4 Kd4 74. Kb5 Qd6 75. Ka4 Qb6 76. Ka3 Kc3 77. Ka2 Qb2# d4 d5 e4",
          "positional_tokens": {
            "prompt_tokens": 116,
            "completion_tokens": 636,
            "total_tokens": 752,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxf3",
            "Kxf3",
            "Kxd3",
            "Kxf2",
            "g4",
            "Kf3",
            "g5",
            "Kf4",
            "Ke2",
            "Kxg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "e5",
            "Kf3",
            "e4+",
            "Ke3",
            "Ke5",
            "Kf2",
            "Kf4",
            "Ke2",
            "e3",
            "Kf1",
            "Kf3",
            "Ke1",
            "e2",
            "Kd2",
            "Kf2",
            "Kd3",
            "e1=Q",
            "Kd4",
            "Qe6",
            "Kc5",
            "Ke3",
            "Kb4",
            "Kd4",
            "Kb5",
            "Qd6",
            "Ka4",
            "Qb6",
            "Ka3",
            "Kc3",
            "Ka2",
            "Qb2#",
            "d4",
            "d5",
            "e4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "c4",
          "neutral_uci": "c2c4",
          "neutral_response": "2. c4 c6 3. Nf3 Nf6 4. Nc3 dxc4 5. a4 Bf5 6. e3 e6 7. Bxc4 Bb4 8. O-O O-O 9. Qe2 Nbd7 10. e4 Bg6 11. Bd3 Bh5 12. e5 Nd5 13. Nxd5 cxd5 14. Qe3 Bg6 15. Bxg6 hxg6 16. Ng5 Re8 17. Qh3 Nf8 18. Rd1 Rc8 19. Qb3 Qb6 20. Rd3 Be7 21. Qxb6 axb6 22. Nf3 Rc4 23. Bd2 Ra8 24. b3 Rc6 25. Rc1 Rac8 26. Rdc3 Rxc3 27. Rxc3 Rxc3 28. Bxc3 Nd7 29. Kf1 Nb8 30. Ke2 Nc6 31. Kd3 Kf8 32. Ne1 Ke8 33. Nc2 Kd7 34. b4 f6 35. f4 g5 36. g3 gxf4 37. gxf4 fxe5 38. fxe5 Bg5 39. b5 Ne7 40. Bb4 Nf5 41. Bf8 Bf4 42. h3 g5 43. Bb4 Ke8 44. Bd2 Bxd2 45. Kxd2 Kf7 46. Kd3 Kg6 47. Ne1 Kh5 48. Nf3 Nh4 49. Ng1 Ng2 50. Ke2 Kh4 51. Kf3 Nf4 52. Ke3 Nxh3 53. Nf3+ Kg4 54. Nh2+ Kf5 55. Kf3 Nf4 56. Ng4 Ne2 57. Nh6+ Kg6 58. Ng8 Nxd4+ 59. Kg4 Nf5 60. Nf6 d4 61. Ne4 Ne3+ 62. Kf3 Nc4 63. Nd6 Nxe5+ 64. Ke4 d3 65. Ke3 g4 66. Nxb7 g3 67. Nd6 g2 68. Kf2 d2 69. Kxg2 d1=Q 70. Ne4 Qf3+ 71. Kh2 Ng4+ 72. Kg1 Qxe4 73. a5 Qe2 74. axb6 Qf2+ 75. Kh1 Qh2# 0-1 1. d4 d5 2. c4 c6 3. Nf3 Nf6 4. Nc3 dxc4",
          "neutral_tokens": {
            "prompt_tokens": 115,
            "completion_tokens": 640,
            "total_tokens": 755,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "c6",
            "Nf3",
            "Nf6",
            "Nc3",
            "dxc4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "c2c4",
          "positional_uci": "g1f3",
          "neutral_uci": "c2c4",
          "consensus_move": "c2c4",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "c6",
              "Nf3",
              "Nf6",
              "Nc3",
              "dxc4",
              "a4",
              "Bf5",
              "e3",
              "e6",
              "Bxc4",
              "Bb4",
              "O-O",
              "O-O",
              "Qe2",
              "Bg6",
              "Ne5",
              "Nbd7",
              "Nxg6",
              "hxg6",
              "Rd1",
              "Qa5",
              "Bd2",
              "e5",
              "dxe5",
              "Nxe5",
              "Bb3",
              "Rad8",
              "Be1",
              "Rxd1",
              "Rxd1",
              "Rd8",
              "Rxd8+",
              "Qxd8",
              "Qd1",
              "Qxd1",
              "Bxd1",
              "Nd3",
              "Kf1",
              "Nxb2",
              "Bb3",
              "Nd3",
              "Bd2",
              "Nc5",
              "Bc2",
              "Bxc3",
              "Bxc3",
              "Nd5",
              "Bd4",
              "b6",
              "a5",
              "Kf8",
              "axb6",
              "axb6",
              "Ke2",
              "Ne6",
              "Bb2",
              "Ke7",
              "g3",
              "Kd6",
              "e4",
              "Nb4",
              "Bb3",
              "c5",
              "f4",
              "b5",
              "f5",
              "gxf5",
              "exf5",
              "Nd4+",
              "Bxd4",
              "cxd4",
              "Bxf7",
              "Ke5",
              "Be8",
              "Kxf5",
              "Bxb5",
              "Kg4",
              "Bd7+",
              "Kg5",
              "h4+",
              "Kh5",
              "Kf3",
              "d3",
              "Bg4+",
              "Kh6",
              "Ke3",
              "Kg6",
              "Bf3",
              "Kf5",
              "g4+",
              "Ke5",
              "Be4",
              "d2",
              "Bf3",
              "Nc2+",
              "Kxd2",
              "Nd4",
              "Ke3",
              "Nxf3",
              "Kxf3",
              "Kf6",
              "Kf4",
              "Kg6",
              "Ke5",
              "Kh6",
              "Kf5",
              "Kh7",
              "Kg5",
              "Kh8",
              "Kg6",
              "Kg8",
              "h5",
              "Kh8",
              "Kf7",
              "Kh7",
              "g5",
              "Kh8",
              "h6",
              "gxh6",
              "g6",
              "h5",
              "g7+",
              "Kh7",
              "g8=Q+",
              "Kh6",
              "Qg6#",
              "Nf3",
              "Nf4",
              "Nf5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nxf3",
              "Kxf3",
              "Kxd3",
              "Kxf2",
              "g4",
              "Kf3",
              "g5",
              "Kf4",
              "Ke2",
              "Kxg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "e5",
              "Kf3",
              "e4+",
              "Ke3",
              "Ke5",
              "Kf2",
              "Kf4",
              "Ke2",
              "e3",
              "Kf1",
              "Kf3",
              "Ke1",
              "e2",
              "Kd2",
              "Kf2",
              "Kd3",
              "e1=Q",
              "Kd4",
              "Qe6",
              "Kc5",
              "Ke3",
              "Kb4",
              "Kd4",
              "Kb5",
              "Qd6",
              "Ka4",
              "Qb6",
              "Ka3",
              "Kc3",
              "Ka2",
              "Qb2#",
              "d4",
              "d5",
              "e4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "c6",
              "Nf3",
              "Nf6",
              "Nc3",
              "dxc4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 116,
            "completion_tokens": 525,
            "total_tokens": 641,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 116,
            "completion_tokens": 636,
            "total_tokens": 752,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 115,
            "completion_tokens": 640,
            "total_tokens": 755,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 347,
          "total_completion_tokens": 1801,
          "total_tokens": 2148
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 116,
            "completion_tokens": 525,
            "total_tokens": 641,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 116,
            "completion_tokens": 636,
            "total_tokens": 752,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 115,
            "completion_tokens": 640,
            "total_tokens": 755,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/3p4/3P4/8/PPP1PPPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 5,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf3",
          "aggressive_uci": "g1f3",
          "aggressive_response": "3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. dxc5 Qxd1 8. Rxd1 Bxc5 9. b3 O-O 10. Bb2 b5 11. Be2 Bb7 12. Nbd2 Nbd7 13. Rac1 Rac8 14. a4 bxa4 15. bxa4 Nb6 16. a5 Na4 17. Bxf6 gxf6 18. Nb3 Bb4 19. Rxc8 Rxc8 20. Rd7 Bd5 21. Nfd4 Nc5 22. Nxc5 Rxc5 23. h3 Rxa5 24. Bh5 e5 25. Nf5 Be6 26. Rd8+ Bf8 27. Ne7+ Kg7 28. Nc6 Ra1+ 29. Kh2 a5 30. Ra8 a4 31. Nd8 Bd5 32. Ra7 a3 33. Bxf7 Bxf7 34. Nxf7 Kg6 35. Nh8+ Kf5 36. Rxh7 a2 37. Ra7 Bb4 38. Ra4 Bc3 39. Nf7 Ke6 40. Nd8+ Kd5 41. Nb7 Kc6 42. Nd8+ Kb5 43. Ra8 Ba5 44. Nb7 Rh1+ 45. Kxh1 a1=Q+ 46. Kh2 Bc7 47. Rxa1 e4+ 48. g3 Kc6 49. Na5+ Kd5 50. Nb3 Kc4 51. Nd4 Be5 52. Rc1+ Kd3 53. Rd1+ Kc3 54. Kg2 Kc4 55. h4 Kd5 56. h5 Kd6 57. h6 Ke7 58. h7 f5 59. Nxf5+ Kf7 60. Rd5 Bf6 61. Rd6 Be5 62. Rd7+ Kg6 63. Nh4+ Kh6 64. Re7 Bf6 65. Re6 Kxh7 66. Rxf6 Kg7 67. Rf4 Kh6 68. Rxe4 Kg5 69. Rf4 Kh5 70. e4 Kg5 71. e5 Kh5 72. e6 Kg5 73. e7 Kh5 74. e8=Q+ Kg5 75. Qg6# e5 Qg6# e4 Qg6#",
          "aggressive_tokens": {
            "prompt_tokens": 124,
            "completion_tokens": 601,
            "total_tokens": 725,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "e3",
            "e6",
            "Bxc4",
            "c5",
            "O-O",
            "a6",
            "dxc5",
            "Qxd1",
            "Rxd1",
            "Bxc5",
            "b3",
            "O-O",
            "Bb2",
            "b5",
            "Be2",
            "Bb7",
            "Nbd2",
            "Nbd7",
            "Rac1",
            "Rac8",
            "a4",
            "bxa4",
            "bxa4",
            "Nb6",
            "a5",
            "Na4",
            "Bxf6",
            "gxf6",
            "Nb3",
            "Bb4",
            "Rxc8",
            "Rxc8",
            "Rd7",
            "Bd5",
            "Nfd4",
            "Nc5",
            "Nxc5",
            "Rxc5",
            "h3",
            "Rxa5",
            "Bh5",
            "e5",
            "Nf5",
            "Be6",
            "Rd8+",
            "Bf8",
            "Ne7+",
            "Kg7",
            "Nc6",
            "Ra1+",
            "Kh2",
            "a5",
            "Ra8",
            "a4",
            "Nd8",
            "Bd5",
            "Ra7",
            "a3",
            "Bxf7",
            "Bxf7",
            "Nxf7",
            "Kg6",
            "Nh8+",
            "Kf5",
            "Rxh7",
            "a2",
            "Ra7",
            "Bb4",
            "Ra4",
            "Bc3",
            "Nf7",
            "Ke6",
            "Nd8+",
            "Kd5",
            "Nb7",
            "Kc6",
            "Nd8+",
            "Kb5",
            "Ra8",
            "Ba5",
            "Nb7",
            "Rh1+",
            "Kxh1",
            "a1=Q+",
            "Kh2",
            "Bc7",
            "Rxa1",
            "e4+",
            "g3",
            "Kc6",
            "Na5+",
            "Kd5",
            "Nb3",
            "Kc4",
            "Nd4",
            "Be5",
            "Rc1+",
            "Kd3",
            "Rd1+",
            "Kc3",
            "Kg2",
            "Kc4",
            "h4",
            "Kd5",
            "h5",
            "Kd6",
            "h6",
            "Ke7",
            "h7",
            "f5",
            "Nxf5+",
            "Kf7",
            "Rd5",
            "Bf6",
            "Rd6",
            "Be5",
            "Rd7+",
            "Kg6",
            "Nh4+",
            "Kh6",
            "Re7",
            "Bf6",
            "Re6",
            "Kxh7",
            "Rxf6",
            "Kg7",
            "Rf4",
            "Kh6",
            "Rxe4",
            "Kg5",
            "Rf4",
            "Kh5",
            "e4",
            "Kg5",
            "e5",
            "Kh5",
            "e6",
            "Kg5",
            "e7",
            "Kh5",
            "e8=Q+",
            "Kg5",
            "Qg6#",
            "e5",
            "Qg6#",
            "e4",
            "Qg6#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf3",
          "positional_uci": "g1f3",
          "positional_response": "3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. dxc5 Qxd1 8. Rxd1 Bxc5 9. b3 b5 10. Be2 Bb7 11. Bb2 Nbd7 12. Nbd2 Ke7 13. Rac1 Rac8 14. a4 bxa4 15. bxa4 Rhd8 16. Nb3 Bb4 17. Nfd4 Nb6 18. Rxc8 Rxc8 19. a5 Nc4 20. Bxc4 Rxc4 21. Rc1 Rxc1+ 22. Bxc1 Bd5 23. Bd2 Bxd2 24. Nxd2 Kd6 25. f3 Kc5 26. Kf2 Kb4 27. e4 Bb7 28. N2b3 Nd7 29. Ke3 e5 30. Nf5 Kxb3 31. Nd6 Bc6 32. Nxf7 Kb4 33. Ng5 h6 34. Ne6 g6 35. f4 exf4+ 36. Nxf4 g5 37. Nd3+ Kxa5 38. Kd4 Kb6 39. e5 Bxg2 40. e6 Nf6 41. Ke5 Ne8 42. Nb4 a5 43. Nd5+ Bxd5 44. Kxd5 a4 45. Kc4 Kc6 46. Kb4 Kd6 47. Kxa4 Kxe6 48. Kb4 Kf5 49. Kc3 Kg4 50. Kd2 Kh3 51. Ke2 Kxh2 52. Kf3 h5 53. Kf2 g4 54. Kf1 g3 55. Ke2 g2 56. Kf3 g1=Q 57. Kf4 Qg4+ 58. Ke5 Kg3 59. Kd5 h4 60. Ke5 h3 61. Kd5 h2 62. Ke5 h1=Q 63. 1-0 Nf6 gxf6+ Kxf6",
          "positional_tokens": {
            "prompt_tokens": 124,
            "completion_tokens": 508,
            "total_tokens": 632,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "e3",
            "e6",
            "Bxc4",
            "c5",
            "O-O",
            "a6",
            "dxc5",
            "Qxd1",
            "Rxd1",
            "Bxc5",
            "b3",
            "b5",
            "Be2",
            "Bb7",
            "Bb2",
            "Nbd7",
            "Nbd2",
            "Ke7",
            "Rac1",
            "Rac8",
            "a4",
            "bxa4",
            "bxa4",
            "Rhd8",
            "Nb3",
            "Bb4",
            "Nfd4",
            "Nb6",
            "Rxc8",
            "Rxc8",
            "a5",
            "Nc4",
            "Bxc4",
            "Rxc4",
            "Rc1",
            "Rxc1+",
            "Bxc1",
            "Bd5",
            "Bd2",
            "Bxd2",
            "Nxd2",
            "Kd6",
            "f3",
            "Kc5",
            "Kf2",
            "Kb4",
            "e4",
            "Bb7",
            "N2b3",
            "Nd7",
            "Ke3",
            "e5",
            "Nf5",
            "Kxb3",
            "Nd6",
            "Bc6",
            "Nxf7",
            "Kb4",
            "Ng5",
            "h6",
            "Ne6",
            "g6",
            "f4",
            "exf4+",
            "Nxf4",
            "g5",
            "Nd3+",
            "Kxa5",
            "Kd4",
            "Kb6",
            "e5",
            "Bxg2",
            "e6",
            "Nf6",
            "Ke5",
            "Ne8",
            "Nb4",
            "a5",
            "Nd5+",
            "Bxd5",
            "Kxd5",
            "a4",
            "Kc4",
            "Kc6",
            "Kb4",
            "Kd6",
            "Kxa4",
            "Kxe6",
            "Kb4",
            "Kf5",
            "Kc3",
            "Kg4",
            "Kd2",
            "Kh3",
            "Ke2",
            "Kxh2",
            "Kf3",
            "h5",
            "Kf2",
            "g4",
            "Kf1",
            "g3",
            "Ke2",
            "g2",
            "Kf3",
            "g1=Q",
            "Kf4",
            "Qg4+",
            "Ke5",
            "Kg3",
            "Kd5",
            "h4",
            "Ke5",
            "h3",
            "Kd5",
            "h2",
            "Ke5",
            "h1=Q",
            "Nf6",
            "gxf6+",
            "Kxf6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf3",
          "neutral_uci": "g1f3",
          "neutral_response": "3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. dxc5 Qxd1 8. Rxd1 Bxc5 9. b3 b5 10. Be2 Bb7 11. Bb2 Nbd7 12. Nbd2 Ke7 13. Rac1 Rac8 14. a4 bxa4 15. bxa4 Rhd8 16. Nb3 Bb4 17. Nfd4 Nb6 18. Rxc8 Rxc8 19. a5 Nc4 20. Bxc4 Rxc4 21. Rc1 Rxc1+ 22. Bxc1 Bd5 23. Bd2 Bxd2 24. Nxd2 Kd6 25. f3 Kc5 26. Kf2 Kb4 27. e4 Bb7 28. N2b3 Nd7 29. Ke3 e5 30. Nf5 Kxb3 31. Nd6 Bc6 32. Nxf7 Kb4 33. Ng5 h6 34. Ne6 g6 35. f4 exf4+ 36. Nxf4 g5 37. Nd3+ Kxa5 38. Kd4 Kb5 39. e5 Bxg2 40. e6 Nf6 41. Ke5 Ne8 42. e7 Kc6 43. Ke6 Bd5+ 44. Ke5 Bc4 45. Nb4+ Kd7 46. Kd4 Bb5 47. Nd5 Ke6 48. Kc5 h5 49. h3 g4 50. hxg4 hxg4 51. Ne3 g3 52. Kb6 Kxe7 53. Ka5 Kd6 54. Kb4 Ke5 55. Kc3 Ke4 56. Kd2 Kf3 57. Nf5 g2 58. Nh4+ Kf2 59. Nxg2 Kxg2 60. Kc3 Kf3 61. Kb4 Ke4 62. Ka5 Kd5 63. Kb4 Kc6 64. Ka5 Nc7 65. Kb4 Kb6 66. Ka3 a5 67. Kb3 Kc5 68. Ka3 Nd5 69. Kb3 a4+ 70. Ka3 Kc4 71. Kb2 Kb4 72. Ka2 Bc4+ 73. Kb2 a3+ 74. Ka1 Kb3 75. Kb1 Nb4 76. Ka1 Nc2+ 77. Kb1 a2+ 78. Kc1 a1=Q+ 79. Kd2 Qe1# 0-1 1. d4 d5 2.",
          "neutral_tokens": {
            "prompt_tokens": 123,
            "completion_tokens": 640,
            "total_tokens": 763,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf6",
            "e3",
            "e6",
            "Bxc4",
            "c5",
            "O-O",
            "a6",
            "dxc5",
            "Qxd1",
            "Rxd1",
            "Bxc5",
            "b3",
            "b5",
            "Be2",
            "Bb7",
            "Bb2",
            "Nbd7",
            "Nbd2",
            "Ke7",
            "Rac1",
            "Rac8",
            "a4",
            "bxa4",
            "bxa4",
            "Rhd8",
            "Nb3",
            "Bb4",
            "Nfd4",
            "Nb6",
            "Rxc8",
            "Rxc8",
            "a5",
            "Nc4",
            "Bxc4",
            "Rxc4",
            "Rc1",
            "Rxc1+",
            "Bxc1",
            "Bd5",
            "Bd2",
            "Bxd2",
            "Nxd2",
            "Kd6",
            "f3",
            "Kc5",
            "Kf2",
            "Kb4",
            "e4",
            "Bb7",
            "N2b3",
            "Nd7",
            "Ke3",
            "e5",
            "Nf5",
            "Kxb3",
            "Nd6",
            "Bc6",
            "Nxf7",
            "Kb4",
            "Ng5",
            "h6",
            "Ne6",
            "g6",
            "f4",
            "exf4+",
            "Nxf4",
            "g5",
            "Nd3+",
            "Kxa5",
            "Kd4",
            "Kb5",
            "e5",
            "Bxg2",
            "e6",
            "Nf6",
            "Ke5",
            "Ne8",
            "e7",
            "Kc6",
            "Ke6",
            "Bd5+",
            "Ke5",
            "Bc4",
            "Nb4+",
            "Kd7",
            "Kd4",
            "Bb5",
            "Nd5",
            "Ke6",
            "Kc5",
            "h5",
            "h3",
            "g4",
            "hxg4",
            "hxg4",
            "Ne3",
            "g3",
            "Kb6",
            "Kxe7",
            "Ka5",
            "Kd6",
            "Kb4",
            "Ke5",
            "Kc3",
            "Ke4",
            "Kd2",
            "Kf3",
            "Nf5",
            "g2",
            "Nh4+",
            "Kf2",
            "Nxg2",
            "Kxg2",
            "Kc3",
            "Kf3",
            "Kb4",
            "Ke4",
            "Ka5",
            "Kd5",
            "Kb4",
            "Kc6",
            "Ka5",
            "Nc7",
            "Kb4",
            "Kb6",
            "Ka3",
            "a5",
            "Kb3",
            "Kc5",
            "Ka3",
            "Nd5",
            "Kb3",
            "a4+",
            "Ka3",
            "Kc4",
            "Kb2",
            "Kb4",
            "Ka2",
            "Bc4+",
            "Kb2",
            "a3+",
            "Ka1",
            "Kb3",
            "Kb1",
            "Nb4",
            "Ka1",
            "Nc2+",
            "Kb1",
            "a2+",
            "Kc1",
            "a1=Q+",
            "Kd2",
            "Qe1#",
            "d4",
            "d5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g1f3",
          "positional_uci": "g1f3",
          "neutral_uci": "g1f3",
          "consensus_move": "g1f3",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf6",
              "e3",
              "e6",
              "Bxc4",
              "c5",
              "O-O",
              "a6",
              "dxc5",
              "Qxd1",
              "Rxd1",
              "Bxc5",
              "b3",
              "O-O",
              "Bb2",
              "b5",
              "Be2",
              "Bb7",
              "Nbd2",
              "Nbd7",
              "Rac1",
              "Rac8",
              "a4",
              "bxa4",
              "bxa4",
              "Nb6",
              "a5",
              "Na4",
              "Bxf6",
              "gxf6",
              "Nb3",
              "Bb4",
              "Rxc8",
              "Rxc8",
              "Rd7",
              "Bd5",
              "Nfd4",
              "Nc5",
              "Nxc5",
              "Rxc5",
              "h3",
              "Rxa5",
              "Bh5",
              "e5",
              "Nf5",
              "Be6",
              "Rd8+",
              "Bf8",
              "Ne7+",
              "Kg7",
              "Nc6",
              "Ra1+",
              "Kh2",
              "a5",
              "Ra8",
              "a4",
              "Nd8",
              "Bd5",
              "Ra7",
              "a3",
              "Bxf7",
              "Bxf7",
              "Nxf7",
              "Kg6",
              "Nh8+",
              "Kf5",
              "Rxh7",
              "a2",
              "Ra7",
              "Bb4",
              "Ra4",
              "Bc3",
              "Nf7",
              "Ke6",
              "Nd8+",
              "Kd5",
              "Nb7",
              "Kc6",
              "Nd8+",
              "Kb5",
              "Ra8",
              "Ba5",
              "Nb7",
              "Rh1+",
              "Kxh1",
              "a1=Q+",
              "Kh2",
              "Bc7",
              "Rxa1",
              "e4+",
              "g3",
              "Kc6",
              "Na5+",
              "Kd5",
              "Nb3",
              "Kc4",
              "Nd4",
              "Be5",
              "Rc1+",
              "Kd3",
              "Rd1+",
              "Kc3",
              "Kg2",
              "Kc4",
              "h4",
              "Kd5",
              "h5",
              "Kd6",
              "h6",
              "Ke7",
              "h7",
              "f5",
              "Nxf5+",
              "Kf7",
              "Rd5",
              "Bf6",
              "Rd6",
              "Be5",
              "Rd7+",
              "Kg6",
              "Nh4+",
              "Kh6",
              "Re7",
              "Bf6",
              "Re6",
              "Kxh7",
              "Rxf6",
              "Kg7",
              "Rf4",
              "Kh6",
              "Rxe4",
              "Kg5",
              "Rf4",
              "Kh5",
              "e4",
              "Kg5",
              "e5",
              "Kh5",
              "e6",
              "Kg5",
              "e7",
              "Kh5",
              "e8=Q+",
              "Kg5",
              "Qg6#",
              "e5",
              "Qg6#",
              "e4",
              "Qg6#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf6",
              "e3",
              "e6",
              "Bxc4",
              "c5",
              "O-O",
              "a6",
              "dxc5",
              "Qxd1",
              "Rxd1",
              "Bxc5",
              "b3",
              "b5",
              "Be2",
              "Bb7",
              "Bb2",
              "Nbd7",
              "Nbd2",
              "Ke7",
              "Rac1",
              "Rac8",
              "a4",
              "bxa4",
              "bxa4",
              "Rhd8",
              "Nb3",
              "Bb4",
              "Nfd4",
              "Nb6",
              "Rxc8",
              "Rxc8",
              "a5",
              "Nc4",
              "Bxc4",
              "Rxc4",
              "Rc1",
              "Rxc1+",
              "Bxc1",
              "Bd5",
              "Bd2",
              "Bxd2",
              "Nxd2",
              "Kd6",
              "f3",
              "Kc5",
              "Kf2",
              "Kb4",
              "e4",
              "Bb7",
              "N2b3",
              "Nd7",
              "Ke3",
              "e5",
              "Nf5",
              "Kxb3",
              "Nd6",
              "Bc6",
              "Nxf7",
              "Kb4",
              "Ng5",
              "h6",
              "Ne6",
              "g6",
              "f4",
              "exf4+",
              "Nxf4",
              "g5",
              "Nd3+",
              "Kxa5",
              "Kd4",
              "Kb6",
              "e5",
              "Bxg2",
              "e6",
              "Nf6",
              "Ke5",
              "Ne8",
              "Nb4",
              "a5",
              "Nd5+",
              "Bxd5",
              "Kxd5",
              "a4",
              "Kc4",
              "Kc6",
              "Kb4",
              "Kd6",
              "Kxa4",
              "Kxe6",
              "Kb4",
              "Kf5",
              "Kc3",
              "Kg4",
              "Kd2",
              "Kh3",
              "Ke2",
              "Kxh2",
              "Kf3",
              "h5",
              "Kf2",
              "g4",
              "Kf1",
              "g3",
              "Ke2",
              "g2",
              "Kf3",
              "g1=Q",
              "Kf4",
              "Qg4+",
              "Ke5",
              "Kg3",
              "Kd5",
              "h4",
              "Ke5",
              "h3",
              "Kd5",
              "h2",
              "Ke5",
              "h1=Q",
              "Nf6",
              "gxf6+",
              "Kxf6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf6",
              "e3",
              "e6",
              "Bxc4",
              "c5",
              "O-O",
              "a6",
              "dxc5",
              "Qxd1",
              "Rxd1",
              "Bxc5",
              "b3",
              "b5",
              "Be2",
              "Bb7",
              "Bb2",
              "Nbd7",
              "Nbd2",
              "Ke7",
              "Rac1",
              "Rac8",
              "a4",
              "bxa4",
              "bxa4",
              "Rhd8",
              "Nb3",
              "Bb4",
              "Nfd4",
              "Nb6",
              "Rxc8",
              "Rxc8",
              "a5",
              "Nc4",
              "Bxc4",
              "Rxc4",
              "Rc1",
              "Rxc1+",
              "Bxc1",
              "Bd5",
              "Bd2",
              "Bxd2",
              "Nxd2",
              "Kd6",
              "f3",
              "Kc5",
              "Kf2",
              "Kb4",
              "e4",
              "Bb7",
              "N2b3",
              "Nd7",
              "Ke3",
              "e5",
              "Nf5",
              "Kxb3",
              "Nd6",
              "Bc6",
              "Nxf7",
              "Kb4",
              "Ng5",
              "h6",
              "Ne6",
              "g6",
              "f4",
              "exf4+",
              "Nxf4",
              "g5",
              "Nd3+",
              "Kxa5",
              "Kd4",
              "Kb5",
              "e5",
              "Bxg2",
              "e6",
              "Nf6",
              "Ke5",
              "Ne8",
              "e7",
              "Kc6",
              "Ke6",
              "Bd5+",
              "Ke5",
              "Bc4",
              "Nb4+",
              "Kd7",
              "Kd4",
              "Bb5",
              "Nd5",
              "Ke6",
              "Kc5",
              "h5",
              "h3",
              "g4",
              "hxg4",
              "hxg4",
              "Ne3",
              "g3",
              "Kb6",
              "Kxe7",
              "Ka5",
              "Kd6",
              "Kb4",
              "Ke5",
              "Kc3",
              "Ke4",
              "Kd2",
              "Kf3",
              "Nf5",
              "g2",
              "Nh4+",
              "Kf2",
              "Nxg2",
              "Kxg2",
              "Kc3",
              "Kf3",
              "Kb4",
              "Ke4",
              "Ka5",
              "Kd5",
              "Kb4",
              "Kc6",
              "Ka5",
              "Nc7",
              "Kb4",
              "Kb6",
              "Ka3",
              "a5",
              "Kb3",
              "Kc5",
              "Ka3",
              "Nd5",
              "Kb3",
              "a4+",
              "Ka3",
              "Kc4",
              "Kb2",
              "Kb4",
              "Ka2",
              "Bc4+",
              "Kb2",
              "a3+",
              "Ka1",
              "Kb3",
              "Kb1",
              "Nb4",
              "Ka1",
              "Nc2+",
              "Kb1",
              "a2+",
              "Kc1",
              "a1=Q+",
              "Kd2",
              "Qe1#",
              "d4",
              "d5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 124,
            "completion_tokens": 601,
            "total_tokens": 725,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 124,
            "completion_tokens": 508,
            "total_tokens": 632,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 123,
            "completion_tokens": 640,
            "total_tokens": 763,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 371,
          "total_completion_tokens": 1749,
          "total_tokens": 2120
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 124,
            "completion_tokens": 601,
            "total_tokens": 725,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 124,
            "completion_tokens": 508,
            "total_tokens": 632,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 123,
            "completion_tokens": 640,
            "total_tokens": 763,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/8/2pP4/8/PP2PPPP/RNBQKBNR w KQkq - 0 3"
    },
    {
      "turn": 7,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc3",
          "aggressive_uci": "b1c3",
          "aggressive_response": "4. Nc3 Nf6 5. e4 Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Na5 14. Bc2 c5 15. dxc5 Qc7 16. f4 Rfd8 17. Qe2 Bxc3 18. bxc3 h6 19. f5 Bh7 20. g5 hxg5 21. Bxg5 Qxc5 22. Bxf6 gxf6 23. Rhg1+ Kh8 24. Qh5 Qxc3 25. Qxf7 Rg8 26. Nb1 Qe3+ 27. Kb2 Nc4+ 28. Ka1 Qh6 29. fxe6 Ne3 30. Rxg8+ Rxg8 31. Rd2 Nxc2+ 32. Rxc2 Bxe4 33. Rc7 Bg6 34. Qxf6+ Rg7 35. e7 Kh7 36. Rc8 Rf7 37. Rh8# e4 Nc6 Qe7",
          "aggressive_tokens": {
            "prompt_tokens": 133,
            "completion_tokens": 291,
            "total_tokens": 424,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "e4",
            "Bg4",
            "Be3",
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O",
            "O-O-O",
            "a6",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Nd2",
            "b5",
            "Bb3",
            "Na5",
            "Bc2",
            "c5",
            "dxc5",
            "Qc7",
            "f4",
            "Rfd8",
            "Qe2",
            "Bxc3",
            "bxc3",
            "h6",
            "f5",
            "Bh7",
            "g5",
            "hxg5",
            "Bxg5",
            "Qxc5",
            "Bxf6",
            "gxf6",
            "Rhg1+",
            "Kh8",
            "Qh5",
            "Qxc3",
            "Qxf7",
            "Rg8",
            "Nb1",
            "Qe3+",
            "Kb2",
            "Nc4+",
            "Ka1",
            "Qh6",
            "fxe6",
            "Ne3",
            "Rxg8+",
            "Rxg8",
            "Rd2",
            "Nxc2+",
            "Rxc2",
            "Bxe4",
            "Rc7",
            "Bg6",
            "Qxf6+",
            "Rg7",
            "e7",
            "Kh7",
            "Rc8",
            "Rf7",
            "Rh8#",
            "e4",
            "Nc6",
            "Qe7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e3",
          "positional_uci": "e2e3",
          "positional_response": "4. e3 e5 5. Bxc4 exd4 6. exd4 Bg4 7. O-O Nf6 8. Re1+ Be7 9. d5 Na5 10. Qa4+ c6 11. dxc6 Nxc6 12. Ne5 Bd7 13. Nxf7 Qa5 14. Qxa5 Nxa5 15. Nxh8 Nxc4 16. b3 Nb6 17. Ba3 Nbd5 18. Nc3 Kf8 19. Nxd5 Nxd5 20. Rad1 Bxa3 21. Rxd5 Bc6 22. Rf5+ Kg8 23. Nf7 Rf8 24. Nh6+ gxh6 25. Rxf8+ Kxf8 26. Re6 Kg7 27. f3 Bc5+ 28. Kh1 h5 29. Re5 Bd6 30. Rxh5 Kg6 31. Rh4 h5 32. Rd4 Bc5 33. Rd8 h4 34. h3 Kf5 35. Rh8 Bf2 36. Rf8+ Ke5 37. f4+ Ke4 38. f5 Ke3 39. f6 Ke2 40. f7 Kf1 41. Rg8 Bc5 42. f8=Q+ Bxf8 43. Rxf8+ Ke2 44. Rh8 Kf2 45. Rf8+ Kg3 46. Rg8+ Kf2 47. Kh2 a6 48. g4 hxg3+ 49. Rxg3 Ke2 50. h4 Kd2 51. h5 Kc2 52. h6 Kb2 53. h7 Kxa2 54. h8=Q Ka3 55. Qf8+ Kb2 56. Qf2+ Ka3 57. b4+ Kxb4 58. Qb2+ Kc5 59. Rc3+ Kd6 60. Qd2+ Kc7 61. Qf4+ Kb6 62. Rb3+ Ka7 63. Qd4+ Kb8 64. Qd8+ Ka7 65. Qb6+ Kb8 66. Rd3 Kc8 67. Rd8# Bb5 68. Qc5+ Kb8 69. Qd6+ Ka7 70. Qc5+ b6 71. Qc7# Bxd7 72. Rxd7+ Ka8 73. Rd8# Bxd8",
          "positional_tokens": {
            "prompt_tokens": 133,
            "completion_tokens": 595,
            "total_tokens": 728,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Bxc4",
            "exd4",
            "exd4",
            "Bg4",
            "O-O",
            "Nf6",
            "Re1+",
            "Be7",
            "d5",
            "Na5",
            "Qa4+",
            "c6",
            "dxc6",
            "Nxc6",
            "Ne5",
            "Bd7",
            "Nxf7",
            "Qa5",
            "Qxa5",
            "Nxa5",
            "Nxh8",
            "Nxc4",
            "b3",
            "Nb6",
            "Ba3",
            "Nbd5",
            "Nc3",
            "Kf8",
            "Nxd5",
            "Nxd5",
            "Rad1",
            "Bxa3",
            "Rxd5",
            "Bc6",
            "Rf5+",
            "Kg8",
            "Nf7",
            "Rf8",
            "Nh6+",
            "gxh6",
            "Rxf8+",
            "Kxf8",
            "Re6",
            "Kg7",
            "f3",
            "Bc5+",
            "Kh1",
            "h5",
            "Re5",
            "Bd6",
            "Rxh5",
            "Kg6",
            "Rh4",
            "h5",
            "Rd4",
            "Bc5",
            "Rd8",
            "h4",
            "h3",
            "Kf5",
            "Rh8",
            "Bf2",
            "Rf8+",
            "Ke5",
            "f4+",
            "Ke4",
            "f5",
            "Ke3",
            "f6",
            "Ke2",
            "f7",
            "Kf1",
            "Rg8",
            "Bc5",
            "f8=Q+",
            "Bxf8",
            "Rxf8+",
            "Ke2",
            "Rh8",
            "Kf2",
            "Rf8+",
            "Kg3",
            "Rg8+",
            "Kf2",
            "Kh2",
            "a6",
            "g4",
            "hxg3+",
            "Rxg3",
            "Ke2",
            "h4",
            "Kd2",
            "h5",
            "Kc2",
            "h6",
            "Kb2",
            "h7",
            "Kxa2",
            "h8=Q",
            "Ka3",
            "Qf8+",
            "Kb2",
            "Qf2+",
            "Ka3",
            "b4+",
            "Kxb4",
            "Qb2+",
            "Kc5",
            "Rc3+",
            "Kd6",
            "Qd2+",
            "Kc7",
            "Qf4+",
            "Kb6",
            "Rb3+",
            "Ka7",
            "Qd4+",
            "Kb8",
            "Qd8+",
            "Ka7",
            "Qb6+",
            "Kb8",
            "Rd3",
            "Kc8",
            "Rd8#",
            "Bb5",
            "Qc5+",
            "Kb8",
            "Qd6+",
            "Ka7",
            "Qc5+",
            "b6",
            "Qc7#",
            "Bxd7",
            "Rxd7+",
            "Ka8",
            "Rd8#",
            "Bxd8"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc3",
          "neutral_uci": "b1c3",
          "neutral_response": "4. Nc3 Nf6 5. e4 Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Na5 14. Bc2 c5 15. dxc5 Qc7 16. f4 Rfd8 17. Qe2 Bxc3 18. bxc3 h6 19. f5 exf5 20. gxf5 Bh7 21. Rhg1 Kh8 22. Qg2 Rg8 23. Bd4 Qe7 24. e5 Nd7 25. f6 gxf6 26. exf6 Qe6 27. Qg7+ Rxg7 28. fxg7+ Kg8 29. Bxh7+ Kxh7 30. Rde1 Qxa2 31. Ne4 Nb3+ 32. Kd1 Nxd4 33. cxd4 Qa1+ 34. Ke2 Qxd4 35. Kf3 Ne5+ 36. Kf4 Ng6+ 37. Kf5 Qe5+ 38. Kg4 Qf4+ 39. Kh5 Qh4# 0-1\nNf6+ Nxf6+ gxf6",
          "neutral_tokens": {
            "prompt_tokens": 132,
            "completion_tokens": 323,
            "total_tokens": 455,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "e4",
            "Bg4",
            "Be3",
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O",
            "O-O-O",
            "a6",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Nd2",
            "b5",
            "Bb3",
            "Na5",
            "Bc2",
            "c5",
            "dxc5",
            "Qc7",
            "f4",
            "Rfd8",
            "Qe2",
            "Bxc3",
            "bxc3",
            "h6",
            "f5",
            "exf5",
            "gxf5",
            "Bh7",
            "Rhg1",
            "Kh8",
            "Qg2",
            "Rg8",
            "Bd4",
            "Qe7",
            "e5",
            "Nd7",
            "f6",
            "gxf6",
            "exf6",
            "Qe6",
            "Qg7+",
            "Rxg7",
            "fxg7+",
            "Kg8",
            "Bxh7+",
            "Kxh7",
            "Rde1",
            "Qxa2",
            "Ne4",
            "Nb3+",
            "Kd1",
            "Nxd4",
            "cxd4",
            "Qa1+",
            "Ke2",
            "Qxd4",
            "Kf3",
            "Ne5+",
            "Kf4",
            "Ng6+",
            "Kf5",
            "Qe5+",
            "Kg4",
            "Qf4+",
            "Kh5",
            "Qh4#",
            "Nf6+",
            "Nxf6+",
            "gxf6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b1c3",
          "positional_uci": "e2e3",
          "neutral_uci": "b1c3",
          "consensus_move": "b1c3",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf6",
              "e4",
              "Bg4",
              "Be3",
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O",
              "O-O-O",
              "a6",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Nd2",
              "b5",
              "Bb3",
              "Na5",
              "Bc2",
              "c5",
              "dxc5",
              "Qc7",
              "f4",
              "Rfd8",
              "Qe2",
              "Bxc3",
              "bxc3",
              "h6",
              "f5",
              "Bh7",
              "g5",
              "hxg5",
              "Bxg5",
              "Qxc5",
              "Bxf6",
              "gxf6",
              "Rhg1+",
              "Kh8",
              "Qh5",
              "Qxc3",
              "Qxf7",
              "Rg8",
              "Nb1",
              "Qe3+",
              "Kb2",
              "Nc4+",
              "Ka1",
              "Qh6",
              "fxe6",
              "Ne3",
              "Rxg8+",
              "Rxg8",
              "Rd2",
              "Nxc2+",
              "Rxc2",
              "Bxe4",
              "Rc7",
              "Bg6",
              "Qxf6+",
              "Rg7",
              "e7",
              "Kh7",
              "Rc8",
              "Rf7",
              "Rh8#",
              "e4",
              "Nc6",
              "Qe7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Bxc4",
              "exd4",
              "exd4",
              "Bg4",
              "O-O",
              "Nf6",
              "Re1+",
              "Be7",
              "d5",
              "Na5",
              "Qa4+",
              "c6",
              "dxc6",
              "Nxc6",
              "Ne5",
              "Bd7",
              "Nxf7",
              "Qa5",
              "Qxa5",
              "Nxa5",
              "Nxh8",
              "Nxc4",
              "b3",
              "Nb6",
              "Ba3",
              "Nbd5",
              "Nc3",
              "Kf8",
              "Nxd5",
              "Nxd5",
              "Rad1",
              "Bxa3",
              "Rxd5",
              "Bc6",
              "Rf5+",
              "Kg8",
              "Nf7",
              "Rf8",
              "Nh6+",
              "gxh6",
              "Rxf8+",
              "Kxf8",
              "Re6",
              "Kg7",
              "f3",
              "Bc5+",
              "Kh1",
              "h5",
              "Re5",
              "Bd6",
              "Rxh5",
              "Kg6",
              "Rh4",
              "h5",
              "Rd4",
              "Bc5",
              "Rd8",
              "h4",
              "h3",
              "Kf5",
              "Rh8",
              "Bf2",
              "Rf8+",
              "Ke5",
              "f4+",
              "Ke4",
              "f5",
              "Ke3",
              "f6",
              "Ke2",
              "f7",
              "Kf1",
              "Rg8",
              "Bc5",
              "f8=Q+",
              "Bxf8",
              "Rxf8+",
              "Ke2",
              "Rh8",
              "Kf2",
              "Rf8+",
              "Kg3",
              "Rg8+",
              "Kf2",
              "Kh2",
              "a6",
              "g4",
              "hxg3+",
              "Rxg3",
              "Ke2",
              "h4",
              "Kd2",
              "h5",
              "Kc2",
              "h6",
              "Kb2",
              "h7",
              "Kxa2",
              "h8=Q",
              "Ka3",
              "Qf8+",
              "Kb2",
              "Qf2+",
              "Ka3",
              "b4+",
              "Kxb4",
              "Qb2+",
              "Kc5",
              "Rc3+",
              "Kd6",
              "Qd2+",
              "Kc7",
              "Qf4+",
              "Kb6",
              "Rb3+",
              "Ka7",
              "Qd4+",
              "Kb8",
              "Qd8+",
              "Ka7",
              "Qb6+",
              "Kb8",
              "Rd3",
              "Kc8",
              "Rd8#",
              "Bb5",
              "Qc5+",
              "Kb8",
              "Qd6+",
              "Ka7",
              "Qc5+",
              "b6",
              "Qc7#",
              "Bxd7",
              "Rxd7+",
              "Ka8",
              "Rd8#",
              "Bxd8"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf6",
              "e4",
              "Bg4",
              "Be3",
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O",
              "O-O-O",
              "a6",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Nd2",
              "b5",
              "Bb3",
              "Na5",
              "Bc2",
              "c5",
              "dxc5",
              "Qc7",
              "f4",
              "Rfd8",
              "Qe2",
              "Bxc3",
              "bxc3",
              "h6",
              "f5",
              "exf5",
              "gxf5",
              "Bh7",
              "Rhg1",
              "Kh8",
              "Qg2",
              "Rg8",
              "Bd4",
              "Qe7",
              "e5",
              "Nd7",
              "f6",
              "gxf6",
              "exf6",
              "Qe6",
              "Qg7+",
              "Rxg7",
              "fxg7+",
              "Kg8",
              "Bxh7+",
              "Kxh7",
              "Rde1",
              "Qxa2",
              "Ne4",
              "Nb3+",
              "Kd1",
              "Nxd4",
              "cxd4",
              "Qa1+",
              "Ke2",
              "Qxd4",
              "Kf3",
              "Ne5+",
              "Kf4",
              "Ng6+",
              "Kf5",
              "Qe5+",
              "Kg4",
              "Qf4+",
              "Kh5",
              "Qh4#",
              "Nf6+",
              "Nxf6+",
              "gxf6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 133,
            "completion_tokens": 291,
            "total_tokens": 424,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 133,
            "completion_tokens": 595,
            "total_tokens": 728,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 132,
            "completion_tokens": 323,
            "total_tokens": 455,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 398,
          "total_completion_tokens": 1209,
          "total_tokens": 1607
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 133,
            "completion_tokens": 291,
            "total_tokens": 424,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 133,
            "completion_tokens": 595,
            "total_tokens": 728,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 132,
            "completion_tokens": 323,
            "total_tokens": 455,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkbnr/ppp1pppp/2n5/8/2pP4/5N2/PP2PPPP/RNBQKB1R w KQkq - 2 4"
    },
    {
      "turn": 9,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": "e2e4",
          "aggressive_response": "5. e4 Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Ne5 14. Qc2 Nc6 15. f4 Nxd4 16. Bxd4 Qxd4 17. f5 exf5 18. exf5 Bh5 19. gxh5 Nxh5 20. Nde4 Qe3+ 21. Kb1 c5 22. Nd5 Qf3 23. Rhf1 Qxh3 24. Rh1 Qf3 25. Rdf1 Qg4 26. Rfg1 Qf3 27. Rxh5 Qxh5 28. Nef6+ Kh8 29. Nxh5 c4 30. Rxg7 cxb3 31. Rxh7+ Kxh7 32. Nhf6+ Kg7 33. Qg2+ Kh6 34. Qh3+ Kg5 35. Qg4+ Kh6 36. Qh5+ Kg7 37. Qh7# Bb4 38. Qh7# Bb4 39. Qh7# Bb4 40. Qh7# Bb4 41. Qh7# Bb4 42. Qh7# Bb4 43. Qh7# Bb4 44. Qh7# Bb4 45. Qh7# Bb4 46. Qh7# Bb4 47. Qh7# Bb4 48. Qh7# Bb4 49. Qh7# Bb4 50. Qh7# Bb4 51. Qh7# Bb4 52. Qh7# Bb4 53. Qh7# Bb4 54. Qh7# Bb4 55. Qh7# Bb4 56. Qh7# Bb4 57. Qh7# Bb4 58. Qh7# Bb4 59. Qh7# Bb4 60. Qh7# Bb4 61. Qh7# Bb4 62. Qh7# Bb4 63. Qh7# Bb4 64. Qh7# Bb4 65. Qh7# Bb4 66. Qh7# Bb4 67. Qh7# Bb4 68. Qh7# Bb4 69. Qh7# Bb4 70. Qh7# Bb4 71. Qh7# Bb4 72. Qh7# Bb4 73. Qh7",
          "aggressive_tokens": {
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bg4",
            "Be3",
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O",
            "O-O-O",
            "a6",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Nd2",
            "b5",
            "Bb3",
            "Ne5",
            "Qc2",
            "Nc6",
            "f4",
            "Nxd4",
            "Bxd4",
            "Qxd4",
            "f5",
            "exf5",
            "exf5",
            "Bh5",
            "gxh5",
            "Nxh5",
            "Nde4",
            "Qe3+",
            "Kb1",
            "c5",
            "Nd5",
            "Qf3",
            "Rhf1",
            "Qxh3",
            "Rh1",
            "Qf3",
            "Rdf1",
            "Qg4",
            "Rfg1",
            "Qf3",
            "Rxh5",
            "Qxh5",
            "Nef6+",
            "Kh8",
            "Nxh5",
            "c4",
            "Rxg7",
            "cxb3",
            "Rxh7+",
            "Kxh7",
            "Nhf6+",
            "Kg7",
            "Qg2+",
            "Kh6",
            "Qh3+",
            "Kg5",
            "Qg4+",
            "Kh6",
            "Qh5+",
            "Kg7",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "5. e4 Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Na5 14. Bc2 c5 15. f4 c4 16. Qe2 Bxc3 17. bxc3 Qe7 18. f5 Qa3+ 19. Kb1 Qxc3 20. fxg6 Qb4+ 21. Ka1 Qc3+ 22. Kb1 Qb4+ 23. Ka1 Qc3+ 24. Kb1 Qb4+ 25. Ka1 Qc3+ 26. Kb1 Qb4+ 27. Ka1 Qc3+ 28. Kb1 Qb4+ 29. Ka1 Qc3+ 30. Kb1 Qb4+ 31. Ka1 Qc3+ 32. Kb1 Qb4+ 33. Ka1 Qc3+ 34. Kb1 Qb4+ 35. Ka1 Qc3+ 36. Kb1 Qb4+ 37. Ka1 Qc3+ 38. Kb1 Qb4+ 39. Ka1 Qc3+ 40. Kb1 Qb4+ 41. Ka1 Qc3+ 42. Kb1 Qb4+ 43. Ka1 Qc3+ 44. Kb1 Qb4+ 45. Ka1 Qc3+ 46. Kb1 Qb4+ 47. Ka1 Qc3+ 48. Kb1 Qb4+ 49. Ka1 Qc3+ 50. Kb1 Qb4+ 51. Ka1 Qc3+ 52. Kb1 Qb4+ 53. Ka1 Qc3+ 54. Kb1 Qb4+ 55. Ka1 Qc3+ 56. Kb1 Qb4+ 57. Ka1 Qc3+ 58. Kb1 Qb4+ 59. Ka1 Qc3+ 60. Kb1 Qb4+ 61. Ka1 Qc3+ 62. Kb1 Qb4+ 63. Ka1 Qc3+ 64. Kb1 Qb4+ 65. Ka1 Qc3+ 66. Kb1 Qb4+ 67. Ka1 Qc3+ 68. Kb1 Qb4+ 69. Ka1 Qc3+ 70. Kb1 Qb4+ 71. Ka1 Qc3+ 72. Kb1 Qb4+ 73. Ka1 Qc3+ 74. Kb1 Qb4+ 75. Ka1 Qc3+ 76. Kb1 Qb4+ 77. Ka1 Q",
          "positional_tokens": {
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bg4",
            "Be3",
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O",
            "O-O-O",
            "a6",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Nd2",
            "b5",
            "Bb3",
            "Na5",
            "Bc2",
            "c5",
            "f4",
            "c4",
            "Qe2",
            "Bxc3",
            "bxc3",
            "Qe7",
            "f5",
            "Qa3+",
            "Kb1",
            "Qxc3",
            "fxg6",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bg4",
          "neutral_uci": null,
          "neutral_response": "5. e4 Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Na5 14. Bc2 c5 15. f4 cxd4 16. Bxd4 h6 17. f5 exf5 18. gxf5 Bh7 19. Rhg1 Kh8 20. Rxg7 Kxg7 21. Rg1+ Kh8 22. Nd5 Be7 23. Qe3 Nc6 24. Qxh6 Nxd4 25. Qg7# 1-0\n5. ... Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Na5 14. Bc2 c5 15. f4 cxd4 16. Bxd4 h6 17. f5 exf5 18. gxf5 Bh7 19. Rhg1 Kh8 20. Rxg7 Kxg7 21. Rg1+ Kh8 22. Nd5 Be7 23. Qe3 Nc6 24. Qxh6 Nxd4 25. Qg7# 1-0 25. ... Nxd4 26. Qg7# 1-0",
          "neutral_tokens": {
            "prompt_tokens": 141,
            "completion_tokens": 365,
            "total_tokens": 506,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Be3",
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O",
            "O-O-O",
            "a6",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Nd2",
            "b5",
            "Bb3",
            "Na5",
            "Bc2",
            "c5",
            "f4",
            "cxd4",
            "Bxd4",
            "h6",
            "f5",
            "exf5",
            "gxf5",
            "Bh7",
            "Rhg1",
            "Kh8",
            "Rxg7",
            "Kxg7",
            "Rg1+",
            "Kh8",
            "Nd5",
            "Be7",
            "Qe3",
            "Nc6",
            "Qxh6",
            "Nxd4",
            "Qg7#",
            "Nxd4",
            "Qg7#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e2e4",
          "positional_uci": "e2e4",
          "neutral_uci": null,
          "consensus_move": "e2e4",
          "source_agents": [
            "aggressive",
            "positional"
          ],
          "selected_agent": "aggressive"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bg4",
              "Be3",
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O",
              "O-O-O",
              "a6",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Nd2",
              "b5",
              "Bb3",
              "Ne5",
              "Qc2",
              "Nc6",
              "f4",
              "Nxd4",
              "Bxd4",
              "Qxd4",
              "f5",
              "exf5",
              "exf5",
              "Bh5",
              "gxh5",
              "Nxh5",
              "Nde4",
              "Qe3+",
              "Kb1",
              "c5",
              "Nd5",
              "Qf3",
              "Rhf1",
              "Qxh3",
              "Rh1",
              "Qf3",
              "Rdf1",
              "Qg4",
              "Rfg1",
              "Qf3",
              "Rxh5",
              "Qxh5",
              "Nef6+",
              "Kh8",
              "Nxh5",
              "c4",
              "Rxg7",
              "cxb3",
              "Rxh7+",
              "Kxh7",
              "Nhf6+",
              "Kg7",
              "Qg2+",
              "Kh6",
              "Qh3+",
              "Kg5",
              "Qg4+",
              "Kh6",
              "Qh5+",
              "Kg7",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bg4",
              "Be3",
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O",
              "O-O-O",
              "a6",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Nd2",
              "b5",
              "Bb3",
              "Na5",
              "Bc2",
              "c5",
              "f4",
              "c4",
              "Qe2",
              "Bxc3",
              "bxc3",
              "Qe7",
              "f5",
              "Qa3+",
              "Kb1",
              "Qxc3",
              "fxg6",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Be3",
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O",
              "O-O-O",
              "a6",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Nd2",
              "b5",
              "Bb3",
              "Na5",
              "Bc2",
              "c5",
              "f4",
              "cxd4",
              "Bxd4",
              "h6",
              "f5",
              "exf5",
              "gxf5",
              "Bh7",
              "Rhg1",
              "Kh8",
              "Rxg7",
              "Kxg7",
              "Rg1+",
              "Kh8",
              "Nd5",
              "Be7",
              "Qe3",
              "Nc6",
              "Qxh6",
              "Nxd4",
              "Qg7#",
              "Nxd4",
              "Qg7#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 141,
            "completion_tokens": 365,
            "total_tokens": 506,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 425,
          "total_completion_tokens": 1645,
          "total_tokens": 2070
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 141,
            "completion_tokens": 365,
            "total_tokens": 506,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp1pppp/2n2n2/8/2pP4/2N2N2/PP2PPPP/R1BQKB1R w KQkq - 4 5"
    },
    {
      "turn": 11,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d5",
          "aggressive_uci": "d4d5",
          "aggressive_response": "6. d5 Ne5 7. Bf4 Nfd7 8. Bxe5 Nxe5 9. Bxc4 Bxf3 10. gxf3 Nxc4 11. Qa4+ Qd7 12. Qxc4 O-O-O 13. O-O-O Kb8 14. Kb1 g6 15. Rc1 Bg7 16. Nb5 Rc8 17. Qc5 b6 18. Qc4 e6 19. Rhd1 exd5 20. Rxd5 Qe7 21. Qa4 c6 22. Rxc6 Rxc6 23. Nd4 Bxd4 24. Qxc6 Rd8 25. Rxd8+ Qxd8 26. Qc4 Qd7 27. Qb4 Bc5 28. Qc3 Qd1+ 29. Qc1 Qxc1+ 30. Kxc1 Bxf2 31. Kd2 Kc7 32. Kd3 Kd6 33. Kc4 Ke5 34. Kb5 Kf4 35. Ka6 Kxf3 36. Kxa7 Kxe4 37. Ka6 f5 38. Kb5 f4 39. Kc4 f3 40. Kc3 Be3 41. Kc2 f2 42. Kd1 f1=Q+ 43. Kc2 Qd3# f2 Qd2+ Kb1 Qc1#",
          "aggressive_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 338,
            "total_tokens": 488,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Ne5",
            "Bf4",
            "Nfd7",
            "Bxe5",
            "Nxe5",
            "Bxc4",
            "Bxf3",
            "gxf3",
            "Nxc4",
            "Qa4+",
            "Qd7",
            "Qxc4",
            "O-O-O",
            "O-O-O",
            "Kb8",
            "Kb1",
            "g6",
            "Rc1",
            "Bg7",
            "Nb5",
            "Rc8",
            "Qc5",
            "b6",
            "Qc4",
            "e6",
            "Rhd1",
            "exd5",
            "Rxd5",
            "Qe7",
            "Qa4",
            "c6",
            "Rxc6",
            "Rxc6",
            "Nd4",
            "Bxd4",
            "Qxc6",
            "Rd8",
            "Rxd8+",
            "Qxd8",
            "Qc4",
            "Qd7",
            "Qb4",
            "Bc5",
            "Qc3",
            "Qd1+",
            "Qc1",
            "Qxc1+",
            "Kxc1",
            "Bxf2",
            "Kd2",
            "Kc7",
            "Kd3",
            "Kd6",
            "Kc4",
            "Ke5",
            "Kb5",
            "Kf4",
            "Ka6",
            "Kxf3",
            "Kxa7",
            "Kxe4",
            "Ka6",
            "f5",
            "Kb5",
            "f4",
            "Kc4",
            "f3",
            "Kc3",
            "Be3",
            "Kc2",
            "f2",
            "Kd1",
            "f1=Q+",
            "Kc2",
            "Qd3#",
            "f2",
            "Qd2+",
            "Kb1",
            "Qc1#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d5",
          "positional_uci": "d4d5",
          "positional_response": "6. d5 Ne5 7. Bf4 Nfd7 8. Bxe5 Nxe5 9. Bxc4 Bxf3 10. gxf3 Nxc4 11. Qa4+ Qd7 12. Qxc4 O-O-O 13. O-O-O e6 14. dxe6 Qxe6 15. Rxd8+ Kxd8 16. Rd1+ Bd6 17. Qd4 Kc8 18. Qxa7 Rd8 19. Qa8+ Kd7 20. Qxb7 Ke8 21. Nd5 Qh6+ 22. Kb1 Qxh2 23. Nxc7+ Bxc7 24. Qc6+ Kf8 25. Qc5+ Ke8 26. Qc6+ Kf8 27. Qc5+ Ke8 28. Qc6+ Kf8 29. Qc5+ Ke8 30. Qc6+ Kf8 31. Qc5+ Ke8 32. Qc6+ Kf8 33. Qc5+ Ke8 34. Qc6+ Kf8 35. Qc5+ Ke8 36. Qc6+ Kf8 37. Qc5+ Ke8 38. Qc6+ Kf8 39. Qc5+ Ke8 40. Qc6+ Kf8 41. Qc5+ Ke8 42. Qc6+ Kf8 43. Qc5+ Ke8 44. Qc6+ Kf8 45. Qc5+ Ke8 46. Qc6+ Kf8 47. Qc5+ Ke8 48. Qc6+ Kf8 49. Qc5+ Ke8 50. Qc6+ Kf8 51. Qc5+ Ke8 52. Qc6+ Kf8 53. Qc5+ Ke8 54. Qc6+ Kf8 55. Qc5+ Ke8 56. Qc6+ Kf8 57. Qc5+ Ke8 58. Qc6+ Kf8 59. Qc5+ Ke8 60. Qc6+ Kf8 61. Qc5+ Ke8 62. Qc6+ Kf8 63. Qc5+ Ke8 64. Qc6+ Kf8 65. Qc5+ Ke8 66. Qc6+ Kf8 67. Qc5+ Ke8 68. Qc6+ Kf8 69. Qc5+ Ke8 70. Qc6+ Kf8 71. Qc5+ Ke8 72. Qc6+ Kf8 73. Qc5+ Ke8 74. Qc",
          "positional_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 640,
            "total_tokens": 790,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Ne5",
            "Bf4",
            "Nfd7",
            "Bxe5",
            "Nxe5",
            "Bxc4",
            "Bxf3",
            "gxf3",
            "Nxc4",
            "Qa4+",
            "Qd7",
            "Qxc4",
            "O-O-O",
            "O-O-O",
            "e6",
            "dxe6",
            "Qxe6",
            "Rxd8+",
            "Kxd8",
            "Rd1+",
            "Bd6",
            "Qd4",
            "Kc8",
            "Qxa7",
            "Rd8",
            "Qa8+",
            "Kd7",
            "Qxb7",
            "Ke8",
            "Nd5",
            "Qh6+",
            "Kb1",
            "Qxh2",
            "Nxc7+",
            "Bxc7",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d5",
          "neutral_uci": "d4d5",
          "neutral_response": "6. d5 Ne5 7. Bf4 Nfd7 8. Bxe5 Nxe5 9. Bxc4 Bxf3 10. gxf3 Nxc4 11. Qa4+ Qd7 12. Qxc4 O-O-O 13. O-O-O e6 14. dxe6 Qxe6 15. Rxd8+ Kxd8 16. Rd1+ Kc8 17. Qd4 Bd6 18. Qxa7 Rd8 19. Qa8+ Kd7 20. Qxb7 Ke8 21. Qc6+ Kf8 22. Kb1 Rb8 23. Nb5 Qe5 24. Nxd6 Qxb2# 1-0\n\n1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Ne5 7. Bf4 Nfd7 8. Bxe5 Nxe5 9. Bxc4 Bxf3 10. gxf3 Nxc4 11. Qa4+ Qd7 12. Qxc4 O-O-O 13. O-O-O e6 14. dxe6 Qxe6 15. Rxd8+ Kxd8 16. Rd1+ Kc8 17. Qd4 Bd6 18. Qxa7 Rd8 19. Qa8+ Kd7 20. Qxb7 Ke8 21. Qc6+ Kf8 22. Kb1 Rb8 23. Nb5 Qe5 24. Nxd6 Qxb2# 1-0 d5 dxc4 Nf3 Nc6 Nc3 Nf6 e4 Bg4 d5 Ne5 Bf4 Nfd7 Bxe5 Nxe5 Bxc4 Bxf3 gxf3 Nxc4 Qa4+ Qd7 Qxc4 O-O-O O-O-O e6 dxe6 Qxe6 Rxd8+ Kxd8 Rd1+ Kc8 Qd4 Bd6 Qxa7 Rd8 Qa8+ Kd7 Qxb7 Ke8 Qc6+ Kf8 Kb1 Rb8 Nb5 Qe5 Nxd6 Qxb2# 1-0",
          "neutral_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 522,
            "total_tokens": 671,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Ne5",
            "Bf4",
            "Nfd7",
            "Bxe5",
            "Nxe5",
            "Bxc4",
            "Bxf3",
            "gxf3",
            "Nxc4",
            "Qa4+",
            "Qd7",
            "Qxc4",
            "O-O-O",
            "O-O-O",
            "e6",
            "dxe6",
            "Qxe6",
            "Rxd8+",
            "Kxd8",
            "Rd1+",
            "Kc8",
            "Qd4",
            "Bd6",
            "Qxa7",
            "Rd8",
            "Qa8+",
            "Kd7",
            "Qxb7",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Kb1",
            "Rb8",
            "Nb5",
            "Qe5",
            "Nxd6",
            "Qxb2#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d4d5",
          "positional_uci": "d4d5",
          "neutral_uci": "d4d5",
          "consensus_move": "d4d5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Ne5",
              "Bf4",
              "Nfd7",
              "Bxe5",
              "Nxe5",
              "Bxc4",
              "Bxf3",
              "gxf3",
              "Nxc4",
              "Qa4+",
              "Qd7",
              "Qxc4",
              "O-O-O",
              "O-O-O",
              "Kb8",
              "Kb1",
              "g6",
              "Rc1",
              "Bg7",
              "Nb5",
              "Rc8",
              "Qc5",
              "b6",
              "Qc4",
              "e6",
              "Rhd1",
              "exd5",
              "Rxd5",
              "Qe7",
              "Qa4",
              "c6",
              "Rxc6",
              "Rxc6",
              "Nd4",
              "Bxd4",
              "Qxc6",
              "Rd8",
              "Rxd8+",
              "Qxd8",
              "Qc4",
              "Qd7",
              "Qb4",
              "Bc5",
              "Qc3",
              "Qd1+",
              "Qc1",
              "Qxc1+",
              "Kxc1",
              "Bxf2",
              "Kd2",
              "Kc7",
              "Kd3",
              "Kd6",
              "Kc4",
              "Ke5",
              "Kb5",
              "Kf4",
              "Ka6",
              "Kxf3",
              "Kxa7",
              "Kxe4",
              "Ka6",
              "f5",
              "Kb5",
              "f4",
              "Kc4",
              "f3",
              "Kc3",
              "Be3",
              "Kc2",
              "f2",
              "Kd1",
              "f1=Q+",
              "Kc2",
              "Qd3#",
              "f2",
              "Qd2+",
              "Kb1",
              "Qc1#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Ne5",
              "Bf4",
              "Nfd7",
              "Bxe5",
              "Nxe5",
              "Bxc4",
              "Bxf3",
              "gxf3",
              "Nxc4",
              "Qa4+",
              "Qd7",
              "Qxc4",
              "O-O-O",
              "O-O-O",
              "e6",
              "dxe6",
              "Qxe6",
              "Rxd8+",
              "Kxd8",
              "Rd1+",
              "Bd6",
              "Qd4",
              "Kc8",
              "Qxa7",
              "Rd8",
              "Qa8+",
              "Kd7",
              "Qxb7",
              "Ke8",
              "Nd5",
              "Qh6+",
              "Kb1",
              "Qxh2",
              "Nxc7+",
              "Bxc7",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Ne5",
              "Bf4",
              "Nfd7",
              "Bxe5",
              "Nxe5",
              "Bxc4",
              "Bxf3",
              "gxf3",
              "Nxc4",
              "Qa4+",
              "Qd7",
              "Qxc4",
              "O-O-O",
              "O-O-O",
              "e6",
              "dxe6",
              "Qxe6",
              "Rxd8+",
              "Kxd8",
              "Rd1+",
              "Kc8",
              "Qd4",
              "Bd6",
              "Qxa7",
              "Rd8",
              "Qa8+",
              "Kd7",
              "Qxb7",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Kb1",
              "Rb8",
              "Nb5",
              "Qe5",
              "Nxd6",
              "Qxb2#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 150,
            "completion_tokens": 338,
            "total_tokens": 488,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 150,
            "completion_tokens": 640,
            "total_tokens": 790,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 149,
            "completion_tokens": 522,
            "total_tokens": 671,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 449,
          "total_completion_tokens": 1500,
          "total_tokens": 1949
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 150,
            "completion_tokens": 338,
            "total_tokens": 488,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 150,
            "completion_tokens": 640,
            "total_tokens": 790,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 149,
            "completion_tokens": 522,
            "total_tokens": 671,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/2n2n2/8/2pPP1b1/2N2N2/PP3PPP/R1BQKB1R w KQkq - 1 6"
    },
    {
      "turn": 13,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qa4+",
          "aggressive_uci": "d1a4",
          "aggressive_response": "7. Qa4+ c6 8. b4 cxb3 9. axb3 Bxf3 10. gxf3 b6 11. dxc6 Qc7 12. Nd5 Nxd5 13. exd5 e6 14. Bf4 Bd6 15. Bxd6 Qxd6 16. c7+ Ke7 17. Rc1 Rhc8 18. Ba6 Rxc7 19. Rxc7+ Qxc7 20. O-O exd5 21. Re1+ Kf8 22. Qb4+ Kg8 23. Re7 Qc1+ 24. Kg2 Qg5+ 25. Kf1 Nc6 26. Qd6 Nxe7 27. Bb7 Re8 28. Qd7 Kf8 29. Bc6 Nxc6 30. Qxc6 Qe5 31. Kg2 Qg5+ 32. Kf1 Qe5 33. Kg2 Qg5+ 34. Kf1 Qe5 35. Kg2 Qg5+ 36. Kf1 Qe5 37. Kg2 Qg5+ 38. Kf1 Qe5 39. Kg2 Qg5+ 40. Kf1 Qe5 41. Kg2 Qg5+ 42. Kf1 Qe5 43. Kg2 Qg5+ 44. Kf1 Qe5 45. Kg2 Qg5+ 46. Kf1 Qe5 47. Kg2 Qg5+ 48. Kf1 Qe5 49. Kg2 Qg5+ 50. Kf1 Qe5 51. Kg2 Qg5+ 52. Kf1 Qe5 53. Kg2 Qg5+ 54. Kf1 Qe5 55. Kg2 Qg5+ 56. Kf1 Qe5 57. Kg2 Qg5+ 58. Kf1 Qe5 59. Kg2 Qg5+ 60. Kf1 Qe5 61. Kg2 Qg5+ 62. Kf1 Qe5 63. Kg2 Qg5+ 64. Kf1 Qe5 65. Kg2 Qg5+ 66. Kf1 Qe5 67. Kg2 Qg5+ 68. Kf1 Qe5 69. Kg2 Qg5+ 70. Kf1 Qe5 71. Kg2 Qg5+ 72. Kf1 Qe5 73. Kg2 Qg5+ 74. Kf1 Qe5 75. Kg2 Qg5+ 76. Kf1 Qe5 77. Kg2 Qg5+ 78. Kf1 Qe5",
          "aggressive_tokens": {
            "prompt_tokens": 157,
            "completion_tokens": 640,
            "total_tokens": 797,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "c6",
            "b4",
            "cxb3",
            "axb3",
            "Bxf3",
            "gxf3",
            "b6",
            "dxc6",
            "Qc7",
            "Nd5",
            "Nxd5",
            "exd5",
            "e6",
            "Bf4",
            "Bd6",
            "Bxd6",
            "Qxd6",
            "c7+",
            "Ke7",
            "Rc1",
            "Rhc8",
            "Ba6",
            "Rxc7",
            "Rxc7+",
            "Qxc7",
            "O-O",
            "exd5",
            "Re1+",
            "Kf8",
            "Qb4+",
            "Kg8",
            "Re7",
            "Qc1+",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Nc6",
            "Qd6",
            "Nxe7",
            "Bb7",
            "Re8",
            "Qd7",
            "Kf8",
            "Bc6",
            "Nxc6",
            "Qxc6",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qa4+",
          "positional_uci": "d1a4",
          "positional_response": "7. Qa4+ c6 8. b4 cxb3 9. axb3 Bxf3 10. gxf3 b6 11. dxc6 Qc7 12. Nd5 Nxd5 13. exd5 e6 14. Bf4 Bd6 15. Bxd6 Qxd6 16. c7+ Ke7 17. Rc1 Rhc8 18. Ba6 Rxc7 19. Rxc7+ Qxc7 20. O-O Rd8 21. dxe6 fxe6 22. Qh4+ Kf8 23. Qxh7 Nxb3 24. Qh8+ Kf7 25. Qh5+ Kg8 26. Re1 Nc5 27. Bc4 Qf7 28. Qg4 Rd6 29. Kh1 a5 30. Rg1 a4 31. h4 a3 32. h5 Nd3 33. Bxd3 Rxd3 34. h6 Qxf3+ 35. Qxf3 Rxf3 36. Rxg7+ Kh8 37. Kg2 Rf6 38. Ra7 Rxh6 39. Rxa3 Kg7 40. Rb3 e5 41. Kf3 Kf7 42. Ke4 Re6 43. Rb5 Kf6 44. f3 Kf7 45. Rxe5 Rxe5+ 46. Kxe5 Ke7 47. Kd5 Kf6 48. Kc6 Kf5 49. Kxb6 Kf4 50. Kc5 Kxf3 51.  Kd4 Kf4 52. Kd3 Kf3 53. Kd2 Kf2 54. Kd1 Kf1 55. Kd2 Kf2 56. Kd3 Kf3 57. Kd4 Kf4 58. Kd5 Kf5 59. Kd6 Kf6 60. Kd7 Kf7 61. Kd8 Kf8 62. Kd7 Kf7 63. Kd6 Kf6 64. Kd5 Kf5 65. Kd4 Kf4 66. Kd3 Kf3 67. Kd2 Kf2 68. Kd1 Kf1 69. Kd2 Kf2 70. Kd3 Kf3 71. Kd4 Kf4 72. Kd5 Kf5 73. Kd6 Kf6 74. Kd7 Kf7 75. Kd8 Kf8 76. Kd7 Kf7 77. Kd6 Kf6 78. Kd5 Kf5 79. Kd4 Kf4 80. K",
          "positional_tokens": {
            "prompt_tokens": 157,
            "completion_tokens": 640,
            "total_tokens": 797,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "c6",
            "b4",
            "cxb3",
            "axb3",
            "Bxf3",
            "gxf3",
            "b6",
            "dxc6",
            "Qc7",
            "Nd5",
            "Nxd5",
            "exd5",
            "e6",
            "Bf4",
            "Bd6",
            "Bxd6",
            "Qxd6",
            "c7+",
            "Ke7",
            "Rc1",
            "Rhc8",
            "Ba6",
            "Rxc7",
            "Rxc7+",
            "Qxc7",
            "O-O",
            "Rd8",
            "dxe6",
            "fxe6",
            "Qh4+",
            "Kf8",
            "Qxh7",
            "Nxb3",
            "Qh8+",
            "Kf7",
            "Qh5+",
            "Kg8",
            "Re1",
            "Nc5",
            "Bc4",
            "Qf7",
            "Qg4",
            "Rd6",
            "Kh1",
            "a5",
            "Rg1",
            "a4",
            "h4",
            "a3",
            "h5",
            "Nd3",
            "Bxd3",
            "Rxd3",
            "h6",
            "Qxf3+",
            "Qxf3",
            "Rxf3",
            "Rxg7+",
            "Kh8",
            "Kg2",
            "Rf6",
            "Ra7",
            "Rxh6",
            "Rxa3",
            "Kg7",
            "Rb3",
            "e5",
            "Kf3",
            "Kf7",
            "Ke4",
            "Re6",
            "Rb5",
            "Kf6",
            "f3",
            "Kf7",
            "Rxe5",
            "Rxe5+",
            "Kxe5",
            "Ke7",
            "Kd5",
            "Kf6",
            "Kc6",
            "Kf5",
            "Kxb6",
            "Kf4",
            "Kc5",
            "Kxf3",
            "Kd4",
            "Kf4",
            "Kd3",
            "Kf3",
            "Kd2",
            "Kf2",
            "Kd1",
            "Kf1",
            "Kd2",
            "Kf2",
            "Kd3",
            "Kf3",
            "Kd4",
            "Kf4",
            "Kd5",
            "Kf5",
            "Kd6",
            "Kf6",
            "Kd7",
            "Kf7",
            "Kd8",
            "Kf8",
            "Kd7",
            "Kf7",
            "Kd6",
            "Kf6",
            "Kd5",
            "Kf5",
            "Kd4",
            "Kf4",
            "Kd3",
            "Kf3",
            "Kd2",
            "Kf2",
            "Kd1",
            "Kf1",
            "Kd2",
            "Kf2",
            "Kd3",
            "Kf3",
            "Kd4",
            "Kf4",
            "Kd5",
            "Kf5",
            "Kd6",
            "Kf6",
            "Kd7",
            "Kf7",
            "Kd8",
            "Kf8",
            "Kd7",
            "Kf7",
            "Kd6",
            "Kf6",
            "Kd5",
            "Kf5",
            "Kd4",
            "Kf4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qa4+",
          "neutral_uci": "d1a4",
          "neutral_response": "7. Qa4+ c6 8. b4 cxb3 9. axb3 Bxf3 10. gxf3 b6 11. b4 Nb7 12. dxc6 Nd6 13. c7+ Qd7 14. Nb5 Nc8 15. Bf4 e6 16. Rd1 Qe7 17. Nd6# cxd6 18. Bb5+ Nd7 19. e5 d5 20. Bc6 Qxb4+ 21. Qxb4 Bxb4+ 22. Ke2 Ke7 23. Bxa8 Nc5 24. Bc6 a5 25. Rhg1 g6 26. Bg5+ Kf8 27. Bf6 Rg8 28. Rg4 Na7 29. Bxd5 exd5 30. Rxd5 Ne6 31. Rd8+ Nxd8 32. cxd8=Q# Rxd8 33. Bxd8 Nc6 34. Bxb6 Nxe5 35. Re4 Nd7 36. Bd4 f5 37. Re6 Kf7 38. Ra6 Ke7 39. Ra7 Ke6 40. Kd3 h5 41. Kc4 Ne5+ 42. Bxe5 Kxe5 43. Ra6 g5 44. Rh6 h4 45. Rg6 Kf4 46. Rf6 Be1 47. Kd3 Bxf2 48. Ke2 Bg1 49. Ra6 Bxh2 50. Rxa5 Bg1 51. Ra4+ Kg3 52. Ra5 Kf4 53. Ra4+ Ke5 54. Ra5+ Kf6 55. Ra6+ Kg7 56. Kf1 Bd4 57. Kg2 Be5 58. Kh3 Bf6 59. Ra5 Kg6 60. Ra6 Kf7 61. Ra7+ Ke6 62. Ra6+ Ke5 63. Ra5+ Kf4 64. Ra6 Be5 65. Ra3 Bd4 66. Ra4 Ke3 67. Ra3+ Kf2 68. Ra2+ Kxf3 69. Ra3+ Be3 70. Ra5 Ke4 71. Ra4+ Bd4 72. Ra5 Bf2 73. Ra4+ Kf3 74. Ra3+ Be3 75. Ra5 g4+ 76. Kxh4 f4 77. Ra3 g3 78. Kh3 g2 79. Ra1 g1=Q 80. Rxg1 Bxg1 81. Kh4 Kg2 82. Kg4 f3 83. Kf4 f2 84. Ke3 f1",
          "neutral_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "c6",
            "b4",
            "cxb3",
            "axb3",
            "Bxf3",
            "gxf3",
            "b6",
            "b4",
            "Nb7",
            "dxc6",
            "Nd6",
            "c7+",
            "Qd7",
            "Nb5",
            "Nc8",
            "Bf4",
            "e6",
            "Rd1",
            "Qe7",
            "Nd6#",
            "cxd6",
            "Bb5+",
            "Nd7",
            "e5",
            "d5",
            "Bc6",
            "Qxb4+",
            "Qxb4",
            "Bxb4+",
            "Ke2",
            "Ke7",
            "Bxa8",
            "Nc5",
            "Bc6",
            "a5",
            "Rhg1",
            "g6",
            "Bg5+",
            "Kf8",
            "Bf6",
            "Rg8",
            "Rg4",
            "Na7",
            "Bxd5",
            "exd5",
            "Rxd5",
            "Ne6",
            "Rd8+",
            "Nxd8",
            "cxd8=Q#",
            "Rxd8",
            "Bxd8",
            "Nc6",
            "Bxb6",
            "Nxe5",
            "Re4",
            "Nd7",
            "Bd4",
            "f5",
            "Re6",
            "Kf7",
            "Ra6",
            "Ke7",
            "Ra7",
            "Ke6",
            "Kd3",
            "h5",
            "Kc4",
            "Ne5+",
            "Bxe5",
            "Kxe5",
            "Ra6",
            "g5",
            "Rh6",
            "h4",
            "Rg6",
            "Kf4",
            "Rf6",
            "Be1",
            "Kd3",
            "Bxf2",
            "Ke2",
            "Bg1",
            "Ra6",
            "Bxh2",
            "Rxa5",
            "Bg1",
            "Ra4+",
            "Kg3",
            "Ra5",
            "Kf4",
            "Ra4+",
            "Ke5",
            "Ra5+",
            "Kf6",
            "Ra6+",
            "Kg7",
            "Kf1",
            "Bd4",
            "Kg2",
            "Be5",
            "Kh3",
            "Bf6",
            "Ra5",
            "Kg6",
            "Ra6",
            "Kf7",
            "Ra7+",
            "Ke6",
            "Ra6+",
            "Ke5",
            "Ra5+",
            "Kf4",
            "Ra6",
            "Be5",
            "Ra3",
            "Bd4",
            "Ra4",
            "Ke3",
            "Ra3+",
            "Kf2",
            "Ra2+",
            "Kxf3",
            "Ra3+",
            "Be3",
            "Ra5",
            "Ke4",
            "Ra4+",
            "Bd4",
            "Ra5",
            "Bf2",
            "Ra4+",
            "Kf3",
            "Ra3+",
            "Be3",
            "Ra5",
            "g4+",
            "Kxh4",
            "f4",
            "Ra3",
            "g3",
            "Kh3",
            "g2",
            "Ra1",
            "g1=Q",
            "Rxg1",
            "Bxg1",
            "Kh4",
            "Kg2",
            "Kg4",
            "f3",
            "Kf4",
            "f2",
            "Ke3",
            "f1"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d1a4",
          "positional_uci": "d1a4",
          "neutral_uci": "d1a4",
          "consensus_move": "d1a4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "c6",
              "b4",
              "cxb3",
              "axb3",
              "Bxf3",
              "gxf3",
              "b6",
              "dxc6",
              "Qc7",
              "Nd5",
              "Nxd5",
              "exd5",
              "e6",
              "Bf4",
              "Bd6",
              "Bxd6",
              "Qxd6",
              "c7+",
              "Ke7",
              "Rc1",
              "Rhc8",
              "Ba6",
              "Rxc7",
              "Rxc7+",
              "Qxc7",
              "O-O",
              "exd5",
              "Re1+",
              "Kf8",
              "Qb4+",
              "Kg8",
              "Re7",
              "Qc1+",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Nc6",
              "Qd6",
              "Nxe7",
              "Bb7",
              "Re8",
              "Qd7",
              "Kf8",
              "Bc6",
              "Nxc6",
              "Qxc6",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "c6",
              "b4",
              "cxb3",
              "axb3",
              "Bxf3",
              "gxf3",
              "b6",
              "dxc6",
              "Qc7",
              "Nd5",
              "Nxd5",
              "exd5",
              "e6",
              "Bf4",
              "Bd6",
              "Bxd6",
              "Qxd6",
              "c7+",
              "Ke7",
              "Rc1",
              "Rhc8",
              "Ba6",
              "Rxc7",
              "Rxc7+",
              "Qxc7",
              "O-O",
              "Rd8",
              "dxe6",
              "fxe6",
              "Qh4+",
              "Kf8",
              "Qxh7",
              "Nxb3",
              "Qh8+",
              "Kf7",
              "Qh5+",
              "Kg8",
              "Re1",
              "Nc5",
              "Bc4",
              "Qf7",
              "Qg4",
              "Rd6",
              "Kh1",
              "a5",
              "Rg1",
              "a4",
              "h4",
              "a3",
              "h5",
              "Nd3",
              "Bxd3",
              "Rxd3",
              "h6",
              "Qxf3+",
              "Qxf3",
              "Rxf3",
              "Rxg7+",
              "Kh8",
              "Kg2",
              "Rf6",
              "Ra7",
              "Rxh6",
              "Rxa3",
              "Kg7",
              "Rb3",
              "e5",
              "Kf3",
              "Kf7",
              "Ke4",
              "Re6",
              "Rb5",
              "Kf6",
              "f3",
              "Kf7",
              "Rxe5",
              "Rxe5+",
              "Kxe5",
              "Ke7",
              "Kd5",
              "Kf6",
              "Kc6",
              "Kf5",
              "Kxb6",
              "Kf4",
              "Kc5",
              "Kxf3",
              "Kd4",
              "Kf4",
              "Kd3",
              "Kf3",
              "Kd2",
              "Kf2",
              "Kd1",
              "Kf1",
              "Kd2",
              "Kf2",
              "Kd3",
              "Kf3",
              "Kd4",
              "Kf4",
              "Kd5",
              "Kf5",
              "Kd6",
              "Kf6",
              "Kd7",
              "Kf7",
              "Kd8",
              "Kf8",
              "Kd7",
              "Kf7",
              "Kd6",
              "Kf6",
              "Kd5",
              "Kf5",
              "Kd4",
              "Kf4",
              "Kd3",
              "Kf3",
              "Kd2",
              "Kf2",
              "Kd1",
              "Kf1",
              "Kd2",
              "Kf2",
              "Kd3",
              "Kf3",
              "Kd4",
              "Kf4",
              "Kd5",
              "Kf5",
              "Kd6",
              "Kf6",
              "Kd7",
              "Kf7",
              "Kd8",
              "Kf8",
              "Kd7",
              "Kf7",
              "Kd6",
              "Kf6",
              "Kd5",
              "Kf5",
              "Kd4",
              "Kf4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "c6",
              "b4",
              "cxb3",
              "axb3",
              "Bxf3",
              "gxf3",
              "b6",
              "b4",
              "Nb7",
              "dxc6",
              "Nd6",
              "c7+",
              "Qd7",
              "Nb5",
              "Nc8",
              "Bf4",
              "e6",
              "Rd1",
              "Qe7",
              "Nd6#",
              "cxd6",
              "Bb5+",
              "Nd7",
              "e5",
              "d5",
              "Bc6",
              "Qxb4+",
              "Qxb4",
              "Bxb4+",
              "Ke2",
              "Ke7",
              "Bxa8",
              "Nc5",
              "Bc6",
              "a5",
              "Rhg1",
              "g6",
              "Bg5+",
              "Kf8",
              "Bf6",
              "Rg8",
              "Rg4",
              "Na7",
              "Bxd5",
              "exd5",
              "Rxd5",
              "Ne6",
              "Rd8+",
              "Nxd8",
              "cxd8=Q#",
              "Rxd8",
              "Bxd8",
              "Nc6",
              "Bxb6",
              "Nxe5",
              "Re4",
              "Nd7",
              "Bd4",
              "f5",
              "Re6",
              "Kf7",
              "Ra6",
              "Ke7",
              "Ra7",
              "Ke6",
              "Kd3",
              "h5",
              "Kc4",
              "Ne5+",
              "Bxe5",
              "Kxe5",
              "Ra6",
              "g5",
              "Rh6",
              "h4",
              "Rg6",
              "Kf4",
              "Rf6",
              "Be1",
              "Kd3",
              "Bxf2",
              "Ke2",
              "Bg1",
              "Ra6",
              "Bxh2",
              "Rxa5",
              "Bg1",
              "Ra4+",
              "Kg3",
              "Ra5",
              "Kf4",
              "Ra4+",
              "Ke5",
              "Ra5+",
              "Kf6",
              "Ra6+",
              "Kg7",
              "Kf1",
              "Bd4",
              "Kg2",
              "Be5",
              "Kh3",
              "Bf6",
              "Ra5",
              "Kg6",
              "Ra6",
              "Kf7",
              "Ra7+",
              "Ke6",
              "Ra6+",
              "Ke5",
              "Ra5+",
              "Kf4",
              "Ra6",
              "Be5",
              "Ra3",
              "Bd4",
              "Ra4",
              "Ke3",
              "Ra3+",
              "Kf2",
              "Ra2+",
              "Kxf3",
              "Ra3+",
              "Be3",
              "Ra5",
              "Ke4",
              "Ra4+",
              "Bd4",
              "Ra5",
              "Bf2",
              "Ra4+",
              "Kf3",
              "Ra3+",
              "Be3",
              "Ra5",
              "g4+",
              "Kxh4",
              "f4",
              "Ra3",
              "g3",
              "Kh3",
              "g2",
              "Ra1",
              "g1=Q",
              "Rxg1",
              "Bxg1",
              "Kh4",
              "Kg2",
              "Kg4",
              "f3",
              "Kf4",
              "f2",
              "Ke3",
              "f1"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 157,
            "completion_tokens": 640,
            "total_tokens": 797,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 157,
            "completion_tokens": 640,
            "total_tokens": 797,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 470,
          "total_completion_tokens": 1920,
          "total_tokens": 2390
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 157,
            "completion_tokens": 640,
            "total_tokens": 797,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 157,
            "completion_tokens": 640,
            "total_tokens": 797,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/5n2/n2P4/2p1P1b1/2N2N2/PP3PPP/R1BQKB1R w KQkq - 1 7"
    },
    {
      "turn": 15,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "dxc6",
          "aggressive_uci": "d5c6",
          "aggressive_response": "8. dxc6 bxc6 9. Qxc6+ Nd7 10. Ne5 Be6 11. Bxc4 Bxc4 12. Nxc4 e6 13. Nb5 Bb4+ 14. Bd2 Bxd2+ 15. Nxd2 O-O 16. Qxc7 Qxc7 17. Nxc7 Rac8 18. Nb5 Rc2 19. b3 a6 20. Nd4 Rb2 21. Nc4 *",
          "aggressive_tokens": {
            "prompt_tokens": 167,
            "completion_tokens": 114,
            "total_tokens": 281,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "bxc6",
            "Qxc6+",
            "Nd7",
            "Ne5",
            "Be6",
            "Bxc4",
            "Bxc4",
            "Nxc4",
            "e6",
            "Nb5",
            "Bb4+",
            "Bd2",
            "Bxd2+",
            "Nxd2",
            "O-O",
            "Qxc7",
            "Qxc7",
            "Nxc7",
            "Rac8",
            "Nb5",
            "Rc2",
            "b3",
            "a6",
            "Nd4",
            "Rb2",
            "Nc4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "dxc6",
          "positional_uci": "d5c6",
          "positional_response": "8. dxc6 bxc6 9. Qxc6+ Nd7 10. Ne5 Be6 11. Bxc4 Bxc4 12. Nxc4 e6 13. Nb5 Bb4+ 14. Bd2 Bxd2+ 15. Nxd2 O-O 16. Qxc7 Qxc7 17. Nxc7 Rac8 18. Nb5 Rc2 19. b3 a6 20. Nd4 Rc5 21. O-O Rfc8 22. Nc4 Ne5 23. Nxe5 Rxe5 24. f3 Rec5 25. Rfd1 e5 26. Nf5 Kf8 27. Rd5 Rc1+ 28. Rxc1 Rxc1+ 29. Kf2 Rc2+ 30. Kg3 Rxa2 31. Rd8# g6 32. Nd6+ Ke7 33. Nb7 Rb2 34. Rd3 f5 35. exf5 gxf5 36. Nc5 f4+ 37. Kh3 a5 38. Rd7+ Kf6 39. Rxh7 Rc2 40. Ne4+ Kg6 41. Ra7 Ra2 42. b4 a4 43. b5 a3 44. b6 Rb2 45. b7 a2 46. Nc5 Kf5 47. Rxa2 Rb6 48. Ra8 Rh6# Rg6 50. b8=Q Rh6# Rg6 52. Qf8+ Kg5 53. Ne4+ Kh5 54. Qh8+ Rh6 55. Qxe5+ Kg6+ 56. Kg4 Rxh2 57. Qf6+ Kh7 58. Rh8# Kg6 60. Qf6# Kh5 61. Qg5#",
          "positional_tokens": {
            "prompt_tokens": 167,
            "completion_tokens": 426,
            "total_tokens": 593,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "bxc6",
            "Qxc6+",
            "Nd7",
            "Ne5",
            "Be6",
            "Bxc4",
            "Bxc4",
            "Nxc4",
            "e6",
            "Nb5",
            "Bb4+",
            "Bd2",
            "Bxd2+",
            "Nxd2",
            "O-O",
            "Qxc7",
            "Qxc7",
            "Nxc7",
            "Rac8",
            "Nb5",
            "Rc2",
            "b3",
            "a6",
            "Nd4",
            "Rc5",
            "O-O",
            "Rfc8",
            "Nc4",
            "Ne5",
            "Nxe5",
            "Rxe5",
            "f3",
            "Rec5",
            "Rfd1",
            "e5",
            "Nf5",
            "Kf8",
            "Rd5",
            "Rc1+",
            "Rxc1",
            "Rxc1+",
            "Kf2",
            "Rc2+",
            "Kg3",
            "Rxa2",
            "Rd8#",
            "g6",
            "Nd6+",
            "Ke7",
            "Nb7",
            "Rb2",
            "Rd3",
            "f5",
            "exf5",
            "gxf5",
            "Nc5",
            "f4+",
            "Kh3",
            "a5",
            "Rd7+",
            "Kf6",
            "Rxh7",
            "Rc2",
            "Ne4+",
            "Kg6",
            "Ra7",
            "Ra2",
            "b4",
            "a4",
            "b5",
            "a3",
            "b6",
            "Rb2",
            "b7",
            "a2",
            "Nc5",
            "Kf5",
            "Rxa2",
            "Rb6",
            "Ra8",
            "Rh6#",
            "Rg6",
            "b8=Q",
            "Rh6#",
            "Rg6",
            "Qf8+",
            "Kg5",
            "Ne4+",
            "Kh5",
            "Qh8+",
            "Rh6",
            "Qxe5+",
            "Kg6+",
            "Kg4",
            "Rxh2",
            "Qf6+",
            "Kh7",
            "Rh8#",
            "Kg6",
            "Qf6#",
            "Kh5",
            "Qg5#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "dxc6",
          "neutral_uci": "d5c6",
          "neutral_response": "8. dxc6 bxc6 9. Qxc6+ Nd7 10. Ne5 Be6 11. Bxc4 Bxc4 12. Nxc4 e6 13. Nb5 Bb4+ 14. Bd2 Bxd2+ 15. Nxd2 O-O 16. Qxc7 Qxc7 17. Nxc7 Rac8 18. Nb5 Rc2 19. b3 a6 20. Nd4 Rb2 21. Nc4 Rxf2 22. Kxf2 Nc5 23. Ke3 Rd8 24. Rhd1 f6 25. Nc6 Rxd1 26. Rxd1 Kf7 27. b4 Na4 28. Rd7+ Kg6 29. Ne7+ Kh6 30. Ng8+ Kg6 31. Nd6 Nc3 32. Ne8 Nxa2 33. Rxg7+ Kh5 34. Nexf6+ Kh4 35. Rg4# Nc3 36. Kf3 h5 37. g3+ Kh3 38. Rh4# Kg1 39. Nxh5 Kf1 40. Nf4 Ke1 41. Nxe6 Kd2 42. Nc5 Kc2 43. Nxa6 Kb3 44. e5 Nd5 45. e6 Kc3 46. b5 Kd3 47. Nb4+ Nxb4 48. Rxb4 Kc3 49. Re4 Kd3 50. e7 Kc3 51. e8=Q Kd3 52. Qd7+ Kc3 53. Qd4+ Kc2 54. Re2+ Kb3 55. Rb2+ Ka3 56. Qb4# Kg1 57. Rb1+ Kxh2 58. Qh4# Kg1 59. Qh1# Kh2 60. Qg2# Kg1 61. Qg1# Kh2 62. Qh1# Kg1 63. Qh1# Kh2 64. Qg2# Kg1 65. Qg1# Kh2 66. Qh1# Kg1 67. Qh1# Kh2 68. Qg2# Kg1 69. Qg1# Kh2 70. Qh1# Kg1 71. Qh1# Kh2 72. Qg2# Kg1 73. Qg1# Kh2 74. Qh1# Kg1 75. Qh1# Kh2 76. Qg2# Kg1 77. Qg1# Kh2 78. Qh1# Kg1 79. Qh1# Kh2 80. Qg2# Kg1 81. Qg1# Kh2",
          "neutral_tokens": {
            "prompt_tokens": 166,
            "completion_tokens": 640,
            "total_tokens": 806,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "bxc6",
            "Qxc6+",
            "Nd7",
            "Ne5",
            "Be6",
            "Bxc4",
            "Bxc4",
            "Nxc4",
            "e6",
            "Nb5",
            "Bb4+",
            "Bd2",
            "Bxd2+",
            "Nxd2",
            "O-O",
            "Qxc7",
            "Qxc7",
            "Nxc7",
            "Rac8",
            "Nb5",
            "Rc2",
            "b3",
            "a6",
            "Nd4",
            "Rb2",
            "Nc4",
            "Rxf2",
            "Kxf2",
            "Nc5",
            "Ke3",
            "Rd8",
            "Rhd1",
            "f6",
            "Nc6",
            "Rxd1",
            "Rxd1",
            "Kf7",
            "b4",
            "Na4",
            "Rd7+",
            "Kg6",
            "Ne7+",
            "Kh6",
            "Ng8+",
            "Kg6",
            "Nd6",
            "Nc3",
            "Ne8",
            "Nxa2",
            "Rxg7+",
            "Kh5",
            "Nexf6+",
            "Kh4",
            "Rg4#",
            "Nc3",
            "Kf3",
            "h5",
            "g3+",
            "Kh3",
            "Rh4#",
            "Kg1",
            "Nxh5",
            "Kf1",
            "Nf4",
            "Ke1",
            "Nxe6",
            "Kd2",
            "Nc5",
            "Kc2",
            "Nxa6",
            "Kb3",
            "e5",
            "Nd5",
            "e6",
            "Kc3",
            "b5",
            "Kd3",
            "Nb4+",
            "Nxb4",
            "Rxb4",
            "Kc3",
            "Re4",
            "Kd3",
            "e7",
            "Kc3",
            "e8=Q",
            "Kd3",
            "Qd7+",
            "Kc3",
            "Qd4+",
            "Kc2",
            "Re2+",
            "Kb3",
            "Rb2+",
            "Ka3",
            "Qb4#",
            "Kg1",
            "Rb1+",
            "Kxh2",
            "Qh4#",
            "Kg1",
            "Qh1#",
            "Kh2",
            "Qg2#",
            "Kg1",
            "Qg1#",
            "Kh2",
            "Qh1#",
            "Kg1",
            "Qh1#",
            "Kh2",
            "Qg2#",
            "Kg1",
            "Qg1#",
            "Kh2",
            "Qh1#",
            "Kg1",
            "Qh1#",
            "Kh2",
            "Qg2#",
            "Kg1",
            "Qg1#",
            "Kh2",
            "Qh1#",
            "Kg1",
            "Qh1#",
            "Kh2",
            "Qg2#",
            "Kg1",
            "Qg1#",
            "Kh2",
            "Qh1#",
            "Kg1",
            "Qh1#",
            "Kh2",
            "Qg2#",
            "Kg1",
            "Qg1#",
            "Kh2",
            "Qh1#",
            "Kg1",
            "Qh1#",
            "Kh2",
            "Qg2#",
            "Kg1",
            "Qg1#",
            "Kh2"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d5c6",
          "positional_uci": "d5c6",
          "neutral_uci": "d5c6",
          "consensus_move": "d5c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "bxc6",
              "Qxc6+",
              "Nd7",
              "Ne5",
              "Be6",
              "Bxc4",
              "Bxc4",
              "Nxc4",
              "e6",
              "Nb5",
              "Bb4+",
              "Bd2",
              "Bxd2+",
              "Nxd2",
              "O-O",
              "Qxc7",
              "Qxc7",
              "Nxc7",
              "Rac8",
              "Nb5",
              "Rc2",
              "b3",
              "a6",
              "Nd4",
              "Rb2",
              "Nc4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "bxc6",
              "Qxc6+",
              "Nd7",
              "Ne5",
              "Be6",
              "Bxc4",
              "Bxc4",
              "Nxc4",
              "e6",
              "Nb5",
              "Bb4+",
              "Bd2",
              "Bxd2+",
              "Nxd2",
              "O-O",
              "Qxc7",
              "Qxc7",
              "Nxc7",
              "Rac8",
              "Nb5",
              "Rc2",
              "b3",
              "a6",
              "Nd4",
              "Rc5",
              "O-O",
              "Rfc8",
              "Nc4",
              "Ne5",
              "Nxe5",
              "Rxe5",
              "f3",
              "Rec5",
              "Rfd1",
              "e5",
              "Nf5",
              "Kf8",
              "Rd5",
              "Rc1+",
              "Rxc1",
              "Rxc1+",
              "Kf2",
              "Rc2+",
              "Kg3",
              "Rxa2",
              "Rd8#",
              "g6",
              "Nd6+",
              "Ke7",
              "Nb7",
              "Rb2",
              "Rd3",
              "f5",
              "exf5",
              "gxf5",
              "Nc5",
              "f4+",
              "Kh3",
              "a5",
              "Rd7+",
              "Kf6",
              "Rxh7",
              "Rc2",
              "Ne4+",
              "Kg6",
              "Ra7",
              "Ra2",
              "b4",
              "a4",
              "b5",
              "a3",
              "b6",
              "Rb2",
              "b7",
              "a2",
              "Nc5",
              "Kf5",
              "Rxa2",
              "Rb6",
              "Ra8",
              "Rh6#",
              "Rg6",
              "b8=Q",
              "Rh6#",
              "Rg6",
              "Qf8+",
              "Kg5",
              "Ne4+",
              "Kh5",
              "Qh8+",
              "Rh6",
              "Qxe5+",
              "Kg6+",
              "Kg4",
              "Rxh2",
              "Qf6+",
              "Kh7",
              "Rh8#",
              "Kg6",
              "Qf6#",
              "Kh5",
              "Qg5#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "bxc6",
              "Qxc6+",
              "Nd7",
              "Ne5",
              "Be6",
              "Bxc4",
              "Bxc4",
              "Nxc4",
              "e6",
              "Nb5",
              "Bb4+",
              "Bd2",
              "Bxd2+",
              "Nxd2",
              "O-O",
              "Qxc7",
              "Qxc7",
              "Nxc7",
              "Rac8",
              "Nb5",
              "Rc2",
              "b3",
              "a6",
              "Nd4",
              "Rb2",
              "Nc4",
              "Rxf2",
              "Kxf2",
              "Nc5",
              "Ke3",
              "Rd8",
              "Rhd1",
              "f6",
              "Nc6",
              "Rxd1",
              "Rxd1",
              "Kf7",
              "b4",
              "Na4",
              "Rd7+",
              "Kg6",
              "Ne7+",
              "Kh6",
              "Ng8+",
              "Kg6",
              "Nd6",
              "Nc3",
              "Ne8",
              "Nxa2",
              "Rxg7+",
              "Kh5",
              "Nexf6+",
              "Kh4",
              "Rg4#",
              "Nc3",
              "Kf3",
              "h5",
              "g3+",
              "Kh3",
              "Rh4#",
              "Kg1",
              "Nxh5",
              "Kf1",
              "Nf4",
              "Ke1",
              "Nxe6",
              "Kd2",
              "Nc5",
              "Kc2",
              "Nxa6",
              "Kb3",
              "e5",
              "Nd5",
              "e6",
              "Kc3",
              "b5",
              "Kd3",
              "Nb4+",
              "Nxb4",
              "Rxb4",
              "Kc3",
              "Re4",
              "Kd3",
              "e7",
              "Kc3",
              "e8=Q",
              "Kd3",
              "Qd7+",
              "Kc3",
              "Qd4+",
              "Kc2",
              "Re2+",
              "Kb3",
              "Rb2+",
              "Ka3",
              "Qb4#",
              "Kg1",
              "Rb1+",
              "Kxh2",
              "Qh4#",
              "Kg1",
              "Qh1#",
              "Kh2",
              "Qg2#",
              "Kg1",
              "Qg1#",
              "Kh2",
              "Qh1#",
              "Kg1",
              "Qh1#",
              "Kh2",
              "Qg2#",
              "Kg1",
              "Qg1#",
              "Kh2",
              "Qh1#",
              "Kg1",
              "Qh1#",
              "Kh2",
              "Qg2#",
              "Kg1",
              "Qg1#",
              "Kh2",
              "Qh1#",
              "Kg1",
              "Qh1#",
              "Kh2",
              "Qg2#",
              "Kg1",
              "Qg1#",
              "Kh2",
              "Qh1#",
              "Kg1",
              "Qh1#",
              "Kh2",
              "Qg2#",
              "Kg1",
              "Qg1#",
              "Kh2",
              "Qh1#",
              "Kg1",
              "Qh1#",
              "Kh2",
              "Qg2#",
              "Kg1",
              "Qg1#",
              "Kh2"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 167,
            "completion_tokens": 114,
            "total_tokens": 281,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 167,
            "completion_tokens": 426,
            "total_tokens": 593,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 166,
            "completion_tokens": 640,
            "total_tokens": 806,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 500,
          "total_completion_tokens": 1180,
          "total_tokens": 1680
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 167,
            "completion_tokens": 114,
            "total_tokens": 281,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 167,
            "completion_tokens": 426,
            "total_tokens": 593,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 166,
            "completion_tokens": 640,
            "total_tokens": 806,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/2n2n2/3P4/Q1p1P1b1/2N2N2/PP3PPP/R1B1KB1R w KQkq - 3 8"
    },
    {
      "turn": 17,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bxc4",
          "aggressive_uci": "f1c4",
          "aggressive_response": "9. Bxc4 Bxf3 10. gxf3 e6 11. Bg5 Be7 12. Rd1 Qc8 13. Ba6 Qb8 14. Bb7 O-O 15.\nBxa8 Qxa8 16. e5 Nd5 17. Nxd5 Bxg5 18. Nxc7 Qc8 19. Rd7 Rd8 20. Qxa7 Rxd7 21.\ncxd7 Qxd7 22. Qb8+ Bd8 23. O-O Qxc7 24. Qxc7 Bxc7 25. Rc1 Bd8 26. Rc8 Kf8 27.\nRxd8+ Ke7 28. Rb8 f6 29. Rb7+ Kf8 30. exf6 gxf6 31. Rxb6 Kf7 32. a4 e5 33. a5\nKg6 34. a6 Kf5 35. a7 Kf4 36. a8=Q f5 37. Rb4+ e4 38. Qb8+ Kxf3 39. Qg3+ Ke2\n40. Qe3+ Kd1 41. Rc4 f4 42. Rc1# Qxc1 43. Qxc1+ Kxc1 44. b4 Kd2 45. b5 e3\n46. fxe3 fxe3 47. b6 e2 48. b7 e1=Q+ 49. Kg2 Qe4+ 50. Kg3 Qxb7 51. h4 Qe4\n52. h5 Ke3 53. h6 Qf4+ 54. Kh3 Kf3 55. b4 Qg3# Qg2+ 57. Kh4 Qg4# Qg4#",
          "aggressive_tokens": {
            "prompt_tokens": 175,
            "completion_tokens": 416,
            "total_tokens": 591,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bxf3",
            "gxf3",
            "e6",
            "Bg5",
            "Be7",
            "Rd1",
            "Qc8",
            "Ba6",
            "Qb8",
            "Bb7",
            "O-O",
            "Bxa8",
            "Qxa8",
            "e5",
            "Nd5",
            "Nxd5",
            "Bxg5",
            "Nxc7",
            "Qc8",
            "Rd7",
            "Rd8",
            "Qxa7",
            "Rxd7",
            "cxd7",
            "Qxd7",
            "Qb8+",
            "Bd8",
            "O-O",
            "Qxc7",
            "Qxc7",
            "Bxc7",
            "Rc1",
            "Bd8",
            "Rc8",
            "Kf8",
            "Rxd8+",
            "Ke7",
            "Rb8",
            "f6",
            "Rb7+",
            "Kf8",
            "exf6",
            "gxf6",
            "Rxb6",
            "Kf7",
            "a4",
            "e5",
            "a5",
            "Kg6",
            "a6",
            "Kf5",
            "a7",
            "Kf4",
            "a8=Q",
            "f5",
            "Rb4+",
            "e4",
            "Qb8+",
            "Kxf3",
            "Qg3+",
            "Ke2",
            "Qe3+",
            "Kd1",
            "Rc4",
            "f4",
            "Rc1#",
            "Qxc1",
            "Qxc1+",
            "Kxc1",
            "b4",
            "Kd2",
            "b5",
            "e3",
            "fxe3",
            "fxe3",
            "b6",
            "e2",
            "b7",
            "e1=Q+",
            "Kg2",
            "Qe4+",
            "Kg3",
            "Qxb7",
            "h4",
            "Qe4",
            "h5",
            "Ke3",
            "h6",
            "Qf4+",
            "Kh3",
            "Kf3",
            "b4",
            "Qg3#",
            "Qg2+",
            "Kh4",
            "Qg4#",
            "Qg4#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bxc4",
          "positional_uci": "f1c4",
          "positional_response": "9. Bxc4 e6 10. Ne5 Bd6 11. Nxg4 Nxg4 12. h3 Ne5 13. O-O O-O 14. Be3 Qh4 15.\nBe2 f5 16. exf5 Qxa4 17. Nxa4 exf5 18. Rac1 f4 19. Bd4 f3 20. Bxf3 Nxf3+ 21.\ngxf3 Rxf3 22. Kg2 Raf8 23. Rc3 R3f4 24. Rd3 R8f5 25. Nc3 Rg5+ 26. Kh1 Rh4 27.\nNe2 Rgh5 28. Kg2 Rg5+ 29. Ng3 Rd5 30. Rfd1 c5 31. Bc3 Rxd3 32. Rxd3 Bxg3 33.\nfxg3 Ra4 34. a3 b5 35. Rd7 b4 36. axb4 cxb4 37. Rxg7+ Kf8 38. Rg4 a5 39. Kf3\nRa1 40. Bd4 Rf1+ 41. Ke4 Re1+ 42. Kd5 Rd1 43. Kc5 Rc1+ 44. Kb5 Ra1 45. Bb6\nRb1 46. Bd4 Ra1 47. Rf4+ Ke7 48. Rf5 Ke6 49. Rh5 Rd1 50. Bb6 Rd2 51. b3 Rd3\n52. Kc4 Rxg3 53. Bxa5 Kf6 54. Rxh7 Kg6 55. Rh4 Kg5 56. Rg4+ Rxg4+ 57. hxg4\nKxg4 58. Bxb4 Kf5 59. Kd5 Kf6 60. Kd6 Kf7 61. Bc5 Ke8 62. Kc7 Kf7 63. b4 Ke6\n64. b5 Kd5 65. b6 Kxc5 66. b7 Kd5 67. b8=Q Ke6 68. Qb5 Kf6 69. Qd5 Ke7 70.\nQd6+ Kf7 71. Kd7 Kg7 72. Qe6 Kf8 73. Qe7+ Kg8 74. Ke6 Kh8 75. Kf6 Kg8 76.\nQg7# e4 e5 e6",
          "positional_tokens": {
            "prompt_tokens": 175,
            "completion_tokens": 572,
            "total_tokens": 747,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e6",
            "Ne5",
            "Bd6",
            "Nxg4",
            "Nxg4",
            "h3",
            "Ne5",
            "O-O",
            "O-O",
            "Be3",
            "Qh4",
            "Be2",
            "f5",
            "exf5",
            "Qxa4",
            "Nxa4",
            "exf5",
            "Rac1",
            "f4",
            "Bd4",
            "f3",
            "Bxf3",
            "Nxf3+",
            "gxf3",
            "Rxf3",
            "Kg2",
            "Raf8",
            "Rc3",
            "R3f4",
            "Rd3",
            "R8f5",
            "Nc3",
            "Rg5+",
            "Kh1",
            "Rh4",
            "Ne2",
            "Rgh5",
            "Kg2",
            "Rg5+",
            "Ng3",
            "Rd5",
            "Rfd1",
            "c5",
            "Bc3",
            "Rxd3",
            "Rxd3",
            "Bxg3",
            "fxg3",
            "Ra4",
            "a3",
            "b5",
            "Rd7",
            "b4",
            "axb4",
            "cxb4",
            "Rxg7+",
            "Kf8",
            "Rg4",
            "a5",
            "Kf3",
            "Ra1",
            "Bd4",
            "Rf1+",
            "Ke4",
            "Re1+",
            "Kd5",
            "Rd1",
            "Kc5",
            "Rc1+",
            "Kb5",
            "Ra1",
            "Bb6",
            "Rb1",
            "Bd4",
            "Ra1",
            "Rf4+",
            "Ke7",
            "Rf5",
            "Ke6",
            "Rh5",
            "Rd1",
            "Bb6",
            "Rd2",
            "b3",
            "Rd3",
            "Kc4",
            "Rxg3",
            "Bxa5",
            "Kf6",
            "Rxh7",
            "Kg6",
            "Rh4",
            "Kg5",
            "Rg4+",
            "Rxg4+",
            "hxg4",
            "Kxg4",
            "Bxb4",
            "Kf5",
            "Kd5",
            "Kf6",
            "Kd6",
            "Kf7",
            "Bc5",
            "Ke8",
            "Kc7",
            "Kf7",
            "b4",
            "Ke6",
            "b5",
            "Kd5",
            "b6",
            "Kxc5",
            "b7",
            "Kd5",
            "b8=Q",
            "Ke6",
            "Qb5",
            "Kf6",
            "Qd5",
            "Ke7",
            "Qd6+",
            "Kf7",
            "Kd7",
            "Kg7",
            "Qe6",
            "Kf8",
            "Qe7+",
            "Kg8",
            "Ke6",
            "Kh8",
            "Kf6",
            "Kg8",
            "Qg7#",
            "e4",
            "e5",
            "e6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bxc4",
          "neutral_uci": "f1c4",
          "neutral_response": "9. Bxc4 e6 10. Ne5 Bd6 11. Nxg4 Nxg4 12. h3 Ne5 13. O-O O-O 14. Be3 Qh4 15.\nBe2 f5 16. exf5 Qxa4 17. Nxa4 exf5 18. Nc3 f4 19. Bd4 Nxc6 20. Bc4+ Kh8 21.\nRad1 Rad8 22. Nb5 Nxd4 23. Nxd4 Be5 24. Ne6 Rxd1 25. Rxd1 Re8 26. Rd8 Rxd8\n27. Nxd8 Bxb2 28. Nc6 a5 29. Kf1 g5 30. Ke2 Kg7 31. Kd3 Kf6 32. Ke4 h5 33.\nNd4 Bxd4 34. Kxd4 Kf5 35. Bd3+ Ke6 36. Ke4 c6 37. Bc4+ Kf6 38. a4 h4 39.\nBb3 b5 40. axb5 cxb5 41. Kd5 a4 42. Ba2 b4 43. Kc4 b3 44. Bb1 Ke5 45. Kb4\nKd4 46. Kxa4 Kc3 47. Ka3 b2 48. Ka2 Kd2 49. Kxb2 Ke2 50. f3 Kf2 51. Bf5\nKxg2 52. Bg4 Kf2 53. Kc3 Ke3 54. Kc4 Kf2 55. Kd5 Kg3 56. Ke5 Kf2 57. Kf5\nKe3 58. Kxg5 Kd4 59. Kxf4 Kd5 60. Kg5 Ke5 61. Kxh4 Kf6 62. Kh5 Kg7 63. Kg5\nKh7 64. Kf6 Kh6 65. f4 Kh7 66. f5 Kh6 67. Ke7 Kg7 68. f6+ Kg8 69. f7+ Kh7\n70. f8=Q Kg6 71. Qf6+ Kh7 72. Bf5+ Kg8 73. Qf7+ Kh8 74. Qh7# e4 f3 gxf3",
          "neutral_tokens": {
            "prompt_tokens": 174,
            "completion_tokens": 545,
            "total_tokens": 719,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e6",
            "Ne5",
            "Bd6",
            "Nxg4",
            "Nxg4",
            "h3",
            "Ne5",
            "O-O",
            "O-O",
            "Be3",
            "Qh4",
            "Be2",
            "f5",
            "exf5",
            "Qxa4",
            "Nxa4",
            "exf5",
            "Nc3",
            "f4",
            "Bd4",
            "Nxc6",
            "Bc4+",
            "Kh8",
            "Rad1",
            "Rad8",
            "Nb5",
            "Nxd4",
            "Nxd4",
            "Be5",
            "Ne6",
            "Rxd1",
            "Rxd1",
            "Re8",
            "Rd8",
            "Rxd8",
            "Nxd8",
            "Bxb2",
            "Nc6",
            "a5",
            "Kf1",
            "g5",
            "Ke2",
            "Kg7",
            "Kd3",
            "Kf6",
            "Ke4",
            "h5",
            "Nd4",
            "Bxd4",
            "Kxd4",
            "Kf5",
            "Bd3+",
            "Ke6",
            "Ke4",
            "c6",
            "Bc4+",
            "Kf6",
            "a4",
            "h4",
            "Bb3",
            "b5",
            "axb5",
            "cxb5",
            "Kd5",
            "a4",
            "Ba2",
            "b4",
            "Kc4",
            "b3",
            "Bb1",
            "Ke5",
            "Kb4",
            "Kd4",
            "Kxa4",
            "Kc3",
            "Ka3",
            "b2",
            "Ka2",
            "Kd2",
            "Kxb2",
            "Ke2",
            "f3",
            "Kf2",
            "Bf5",
            "Kxg2",
            "Bg4",
            "Kf2",
            "Kc3",
            "Ke3",
            "Kc4",
            "Kf2",
            "Kd5",
            "Kg3",
            "Ke5",
            "Kf2",
            "Kf5",
            "Ke3",
            "Kxg5",
            "Kd4",
            "Kxf4",
            "Kd5",
            "Kg5",
            "Ke5",
            "Kxh4",
            "Kf6",
            "Kh5",
            "Kg7",
            "Kg5",
            "Kh7",
            "Kf6",
            "Kh6",
            "f4",
            "Kh7",
            "f5",
            "Kh6",
            "Ke7",
            "Kg7",
            "f6+",
            "Kg8",
            "f7+",
            "Kh7",
            "f8=Q",
            "Kg6",
            "Qf6+",
            "Kh7",
            "Bf5+",
            "Kg8",
            "Qf7+",
            "Kh8",
            "Qh7#",
            "e4",
            "f3",
            "gxf3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f1c4",
          "positional_uci": "f1c4",
          "neutral_uci": "f1c4",
          "consensus_move": "f1c4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bxf3",
              "gxf3",
              "e6",
              "Bg5",
              "Be7",
              "Rd1",
              "Qc8",
              "Ba6",
              "Qb8",
              "Bb7",
              "O-O",
              "Bxa8",
              "Qxa8",
              "e5",
              "Nd5",
              "Nxd5",
              "Bxg5",
              "Nxc7",
              "Qc8",
              "Rd7",
              "Rd8",
              "Qxa7",
              "Rxd7",
              "cxd7",
              "Qxd7",
              "Qb8+",
              "Bd8",
              "O-O",
              "Qxc7",
              "Qxc7",
              "Bxc7",
              "Rc1",
              "Bd8",
              "Rc8",
              "Kf8",
              "Rxd8+",
              "Ke7",
              "Rb8",
              "f6",
              "Rb7+",
              "Kf8",
              "exf6",
              "gxf6",
              "Rxb6",
              "Kf7",
              "a4",
              "e5",
              "a5",
              "Kg6",
              "a6",
              "Kf5",
              "a7",
              "Kf4",
              "a8=Q",
              "f5",
              "Rb4+",
              "e4",
              "Qb8+",
              "Kxf3",
              "Qg3+",
              "Ke2",
              "Qe3+",
              "Kd1",
              "Rc4",
              "f4",
              "Rc1#",
              "Qxc1",
              "Qxc1+",
              "Kxc1",
              "b4",
              "Kd2",
              "b5",
              "e3",
              "fxe3",
              "fxe3",
              "b6",
              "e2",
              "b7",
              "e1=Q+",
              "Kg2",
              "Qe4+",
              "Kg3",
              "Qxb7",
              "h4",
              "Qe4",
              "h5",
              "Ke3",
              "h6",
              "Qf4+",
              "Kh3",
              "Kf3",
              "b4",
              "Qg3#",
              "Qg2+",
              "Kh4",
              "Qg4#",
              "Qg4#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e6",
              "Ne5",
              "Bd6",
              "Nxg4",
              "Nxg4",
              "h3",
              "Ne5",
              "O-O",
              "O-O",
              "Be3",
              "Qh4",
              "Be2",
              "f5",
              "exf5",
              "Qxa4",
              "Nxa4",
              "exf5",
              "Rac1",
              "f4",
              "Bd4",
              "f3",
              "Bxf3",
              "Nxf3+",
              "gxf3",
              "Rxf3",
              "Kg2",
              "Raf8",
              "Rc3",
              "R3f4",
              "Rd3",
              "R8f5",
              "Nc3",
              "Rg5+",
              "Kh1",
              "Rh4",
              "Ne2",
              "Rgh5",
              "Kg2",
              "Rg5+",
              "Ng3",
              "Rd5",
              "Rfd1",
              "c5",
              "Bc3",
              "Rxd3",
              "Rxd3",
              "Bxg3",
              "fxg3",
              "Ra4",
              "a3",
              "b5",
              "Rd7",
              "b4",
              "axb4",
              "cxb4",
              "Rxg7+",
              "Kf8",
              "Rg4",
              "a5",
              "Kf3",
              "Ra1",
              "Bd4",
              "Rf1+",
              "Ke4",
              "Re1+",
              "Kd5",
              "Rd1",
              "Kc5",
              "Rc1+",
              "Kb5",
              "Ra1",
              "Bb6",
              "Rb1",
              "Bd4",
              "Ra1",
              "Rf4+",
              "Ke7",
              "Rf5",
              "Ke6",
              "Rh5",
              "Rd1",
              "Bb6",
              "Rd2",
              "b3",
              "Rd3",
              "Kc4",
              "Rxg3",
              "Bxa5",
              "Kf6",
              "Rxh7",
              "Kg6",
              "Rh4",
              "Kg5",
              "Rg4+",
              "Rxg4+",
              "hxg4",
              "Kxg4",
              "Bxb4",
              "Kf5",
              "Kd5",
              "Kf6",
              "Kd6",
              "Kf7",
              "Bc5",
              "Ke8",
              "Kc7",
              "Kf7",
              "b4",
              "Ke6",
              "b5",
              "Kd5",
              "b6",
              "Kxc5",
              "b7",
              "Kd5",
              "b8=Q",
              "Ke6",
              "Qb5",
              "Kf6",
              "Qd5",
              "Ke7",
              "Qd6+",
              "Kf7",
              "Kd7",
              "Kg7",
              "Qe6",
              "Kf8",
              "Qe7+",
              "Kg8",
              "Ke6",
              "Kh8",
              "Kf6",
              "Kg8",
              "Qg7#",
              "e4",
              "e5",
              "e6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e6",
              "Ne5",
              "Bd6",
              "Nxg4",
              "Nxg4",
              "h3",
              "Ne5",
              "O-O",
              "O-O",
              "Be3",
              "Qh4",
              "Be2",
              "f5",
              "exf5",
              "Qxa4",
              "Nxa4",
              "exf5",
              "Nc3",
              "f4",
              "Bd4",
              "Nxc6",
              "Bc4+",
              "Kh8",
              "Rad1",
              "Rad8",
              "Nb5",
              "Nxd4",
              "Nxd4",
              "Be5",
              "Ne6",
              "Rxd1",
              "Rxd1",
              "Re8",
              "Rd8",
              "Rxd8",
              "Nxd8",
              "Bxb2",
              "Nc6",
              "a5",
              "Kf1",
              "g5",
              "Ke2",
              "Kg7",
              "Kd3",
              "Kf6",
              "Ke4",
              "h5",
              "Nd4",
              "Bxd4",
              "Kxd4",
              "Kf5",
              "Bd3+",
              "Ke6",
              "Ke4",
              "c6",
              "Bc4+",
              "Kf6",
              "a4",
              "h4",
              "Bb3",
              "b5",
              "axb5",
              "cxb5",
              "Kd5",
              "a4",
              "Ba2",
              "b4",
              "Kc4",
              "b3",
              "Bb1",
              "Ke5",
              "Kb4",
              "Kd4",
              "Kxa4",
              "Kc3",
              "Ka3",
              "b2",
              "Ka2",
              "Kd2",
              "Kxb2",
              "Ke2",
              "f3",
              "Kf2",
              "Bf5",
              "Kxg2",
              "Bg4",
              "Kf2",
              "Kc3",
              "Ke3",
              "Kc4",
              "Kf2",
              "Kd5",
              "Kg3",
              "Ke5",
              "Kf2",
              "Kf5",
              "Ke3",
              "Kxg5",
              "Kd4",
              "Kxf4",
              "Kd5",
              "Kg5",
              "Ke5",
              "Kxh4",
              "Kf6",
              "Kh5",
              "Kg7",
              "Kg5",
              "Kh7",
              "Kf6",
              "Kh6",
              "f4",
              "Kh7",
              "f5",
              "Kh6",
              "Ke7",
              "Kg7",
              "f6+",
              "Kg8",
              "f7+",
              "Kh7",
              "f8=Q",
              "Kg6",
              "Qf6+",
              "Kh7",
              "Bf5+",
              "Kg8",
              "Qf7+",
              "Kh8",
              "Qh7#",
              "e4",
              "f3",
              "gxf3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 175,
            "completion_tokens": 416,
            "total_tokens": 591,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 175,
            "completion_tokens": 572,
            "total_tokens": 747,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 174,
            "completion_tokens": 545,
            "total_tokens": 719,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 524,
          "total_completion_tokens": 1533,
          "total_tokens": 2057
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 175,
            "completion_tokens": 416,
            "total_tokens": 591,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 175,
            "completion_tokens": 572,
            "total_tokens": 747,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 174,
            "completion_tokens": 545,
            "total_tokens": 719,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/p1p1pppp/1pP2n2/8/Q1p1P1b1/2N2N2/PP3PPP/R1B1KB1R w KQkq - 0 9"
    },
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e5",
          "aggressive_uci": "e7e5",
          "aggressive_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6\n4. Ba4 Nf6 5. O-O Be7 6. Re1 b5\n7. Bb3 d6 8. c3 O-O 9. h3 Bb7\n10. d4 Re8 11. Nbd2 Bf8 12. a4 h6\n13. Bc2 exd4 14. cxd4 Nb4 15. Bb1 c5\n16. d5 Nd7 17. Ra3 c4 18. Nd4 Qf6\n19. N2f3 Nc5 20. axb5 Nbd3 21. Bxd3 Nxd3\n22. Re2 axb5 23. Nxb5 Rxa3 24. Nxa3 Ba6\n25. Qa4 Ra8 26. Nxc4 Nxc1 27. Rc2 Nd3\n28. Nb6 Ra7 29. Nd7 Qd8 30. Nxf8 Kxf8\n31. Nd4 Nc5 32. Qa3 Qb6 33. Nc6 Ra8\n34. e5 Bb7 35. Qf3 dxe5 36. Nxe5 Qc7\n37. Rxc5 Ra1+ 38. Kh2 Qxe5+ 39. g3 Qe1\n40. Qg2 Rd1 41. Rb5 Bxd5 42. Rxd5 Rxd5\n43. Qxd5 Qxf2+ 44. Qg2 Qxg2+ 45. Kxg2 Ke7\n46. Kf3 Kd6 47. Ke4 Kc5 48. Ke5 Kc4\n49. Kd6 Kb3 50. Ke7 Kxb2 51. Kxf7 Kc3\n52. Kxg7 Kd4 53. Kxh6 Ke5 54. g4 Kf6\n55. g5+ Kf7 56. Kh7 Kf8 57. g6 Ke7\n58. g7 Kf6 59. g8=Q Kf5 60. Qg4+ Kf6\n61. Qf4+ Ke6 62. Kg6 Kd5 63. Kf6 Kc5\n64. Qe4 Kd6 65. Qd4+ Kc6 66. Ke6 Kb5\n67. Kd6 Ka5 68. Qb2 Ka4 69. Kc5 Ka5\n70. Qa3# d4 e5 Qf3",
          "aggressive_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 590,
            "total_tokens": 704,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qf3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e5",
          "positional_uci": "e7e5",
          "positional_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7\n6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5\n11. d4 Qc7 12. Nbd2 cxd4 13. cxd4 Nc6 14. Nb3 a5\n15. Be3 a4 16. Nbd2 Bd7 17. Rc1 Qb7 18. Bb1 Rfc8\n19. Nf1 Bd8 20. Ng3 Bb6 21. Qd2 Ba5 22. Qe2 Bxe1\n23. Rxe1 Re8 24. Qd2 Rac8 25. Bg5 Kh8 26. Bxf6 gxf6\n27. Qh6 Rg8 28. Qxf6+ Rg7 29. Nh5 Rg8 30. Ng5 Be8\n31. Nxg7 Rxg7 32. dxe5 dxe5 33. Rd1 Qe7 34. Qxe7 Nxe7\n35. Rd8 Rg8 36. Nf3 f6 37. Bd3 Kg7 38. Rb8 Kf7\n39. Bxb5 Bxb5 40. Rxb5 Rc8 41. Rb4 Rc1+ 42. Kh2 Rc2\n43. Kg3 Ng6 44. Ne1 Re2 45. Nd3 Rd2 46. Nc5 Nf4\n47. Nxa4 Nd3 48. Rb7+ Kg6 49. f3 Nf4 50. Kh2 Rxg2+\n51. Kh1 Rf2 52. Nc3 Rxf3 53. a4 Rxh3+ 54. Kg1 Rd3\n55. a5 Rd6 56. Rb6 Rd7 57. a6 Ra7 58. Nb5 Ra8\n59. a7 Ne2+ 60. Kf2 Nd4 61. Rb8 Rxa7 62. Nxa7 h5\n63. Nb5 Ne6 64. Nd6 Kg5 65. Rg8+ Kf4 66. b4 h4\n67. b5 Nc5 68. b6 h3 69. b7 Nxb7 70. Nxb7 Kxe4\n71. Rh8 f5 72. Rxh3 f4 73. Nd6+ Kd5 74. Nf5 Ke4\n75. Nh4 Kd5 76. Ng2 Ke4 77. Rh4 Kf5 78. Nxf4 exf4\n79. Kf3 Kg5 80. Rxf4 Kg6",
          "positional_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Qc7",
            "Nbd2",
            "cxd4",
            "cxd4",
            "Nc6",
            "Nb3",
            "a5",
            "Be3",
            "a4",
            "Nbd2",
            "Bd7",
            "Rc1",
            "Qb7",
            "Bb1",
            "Rfc8",
            "Nf1",
            "Bd8",
            "Ng3",
            "Bb6",
            "Qd2",
            "Ba5",
            "Qe2",
            "Bxe1",
            "Rxe1",
            "Re8",
            "Qd2",
            "Rac8",
            "Bg5",
            "Kh8",
            "Bxf6",
            "gxf6",
            "Qh6",
            "Rg8",
            "Qxf6+",
            "Rg7",
            "Nh5",
            "Rg8",
            "Ng5",
            "Be8",
            "Nxg7",
            "Rxg7",
            "dxe5",
            "dxe5",
            "Rd1",
            "Qe7",
            "Qxe7",
            "Nxe7",
            "Rd8",
            "Rg8",
            "Nf3",
            "f6",
            "Bd3",
            "Kg7",
            "Rb8",
            "Kf7",
            "Bxb5",
            "Bxb5",
            "Rxb5",
            "Rc8",
            "Rb4",
            "Rc1+",
            "Kh2",
            "Rc2",
            "Kg3",
            "Ng6",
            "Ne1",
            "Re2",
            "Nd3",
            "Rd2",
            "Nc5",
            "Nf4",
            "Nxa4",
            "Nd3",
            "Rb7+",
            "Kg6",
            "f3",
            "Nf4",
            "Kh2",
            "Rxg2+",
            "Kh1",
            "Rf2",
            "Nc3",
            "Rxf3",
            "a4",
            "Rxh3+",
            "Kg1",
            "Rd3",
            "a5",
            "Rd6",
            "Rb6",
            "Rd7",
            "a6",
            "Ra7",
            "Nb5",
            "Ra8",
            "a7",
            "Ne2+",
            "Kf2",
            "Nd4",
            "Rb8",
            "Rxa7",
            "Nxa7",
            "h5",
            "Nb5",
            "Ne6",
            "Nd6",
            "Kg5",
            "Rg8+",
            "Kf4",
            "b4",
            "h4",
            "b5",
            "Nc5",
            "b6",
            "h3",
            "b7",
            "Nxb7",
            "Nxb7",
            "Kxe4",
            "Rh8",
            "f5",
            "Rxh3",
            "f4",
            "Nd6+",
            "Kd5",
            "Nf5",
            "Ke4",
            "Nh4",
            "Kd5",
            "Ng2",
            "Ke4",
            "Rh4",
            "Kf5",
            "Nxf4",
            "exf4",
            "Kf3",
            "Kg5",
            "Rxf4",
            "Kg6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e5",
          "neutral_uci": "e7e5",
          "neutral_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6\n4. Ba4 Nf6 5. O-O Be7 6. Re1 b5\n7. Bb3 d6 8. c3 O-O 9. h3 Bb7\n10. d4 Re8 11. Nbd2 Bf8 12. a4 h6\n13. Bc2 exd4 14. cxd4 Nb4 15. Bb1 c5\n16. d5 Nd7 17. Ra3 c4 18. Nd4 Qf6\n19. N2f3 Nc5 20. axb5 Nbd3 21. Bxd3 Nxd3\n22. Re2 axb5 23. Nxb5 Rxa3 24. Nxa3 Ba6\n25. Qa4 Ra8 26. Be3 Nxb2\n27. Qc6 Rc8 28. Qxa6 c3 29. Qxc8 Nd3",
          "neutral_tokens": {
            "prompt_tokens": 113,
            "completion_tokens": 230,
            "total_tokens": 343,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Bb7",
            "d4",
            "Re8",
            "Nbd2",
            "Bf8",
            "a4",
            "h6",
            "Bc2",
            "exd4",
            "cxd4",
            "Nb4",
            "Bb1",
            "c5",
            "d5",
            "Nd7",
            "Ra3",
            "c4",
            "Nd4",
            "Qf6",
            "N2f3",
            "Nc5",
            "axb5",
            "Nbd3",
            "Bxd3",
            "Nxd3",
            "Re2",
            "axb5",
            "Nxb5",
            "Rxa3",
            "Nxa3",
            "Ba6",
            "Qa4",
            "Ra8",
            "Be3",
            "Nxb2",
            "Qc6",
            "Rc8",
            "Qxa6",
            "c3",
            "Qxc8",
            "Nd3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e7e5",
          "positional_uci": "e7e5",
          "neutral_uci": "e7e5",
          "consensus_move": "e7e5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qf3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Qc7",
              "Nbd2",
              "cxd4",
              "cxd4",
              "Nc6",
              "Nb3",
              "a5",
              "Be3",
              "a4",
              "Nbd2",
              "Bd7",
              "Rc1",
              "Qb7",
              "Bb1",
              "Rfc8",
              "Nf1",
              "Bd8",
              "Ng3",
              "Bb6",
              "Qd2",
              "Ba5",
              "Qe2",
              "Bxe1",
              "Rxe1",
              "Re8",
              "Qd2",
              "Rac8",
              "Bg5",
              "Kh8",
              "Bxf6",
              "gxf6",
              "Qh6",
              "Rg8",
              "Qxf6+",
              "Rg7",
              "Nh5",
              "Rg8",
              "Ng5",
              "Be8",
              "Nxg7",
              "Rxg7",
              "dxe5",
              "dxe5",
              "Rd1",
              "Qe7",
              "Qxe7",
              "Nxe7",
              "Rd8",
              "Rg8",
              "Nf3",
              "f6",
              "Bd3",
              "Kg7",
              "Rb8",
              "Kf7",
              "Bxb5",
              "Bxb5",
              "Rxb5",
              "Rc8",
              "Rb4",
              "Rc1+",
              "Kh2",
              "Rc2",
              "Kg3",
              "Ng6",
              "Ne1",
              "Re2",
              "Nd3",
              "Rd2",
              "Nc5",
              "Nf4",
              "Nxa4",
              "Nd3",
              "Rb7+",
              "Kg6",
              "f3",
              "Nf4",
              "Kh2",
              "Rxg2+",
              "Kh1",
              "Rf2",
              "Nc3",
              "Rxf3",
              "a4",
              "Rxh3+",
              "Kg1",
              "Rd3",
              "a5",
              "Rd6",
              "Rb6",
              "Rd7",
              "a6",
              "Ra7",
              "Nb5",
              "Ra8",
              "a7",
              "Ne2+",
              "Kf2",
              "Nd4",
              "Rb8",
              "Rxa7",
              "Nxa7",
              "h5",
              "Nb5",
              "Ne6",
              "Nd6",
              "Kg5",
              "Rg8+",
              "Kf4",
              "b4",
              "h4",
              "b5",
              "Nc5",
              "b6",
              "h3",
              "b7",
              "Nxb7",
              "Nxb7",
              "Kxe4",
              "Rh8",
              "f5",
              "Rxh3",
              "f4",
              "Nd6+",
              "Kd5",
              "Nf5",
              "Ke4",
              "Nh4",
              "Kd5",
              "Ng2",
              "Ke4",
              "Rh4",
              "Kf5",
              "Nxf4",
              "exf4",
              "Kf3",
              "Kg5",
              "Rxf4",
              "Kg6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Bb7",
              "d4",
              "Re8",
              "Nbd2",
              "Bf8",
              "a4",
              "h6",
              "Bc2",
              "exd4",
              "cxd4",
              "Nb4",
              "Bb1",
              "c5",
              "d5",
              "Nd7",
              "Ra3",
              "c4",
              "Nd4",
              "Qf6",
              "N2f3",
              "Nc5",
              "axb5",
              "Nbd3",
              "Bxd3",
              "Nxd3",
              "Re2",
              "axb5",
              "Nxb5",
              "Rxa3",
              "Nxa3",
              "Ba6",
              "Qa4",
              "Ra8",
              "Be3",
              "Nxb2",
              "Qc6",
              "Rc8",
              "Qxa6",
              "c3",
              "Qxc8",
              "Nd3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 114,
            "completion_tokens": 590,
            "total_tokens": 704,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 113,
            "completion_tokens": 230,
            "total_tokens": 343,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 341,
          "total_completion_tokens": 1460,
          "total_tokens": 1801
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 114,
            "completion_tokens": 590,
            "total_tokens": 704,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 113,
            "completion_tokens": 230,
            "total_tokens": 343,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": "e2e4",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 Bc5 4. O-O Nf6 5. Nxe5 Nxe5 6. d4 Bxd4 7. Qxd4 Nc6 8. Bxc6 bxc6 9. Bg5 O-O 10. e5 h6 11. Bh4 g5 12. exf6 gxh4 13. Qg4+ Kh7 14. Qg7# 1-0\n* 1. e4 e5 2. Nf3 Nc6 3. Bb5 Bc5 4. O-O Nf6 5. Nxe5 Nxe5 6. d4 Bxd4 7. Qxd4 Nc6 8. Bxc6 bxc6 9. Bg5 O-O 10. e5 h6 11. Bh4 g5 12. exf6 gxh4 13. Qg4+ Kh7 14. Qg7# 1-0 e4 e5 Nf3 Nc6 Bb5 Bc5 O-O Nf6 Nxe5 Nxe5 d4 Bxd4 Qxd4 Nc6 Bxc6 bxc6 Bg5 O-O e5 h6 Bh4 g5 exf6 gxh4 Qg4+ Kh7 Qg7# 1-0",
          "aggressive_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 317,
            "total_tokens": 427,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bb5",
            "Bc5",
            "O-O",
            "Nf6",
            "Nxe5",
            "Nxe5",
            "d4",
            "Bxd4",
            "Qxd4",
            "Nc6",
            "Bxc6",
            "bxc6",
            "Bg5",
            "O-O",
            "e5",
            "h6",
            "Bh4",
            "g5",
            "exf6",
            "gxh4",
            "Qg4+",
            "Kh7",
            "Qg7#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 Bc5 4. O-O Nge7 5. Nxe5 Nxe5 6. d4 Bxd4 7. Qxd4 N7c6 8. Bxc6 Nxc6 9. Qxg7 Rf8 10. Bh6 Qe7 11. Nc3 d6 12. Nd5 Qe5 13. Qxf8+ Kd7 14. Qxf7+ Kd8 15. Qxc7+ Ke8 16. Qxh7 Be6 17. Nc7+ Kd8 18. Nxa8 Nd4 19. Qc7+ Ke8 20. Qxb7 Ne2+ 21. Kh1 Qh5 22. Nc7+ Kf7 23. Nxe6+ Kxe6 24. Qd5+ Qxd5 25. exd5+ Kxd5 26. Rad1+ Kc4 27. Rfe1 Nd4 28. Re4 d5 29. Rexd4+ Kc5 30. Rxd5+ Kc6 31. Rd6+ Kc5 32. R1d5+ Kc4 33. b3+ Kc3 34. Rc6+ Kb2 35. Bg7+ Kxa2 36. Ra5+ Kb1 37. Ra1# 1-0 e4 Nf6 d4 Nxd4 Nc3 Bb5 O-O Nge7 Nxe5 Nxe5 d4 Bxd4 Qxd4 N7c6 Bxc6 Nxc6 Qxg7 Rf8 Bh6 Qe7 Nc3 d6 Nd5 Qe5 Qxf8+ Kd7 Qxf7+ Kd8 Qxc7+ Ke8 Qxh7 Be6 Nc7+ Kd8 Nxa8 Nd4 Qc7+ Ke8 Qxb7 Ne2+ Kh1 Qh5 Nc7+ Kf7 Nxe6+ Kxe6 Qd5+ Qxd5 exd5+ Kxd5 Rad1+ Kc4 Rfe1 Nd4 Re4 d5 Rexd4+ Kc5 Rxd5+ Kc6 Rd6+ Kc5 R1d5+ Kc4 b3+ Kc3 Rc6+ Kb2 Ra5+ Kb1 Ne2 Rc6+ Kb5 Rd5+ Kb4 a3+ Kxa3 Ra5#",
          "positional_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 563,
            "total_tokens": 673,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "d4",
            "Nxd4",
            "Nc3",
            "Bb5",
            "O-O",
            "Nge7",
            "Nxe5",
            "Nxe5",
            "d4",
            "Bxd4",
            "Qxd4",
            "N7c6",
            "Bxc6",
            "Nxc6",
            "Qxg7",
            "Rf8",
            "Bh6",
            "Qe7",
            "Nc3",
            "d6",
            "Nd5",
            "Qe5",
            "Qxf8+",
            "Kd7",
            "Qxf7+",
            "Kd8",
            "Qxc7+",
            "Ke8",
            "Qxh7",
            "Be6",
            "Nc7+",
            "Kd8",
            "Nxa8",
            "Nd4",
            "Qc7+",
            "Ke8",
            "Qxb7",
            "Ne2+",
            "Kh1",
            "Qh5",
            "Nc7+",
            "Kf7",
            "Nxe6+",
            "Kxe6",
            "Qd5+",
            "Qxd5",
            "exd5+",
            "Kxd5",
            "Rad1+",
            "Kc4",
            "Rfe1",
            "Nd4",
            "Re4",
            "d5",
            "Rexd4+",
            "Kc5",
            "Rxd5+",
            "Kc6",
            "Rd6+",
            "Kc5",
            "R1d5+",
            "Kc4",
            "b3+",
            "Kc3",
            "Rc6+",
            "Kb2",
            "Ra5+",
            "Kb1",
            "Ne2",
            "Rc6+",
            "Kb5",
            "Rd5+",
            "Kb4",
            "a3+",
            "Kxa3",
            "Ra5#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": "d2d4",
          "neutral_response": "d4 d5\nd4 d5 Nf3 Nf6 Bf4 Bf5 e3 e6 c4 c6 Nc3 Nbd7 Be2 Be7 O-O O-O Rc1 Rc8 Qb3 Qb6 c5 Qxb3 axb3 a6 b4 Nh5 Bg3 Nxg3 hxg3 Bf6 Nd2 e5 dxe5 Nxe5 Nb3 Nd3 Bxd3 Bxd3 Rfd1 Rfe8 Nd4 Bg6 Nce2 Bxd4 Nxd4 Re4 Rd2 Rce8 Rcd1 f6 Kf1 Kf7 Ke2 Bh5+ f3 Rxe3+ Kf2 R3e7 Nf5 Rd7 Nd6+ Kf8 Nxe8 Bxe8 Re1 Bg6 Rde2 Bd3 Re8+ Kf7 Rb8 Bb5 Ree8 d4 Rf8+ Kg6 f4 d3 Rd1 Re7 Rd2 Re2+ Kf3 Rxb2 g4 d2 f5+ Kg5 Rxg7+ Kh6 Rgg8 Rb3+ Kf4 d1=Q g5+ fxg5+ Ke5 Qe2+ Kd6 Qe7+ Kc5 Qe3+ Qxe3+ Kg4 Qe4+ Kh5 Qe1+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Qe8+ Kh6 Qe6+ Kh5 Q",
          "neutral_tokens": {
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Rf8+",
            "Kg6",
            "f4",
            "d3",
            "Rd1",
            "Re7",
            "Rd2",
            "Re2+",
            "Kf3",
            "Rxb2",
            "g4",
            "d2",
            "f5+",
            "Kg5",
            "Rxg7+",
            "Kh6",
            "Rgg8",
            "Rb3+",
            "Kf4",
            "d1=Q",
            "g5+",
            "fxg5+",
            "Ke5",
            "Qe2+",
            "Kd6",
            "Qe7+",
            "Kc5",
            "Qe3+",
            "Qxe3+",
            "Kg4",
            "Qe4+",
            "Kh5",
            "Qe1+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5",
            "Qe8+",
            "Kh6",
            "Qe6+",
            "Kh5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e2e4",
          "positional_uci": "e2e4",
          "neutral_uci": "d2d4",
          "consensus_move": "e2e4",
          "source_agents": [
            "aggressive",
            "positional"
          ],
          "selected_agent": "aggressive"
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bb5",
              "Bc5",
              "O-O",
              "Nf6",
              "Nxe5",
              "Nxe5",
              "d4",
              "Bxd4",
              "Qxd4",
              "Nc6",
              "Bxc6",
              "bxc6",
              "Bg5",
              "O-O",
              "e5",
              "h6",
              "Bh4",
              "g5",
              "exf6",
              "gxh4",
              "Qg4+",
              "Kh7",
              "Qg7#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf6",
              "d4",
              "Nxd4",
              "Nc3",
              "Bb5",
              "O-O",
              "Nge7",
              "Nxe5",
              "Nxe5",
              "d4",
              "Bxd4",
              "Qxd4",
              "N7c6",
              "Bxc6",
              "Nxc6",
              "Qxg7",
              "Rf8",
              "Bh6",
              "Qe7",
              "Nc3",
              "d6",
              "Nd5",
              "Qe5",
              "Qxf8+",
              "Kd7",
              "Qxf7+",
              "Kd8",
              "Qxc7+",
              "Ke8",
              "Qxh7",
              "Be6",
              "Nc7+",
              "Kd8",
              "Nxa8",
              "Nd4",
              "Qc7+",
              "Ke8",
              "Qxb7",
              "Ne2+",
              "Kh1",
              "Qh5",
              "Nc7+",
              "Kf7",
              "Nxe6+",
              "Kxe6",
              "Qd5+",
              "Qxd5",
              "exd5+",
              "Kxd5",
              "Rad1+",
              "Kc4",
              "Rfe1",
              "Nd4",
              "Re4",
              "d5",
              "Rexd4+",
              "Kc5",
              "Rxd5+",
              "Kc6",
              "Rd6+",
              "Kc5",
              "R1d5+",
              "Kc4",
              "b3+",
              "Kc3",
              "Rc6+",
              "Kb2",
              "Ra5+",
              "Kb1",
              "Ne2",
              "Rc6+",
              "Kb5",
              "Rd5+",
              "Kb4",
              "a3+",
              "Kxa3",
              "Ra5#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Rf8+",
              "Kg6",
              "f4",
              "d3",
              "Rd1",
              "Re7",
              "Rd2",
              "Re2+",
              "Kf3",
              "Rxb2",
              "g4",
              "d2",
              "f5+",
              "Kg5",
              "Rxg7+",
              "Kh6",
              "Rgg8",
              "Rb3+",
              "Kf4",
              "d1=Q",
              "g5+",
              "fxg5+",
              "Ke5",
              "Qe2+",
              "Kd6",
              "Qe7+",
              "Kc5",
              "Qe3+",
              "Qxe3+",
              "Kg4",
              "Qe4+",
              "Kh5",
              "Qe1+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5",
              "Qe8+",
              "Kh6",
              "Qe6+",
              "Kh5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 110,
            "completion_tokens": 317,
            "total_tokens": 427,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 110,
            "completion_tokens": 563,
            "total_tokens": 673,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 329,
          "total_completion_tokens": 1520,
          "total_tokens": 1849
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 110,
            "completion_tokens": 317,
            "total_tokens": 427,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 110,
            "completion_tokens": 563,
            "total_tokens": 673,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 3,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf3",
          "aggressive_uci": "g1f3",
          "aggressive_response": "2. Nf3 Nc6 *\n3. Bb5 a6 *\n4. Ba4 Nf6 *\n5. O-O Be7 *\n6. Re1 b5 *\n7. Bb3 d6 *\n8. c3 O-O *\n9. h3 Na5 *\n10. Bc2 c5 *\n11. d4 Qc7 *\n12. Nbd2 cxd4 *\n13. cxd4 Nc6 *\n14. Nb3 a5 *\n15. Be3 a4 *\n16. Nbd2 Bd7 *\n17. Rc1 Qb7 *\n18. Nf1 Rfc8 *\n19. Ng3 g6 *\n20. Qd2 Bd8 *\n21. Red1 Ba5 *\n22. Qe2 Bb6 *\n23. dxe5 dxe5 *\n24. Bxb6 Qxb6 *\n25. Rd6 Ne8 *\n26. Rd5 Be6 *\n27. Rxb5 Qa6 *\n28. Bd3 Nd4 *\n29. Nxd4 Rxc1+ *\n30. Kh2 exd4 *\n31. Rb8 Qc6 *\n32. Bb5 Qxb5 *\n33. Qxb5 Rxb8 *\n34. Qxb8 Rc8 *\n35. Qb4 d3 *\n36. Nf1 Rc2 *\n37. Qxa4 Nf6 *\n38. Qd4 Bc4 *\n39. Ne3 Rxf2 *\n40. Nxc4 d2 *\n41. Nxd2 Rf4 *\n42. Qe5 Rf2 *\n43. Kg1 Rxd2 *\n44. Qxf6 Re2 *\n45. e5 Rxb2 *\n46. e6 Rb1+ *\n47. Kh2 fxe6 *\n48. Qxe6+ Kg7 *\n49. Qe5+ Kh6 *\n50. Qf4+ Kg7 *\n51. Qd4+ Kh6 *\n52. Qd2+ Kg7 *\n53. Qc3+ Kh6 *\n54. a4 Rb7 *\n55. a5 Ra7 *\n56. Qe3+ Kh5 *\n57. Qxa7 g5 *\n58. Qxh7# Qh7 *\n59. Qxh7# Kg5 *\n60. Qe7+ Kh5 *\n61. g4+ Kh4 *\n62. Qe1# Kg5 *\n63. Qe5+ Kh4 *\n64. Qg3# Kg5 *\n65. h4+ Kh6 *\n66. Qd6+ Kg7 *\n67. hxg5 Kh7 *\n68. Qe7+ Kg6 *\n69. Qf6+ Kh7 *\n70.",
          "aggressive_tokens": {
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Qc7",
            "Nbd2",
            "cxd4",
            "cxd4",
            "Nc6",
            "Nb3",
            "a5",
            "Be3",
            "a4",
            "Nbd2",
            "Bd7",
            "Rc1",
            "Qb7",
            "Nf1",
            "Rfc8",
            "Ng3",
            "g6",
            "Qd2",
            "Bd8",
            "Red1",
            "Ba5",
            "Qe2",
            "Bb6",
            "dxe5",
            "dxe5",
            "Bxb6",
            "Qxb6",
            "Rd6",
            "Ne8",
            "Rd5",
            "Be6",
            "Rxb5",
            "Qa6",
            "Bd3",
            "Nd4",
            "Nxd4",
            "Rxc1+",
            "Kh2",
            "exd4",
            "Rb8",
            "Qc6",
            "Bb5",
            "Qxb5",
            "Qxb5",
            "Rxb8",
            "Qxb8",
            "Rc8",
            "Qb4",
            "d3",
            "Nf1",
            "Rc2",
            "Qxa4",
            "Nf6",
            "Qd4",
            "Bc4",
            "Ne3",
            "Rxf2",
            "Nxc4",
            "d2",
            "Nxd2",
            "Rf4",
            "Qe5",
            "Rf2",
            "Kg1",
            "Rxd2",
            "Qxf6",
            "Re2",
            "e5",
            "Rxb2",
            "e6",
            "Rb1+",
            "Kh2",
            "fxe6",
            "Qxe6+",
            "Kg7",
            "Qe5+",
            "Kh6",
            "Qf4+",
            "Kg7",
            "Qd4+",
            "Kh6",
            "Qd2+",
            "Kg7",
            "Qc3+",
            "Kh6",
            "a4",
            "Rb7",
            "a5",
            "Ra7",
            "Qe3+",
            "Kh5",
            "Qxa7",
            "g5",
            "Qxh7#",
            "Qh7",
            "Qxh7#",
            "Kg5",
            "Qe7+",
            "Kh5",
            "g4+",
            "Kh4",
            "Qe1#",
            "Kg5",
            "Qe5+",
            "Kh4",
            "Qg3#",
            "Kg5",
            "h4+",
            "Kh6",
            "Qd6+",
            "Kg7",
            "hxg5",
            "Kh7",
            "Qe7+",
            "Kg6",
            "Qf6+",
            "Kh7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf3",
          "positional_uci": "g1f3",
          "positional_response": "2. Nf3 Nc6 *\n3. Bb5 a6 *\n4. Ba4 Nf6 *\n5. O-O Be7 *\n6. Re1 b5 *\n7. Bb3 d6 *\n8. c3 O-O *\n9. h3 Na5 *\n10. Bc2 c5 *\n11. d4 Qc7 *\n12. Nbd2 cxd4 *\n13. cxd4 Nc6 *\n14. Nb3 a5 *\n15. Be3 a4 *\n16. Nbd2 Bd7 *\n17. Rc1 Qb7 *\n18. Bb1 Rfc8 *\n19. Nf1 Bd8 *\n20. Ng3 Bb6 *\n21. Qd2 Ba5 *\n22. Qe2 Bxe1 *\n23. Rxe1 Re8 *\n24. Qd2 Rad8 *\n25. Bg5 Ne7 *\n26. Bxf6 gxf6 *\n27. Qh6 Ng6 *\n28. Nh5 Re6 *\n29. Qg7# 1-0\n\ne4 e5 Nf3 Nc6 Bb5 a6 Ba4 Nf6 O-O Be7 Re1 b5 Bb3 d6 c3 O-O h3 Na5 Bc2 c5 d4 Qc7 Nbd2 cxd4 cxd4 Nc6 Nb3 a5 Be3 a4 Nbd2 Bd7 Rc1 Qb7 Bb1 Rfc8 Nf1 Bd8 Ng3 Bb6 Qd2 Ba5 Qe2 Bxe1 Rxe1 Re8 Qd2 Rad8 Bg5 Ne7 Bxf6 gxf6 Qh6 Ng6 Nh5 Re6 Qg7# 1-0",
          "positional_tokens": {
            "prompt_tokens": 116,
            "completion_tokens": 400,
            "total_tokens": 516,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Qc7",
            "Nbd2",
            "cxd4",
            "cxd4",
            "Nc6",
            "Nb3",
            "a5",
            "Be3",
            "a4",
            "Nbd2",
            "Bd7",
            "Rc1",
            "Qb7",
            "Bb1",
            "Rfc8",
            "Nf1",
            "Bd8",
            "Ng3",
            "Bb6",
            "Qd2",
            "Ba5",
            "Qe2",
            "Bxe1",
            "Rxe1",
            "Re8",
            "Qd2",
            "Rad8",
            "Bg5",
            "Ne7",
            "Bxf6",
            "gxf6",
            "Qh6",
            "Ng6",
            "Nh5",
            "Re6",
            "Qg7#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf3",
          "neutral_uci": "g1f3",
          "neutral_response": "2. Nf3 Nc6 *\n3. Bb5 a6 *\n4. Ba4 Nf6 *\n5. O-O Be7 *\n6. Re1 b5 *\n7. Bb3 d6 *\n8. c3 O-O *\n9. h3 Na5 *\n10. Bc2 c5 *\n11. d4 Qc7 *\n12. Nbd2 cxd4 *\n13. cxd4 Nc6 *\n14. Nb3 a5 *\n15. Be3 a4 *\n16. Nbd2 Bd7 *\n17. Rc1 Qb7 *\n18. Nf1 Rfc8 *\n19. Ng3 g6 *\n20. Qd2 Bd8 *\n21. Red1 Ba5 *\n22. Qe2 Bb6 *\n23. dxe5 dxe5 *\n24. Bxb6 Qxb6 *\n25. Rd6 Ne8 *\n26. Rd5 Be6 *\n27. Rxb5 Qa6 *\n28. Bd3 Nd4 *\n29. Nxd4 Rxc1+ *\n30. Kh2 exd4 *\n31. Rb8 Qc6 *\n32. Bb5 Rxb8 *\n33. Bxc6 Rxc6 *\n34. Qd2 Rd8 *\n35. Ne2 d3 *\n36. Nc3 a3 *\n37. bxa3 Rxc3 *\n38. Qxc3 d2 *\n39. a4 d1=Q *\n40. a5 Qd6+ *\n41. g3 Qd2 *\n42. Qc5 Rc8 *\n43. Qb6 Rc2 *\n44. Kg2 Qe2 *\n45. a6 Qxe4+ *\n46. Kh2 Bd5 *\n47. g4 Qg2# *\n48. Nxg2 Bxg2 *\n49. Kxg2 Rxa2 *\n50. a7 Rxa7 *\n51. Kg3 Re7 *\n52. f4 Kg7 *\n53. Kh4 h6 *\n54. g5 hxg5+ *\n55. fxg5 Re4+ *\n56. Kg3 Nd6 *\n57. h4 Nf5+ *\n58. Kf3 Rxh4 *\n59. Kg2 Rg4+ *\n60. Kf3 Rxg5 *\n61. Kf4 Rg1 *\n62. Kf3 g5 *\n63. Kf2 Rg3 *\n64. Kf1 g4 *\n65. Kf2 Kg6 *\n66. Kf1 Kg5 *\n67. Kf2 Kf4 *\n68. Kf1 Kf3 *\n69. Ke1 Rg2 *\n70. Kf1 Ne3+ *\n71.",
          "neutral_tokens": {
            "prompt_tokens": 115,
            "completion_tokens": 640,
            "total_tokens": 755,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Qc7",
            "Nbd2",
            "cxd4",
            "cxd4",
            "Nc6",
            "Nb3",
            "a5",
            "Be3",
            "a4",
            "Nbd2",
            "Bd7",
            "Rc1",
            "Qb7",
            "Nf1",
            "Rfc8",
            "Ng3",
            "g6",
            "Qd2",
            "Bd8",
            "Red1",
            "Ba5",
            "Qe2",
            "Bb6",
            "dxe5",
            "dxe5",
            "Bxb6",
            "Qxb6",
            "Rd6",
            "Ne8",
            "Rd5",
            "Be6",
            "Rxb5",
            "Qa6",
            "Bd3",
            "Nd4",
            "Nxd4",
            "Rxc1+",
            "Kh2",
            "exd4",
            "Rb8",
            "Qc6",
            "Bb5",
            "Rxb8",
            "Bxc6",
            "Rxc6",
            "Qd2",
            "Rd8",
            "Ne2",
            "d3",
            "Nc3",
            "a3",
            "bxa3",
            "Rxc3",
            "Qxc3",
            "d2",
            "a4",
            "d1=Q",
            "a5",
            "Qd6+",
            "g3",
            "Qd2",
            "Qc5",
            "Rc8",
            "Qb6",
            "Rc2",
            "Kg2",
            "Qe2",
            "a6",
            "Qxe4+",
            "Kh2",
            "Bd5",
            "g4",
            "Qg2#",
            "Nxg2",
            "Bxg2",
            "Kxg2",
            "Rxa2",
            "a7",
            "Rxa7",
            "Kg3",
            "Re7",
            "f4",
            "Kg7",
            "Kh4",
            "h6",
            "g5",
            "hxg5+",
            "fxg5",
            "Re4+",
            "Kg3",
            "Nd6",
            "h4",
            "Nf5+",
            "Kf3",
            "Rxh4",
            "Kg2",
            "Rg4+",
            "Kf3",
            "Rxg5",
            "Kf4",
            "Rg1",
            "Kf3",
            "g5",
            "Kf2",
            "Rg3",
            "Kf1",
            "g4",
            "Kf2",
            "Kg6",
            "Kf1",
            "Kg5",
            "Kf2",
            "Kf4",
            "Kf1",
            "Kf3",
            "Ke1",
            "Rg2",
            "Kf1",
            "Ne3+"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g1f3",
          "positional_uci": "g1f3",
          "neutral_uci": "g1f3",
          "consensus_move": "g1f3",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Qc7",
              "Nbd2",
              "cxd4",
              "cxd4",
              "Nc6",
              "Nb3",
              "a5",
              "Be3",
              "a4",
              "Nbd2",
              "Bd7",
              "Rc1",
              "Qb7",
              "Nf1",
              "Rfc8",
              "Ng3",
              "g6",
              "Qd2",
              "Bd8",
              "Red1",
              "Ba5",
              "Qe2",
              "Bb6",
              "dxe5",
              "dxe5",
              "Bxb6",
              "Qxb6",
              "Rd6",
              "Ne8",
              "Rd5",
              "Be6",
              "Rxb5",
              "Qa6",
              "Bd3",
              "Nd4",
              "Nxd4",
              "Rxc1+",
              "Kh2",
              "exd4",
              "Rb8",
              "Qc6",
              "Bb5",
              "Qxb5",
              "Qxb5",
              "Rxb8",
              "Qxb8",
              "Rc8",
              "Qb4",
              "d3",
              "Nf1",
              "Rc2",
              "Qxa4",
              "Nf6",
              "Qd4",
              "Bc4",
              "Ne3",
              "Rxf2",
              "Nxc4",
              "d2",
              "Nxd2",
              "Rf4",
              "Qe5",
              "Rf2",
              "Kg1",
              "Rxd2",
              "Qxf6",
              "Re2",
              "e5",
              "Rxb2",
              "e6",
              "Rb1+",
              "Kh2",
              "fxe6",
              "Qxe6+",
              "Kg7",
              "Qe5+",
              "Kh6",
              "Qf4+",
              "Kg7",
              "Qd4+",
              "Kh6",
              "Qd2+",
              "Kg7",
              "Qc3+",
              "Kh6",
              "a4",
              "Rb7",
              "a5",
              "Ra7",
              "Qe3+",
              "Kh5",
              "Qxa7",
              "g5",
              "Qxh7#",
              "Qh7",
              "Qxh7#",
              "Kg5",
              "Qe7+",
              "Kh5",
              "g4+",
              "Kh4",
              "Qe1#",
              "Kg5",
              "Qe5+",
              "Kh4",
              "Qg3#",
              "Kg5",
              "h4+",
              "Kh6",
              "Qd6+",
              "Kg7",
              "hxg5",
              "Kh7",
              "Qe7+",
              "Kg6",
              "Qf6+",
              "Kh7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Qc7",
              "Nbd2",
              "cxd4",
              "cxd4",
              "Nc6",
              "Nb3",
              "a5",
              "Be3",
              "a4",
              "Nbd2",
              "Bd7",
              "Rc1",
              "Qb7",
              "Bb1",
              "Rfc8",
              "Nf1",
              "Bd8",
              "Ng3",
              "Bb6",
              "Qd2",
              "Ba5",
              "Qe2",
              "Bxe1",
              "Rxe1",
              "Re8",
              "Qd2",
              "Rad8",
              "Bg5",
              "Ne7",
              "Bxf6",
              "gxf6",
              "Qh6",
              "Ng6",
              "Nh5",
              "Re6",
              "Qg7#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Qc7",
              "Nbd2",
              "cxd4",
              "cxd4",
              "Nc6",
              "Nb3",
              "a5",
              "Be3",
              "a4",
              "Nbd2",
              "Bd7",
              "Rc1",
              "Qb7",
              "Nf1",
              "Rfc8",
              "Ng3",
              "g6",
              "Qd2",
              "Bd8",
              "Red1",
              "Ba5",
              "Qe2",
              "Bb6",
              "dxe5",
              "dxe5",
              "Bxb6",
              "Qxb6",
              "Rd6",
              "Ne8",
              "Rd5",
              "Be6",
              "Rxb5",
              "Qa6",
              "Bd3",
              "Nd4",
              "Nxd4",
              "Rxc1+",
              "Kh2",
              "exd4",
              "Rb8",
              "Qc6",
              "Bb5",
              "Rxb8",
              "Bxc6",
              "Rxc6",
              "Qd2",
              "Rd8",
              "Ne2",
              "d3",
              "Nc3",
              "a3",
              "bxa3",
              "Rxc3",
              "Qxc3",
              "d2",
              "a4",
              "d1=Q",
              "a5",
              "Qd6+",
              "g3",
              "Qd2",
              "Qc5",
              "Rc8",
              "Qb6",
              "Rc2",
              "Kg2",
              "Qe2",
              "a6",
              "Qxe4+",
              "Kh2",
              "Bd5",
              "g4",
              "Qg2#",
              "Nxg2",
              "Bxg2",
              "Kxg2",
              "Rxa2",
              "a7",
              "Rxa7",
              "Kg3",
              "Re7",
              "f4",
              "Kg7",
              "Kh4",
              "h6",
              "g5",
              "hxg5+",
              "fxg5",
              "Re4+",
              "Kg3",
              "Nd6",
              "h4",
              "Nf5+",
              "Kf3",
              "Rxh4",
              "Kg2",
              "Rg4+",
              "Kf3",
              "Rxg5",
              "Kf4",
              "Rg1",
              "Kf3",
              "g5",
              "Kf2",
              "Rg3",
              "Kf1",
              "g4",
              "Kf2",
              "Kg6",
              "Kf1",
              "Kg5",
              "Kf2",
              "Kf4",
              "Kf1",
              "Kf3",
              "Ke1",
              "Rg2",
              "Kf1",
              "Ne3+"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 116,
            "completion_tokens": 400,
            "total_tokens": 516,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 115,
            "completion_tokens": 640,
            "total_tokens": 755,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 347,
          "total_completion_tokens": 1680,
          "total_tokens": 2027
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 116,
            "completion_tokens": 400,
            "total_tokens": 516,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 115,
            "completion_tokens": 640,
            "total_tokens": 755,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 5,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bb5",
          "aggressive_uci": "f1b5",
          "aggressive_response": "3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 O-O 8. c3 d5 9. exd5 Nxd5 10. Nxe5 Nxe5 11. Rxe5 c6 12. d4 Bd6 13. Re1 Qh4 14. g3 Qh3 15. Be3 Bg4 16. Qd3 Rae8 17. Nd2 Re6 18. a4 Qh5 19. axb5 axb5 20. Ra6 Bc7 21. Bxd5 Qxd5 22. c4 bxc4 23. Qxc4 Qh5 24. Rxc6 Rxc6 25. Qxc6 Ba5 26. Ra1 Bxd2 27. Bxd2 Bf3 28. Qc4 Bd5 29. Qd3 Re8 30. Re1 Rxe1+ 31. Bxe1 Qh3 32. Qf1 Qg4 33. Qd3 Qh3 34. Qf1 Qg4 35. Qd3 Qh3 36. Qf1 Qg4 37. Qd3 Qh3 38. Qf1 Qg4 39. Qd3 Qh3 40. Qf1 Qg4 41. Qd3 Qh3 42. Qf1 Qg4 43. Qd3 Qh3 44. Qf1 Qg4 45. Qd3 Qh3 46. Qf1 Qg4 47. Qd3 Qh3 48. Qf1 Qg4 49. Qd3 Qh3 50. Qf1 Qg4 51. Qd3 Qh3 52. Qf1 Qg4 53. Qd3 Qh3 54. Qf1 Qg4 55. Qd3 Qh3 56. Qf1 Qg4 57. Qd3 Qh3 58. Qf1 Qg4 59. Qd3 Qh3 60. Qf1 Qg4 61. Qd3 Qh3 62. Qf1 Qg4 63. Qd3 Qh3 64. Qf1 Qg4 65. Qd3 Qh3 66. Qf1 Qg4 67. Qd3 Qh3 68. Qf1 Qg4 69. Qd3 Qh3 70. Qf1 Qg4 71. Qd3 Qh3 72. Qf1 Qg4 73. Qd3 Qh3 74. Qf1 Qg4 75. Qd3 Qh3 76. Qf1 Qg",
          "aggressive_tokens": {
            "prompt_tokens": 125,
            "completion_tokens": 640,
            "total_tokens": 765,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "O-O",
            "c3",
            "d5",
            "exd5",
            "Nxd5",
            "Nxe5",
            "Nxe5",
            "Rxe5",
            "c6",
            "d4",
            "Bd6",
            "Re1",
            "Qh4",
            "g3",
            "Qh3",
            "Be3",
            "Bg4",
            "Qd3",
            "Rae8",
            "Nd2",
            "Re6",
            "a4",
            "Qh5",
            "axb5",
            "axb5",
            "Ra6",
            "Bc7",
            "Bxd5",
            "Qxd5",
            "c4",
            "bxc4",
            "Qxc4",
            "Qh5",
            "Rxc6",
            "Rxc6",
            "Qxc6",
            "Ba5",
            "Ra1",
            "Bxd2",
            "Bxd2",
            "Bf3",
            "Qc4",
            "Bd5",
            "Qd3",
            "Re8",
            "Re1",
            "Rxe1+",
            "Bxe1",
            "Qh3",
            "Qf1",
            "Qg4",
            "Qd3",
            "Qh3",
            "Qf1",
            "Qg4",
            "Qd3",
            "Qh3",
            "Qf1",
            "Qg4",
            "Qd3",
            "Qh3",
            "Qf1",
            "Qg4",
            "Qd3",
            "Qh3",
            "Qf1",
            "Qg4",
            "Qd3",
            "Qh3",
            "Qf1",
            "Qg4",
            "Qd3",
            "Qh3",
            "Qf1",
            "Qg4",
            "Qd3",
            "Qh3",
            "Qf1",
            "Qg4",
            "Qd3",
            "Qh3",
            "Qf1",
            "Qg4",
            "Qd3",
            "Qh3",
            "Qf1",
            "Qg4",
            "Qd3",
            "Qh3",
            "Qf1",
            "Qg4",
            "Qd3",
            "Qh3",
            "Qf1",
            "Qg4",
            "Qd3",
            "Qh3",
            "Qf1",
            "Qg4",
            "Qd3",
            "Qh3",
            "Qf1",
            "Qg4",
            "Qd3",
            "Qh3",
            "Qf1",
            "Qg4",
            "Qd3",
            "Qh3",
            "Qf1",
            "Qg4",
            "Qd3",
            "Qh3",
            "Qf1",
            "Qg4",
            "Qd3",
            "Qh3",
            "Qf1",
            "Qg4",
            "Qd3",
            "Qh3",
            "Qf1",
            "Qg4",
            "Qd3",
            "Qh3",
            "Qf1",
            "Qg4",
            "Qd3",
            "Qh3",
            "Qf1",
            "Qg4",
            "Qd3",
            "Qh3",
            "Qf1",
            "Qg4",
            "Qd3",
            "Qh3",
            "Qf1"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bb5",
          "positional_uci": "f1b5",
          "positional_response": "3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 O-O 8. c3 d5 9. exd5 Nxd5 10. Nxe5 Nxe5 11. Rxe5 c6 12. d4 Bd6 13. Re1 Qh4 14. g3 Qh3 15. Be3 Bg4 16. Qd3 Rae8 17. Nd2 Re6 18. Qf1 Qh5 19. a4 Rfe8 20. axb5 axb5 21. Bxd5 Qxd5 22. Qg2 Qh5 23. Ra6 Bf8 24. Rxc6 Bh3 25. Qh1 Rxc6 26. Qxc6 Re6 27. Qf3 Bg4 28. Qg2 Bh3 29. Qf3 Bg4 30. Qh1 Bh3 31. Qf3 Bg4 32. Qh1 Bh3 33. Qf3 Bg4 34. Qh1 Bh3 35. Qf3 Bg4 36. Qh1 Bh3 37. Qf3 Bg4 38. Qh1 Bh3 39. Qf3 Bg4 40. Qh1 Bh3 41. Qf3 Bg4 42. Qh1 Bh3 43. Qf3 Bg4 44. Qh1 Bh3 45. Qf3 Bg4 46. Qh1 Bh3 47. Qf3 Bg4 48. Qh1 Bh3 49. Qf3 Bg4 50. Qh1 Bh3 51. Qf3 Bg4 52. Qh1 Bh3 53. Qf3 Bg4 54. Qh1 Bh3 55. Qf3 Bg4 56. Qh1 Bh3 57. Qf3 Bg4 58. Qh1 Bh3 59. Qf3 Bg4 60. Qh1 Bh3 61. Qf3 Bg4 62. Qh1 Bh3 63. Qf3 Bg4 64. Qh1 Bh3 65. Qf3 Bg4 66. Qh1 Bh3 67. Qf3 Bg4 68. Qh1 Bh3 69. Qf3 Bg4 70. Qh1 Bh3 71. Qf3 Bg4 72. Qh1 Bh3 73. Qf3 Bg4 74. Qh1 Bh3 75. Qf3 Bg4 76. Qh1 Bh3 77. Qf3 Bg4 78. Qh1 Bh3 79. Q",
          "positional_tokens": {
            "prompt_tokens": 125,
            "completion_tokens": 640,
            "total_tokens": 765,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "O-O",
            "c3",
            "d5",
            "exd5",
            "Nxd5",
            "Nxe5",
            "Nxe5",
            "Rxe5",
            "c6",
            "d4",
            "Bd6",
            "Re1",
            "Qh4",
            "g3",
            "Qh3",
            "Be3",
            "Bg4",
            "Qd3",
            "Rae8",
            "Nd2",
            "Re6",
            "Qf1",
            "Qh5",
            "a4",
            "Rfe8",
            "axb5",
            "axb5",
            "Bxd5",
            "Qxd5",
            "Qg2",
            "Qh5",
            "Ra6",
            "Bf8",
            "Rxc6",
            "Bh3",
            "Qh1",
            "Rxc6",
            "Qxc6",
            "Re6",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qh1",
            "Bh3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bb5",
          "neutral_uci": "f1b5",
          "neutral_response": "3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 O-O 8. c3 d5 9. exd5 Nxd5 10. Nxe5 Nxe5 11. Rxe5 c6 12. d4 Bd6 13. Re1 Qh4 14. g3 Qh3 15. Be3 Bg4 16. Qd3 Rae8 17. Nd2 Qh5 18. a4 Re6 19. axb5 axb5 20. Ra6 Bb8 21. Qf1 Rfe8 22. Bxd5 Qxd5 23. Qg2 Qh5 24. Rxc6 Bh3 25. Qf3 Bg4 26. Qg2 Bh3 27. Qf3 Bg4 28. Qg2 Bh3 29. Qf3 Bg4 30. Qg2 Bh3 31. Qf3 Bg4 32. Qg2 Bh3 33. Qf3 Bg4 34. Qg2 Bh3 35. Qf3 Bg4 36. Qg2 Bh3 37. Qf3 Bg4 38. Qg2 Bh3 39. Qf3 Bg4 40. Qg2 Bh3 41. Qf3 Bg4 42. Qg2 Bh3 43. Qf3 Bg4 44. Qg2 Bh3 45. Qf3 Bg4 46. Qg2 Bh3 47. Qf3 Bg4 48. Qg2 Bh3 49. Qf3 Bg4 50. Qg2 Bh3 51. Qf3 Bg4 52. Qg2 Bh3 53. Qf3 Bg4 54. Qg2 Bh3 55. Qf3 Bg4 56. Qg2 Bh3 57. Qf3 Bg4 58. Qg2 Bh3 59. Qf3 Bg4 60. Qg2 Bh3 61. Qf3 Bg4 62. Qg2 Bh3 63. Qf3 Bg4 64. Qg2 Bh3 65. Qf3 Bg4 66. Qg2 Bh3 67. Qf3 Bg4 68. Qg2 Bh3 69. Qf3 Bg4 70. Qg2 Bh3 71. Qf3 Bg4 72. Qg2 Bh3 73. Qf3 Bg4 74. Qg2 Bh3 75. Qf3 Bg4 76. Qg2 Bh3 77. Qf3 Bg4 78. Qg2 Bh3 79. Q",
          "neutral_tokens": {
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "O-O",
            "c3",
            "d5",
            "exd5",
            "Nxd5",
            "Nxe5",
            "Nxe5",
            "Rxe5",
            "c6",
            "d4",
            "Bd6",
            "Re1",
            "Qh4",
            "g3",
            "Qh3",
            "Be3",
            "Bg4",
            "Qd3",
            "Rae8",
            "Nd2",
            "Qh5",
            "a4",
            "Re6",
            "axb5",
            "axb5",
            "Ra6",
            "Bb8",
            "Qf1",
            "Rfe8",
            "Bxd5",
            "Qxd5",
            "Qg2",
            "Qh5",
            "Rxc6",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f1b5",
          "positional_uci": "f1b5",
          "neutral_uci": "f1b5",
          "consensus_move": "f1b5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "O-O",
              "c3",
              "d5",
              "exd5",
              "Nxd5",
              "Nxe5",
              "Nxe5",
              "Rxe5",
              "c6",
              "d4",
              "Bd6",
              "Re1",
              "Qh4",
              "g3",
              "Qh3",
              "Be3",
              "Bg4",
              "Qd3",
              "Rae8",
              "Nd2",
              "Re6",
              "a4",
              "Qh5",
              "axb5",
              "axb5",
              "Ra6",
              "Bc7",
              "Bxd5",
              "Qxd5",
              "c4",
              "bxc4",
              "Qxc4",
              "Qh5",
              "Rxc6",
              "Rxc6",
              "Qxc6",
              "Ba5",
              "Ra1",
              "Bxd2",
              "Bxd2",
              "Bf3",
              "Qc4",
              "Bd5",
              "Qd3",
              "Re8",
              "Re1",
              "Rxe1+",
              "Bxe1",
              "Qh3",
              "Qf1",
              "Qg4",
              "Qd3",
              "Qh3",
              "Qf1",
              "Qg4",
              "Qd3",
              "Qh3",
              "Qf1",
              "Qg4",
              "Qd3",
              "Qh3",
              "Qf1",
              "Qg4",
              "Qd3",
              "Qh3",
              "Qf1",
              "Qg4",
              "Qd3",
              "Qh3",
              "Qf1",
              "Qg4",
              "Qd3",
              "Qh3",
              "Qf1",
              "Qg4",
              "Qd3",
              "Qh3",
              "Qf1",
              "Qg4",
              "Qd3",
              "Qh3",
              "Qf1",
              "Qg4",
              "Qd3",
              "Qh3",
              "Qf1",
              "Qg4",
              "Qd3",
              "Qh3",
              "Qf1",
              "Qg4",
              "Qd3",
              "Qh3",
              "Qf1",
              "Qg4",
              "Qd3",
              "Qh3",
              "Qf1",
              "Qg4",
              "Qd3",
              "Qh3",
              "Qf1",
              "Qg4",
              "Qd3",
              "Qh3",
              "Qf1",
              "Qg4",
              "Qd3",
              "Qh3",
              "Qf1",
              "Qg4",
              "Qd3",
              "Qh3",
              "Qf1",
              "Qg4",
              "Qd3",
              "Qh3",
              "Qf1",
              "Qg4",
              "Qd3",
              "Qh3",
              "Qf1",
              "Qg4",
              "Qd3",
              "Qh3",
              "Qf1",
              "Qg4",
              "Qd3",
              "Qh3",
              "Qf1",
              "Qg4",
              "Qd3",
              "Qh3",
              "Qf1",
              "Qg4",
              "Qd3",
              "Qh3",
              "Qf1"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "O-O",
              "c3",
              "d5",
              "exd5",
              "Nxd5",
              "Nxe5",
              "Nxe5",
              "Rxe5",
              "c6",
              "d4",
              "Bd6",
              "Re1",
              "Qh4",
              "g3",
              "Qh3",
              "Be3",
              "Bg4",
              "Qd3",
              "Rae8",
              "Nd2",
              "Re6",
              "Qf1",
              "Qh5",
              "a4",
              "Rfe8",
              "axb5",
              "axb5",
              "Bxd5",
              "Qxd5",
              "Qg2",
              "Qh5",
              "Ra6",
              "Bf8",
              "Rxc6",
              "Bh3",
              "Qh1",
              "Rxc6",
              "Qxc6",
              "Re6",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qh1",
              "Bh3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "O-O",
              "c3",
              "d5",
              "exd5",
              "Nxd5",
              "Nxe5",
              "Nxe5",
              "Rxe5",
              "c6",
              "d4",
              "Bd6",
              "Re1",
              "Qh4",
              "g3",
              "Qh3",
              "Be3",
              "Bg4",
              "Qd3",
              "Rae8",
              "Nd2",
              "Qh5",
              "a4",
              "Re6",
              "axb5",
              "axb5",
              "Ra6",
              "Bb8",
              "Qf1",
              "Rfe8",
              "Bxd5",
              "Qxd5",
              "Qg2",
              "Qh5",
              "Rxc6",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 125,
            "completion_tokens": 640,
            "total_tokens": 765,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 125,
            "completion_tokens": 640,
            "total_tokens": 765,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 374,
          "total_completion_tokens": 1920,
          "total_tokens": 2294
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 125,
            "completion_tokens": 640,
            "total_tokens": 765,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 125,
            "completion_tokens": 640,
            "total_tokens": 765,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bqkbnr/pppp1ppp/2n5/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3"
    },
    {
      "turn": 7,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bxc6",
          "aggressive_uci": "b5c6",
          "aggressive_response": "4. Bxc6 dxc6 5. O-O Bg4 6. h3 h5 7. d3 Qf6 8. Nbd2 Ne7 9. Re1 Ng6 10. hxg4 hxg4 11. Nh2 Rxh2 12. Kxh2 Qh4+ 13. Kg1 Bc5 14. d4 Bxd4 15. Qe2 Nf4 16. Qf1 g3 17. Re2 Qh2# e4 e5 Nf3 Nc6 Bb5 a6 Bxc6 dxc6 O-O Bg4 h3 h5 d3 Qf6 Nbd2 Ne7 Re1 Ng6 hxg4 hxg4 Nh2 Rxh2 Kxh2 Qh4+ Kg1 Bc5 d4 Bxd4 Qe2 Nf4 Qf1 g3 Re2 Qh2#",
          "aggressive_tokens": {
            "prompt_tokens": 133,
            "completion_tokens": 207,
            "total_tokens": 340,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "dxc6",
            "O-O",
            "Bg4",
            "h3",
            "h5",
            "d3",
            "Qf6",
            "Nbd2",
            "Ne7",
            "Re1",
            "Ng6",
            "hxg4",
            "hxg4",
            "Nh2",
            "Rxh2",
            "Kxh2",
            "Qh4+",
            "Kg1",
            "Bc5",
            "d4",
            "Bxd4",
            "Qe2",
            "Nf4",
            "Qf1",
            "g3",
            "Re2",
            "Qh2#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bxc6",
          "positional_uci": "b5c6",
          "positional_response": "4. Bxc6 dxc6 5. O-O Bg4 6. h3 h5 7. d3 Qf6 8. Nbd2 Ne7 9. Re1 Ng6 10. d4 O-O-O 11. hxg4 hxg4 12. Nh2 Rxh2 13. Qxg4+ Kb8 14. Kxh2 Qxf2 15. Nf3 Bd6 16. Kh1 Rh8+ 17. Nh2 Qxe1+ 18. Nf1 Qxf1# 19. Kh2 exd4+ 20. g3 Qf2+ 21. Kh1 Qf1+ 22. Kh2 Ne5 23. Qf5 Nf3+ 24. Qxf3 Qxf3 25. Bf4 Bxf4 26. gxf4 Rxh3+ 27. Kg1 Rh1# 28. Kxh1 Qh3+ 29. Kg1 Qg3+ 30. Kh1 Qxf4 31. Kg1 Qxe4 32. Kh2 Qxc2+ 33. Kh3 Qxb2 34. Kh4 Qxa1 35. Kh5 Qxa2 36. Kg5 Qe6 37. Kh5 d3 38. Kg5 d2 39. Kh5 d1=Q+ 40. Kg5 Qdg4# 41. Kh5 Qeg6# 42. Kh4 Q6g5# 43. Kh3 Qg3# 44. Kh4 Q5g4# 45. Kh5 Qg6# 46. Kh4 Q3g4# 47. Kh5 Q6g5# 48. Kh4 Q5h5# 49. Kg3 Qhh3+ 50. Kf2 Qgg2+ 51. Ke1 Qhh1# 52. Kd2 Qhh3 53. Kc1 Qhh1# 54. Kd2 Qhh3 55. Kc1 Qhh1# 56. Kd2 Qhh3 57. Kc1 Qhh1# 58. Kd2 Qhh3 59. Kc1 Qhh1# 60. Kd2 Qhh3 61. Kc1 Qhh1# 62. Kd2 Qhh3 63. Kc1 Qhh1# 64. Kd2 Qhh3 65. Kc1 Qhh1# 66. Kd2 Qhh3 67. Kc1 Qhh1# 68. Kd2 Qhh3 69. Kc1 Qhh1# 70. Kd2 Qhh3 71. Kc1 Qhh1# 72. Kd2 Qhh3 73. Kc1 Qhh1# 74. Kd2 Qhh3",
          "positional_tokens": {
            "prompt_tokens": 133,
            "completion_tokens": 640,
            "total_tokens": 773,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "dxc6",
            "O-O",
            "Bg4",
            "h3",
            "h5",
            "d3",
            "Qf6",
            "Nbd2",
            "Ne7",
            "Re1",
            "Ng6",
            "d4",
            "O-O-O",
            "hxg4",
            "hxg4",
            "Nh2",
            "Rxh2",
            "Qxg4+",
            "Kb8",
            "Kxh2",
            "Qxf2",
            "Nf3",
            "Bd6",
            "Kh1",
            "Rh8+",
            "Nh2",
            "Qxe1+",
            "Nf1",
            "Qxf1#",
            "Kh2",
            "exd4+",
            "g3",
            "Qf2+",
            "Kh1",
            "Qf1+",
            "Kh2",
            "Ne5",
            "Qf5",
            "Nf3+",
            "Qxf3",
            "Qxf3",
            "Bf4",
            "Bxf4",
            "gxf4",
            "Rxh3+",
            "Kg1",
            "Rh1#",
            "Kxh1",
            "Qh3+",
            "Kg1",
            "Qg3+",
            "Kh1",
            "Qxf4",
            "Kg1",
            "Qxe4",
            "Kh2",
            "Qxc2+",
            "Kh3",
            "Qxb2",
            "Kh4",
            "Qxa1",
            "Kh5",
            "Qxa2",
            "Kg5",
            "Qe6",
            "Kh5",
            "d3",
            "Kg5",
            "d2",
            "Kh5",
            "d1=Q+",
            "Kg5",
            "Qdg4#",
            "Kh5",
            "Qeg6#",
            "Kh4",
            "Q6g5#",
            "Kh3",
            "Qg3#",
            "Kh4",
            "Q5g4#",
            "Kh5",
            "Qg6#",
            "Kh4",
            "Q3g4#",
            "Kh5",
            "Q6g5#",
            "Kh4",
            "Q5h5#",
            "Kg3",
            "Qhh3+",
            "Kf2",
            "Qgg2+",
            "Ke1",
            "Qhh1#",
            "Kd2",
            "Qhh3",
            "Kc1",
            "Qhh1#",
            "Kd2",
            "Qhh3",
            "Kc1",
            "Qhh1#",
            "Kd2",
            "Qhh3",
            "Kc1",
            "Qhh1#",
            "Kd2",
            "Qhh3",
            "Kc1",
            "Qhh1#",
            "Kd2",
            "Qhh3",
            "Kc1",
            "Qhh1#",
            "Kd2",
            "Qhh3",
            "Kc1",
            "Qhh1#",
            "Kd2",
            "Qhh3",
            "Kc1",
            "Qhh1#",
            "Kd2",
            "Qhh3",
            "Kc1",
            "Qhh1#",
            "Kd2",
            "Qhh3",
            "Kc1",
            "Qhh1#",
            "Kd2",
            "Qhh3",
            "Kc1",
            "Qhh1#",
            "Kd2",
            "Qhh3",
            "Kc1",
            "Qhh1#",
            "Kd2",
            "Qhh3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bxc6",
          "neutral_uci": "b5c6",
          "neutral_response": "4. Bxc6 dxc6 5. O-O Bg4 6. h3 h5 7. d3 Qf6 8. Nbd2 Ne7 9. Re1 Ng6 10. d4 Bd6 11. hxg4 hxg4 12. Nh2 Rxh2 13. Kxh2 Qxf2 14. Nf1 O-O-O 15. Kh1 Rh8+ 16. Nh2 Rxh2+ 17. Kxh2 Nh4 18. Qxg4+ f5 19. Qh3 Nf3+ 20. Qxf3 Qh4+ 21. Qh3 Qxe1 22. Qxf5+ Kb8 23. dxe5 Qh4+ 24. Qh3 Bxe5+ 25. g3 Qxe4 26. Qg2 Qh4+ 27. Kg1 Bxg3 28. Be3 Be5 29. Rf1 b6 30. c3 Kb7 31. Bf2 Qh5 32. Bg3 Bf6 33. Qf3 Qg5 34. Qf4 Qc5+ 35. Kg2 Qd5+ 36. Rf3 Qxa2 37. Qxc7+ Ka8 38. Qb8# e4 d5 Qd6# e4 d5 Qd6# e4 d5 Qd6",
          "neutral_tokens": {
            "prompt_tokens": 132,
            "completion_tokens": 327,
            "total_tokens": 459,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "dxc6",
            "O-O",
            "Bg4",
            "h3",
            "h5",
            "d3",
            "Qf6",
            "Nbd2",
            "Ne7",
            "Re1",
            "Ng6",
            "d4",
            "Bd6",
            "hxg4",
            "hxg4",
            "Nh2",
            "Rxh2",
            "Kxh2",
            "Qxf2",
            "Nf1",
            "O-O-O",
            "Kh1",
            "Rh8+",
            "Nh2",
            "Rxh2+",
            "Kxh2",
            "Nh4",
            "Qxg4+",
            "f5",
            "Qh3",
            "Nf3+",
            "Qxf3",
            "Qh4+",
            "Qh3",
            "Qxe1",
            "Qxf5+",
            "Kb8",
            "dxe5",
            "Qh4+",
            "Qh3",
            "Bxe5+",
            "g3",
            "Qxe4",
            "Qg2",
            "Qh4+",
            "Kg1",
            "Bxg3",
            "Be3",
            "Be5",
            "Rf1",
            "b6",
            "c3",
            "Kb7",
            "Bf2",
            "Qh5",
            "Bg3",
            "Bf6",
            "Qf3",
            "Qg5",
            "Qf4",
            "Qc5+",
            "Kg2",
            "Qd5+",
            "Rf3",
            "Qxa2",
            "Qxc7+",
            "Ka8",
            "Qb8#",
            "e4",
            "d5",
            "Qd6#",
            "e4",
            "d5",
            "Qd6#",
            "e4",
            "d5",
            "Qd6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b5c6",
          "positional_uci": "b5c6",
          "neutral_uci": "b5c6",
          "consensus_move": "b5c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "dxc6",
              "O-O",
              "Bg4",
              "h3",
              "h5",
              "d3",
              "Qf6",
              "Nbd2",
              "Ne7",
              "Re1",
              "Ng6",
              "hxg4",
              "hxg4",
              "Nh2",
              "Rxh2",
              "Kxh2",
              "Qh4+",
              "Kg1",
              "Bc5",
              "d4",
              "Bxd4",
              "Qe2",
              "Nf4",
              "Qf1",
              "g3",
              "Re2",
              "Qh2#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "dxc6",
              "O-O",
              "Bg4",
              "h3",
              "h5",
              "d3",
              "Qf6",
              "Nbd2",
              "Ne7",
              "Re1",
              "Ng6",
              "d4",
              "O-O-O",
              "hxg4",
              "hxg4",
              "Nh2",
              "Rxh2",
              "Qxg4+",
              "Kb8",
              "Kxh2",
              "Qxf2",
              "Nf3",
              "Bd6",
              "Kh1",
              "Rh8+",
              "Nh2",
              "Qxe1+",
              "Nf1",
              "Qxf1#",
              "Kh2",
              "exd4+",
              "g3",
              "Qf2+",
              "Kh1",
              "Qf1+",
              "Kh2",
              "Ne5",
              "Qf5",
              "Nf3+",
              "Qxf3",
              "Qxf3",
              "Bf4",
              "Bxf4",
              "gxf4",
              "Rxh3+",
              "Kg1",
              "Rh1#",
              "Kxh1",
              "Qh3+",
              "Kg1",
              "Qg3+",
              "Kh1",
              "Qxf4",
              "Kg1",
              "Qxe4",
              "Kh2",
              "Qxc2+",
              "Kh3",
              "Qxb2",
              "Kh4",
              "Qxa1",
              "Kh5",
              "Qxa2",
              "Kg5",
              "Qe6",
              "Kh5",
              "d3",
              "Kg5",
              "d2",
              "Kh5",
              "d1=Q+",
              "Kg5",
              "Qdg4#",
              "Kh5",
              "Qeg6#",
              "Kh4",
              "Q6g5#",
              "Kh3",
              "Qg3#",
              "Kh4",
              "Q5g4#",
              "Kh5",
              "Qg6#",
              "Kh4",
              "Q3g4#",
              "Kh5",
              "Q6g5#",
              "Kh4",
              "Q5h5#",
              "Kg3",
              "Qhh3+",
              "Kf2",
              "Qgg2+",
              "Ke1",
              "Qhh1#",
              "Kd2",
              "Qhh3",
              "Kc1",
              "Qhh1#",
              "Kd2",
              "Qhh3",
              "Kc1",
              "Qhh1#",
              "Kd2",
              "Qhh3",
              "Kc1",
              "Qhh1#",
              "Kd2",
              "Qhh3",
              "Kc1",
              "Qhh1#",
              "Kd2",
              "Qhh3",
              "Kc1",
              "Qhh1#",
              "Kd2",
              "Qhh3",
              "Kc1",
              "Qhh1#",
              "Kd2",
              "Qhh3",
              "Kc1",
              "Qhh1#",
              "Kd2",
              "Qhh3",
              "Kc1",
              "Qhh1#",
              "Kd2",
              "Qhh3",
              "Kc1",
              "Qhh1#",
              "Kd2",
              "Qhh3",
              "Kc1",
              "Qhh1#",
              "Kd2",
              "Qhh3",
              "Kc1",
              "Qhh1#",
              "Kd2",
              "Qhh3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "dxc6",
              "O-O",
              "Bg4",
              "h3",
              "h5",
              "d3",
              "Qf6",
              "Nbd2",
              "Ne7",
              "Re1",
              "Ng6",
              "d4",
              "Bd6",
              "hxg4",
              "hxg4",
              "Nh2",
              "Rxh2",
              "Kxh2",
              "Qxf2",
              "Nf1",
              "O-O-O",
              "Kh1",
              "Rh8+",
              "Nh2",
              "Rxh2+",
              "Kxh2",
              "Nh4",
              "Qxg4+",
              "f5",
              "Qh3",
              "Nf3+",
              "Qxf3",
              "Qh4+",
              "Qh3",
              "Qxe1",
              "Qxf5+",
              "Kb8",
              "dxe5",
              "Qh4+",
              "Qh3",
              "Bxe5+",
              "g3",
              "Qxe4",
              "Qg2",
              "Qh4+",
              "Kg1",
              "Bxg3",
              "Be3",
              "Be5",
              "Rf1",
              "b6",
              "c3",
              "Kb7",
              "Bf2",
              "Qh5",
              "Bg3",
              "Bf6",
              "Qf3",
              "Qg5",
              "Qf4",
              "Qc5+",
              "Kg2",
              "Qd5+",
              "Rf3",
              "Qxa2",
              "Qxc7+",
              "Ka8",
              "Qb8#",
              "e4",
              "d5",
              "Qd6#",
              "e4",
              "d5",
              "Qd6#",
              "e4",
              "d5",
              "Qd6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 133,
            "completion_tokens": 207,
            "total_tokens": 340,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 133,
            "completion_tokens": 640,
            "total_tokens": 773,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 132,
            "completion_tokens": 327,
            "total_tokens": 459,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 398,
          "total_completion_tokens": 1174,
          "total_tokens": 1572
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 133,
            "completion_tokens": 207,
            "total_tokens": 340,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 133,
            "completion_tokens": 640,
            "total_tokens": 773,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 132,
            "completion_tokens": 327,
            "total_tokens": 459,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkbnr/1ppp1ppp/p1n5/1B2p3/4P3/5N2/PPPP1PPP/RNBQK2R w KQkq - 0 4"
    },
    {
      "turn": 9,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nxe5",
          "aggressive_uci": "f3e5",
          "aggressive_response": "5. Nxe5 Qe7 6. d4 d6 7. Nxc6 Qxe4+ 8. Qe2 Qxe2+ 9. Kxe2 Bb7 10. d5 Bxc6 11. dxc6 Ne7 12. Re1 Nxc6 13. Kf1+ Be7 14. Nc3 O-O 15. Nd5 Bd8 16. Bf4 Nd4 17. Rac1 Ne6 18. Bg3 Rb8 19. b3 Rb5 20. c4 Ra5 21. Rc2 Nd4 22. Rd2 Nf5 23. Bf4 g5 24. Be3 Nxe3+ 25. Rxe3 c6 26. Ne7+ Bxe7 27. Rxe7 d5 28. cxd5 cxd5 29. Rd7 Rc8 30. R7xd5 Rxd5 31. Rxd5 Rc1+ 32. Ke2 Rc2+ 33. Rd2 Rc6 34. Kd3 Kg7 35. Rc2 Rd6+ 36. Kc4 Rc6+ 37. Kd3 Rd6+ 38. Ke4 Kg6 39. Ke5 Re6+ 40. Kd5 h5 41. Rc6 Kf5 42. Rxe6 fxe6+ 43. Kd4 e5+ 44. Ke3 e4 45. b4 Ke5 46. a4 Kd5 47. b5 axb5 48. axb5 Kc5 49. Kxe4 Kxb5 50. Kf5 Kc5 51. Kxg5 Kd5 52. Kxh5 Ke5 53. Kg5 Ke6 54. h4 Kf7 55. h5 Kg7 56. h6+ Kh7 57. f4 Kg8 58. f5 Kh7 59. f6 Kg8 60. Kg6 Kh8 61. g4 Kg8 62. f7+ Kf8 63. h7 Ke7 64. h8=Q Ke6 65. f8=Q Kd5 66. Qhf6 Ke4 67. Q8e7+ Kd5 68. Qfd6+ Kc4 69. Qec7+ Kb5 70. Qdb6+ Ka4 71. Qca7# Qe7 Nf6+ Kg8 Qe8#",
          "aggressive_tokens": {
            "prompt_tokens": 142,
            "completion_tokens": 566,
            "total_tokens": 708,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe7",
            "d4",
            "d6",
            "Nxc6",
            "Qxe4+",
            "Qe2",
            "Qxe2+",
            "Kxe2",
            "Bb7",
            "d5",
            "Bxc6",
            "dxc6",
            "Ne7",
            "Re1",
            "Nxc6",
            "Kf1+",
            "Be7",
            "Nc3",
            "O-O",
            "Nd5",
            "Bd8",
            "Bf4",
            "Nd4",
            "Rac1",
            "Ne6",
            "Bg3",
            "Rb8",
            "b3",
            "Rb5",
            "c4",
            "Ra5",
            "Rc2",
            "Nd4",
            "Rd2",
            "Nf5",
            "Bf4",
            "g5",
            "Be3",
            "Nxe3+",
            "Rxe3",
            "c6",
            "Ne7+",
            "Bxe7",
            "Rxe7",
            "d5",
            "cxd5",
            "cxd5",
            "Rd7",
            "Rc8",
            "R7xd5",
            "Rxd5",
            "Rxd5",
            "Rc1+",
            "Ke2",
            "Rc2+",
            "Rd2",
            "Rc6",
            "Kd3",
            "Kg7",
            "Rc2",
            "Rd6+",
            "Kc4",
            "Rc6+",
            "Kd3",
            "Rd6+",
            "Ke4",
            "Kg6",
            "Ke5",
            "Re6+",
            "Kd5",
            "h5",
            "Rc6",
            "Kf5",
            "Rxe6",
            "fxe6+",
            "Kd4",
            "e5+",
            "Ke3",
            "e4",
            "b4",
            "Ke5",
            "a4",
            "Kd5",
            "b5",
            "axb5",
            "axb5",
            "Kc5",
            "Kxe4",
            "Kxb5",
            "Kf5",
            "Kc5",
            "Kxg5",
            "Kd5",
            "Kxh5",
            "Ke5",
            "Kg5",
            "Ke6",
            "h4",
            "Kf7",
            "h5",
            "Kg7",
            "h6+",
            "Kh7",
            "f4",
            "Kg8",
            "f5",
            "Kh7",
            "f6",
            "Kg8",
            "Kg6",
            "Kh8",
            "g4",
            "Kg8",
            "f7+",
            "Kf8",
            "h7",
            "Ke7",
            "h8=Q",
            "Ke6",
            "f8=Q",
            "Kd5",
            "Qhf6",
            "Ke4",
            "Q8e7+",
            "Kd5",
            "Qfd6+",
            "Kc4",
            "Qec7+",
            "Kb5",
            "Qdb6+",
            "Ka4",
            "Qca7#",
            "Qe7",
            "Nf6+",
            "Kg8",
            "Qe8#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nxe5",
          "positional_uci": "f3e5",
          "positional_response": "5. Nxe5 Qe7 6. d4 d6 7. Nxc6 Qxe4+ 8. Qe2 Qxe2+ 9. Kxe2 Bb7 10. d5 Bxc6 11. dxc6 Ne7 12. Re1 Nxc6 13. Kf1+ Be7 14. Nc3 O-O 15. Nd5 Bd8 16. c3 Ne5 17. Bf4 Nd3 18. Re2 Nxf4 19. Nxf4 Bg5 20. Nd5 c6 21. Ne7+ Bxe7 22. Rxe7 Rfe8 23. Rae1 Kf8 24. Rxe8+ Rxe8 25. Rxe8+ Kxe8 26. Ke2 Ke7 27. Ke3 Ke6 28. Ke4 f5+ 29. Kd4 c5+ 30. Kc4 Ke5 31. b4 cxb4 32. cxb4 d5+ 33. Kd3 g5 34. a4 h5 35. b5 axb5 36. axb5 Kd6 37. Kd4 f4 38. b6 Kc6 39. b7 Kxb7 40. Kxd5 Kc7 41. Ke5 Kd7 42. Kf5 Ke7 43. Kxg5 Kf7 44. Kxf4 Kg6 45. h4 Kf6 46. g4 hxg4 47. Kxg4 Kg6 48. f4 Kf6 49. h5 Kg7 50. Kg5 Kh7 51. f5 Kg7 52. h6+ Kh7 53. f6 Kg8 54. Kg6 Kh8 55. Kf7 Kh7 56. Ke7 Kxh6 57. f7 Kg7 58. f8=Q+ Kg6 59. Qf6+ Kh5 60. Qg7 Kh4 61. Kf6 Kh3 62. Kf5 Kh2 63. Kf4 Kh1 64. Kf3 Kh2 65. Qg2# e4 d5 Nc3 d4 Nxd4+ Qd5+ Ke6 Qe5+ Kd6 Qe6+ Kc6 Qe5+ Kd6 Qe6+ Kc6 Qe5+ Kd6 Qe6+ Kc6 Qe5+ Kd6 Qe6+ Kc6 Qe5+ Kd6 Qe6+ Kc6 Qe5+ Kd6 Qe6+ Kc6 Qe5+ Kd6 Qe6+ Kc6 Qe5+ Kd6 Qe6+ Kc6 Qe5+ Kd6 Qe6",
          "positional_tokens": {
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Qe7",
            "d4",
            "d6",
            "Nxc6",
            "Qxe4+",
            "Qe2",
            "Qxe2+",
            "Kxe2",
            "Bb7",
            "d5",
            "Bxc6",
            "dxc6",
            "Ne7",
            "Re1",
            "Nxc6",
            "Kf1+",
            "Be7",
            "Nc3",
            "O-O",
            "Nd5",
            "Bd8",
            "c3",
            "Ne5",
            "Bf4",
            "Nd3",
            "Re2",
            "Nxf4",
            "Nxf4",
            "Bg5",
            "Nd5",
            "c6",
            "Ne7+",
            "Bxe7",
            "Rxe7",
            "Rfe8",
            "Rae1",
            "Kf8",
            "Rxe8+",
            "Rxe8",
            "Rxe8+",
            "Kxe8",
            "Ke2",
            "Ke7",
            "Ke3",
            "Ke6",
            "Ke4",
            "f5+",
            "Kd4",
            "c5+",
            "Kc4",
            "Ke5",
            "b4",
            "cxb4",
            "cxb4",
            "d5+",
            "Kd3",
            "g5",
            "a4",
            "h5",
            "b5",
            "axb5",
            "axb5",
            "Kd6",
            "Kd4",
            "f4",
            "b6",
            "Kc6",
            "b7",
            "Kxb7",
            "Kxd5",
            "Kc7",
            "Ke5",
            "Kd7",
            "Kf5",
            "Ke7",
            "Kxg5",
            "Kf7",
            "Kxf4",
            "Kg6",
            "h4",
            "Kf6",
            "g4",
            "hxg4",
            "Kxg4",
            "Kg6",
            "f4",
            "Kf6",
            "h5",
            "Kg7",
            "Kg5",
            "Kh7",
            "f5",
            "Kg7",
            "h6+",
            "Kh7",
            "f6",
            "Kg8",
            "Kg6",
            "Kh8",
            "Kf7",
            "Kh7",
            "Ke7",
            "Kxh6",
            "f7",
            "Kg7",
            "f8=Q+",
            "Kg6",
            "Qf6+",
            "Kh5",
            "Qg7",
            "Kh4",
            "Kf6",
            "Kh3",
            "Kf5",
            "Kh2",
            "Kf4",
            "Kh1",
            "Kf3",
            "Kh2",
            "Qg2#",
            "e4",
            "d5",
            "Nc3",
            "d4",
            "Nxd4+",
            "Qd5+",
            "Ke6",
            "Qe5+",
            "Kd6",
            "Qe6+",
            "Kc6",
            "Qe5+",
            "Kd6",
            "Qe6+",
            "Kc6",
            "Qe5+",
            "Kd6",
            "Qe6+",
            "Kc6",
            "Qe5+",
            "Kd6",
            "Qe6+",
            "Kc6",
            "Qe5+",
            "Kd6",
            "Qe6+",
            "Kc6",
            "Qe5+",
            "Kd6",
            "Qe6+",
            "Kc6",
            "Qe5+",
            "Kd6",
            "Qe6+",
            "Kc6",
            "Qe5+",
            "Kd6",
            "Qe6+",
            "Kc6",
            "Qe5+",
            "Kd6",
            "Qe6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nxe5",
          "neutral_uci": "f3e5",
          "neutral_response": "5. Nxe5 Qe7 6. d4 d6 7. Nxc6 Qxe4+ 8. Qe2 Qxe2+ 9. Kxe2 Bb7 10. d5 Bxc6 11. dxc6 Ne7 12. Re1 Nxc6 13. Kf1+ Be7 14. Nc3 O-O 15. Nd5 Bd8 16. c3 Ne5 17. Bf4 Nd3 18. Re2 Nxf4 19. Nxf4 Bg5 20. Nd5 c6 21. Ne7+ Bxe7 22. Rxe7 Rfe8 23. Rae1 Kf8 24. Rxe8+ Rxe8 25. Rxe8+ Kxe8 26. Ke2 Ke7 27. Kd3 Ke6 28. Kd4 c5+ 29. Kc4 Ke5 30. b4 cxb4 31. cxb4 d5+ 32. Kd3 Kd6 33. Kd4 Kc6 34. a4 f6 35. f4 g6 36. g4 h6 37. h4 Kd6 38. b5 axb5 39. axb5 g5 40. fxg5 fxg5 41. hxg5 hxg5 42. b6 Kc6 43. b7 Kxb7 44. Kxd5 Kc7 45. Ke5 Kd7 46. Kf5 Ke7 47. Kxg5 Kf7 48. Kh6 Kg8 49. Kg6 Kh8 50. Kf7 Kh7 51. g5 Kh8 52. Kg6 Kg8 53. Kh6 Kh8 54. g6 Kg8 55. g7 Kf7 56. Kh7 Kf6 57. g8=Q Kf5 58. Qg6+ Ke5 59. Kg7 Kd5 60. Kf7 Ke5 61. Qf6+ Kd5 62. Ke7 Ke4 63. Ke6 Kd3 64. Qf4 Kc3 65. Kd5 Kd3 66. Qe4+ Kc3 67. Qd4+ Kb3 68. Kc5 Kc2 69. Kc4 Kc1 70. Qf2 Kd1 71. Kd3 Kc1 72. Qc2# Kd1 Kc1 Kd1",
          "neutral_tokens": {
            "prompt_tokens": 141,
            "completion_tokens": 574,
            "total_tokens": 715,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe7",
            "d4",
            "d6",
            "Nxc6",
            "Qxe4+",
            "Qe2",
            "Qxe2+",
            "Kxe2",
            "Bb7",
            "d5",
            "Bxc6",
            "dxc6",
            "Ne7",
            "Re1",
            "Nxc6",
            "Kf1+",
            "Be7",
            "Nc3",
            "O-O",
            "Nd5",
            "Bd8",
            "c3",
            "Ne5",
            "Bf4",
            "Nd3",
            "Re2",
            "Nxf4",
            "Nxf4",
            "Bg5",
            "Nd5",
            "c6",
            "Ne7+",
            "Bxe7",
            "Rxe7",
            "Rfe8",
            "Rae1",
            "Kf8",
            "Rxe8+",
            "Rxe8",
            "Rxe8+",
            "Kxe8",
            "Ke2",
            "Ke7",
            "Kd3",
            "Ke6",
            "Kd4",
            "c5+",
            "Kc4",
            "Ke5",
            "b4",
            "cxb4",
            "cxb4",
            "d5+",
            "Kd3",
            "Kd6",
            "Kd4",
            "Kc6",
            "a4",
            "f6",
            "f4",
            "g6",
            "g4",
            "h6",
            "h4",
            "Kd6",
            "b5",
            "axb5",
            "axb5",
            "g5",
            "fxg5",
            "fxg5",
            "hxg5",
            "hxg5",
            "b6",
            "Kc6",
            "b7",
            "Kxb7",
            "Kxd5",
            "Kc7",
            "Ke5",
            "Kd7",
            "Kf5",
            "Ke7",
            "Kxg5",
            "Kf7",
            "Kh6",
            "Kg8",
            "Kg6",
            "Kh8",
            "Kf7",
            "Kh7",
            "g5",
            "Kh8",
            "Kg6",
            "Kg8",
            "Kh6",
            "Kh8",
            "g6",
            "Kg8",
            "g7",
            "Kf7",
            "Kh7",
            "Kf6",
            "g8=Q",
            "Kf5",
            "Qg6+",
            "Ke5",
            "Kg7",
            "Kd5",
            "Kf7",
            "Ke5",
            "Qf6+",
            "Kd5",
            "Ke7",
            "Ke4",
            "Ke6",
            "Kd3",
            "Qf4",
            "Kc3",
            "Kd5",
            "Kd3",
            "Qe4+",
            "Kc3",
            "Qd4+",
            "Kb3",
            "Kc5",
            "Kc2",
            "Kc4",
            "Kc1",
            "Qf2",
            "Kd1",
            "Kd3",
            "Kc1",
            "Qc2#",
            "Kd1",
            "Kc1",
            "Kd1"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f3e5",
          "positional_uci": "f3e5",
          "neutral_uci": "f3e5",
          "consensus_move": "f3e5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qe7",
              "d4",
              "d6",
              "Nxc6",
              "Qxe4+",
              "Qe2",
              "Qxe2+",
              "Kxe2",
              "Bb7",
              "d5",
              "Bxc6",
              "dxc6",
              "Ne7",
              "Re1",
              "Nxc6",
              "Kf1+",
              "Be7",
              "Nc3",
              "O-O",
              "Nd5",
              "Bd8",
              "Bf4",
              "Nd4",
              "Rac1",
              "Ne6",
              "Bg3",
              "Rb8",
              "b3",
              "Rb5",
              "c4",
              "Ra5",
              "Rc2",
              "Nd4",
              "Rd2",
              "Nf5",
              "Bf4",
              "g5",
              "Be3",
              "Nxe3+",
              "Rxe3",
              "c6",
              "Ne7+",
              "Bxe7",
              "Rxe7",
              "d5",
              "cxd5",
              "cxd5",
              "Rd7",
              "Rc8",
              "R7xd5",
              "Rxd5",
              "Rxd5",
              "Rc1+",
              "Ke2",
              "Rc2+",
              "Rd2",
              "Rc6",
              "Kd3",
              "Kg7",
              "Rc2",
              "Rd6+",
              "Kc4",
              "Rc6+",
              "Kd3",
              "Rd6+",
              "Ke4",
              "Kg6",
              "Ke5",
              "Re6+",
              "Kd5",
              "h5",
              "Rc6",
              "Kf5",
              "Rxe6",
              "fxe6+",
              "Kd4",
              "e5+",
              "Ke3",
              "e4",
              "b4",
              "Ke5",
              "a4",
              "Kd5",
              "b5",
              "axb5",
              "axb5",
              "Kc5",
              "Kxe4",
              "Kxb5",
              "Kf5",
              "Kc5",
              "Kxg5",
              "Kd5",
              "Kxh5",
              "Ke5",
              "Kg5",
              "Ke6",
              "h4",
              "Kf7",
              "h5",
              "Kg7",
              "h6+",
              "Kh7",
              "f4",
              "Kg8",
              "f5",
              "Kh7",
              "f6",
              "Kg8",
              "Kg6",
              "Kh8",
              "g4",
              "Kg8",
              "f7+",
              "Kf8",
              "h7",
              "Ke7",
              "h8=Q",
              "Ke6",
              "f8=Q",
              "Kd5",
              "Qhf6",
              "Ke4",
              "Q8e7+",
              "Kd5",
              "Qfd6+",
              "Kc4",
              "Qec7+",
              "Kb5",
              "Qdb6+",
              "Ka4",
              "Qca7#",
              "Qe7",
              "Nf6+",
              "Kg8",
              "Qe8#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qe7",
              "d4",
              "d6",
              "Nxc6",
              "Qxe4+",
              "Qe2",
              "Qxe2+",
              "Kxe2",
              "Bb7",
              "d5",
              "Bxc6",
              "dxc6",
              "Ne7",
              "Re1",
              "Nxc6",
              "Kf1+",
              "Be7",
              "Nc3",
              "O-O",
              "Nd5",
              "Bd8",
              "c3",
              "Ne5",
              "Bf4",
              "Nd3",
              "Re2",
              "Nxf4",
              "Nxf4",
              "Bg5",
              "Nd5",
              "c6",
              "Ne7+",
              "Bxe7",
              "Rxe7",
              "Rfe8",
              "Rae1",
              "Kf8",
              "Rxe8+",
              "Rxe8",
              "Rxe8+",
              "Kxe8",
              "Ke2",
              "Ke7",
              "Ke3",
              "Ke6",
              "Ke4",
              "f5+",
              "Kd4",
              "c5+",
              "Kc4",
              "Ke5",
              "b4",
              "cxb4",
              "cxb4",
              "d5+",
              "Kd3",
              "g5",
              "a4",
              "h5",
              "b5",
              "axb5",
              "axb5",
              "Kd6",
              "Kd4",
              "f4",
              "b6",
              "Kc6",
              "b7",
              "Kxb7",
              "Kxd5",
              "Kc7",
              "Ke5",
              "Kd7",
              "Kf5",
              "Ke7",
              "Kxg5",
              "Kf7",
              "Kxf4",
              "Kg6",
              "h4",
              "Kf6",
              "g4",
              "hxg4",
              "Kxg4",
              "Kg6",
              "f4",
              "Kf6",
              "h5",
              "Kg7",
              "Kg5",
              "Kh7",
              "f5",
              "Kg7",
              "h6+",
              "Kh7",
              "f6",
              "Kg8",
              "Kg6",
              "Kh8",
              "Kf7",
              "Kh7",
              "Ke7",
              "Kxh6",
              "f7",
              "Kg7",
              "f8=Q+",
              "Kg6",
              "Qf6+",
              "Kh5",
              "Qg7",
              "Kh4",
              "Kf6",
              "Kh3",
              "Kf5",
              "Kh2",
              "Kf4",
              "Kh1",
              "Kf3",
              "Kh2",
              "Qg2#",
              "e4",
              "d5",
              "Nc3",
              "d4",
              "Nxd4+",
              "Qd5+",
              "Ke6",
              "Qe5+",
              "Kd6",
              "Qe6+",
              "Kc6",
              "Qe5+",
              "Kd6",
              "Qe6+",
              "Kc6",
              "Qe5+",
              "Kd6",
              "Qe6+",
              "Kc6",
              "Qe5+",
              "Kd6",
              "Qe6+",
              "Kc6",
              "Qe5+",
              "Kd6",
              "Qe6+",
              "Kc6",
              "Qe5+",
              "Kd6",
              "Qe6+",
              "Kc6",
              "Qe5+",
              "Kd6",
              "Qe6+",
              "Kc6",
              "Qe5+",
              "Kd6",
              "Qe6+",
              "Kc6",
              "Qe5+",
              "Kd6",
              "Qe6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qe7",
              "d4",
              "d6",
              "Nxc6",
              "Qxe4+",
              "Qe2",
              "Qxe2+",
              "Kxe2",
              "Bb7",
              "d5",
              "Bxc6",
              "dxc6",
              "Ne7",
              "Re1",
              "Nxc6",
              "Kf1+",
              "Be7",
              "Nc3",
              "O-O",
              "Nd5",
              "Bd8",
              "c3",
              "Ne5",
              "Bf4",
              "Nd3",
              "Re2",
              "Nxf4",
              "Nxf4",
              "Bg5",
              "Nd5",
              "c6",
              "Ne7+",
              "Bxe7",
              "Rxe7",
              "Rfe8",
              "Rae1",
              "Kf8",
              "Rxe8+",
              "Rxe8",
              "Rxe8+",
              "Kxe8",
              "Ke2",
              "Ke7",
              "Kd3",
              "Ke6",
              "Kd4",
              "c5+",
              "Kc4",
              "Ke5",
              "b4",
              "cxb4",
              "cxb4",
              "d5+",
              "Kd3",
              "Kd6",
              "Kd4",
              "Kc6",
              "a4",
              "f6",
              "f4",
              "g6",
              "g4",
              "h6",
              "h4",
              "Kd6",
              "b5",
              "axb5",
              "axb5",
              "g5",
              "fxg5",
              "fxg5",
              "hxg5",
              "hxg5",
              "b6",
              "Kc6",
              "b7",
              "Kxb7",
              "Kxd5",
              "Kc7",
              "Ke5",
              "Kd7",
              "Kf5",
              "Ke7",
              "Kxg5",
              "Kf7",
              "Kh6",
              "Kg8",
              "Kg6",
              "Kh8",
              "Kf7",
              "Kh7",
              "g5",
              "Kh8",
              "Kg6",
              "Kg8",
              "Kh6",
              "Kh8",
              "g6",
              "Kg8",
              "g7",
              "Kf7",
              "Kh7",
              "Kf6",
              "g8=Q",
              "Kf5",
              "Qg6+",
              "Ke5",
              "Kg7",
              "Kd5",
              "Kf7",
              "Ke5",
              "Qf6+",
              "Kd5",
              "Ke7",
              "Ke4",
              "Ke6",
              "Kd3",
              "Qf4",
              "Kc3",
              "Kd5",
              "Kd3",
              "Qe4+",
              "Kc3",
              "Qd4+",
              "Kb3",
              "Kc5",
              "Kc2",
              "Kc4",
              "Kc1",
              "Qf2",
              "Kd1",
              "Kd3",
              "Kc1",
              "Qc2#",
              "Kd1",
              "Kc1",
              "Kd1"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 142,
            "completion_tokens": 566,
            "total_tokens": 708,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 141,
            "completion_tokens": 574,
            "total_tokens": 715,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 425,
          "total_completion_tokens": 1780,
          "total_tokens": 2205
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 142,
            "completion_tokens": 566,
            "total_tokens": 708,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 141,
            "completion_tokens": 574,
            "total_tokens": 715,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkbnr/2pp1ppp/p1p5/4p3/4P3/5N2/PPPP1PPP/RNBQK2R w KQkq - 0 5"
    },
    {
      "turn": 11,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "O-O",
          "aggressive_uci": "e1g1",
          "aggressive_response": "6. O-O Nxe4 7. Re1 d5 8. d3 f6 9. Qh5+ g6 10. Nxg6 hxg6 11. Qxh8 Kf7 12. dxe4 dxe4 13. Nc3 Bf5 14. Bh6 Bxh6 15. Qxh6 Qd4 16. Rad1 Qb4 17. Qh7+ Ke6 18. Qd7+ Ke5 19. Qxc7+ Ke6 20. Qxc6+ Ke5 21. Qxa8 Qxb2 22. Qd5+ Kf4 23. g3+ Kg5 24. Nxe4+ Kh6 25. Qd2+ g5 26. h4 Bxe4 27. Rxe4 Qxa2 28. hxg5+ fxg5 29. Qd6+ Kh5 30. g4+ Kh4 31. Qh6# e5 Nf6 *",
          "aggressive_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 233,
            "total_tokens": 384,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxe4",
            "Re1",
            "d5",
            "d3",
            "f6",
            "Qh5+",
            "g6",
            "Nxg6",
            "hxg6",
            "Qxh8",
            "Kf7",
            "dxe4",
            "dxe4",
            "Nc3",
            "Bf5",
            "Bh6",
            "Bxh6",
            "Qxh6",
            "Qd4",
            "Rad1",
            "Qb4",
            "Qh7+",
            "Ke6",
            "Qd7+",
            "Ke5",
            "Qxc7+",
            "Ke6",
            "Qxc6+",
            "Ke5",
            "Qxa8",
            "Qxb2",
            "Qd5+",
            "Kf4",
            "g3+",
            "Kg5",
            "Nxe4+",
            "Kh6",
            "Qd2+",
            "g5",
            "h4",
            "Bxe4",
            "Rxe4",
            "Qxa2",
            "hxg5+",
            "fxg5",
            "Qd6+",
            "Kh5",
            "g4+",
            "Kh4",
            "Qh6#",
            "e5",
            "Nf6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "O-O",
          "positional_uci": "e1g1",
          "positional_response": "6. O-O Nxe4 7. Re1 d5 8. d3 f6 9. Qh5+ g6 10. Nxg6 hxg6 11. Qxh8 Kf7 12. dxe4 dxe4 13. Nc3 Bf5 14. Bh6 Bxh6 15. Qxh6 Qd4 16. Rad1 Qb4 17. Qh7+ Ke6 18. Qd7+ Ke5 19. Qxc7+ Ke6 20. Qxc6+ Kf7 21. Qxa8 Qxb2 22. Qa7+ Ke6 23. Qd7+ Ke5 24. Qd5+ Kf4 25. g3+ Kg5 26. Nxe4+ Kh6 27. Qd2+ g5 28. h4 Bxe4 29. Rxe4 Qxa2 30. hxg5+ fxg5 31. Qd6+ Kh5 32. g4+ Kh4 33. Qh6# e5 dxe5 Qxe5+ Kg6 Qxe5+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg6 Qe6+ Kg7 Qe7+ Kg",
          "positional_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 640,
            "total_tokens": 791,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nxe4",
            "Re1",
            "d5",
            "d3",
            "f6",
            "Qh5+",
            "g6",
            "Nxg6",
            "hxg6",
            "Qxh8",
            "Kf7",
            "dxe4",
            "dxe4",
            "Nc3",
            "Bf5",
            "Bh6",
            "Bxh6",
            "Qxh6",
            "Qd4",
            "Rad1",
            "Qb4",
            "Qh7+",
            "Ke6",
            "Qd7+",
            "Ke5",
            "Qxc7+",
            "Ke6",
            "Qxc6+",
            "Kf7",
            "Qxa8",
            "Qxb2",
            "Qa7+",
            "Ke6",
            "Qd7+",
            "Ke5",
            "Qd5+",
            "Kf4",
            "g3+",
            "Kg5",
            "Nxe4+",
            "Kh6",
            "Qd2+",
            "g5",
            "h4",
            "Bxe4",
            "Rxe4",
            "Qxa2",
            "hxg5+",
            "fxg5",
            "Qd6+",
            "Kh5",
            "g4+",
            "Kh4",
            "Qh6#",
            "e5",
            "dxe5",
            "Qxe5+",
            "Kg6",
            "Qxe5+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+",
            "Kg6",
            "Qe6+",
            "Kg7",
            "Qe7+"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "O-O",
          "neutral_uci": "e1g1",
          "neutral_response": "6. O-O Be7 7. d3 O-O 8. Nc3 d6 9. Nxc6 Qd7 10. Nxe7+ Qxe7 11. Bg5 h6 12. Nd5 Qe5 13. Bxf6 gxf6 14. Nxc7 Ra7 15. Nd5 Kh7 16. Qf3 f5 17. exf5 Bxf5 18. Rae1 Qxb2 19. Qxf5+ Kh8 20. Qf6+ Qxf6 21. Nxf6 Kg7 22. Ne8+ Kg6 23. Nxd6 Rd7 24. Nc4 Rc8 25. Ne5+ Kf6 26. Nxd7+ Kg7 27. c4 Rd8 28. Nc5 Rc8 29. Nxa6 Ra8 30. Nb4 Ra4 31. Rb1 Ra3 32. Rb3 Ra4 33. a3 Ra8 34. c5 Rc8 35. c6 Kf6 36. Re1 Kg5 37. Re7 f5 38. c7 Kf6 39. Nd5+ Kg5 40. Rb8 Rxc7 41. Rxc7 f4 42. Rg8+ Kf5 43. Rf7+ Ke5 44. Nxf4 Kd4 45. Re8 Kc3 46. Rd7 Kd2 47. d4 Kc3 48. d5 Kd4 49. d6 Kc5 50. Rc8+ Kb6 51. Rdc7 Ka6 52. d7 Kb6 53. d8=Q Ka6 54. Qd6+ Kb5 55. Qb4+ Ka6 56. Ra8# e5 Nf6+ Kg7",
          "neutral_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 422,
            "total_tokens": 572,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc3",
            "d6",
            "Nxc6",
            "Qd7",
            "Nxe7+",
            "Qxe7",
            "Bg5",
            "h6",
            "Nd5",
            "Qe5",
            "Bxf6",
            "gxf6",
            "Nxc7",
            "Ra7",
            "Nd5",
            "Kh7",
            "Qf3",
            "f5",
            "exf5",
            "Bxf5",
            "Rae1",
            "Qxb2",
            "Qxf5+",
            "Kh8",
            "Qf6+",
            "Qxf6",
            "Nxf6",
            "Kg7",
            "Ne8+",
            "Kg6",
            "Nxd6",
            "Rd7",
            "Nc4",
            "Rc8",
            "Ne5+",
            "Kf6",
            "Nxd7+",
            "Kg7",
            "c4",
            "Rd8",
            "Nc5",
            "Rc8",
            "Nxa6",
            "Ra8",
            "Nb4",
            "Ra4",
            "Rb1",
            "Ra3",
            "Rb3",
            "Ra4",
            "a3",
            "Ra8",
            "c5",
            "Rc8",
            "c6",
            "Kf6",
            "Re1",
            "Kg5",
            "Re7",
            "f5",
            "c7",
            "Kf6",
            "Nd5+",
            "Kg5",
            "Rb8",
            "Rxc7",
            "Rxc7",
            "f4",
            "Rg8+",
            "Kf5",
            "Rf7+",
            "Ke5",
            "Nxf4",
            "Kd4",
            "Re8",
            "Kc3",
            "Rd7",
            "Kd2",
            "d4",
            "Kc3",
            "d5",
            "Kd4",
            "d6",
            "Kc5",
            "Rc8+",
            "Kb6",
            "Rdc7",
            "Ka6",
            "d7",
            "Kb6",
            "d8=Q",
            "Ka6",
            "Qd6+",
            "Kb5",
            "Qb4+",
            "Ka6",
            "Ra8#",
            "e5",
            "Nf6+",
            "Kg7"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e1g1",
          "positional_uci": "e1g1",
          "neutral_uci": "e1g1",
          "consensus_move": "e1g1",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nxe4",
              "Re1",
              "d5",
              "d3",
              "f6",
              "Qh5+",
              "g6",
              "Nxg6",
              "hxg6",
              "Qxh8",
              "Kf7",
              "dxe4",
              "dxe4",
              "Nc3",
              "Bf5",
              "Bh6",
              "Bxh6",
              "Qxh6",
              "Qd4",
              "Rad1",
              "Qb4",
              "Qh7+",
              "Ke6",
              "Qd7+",
              "Ke5",
              "Qxc7+",
              "Ke6",
              "Qxc6+",
              "Ke5",
              "Qxa8",
              "Qxb2",
              "Qd5+",
              "Kf4",
              "g3+",
              "Kg5",
              "Nxe4+",
              "Kh6",
              "Qd2+",
              "g5",
              "h4",
              "Bxe4",
              "Rxe4",
              "Qxa2",
              "hxg5+",
              "fxg5",
              "Qd6+",
              "Kh5",
              "g4+",
              "Kh4",
              "Qh6#",
              "e5",
              "Nf6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nxe4",
              "Re1",
              "d5",
              "d3",
              "f6",
              "Qh5+",
              "g6",
              "Nxg6",
              "hxg6",
              "Qxh8",
              "Kf7",
              "dxe4",
              "dxe4",
              "Nc3",
              "Bf5",
              "Bh6",
              "Bxh6",
              "Qxh6",
              "Qd4",
              "Rad1",
              "Qb4",
              "Qh7+",
              "Ke6",
              "Qd7+",
              "Ke5",
              "Qxc7+",
              "Ke6",
              "Qxc6+",
              "Kf7",
              "Qxa8",
              "Qxb2",
              "Qa7+",
              "Ke6",
              "Qd7+",
              "Ke5",
              "Qd5+",
              "Kf4",
              "g3+",
              "Kg5",
              "Nxe4+",
              "Kh6",
              "Qd2+",
              "g5",
              "h4",
              "Bxe4",
              "Rxe4",
              "Qxa2",
              "hxg5+",
              "fxg5",
              "Qd6+",
              "Kh5",
              "g4+",
              "Kh4",
              "Qh6#",
              "e5",
              "dxe5",
              "Qxe5+",
              "Kg6",
              "Qxe5+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+",
              "Kg6",
              "Qe6+",
              "Kg7",
              "Qe7+"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc3",
              "d6",
              "Nxc6",
              "Qd7",
              "Nxe7+",
              "Qxe7",
              "Bg5",
              "h6",
              "Nd5",
              "Qe5",
              "Bxf6",
              "gxf6",
              "Nxc7",
              "Ra7",
              "Nd5",
              "Kh7",
              "Qf3",
              "f5",
              "exf5",
              "Bxf5",
              "Rae1",
              "Qxb2",
              "Qxf5+",
              "Kh8",
              "Qf6+",
              "Qxf6",
              "Nxf6",
              "Kg7",
              "Ne8+",
              "Kg6",
              "Nxd6",
              "Rd7",
              "Nc4",
              "Rc8",
              "Ne5+",
              "Kf6",
              "Nxd7+",
              "Kg7",
              "c4",
              "Rd8",
              "Nc5",
              "Rc8",
              "Nxa6",
              "Ra8",
              "Nb4",
              "Ra4",
              "Rb1",
              "Ra3",
              "Rb3",
              "Ra4",
              "a3",
              "Ra8",
              "c5",
              "Rc8",
              "c6",
              "Kf6",
              "Re1",
              "Kg5",
              "Re7",
              "f5",
              "c7",
              "Kf6",
              "Nd5+",
              "Kg5",
              "Rb8",
              "Rxc7",
              "Rxc7",
              "f4",
              "Rg8+",
              "Kf5",
              "Rf7+",
              "Ke5",
              "Nxf4",
              "Kd4",
              "Re8",
              "Kc3",
              "Rd7",
              "Kd2",
              "d4",
              "Kc3",
              "d5",
              "Kd4",
              "d6",
              "Kc5",
              "Rc8+",
              "Kb6",
              "Rdc7",
              "Ka6",
              "d7",
              "Kb6",
              "d8=Q",
              "Ka6",
              "Qd6+",
              "Kb5",
              "Qb4+",
              "Ka6",
              "Ra8#",
              "e5",
              "Nf6+",
              "Kg7"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 151,
            "completion_tokens": 233,
            "total_tokens": 384,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 151,
            "completion_tokens": 640,
            "total_tokens": 791,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 150,
            "completion_tokens": 422,
            "total_tokens": 572,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 452,
          "total_completion_tokens": 1295,
          "total_tokens": 1747
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 151,
            "completion_tokens": 233,
            "total_tokens": 384,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 151,
            "completion_tokens": 640,
            "total_tokens": 791,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 150,
            "completion_tokens": 422,
            "total_tokens": 572,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/2pp1ppp/p1p2n2/4N3/4P3/8/PPPP1PPP/RNBQK2R w KQkq - 1 6"
    },
    {
      "turn": 13,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nxc6",
          "aggressive_uci": "e5c6",
          "aggressive_response": "7. Nxc6 Qd7 8. Nd4 Be7 9. Nc3 O-O 10. d3 c5 11. Nf5 Bd8 12. Bg5 h6 13. Bxf6 Bxf6 14. Nd5 Bxb2 15. Rb1 Be5 16. Nb6 Qb7 17. Nxa8 Qxa8 18. Ne7+ Kh8 19. Nxc8 Qxc8 20. f4 Bd4+ 21. Kh1 Qe6 22. c4 f5 23. exf5 Qxf5 24. Qf3 Re8 25. Rfe1 Rf8 26. g3 Qh3 27. Qg2 Qf5 28. Qe4 Qh3 29. Rb6 Bf2 30. Reb1 Bxg3 31. Qg2 Qxg2+ 32. Kxg2 Bxf4 33. Rxa6 Be5 34. Rf1 Rb8 35. Rf2 Rb1 36. a4 Rd1 37. Ra8+ Kh7 38. a5 Rxd3 39. a6 Ra3 40. a7 Kg6 41. Rf3 Ra2+ 42. Rf2 Ra3 43. h3 Rg3+ 44. Kf1 Ra3 45. Rg2+ Kh7 46. h4 Ra1+ 47. Ke2 Ra2+ 48. Kf3 Ra3+ 49. Ke4 Ra4 50. Kd5 Ra6 51. h5 Ra5 52. Rf2 Bf6 53. Re2 Be5 54. Rf2 Bf6 55. Kxd6 Ra6+ 56. Kxc5 Be7+ 57. Kb5 Ra1 58. Rf7 Rb1+ 59. Kc6 Bf6 60. Rh8+ Kxh8 61. a8=Q+ Kh7 62. Rf8 g6 63. Rf7+ Bg7 64. Qf8 Rb6+ 65. Kxb6 gxh5 66. Qxg7#",
          "aggressive_tokens": {
            "prompt_tokens": 158,
            "completion_tokens": 507,
            "total_tokens": 665,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qd7",
            "Nd4",
            "Be7",
            "Nc3",
            "O-O",
            "d3",
            "c5",
            "Nf5",
            "Bd8",
            "Bg5",
            "h6",
            "Bxf6",
            "Bxf6",
            "Nd5",
            "Bxb2",
            "Rb1",
            "Be5",
            "Nb6",
            "Qb7",
            "Nxa8",
            "Qxa8",
            "Ne7+",
            "Kh8",
            "Nxc8",
            "Qxc8",
            "f4",
            "Bd4+",
            "Kh1",
            "Qe6",
            "c4",
            "f5",
            "exf5",
            "Qxf5",
            "Qf3",
            "Re8",
            "Rfe1",
            "Rf8",
            "g3",
            "Qh3",
            "Qg2",
            "Qf5",
            "Qe4",
            "Qh3",
            "Rb6",
            "Bf2",
            "Reb1",
            "Bxg3",
            "Qg2",
            "Qxg2+",
            "Kxg2",
            "Bxf4",
            "Rxa6",
            "Be5",
            "Rf1",
            "Rb8",
            "Rf2",
            "Rb1",
            "a4",
            "Rd1",
            "Ra8+",
            "Kh7",
            "a5",
            "Rxd3",
            "a6",
            "Ra3",
            "a7",
            "Kg6",
            "Rf3",
            "Ra2+",
            "Rf2",
            "Ra3",
            "h3",
            "Rg3+",
            "Kf1",
            "Ra3",
            "Rg2+",
            "Kh7",
            "h4",
            "Ra1+",
            "Ke2",
            "Ra2+",
            "Kf3",
            "Ra3+",
            "Ke4",
            "Ra4",
            "Kd5",
            "Ra6",
            "h5",
            "Ra5",
            "Rf2",
            "Bf6",
            "Re2",
            "Be5",
            "Rf2",
            "Bf6",
            "Kxd6",
            "Ra6+",
            "Kxc5",
            "Be7+",
            "Kb5",
            "Ra1",
            "Rf7",
            "Rb1+",
            "Kc6",
            "Bf6",
            "Rh8+",
            "Kxh8",
            "a8=Q+",
            "Kh7",
            "Rf8",
            "g6",
            "Rf7+",
            "Bg7",
            "Qf8",
            "Rb6+",
            "Kxb6",
            "gxh5",
            "Qxg7#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nxc6",
          "positional_uci": "e5c6",
          "positional_response": "7. Nxc6 Qd7 8. Nd4 Be7 9. Nc3 O-O 10. d3 c5 11. Nf5 Bd8 12. Bg5 h6 13. Bxf6 Bxf6 14. Nd5 Bxb2 15. Rb1 Be5 16. f4 Bd4+ 17. Nxd4 cxd4 18. Nb6 Qc6 19. Nxa8 Qxa8 20. Qh5 Be6 21. f5 Bxa2 22. Rb2 Be6 23. fxe6 fxe6 24. Rxf8+ Qxf8 25. Qg6 Qc8 26. Rb6 a5 27. Rxd6 a4 28. Rxe6 Kf8 29. Qf5+ Kg8 30. Qd5 Kh7 31. Qf5+ Kg8 32. Qg6 Kf8 33. Qf5+ Kg8 34. Qd5 Kh7 35. Qxd4 Qxe6 36. Qxa4 Qb6+ 37. Kf1 Qb1+ 38. Ke2 Qg1 39. Kf3 Qf1+ 40. Kg3 Qe1+ 41. Kh3 Qe3+ 42. g3 Qf3 43. Qd7 Qf1+ 44. Kh4 Qf6+ 45. Kh3 Qf1+ 46. Kg4 Qe2+ 47. Kf5 Qxc2 48. h4 Qf2+ 49. Ke6 Qxg3 50. Qd8 Qg4+ 51. Kd5 Qh5+ 52. Kc4 Qf7+ 53. Qd5 Qc7+ 54. Kd4 Qb6+ 55. Qc5 Qf6+ 56. Qe5 Qxh4 57. Qf5+ Kg8 58. Qe6+ Kh7 59. Qf5+ Kg8 60. Qe6+ Kh7 61. Qf5+ Kg8 62. Qe6+ Kh7 63. Qf5+ Kg8 64. Qe6+ Kh7 65. Qf5+ Kg8 66. Qe6+ Kh7 67. Qf5+ Kg8 68. Qe6+ Kh7 69. Qf5+ Kg8 70. Qe6+ Kh7 71. Qf5+ Kg8 72. Qe6+ Kh7 73. Qf5+ Kg8 74. Qe6+ Kh7 75. Qf5+ Kg8 76. Qe6+ Kh7 77. Qf5+ Kg8 78. Qe6+ Kh7",
          "positional_tokens": {
            "prompt_tokens": 158,
            "completion_tokens": 640,
            "total_tokens": 798,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Qd7",
            "Nd4",
            "Be7",
            "Nc3",
            "O-O",
            "d3",
            "c5",
            "Nf5",
            "Bd8",
            "Bg5",
            "h6",
            "Bxf6",
            "Bxf6",
            "Nd5",
            "Bxb2",
            "Rb1",
            "Be5",
            "f4",
            "Bd4+",
            "Nxd4",
            "cxd4",
            "Nb6",
            "Qc6",
            "Nxa8",
            "Qxa8",
            "Qh5",
            "Be6",
            "f5",
            "Bxa2",
            "Rb2",
            "Be6",
            "fxe6",
            "fxe6",
            "Rxf8+",
            "Qxf8",
            "Qg6",
            "Qc8",
            "Rb6",
            "a5",
            "Rxd6",
            "a4",
            "Rxe6",
            "Kf8",
            "Qf5+",
            "Kg8",
            "Qd5",
            "Kh7",
            "Qf5+",
            "Kg8",
            "Qg6",
            "Kf8",
            "Qf5+",
            "Kg8",
            "Qd5",
            "Kh7",
            "Qxd4",
            "Qxe6",
            "Qxa4",
            "Qb6+",
            "Kf1",
            "Qb1+",
            "Ke2",
            "Qg1",
            "Kf3",
            "Qf1+",
            "Kg3",
            "Qe1+",
            "Kh3",
            "Qe3+",
            "g3",
            "Qf3",
            "Qd7",
            "Qf1+",
            "Kh4",
            "Qf6+",
            "Kh3",
            "Qf1+",
            "Kg4",
            "Qe2+",
            "Kf5",
            "Qxc2",
            "h4",
            "Qf2+",
            "Ke6",
            "Qxg3",
            "Qd8",
            "Qg4+",
            "Kd5",
            "Qh5+",
            "Kc4",
            "Qf7+",
            "Qd5",
            "Qc7+",
            "Kd4",
            "Qb6+",
            "Qc5",
            "Qf6+",
            "Qe5",
            "Qxh4",
            "Qf5+",
            "Kg8",
            "Qe6+",
            "Kh7",
            "Qf5+",
            "Kg8",
            "Qe6+",
            "Kh7",
            "Qf5+",
            "Kg8",
            "Qe6+",
            "Kh7",
            "Qf5+",
            "Kg8",
            "Qe6+",
            "Kh7",
            "Qf5+",
            "Kg8",
            "Qe6+",
            "Kh7",
            "Qf5+",
            "Kg8",
            "Qe6+",
            "Kh7",
            "Qf5+",
            "Kg8",
            "Qe6+",
            "Kh7",
            "Qf5+",
            "Kg8",
            "Qe6+",
            "Kh7",
            "Qf5+",
            "Kg8",
            "Qe6+",
            "Kh7",
            "Qf5+",
            "Kg8",
            "Qe6+",
            "Kh7",
            "Qf5+",
            "Kg8",
            "Qe6+",
            "Kh7"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nxc6",
          "neutral_uci": "e5c6",
          "neutral_response": "7. Nxc6 Qd7 8. Nd4 Be7 9. d3 O-O 10. Nc3 c5 11. Nf5 Bd8 12. Bg5 h6 13. Bxf6 Bxf6 14. Nd5 Bxb2 15. Rb1 Be5 16. f4 Bd4+ 17. Nxd4 cxd4 18. Nb6 Qa7 19. Nxa8 Qxa8 20. Qd2 Be6 21. Qb4 Bxa2 22. Ra1 Be6 23. Qxd6 Rd8 24. Qxa6 Qb8 25. f5 Bc8 26. Qa7 Qb2 27. Qc7 Rf8 28. Ra8 Qc3 29. Qxc3 dxc3 30. Rb1 Bd7 31. Rxf8+ Kxf8 32. Rb3 Ke7 33. Rxc3 Kd6 34. Kf2 Ke5 35. Ke3 g6 36. Rc7 Kd6 37. Rxd7+ Kxd7 38. fxg6 fxg6 39. Kd4 Kd6 40. c4 g5 41. c5+ Kc6 42. Kc4 h5 43. d4 g4 44. d5+ Kc7 45. e5 h4 46. e6 Kd8 47. d6 g3 48. hxg3 hxg3 49. c6 Kc8 50. e7 Kb8 51. e8=Q+ Ka7 52. Qd7+ Kb6 53. Qb7+ Ka5 54. Qb5# Qd7 Qb7 Qb7",
          "neutral_tokens": {
            "prompt_tokens": 157,
            "completion_tokens": 406,
            "total_tokens": 563,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qd7",
            "Nd4",
            "Be7",
            "d3",
            "O-O",
            "Nc3",
            "c5",
            "Nf5",
            "Bd8",
            "Bg5",
            "h6",
            "Bxf6",
            "Bxf6",
            "Nd5",
            "Bxb2",
            "Rb1",
            "Be5",
            "f4",
            "Bd4+",
            "Nxd4",
            "cxd4",
            "Nb6",
            "Qa7",
            "Nxa8",
            "Qxa8",
            "Qd2",
            "Be6",
            "Qb4",
            "Bxa2",
            "Ra1",
            "Be6",
            "Qxd6",
            "Rd8",
            "Qxa6",
            "Qb8",
            "f5",
            "Bc8",
            "Qa7",
            "Qb2",
            "Qc7",
            "Rf8",
            "Ra8",
            "Qc3",
            "Qxc3",
            "dxc3",
            "Rb1",
            "Bd7",
            "Rxf8+",
            "Kxf8",
            "Rb3",
            "Ke7",
            "Rxc3",
            "Kd6",
            "Kf2",
            "Ke5",
            "Ke3",
            "g6",
            "Rc7",
            "Kd6",
            "Rxd7+",
            "Kxd7",
            "fxg6",
            "fxg6",
            "Kd4",
            "Kd6",
            "c4",
            "g5",
            "c5+",
            "Kc6",
            "Kc4",
            "h5",
            "d4",
            "g4",
            "d5+",
            "Kc7",
            "e5",
            "h4",
            "e6",
            "Kd8",
            "d6",
            "g3",
            "hxg3",
            "hxg3",
            "c6",
            "Kc8",
            "e7",
            "Kb8",
            "e8=Q+",
            "Ka7",
            "Qd7+",
            "Kb6",
            "Qb7+",
            "Ka5",
            "Qb5#",
            "Qd7",
            "Qb7",
            "Qb7"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e5c6",
          "positional_uci": "e5c6",
          "neutral_uci": "e5c6",
          "consensus_move": "e5c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qd7",
              "Nd4",
              "Be7",
              "Nc3",
              "O-O",
              "d3",
              "c5",
              "Nf5",
              "Bd8",
              "Bg5",
              "h6",
              "Bxf6",
              "Bxf6",
              "Nd5",
              "Bxb2",
              "Rb1",
              "Be5",
              "Nb6",
              "Qb7",
              "Nxa8",
              "Qxa8",
              "Ne7+",
              "Kh8",
              "Nxc8",
              "Qxc8",
              "f4",
              "Bd4+",
              "Kh1",
              "Qe6",
              "c4",
              "f5",
              "exf5",
              "Qxf5",
              "Qf3",
              "Re8",
              "Rfe1",
              "Rf8",
              "g3",
              "Qh3",
              "Qg2",
              "Qf5",
              "Qe4",
              "Qh3",
              "Rb6",
              "Bf2",
              "Reb1",
              "Bxg3",
              "Qg2",
              "Qxg2+",
              "Kxg2",
              "Bxf4",
              "Rxa6",
              "Be5",
              "Rf1",
              "Rb8",
              "Rf2",
              "Rb1",
              "a4",
              "Rd1",
              "Ra8+",
              "Kh7",
              "a5",
              "Rxd3",
              "a6",
              "Ra3",
              "a7",
              "Kg6",
              "Rf3",
              "Ra2+",
              "Rf2",
              "Ra3",
              "h3",
              "Rg3+",
              "Kf1",
              "Ra3",
              "Rg2+",
              "Kh7",
              "h4",
              "Ra1+",
              "Ke2",
              "Ra2+",
              "Kf3",
              "Ra3+",
              "Ke4",
              "Ra4",
              "Kd5",
              "Ra6",
              "h5",
              "Ra5",
              "Rf2",
              "Bf6",
              "Re2",
              "Be5",
              "Rf2",
              "Bf6",
              "Kxd6",
              "Ra6+",
              "Kxc5",
              "Be7+",
              "Kb5",
              "Ra1",
              "Rf7",
              "Rb1+",
              "Kc6",
              "Bf6",
              "Rh8+",
              "Kxh8",
              "a8=Q+",
              "Kh7",
              "Rf8",
              "g6",
              "Rf7+",
              "Bg7",
              "Qf8",
              "Rb6+",
              "Kxb6",
              "gxh5",
              "Qxg7#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qd7",
              "Nd4",
              "Be7",
              "Nc3",
              "O-O",
              "d3",
              "c5",
              "Nf5",
              "Bd8",
              "Bg5",
              "h6",
              "Bxf6",
              "Bxf6",
              "Nd5",
              "Bxb2",
              "Rb1",
              "Be5",
              "f4",
              "Bd4+",
              "Nxd4",
              "cxd4",
              "Nb6",
              "Qc6",
              "Nxa8",
              "Qxa8",
              "Qh5",
              "Be6",
              "f5",
              "Bxa2",
              "Rb2",
              "Be6",
              "fxe6",
              "fxe6",
              "Rxf8+",
              "Qxf8",
              "Qg6",
              "Qc8",
              "Rb6",
              "a5",
              "Rxd6",
              "a4",
              "Rxe6",
              "Kf8",
              "Qf5+",
              "Kg8",
              "Qd5",
              "Kh7",
              "Qf5+",
              "Kg8",
              "Qg6",
              "Kf8",
              "Qf5+",
              "Kg8",
              "Qd5",
              "Kh7",
              "Qxd4",
              "Qxe6",
              "Qxa4",
              "Qb6+",
              "Kf1",
              "Qb1+",
              "Ke2",
              "Qg1",
              "Kf3",
              "Qf1+",
              "Kg3",
              "Qe1+",
              "Kh3",
              "Qe3+",
              "g3",
              "Qf3",
              "Qd7",
              "Qf1+",
              "Kh4",
              "Qf6+",
              "Kh3",
              "Qf1+",
              "Kg4",
              "Qe2+",
              "Kf5",
              "Qxc2",
              "h4",
              "Qf2+",
              "Ke6",
              "Qxg3",
              "Qd8",
              "Qg4+",
              "Kd5",
              "Qh5+",
              "Kc4",
              "Qf7+",
              "Qd5",
              "Qc7+",
              "Kd4",
              "Qb6+",
              "Qc5",
              "Qf6+",
              "Qe5",
              "Qxh4",
              "Qf5+",
              "Kg8",
              "Qe6+",
              "Kh7",
              "Qf5+",
              "Kg8",
              "Qe6+",
              "Kh7",
              "Qf5+",
              "Kg8",
              "Qe6+",
              "Kh7",
              "Qf5+",
              "Kg8",
              "Qe6+",
              "Kh7",
              "Qf5+",
              "Kg8",
              "Qe6+",
              "Kh7",
              "Qf5+",
              "Kg8",
              "Qe6+",
              "Kh7",
              "Qf5+",
              "Kg8",
              "Qe6+",
              "Kh7",
              "Qf5+",
              "Kg8",
              "Qe6+",
              "Kh7",
              "Qf5+",
              "Kg8",
              "Qe6+",
              "Kh7",
              "Qf5+",
              "Kg8",
              "Qe6+",
              "Kh7",
              "Qf5+",
              "Kg8",
              "Qe6+",
              "Kh7"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qd7",
              "Nd4",
              "Be7",
              "d3",
              "O-O",
              "Nc3",
              "c5",
              "Nf5",
              "Bd8",
              "Bg5",
              "h6",
              "Bxf6",
              "Bxf6",
              "Nd5",
              "Bxb2",
              "Rb1",
              "Be5",
              "f4",
              "Bd4+",
              "Nxd4",
              "cxd4",
              "Nb6",
              "Qa7",
              "Nxa8",
              "Qxa8",
              "Qd2",
              "Be6",
              "Qb4",
              "Bxa2",
              "Ra1",
              "Be6",
              "Qxd6",
              "Rd8",
              "Qxa6",
              "Qb8",
              "f5",
              "Bc8",
              "Qa7",
              "Qb2",
              "Qc7",
              "Rf8",
              "Ra8",
              "Qc3",
              "Qxc3",
              "dxc3",
              "Rb1",
              "Bd7",
              "Rxf8+",
              "Kxf8",
              "Rb3",
              "Ke7",
              "Rxc3",
              "Kd6",
              "Kf2",
              "Ke5",
              "Ke3",
              "g6",
              "Rc7",
              "Kd6",
              "Rxd7+",
              "Kxd7",
              "fxg6",
              "fxg6",
              "Kd4",
              "Kd6",
              "c4",
              "g5",
              "c5+",
              "Kc6",
              "Kc4",
              "h5",
              "d4",
              "g4",
              "d5+",
              "Kc7",
              "e5",
              "h4",
              "e6",
              "Kd8",
              "d6",
              "g3",
              "hxg3",
              "hxg3",
              "c6",
              "Kc8",
              "e7",
              "Kb8",
              "e8=Q+",
              "Ka7",
              "Qd7+",
              "Kb6",
              "Qb7+",
              "Ka5",
              "Qb5#",
              "Qd7",
              "Qb7",
              "Qb7"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 158,
            "completion_tokens": 507,
            "total_tokens": 665,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 158,
            "completion_tokens": 640,
            "total_tokens": 798,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 157,
            "completion_tokens": 406,
            "total_tokens": 563,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 473,
          "total_completion_tokens": 1553,
          "total_tokens": 2026
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 158,
            "completion_tokens": 507,
            "total_tokens": 665,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 158,
            "completion_tokens": 640,
            "total_tokens": 798,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 157,
            "completion_tokens": 406,
            "total_tokens": 563,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/2p2ppp/p1pp1n2/4N3/4P3/8/PPPP1PPP/RNBQ1RK1 w kq - 0 7"
    },
    {
      "turn": 15,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nxe7",
          "aggressive_uci": "c6e7",
          "aggressive_response": "8. Nxe7 Bxe7 9. d3 O-O 10. Nc3 Bg4 11. f3 Be6 12. Be3 c5 13. d4 cxd4 14. Bxd4 Rab8 15. b3 Rfc8 16. Qd2 Nd7 17. Nd5 Bxd5 18. exd5 Bf6 19. Bxf6 Nxf6 20. c4 Re8 21. Rfe1 Rxe1+ 22. Rxe1 Re8 23. Rxe8+ Nxe8 24. Qa5 Kf8 25. Qxa6 Ke7 26. Qb7+ Kf6 27. Qd7 Kg6 28. Qxe8 Kf6 29. Qd7 Kg6 30. Qxd6+ f6 31. c5 Kf7 32. c6 Kg6 33. c7 Kf7 34. c8=Q Kg6 35. Qg4+ Kf7 36. Qdd7+ Kf8 37. Qgxg7# e5 d5 Nc6",
          "aggressive_tokens": {
            "prompt_tokens": 167,
            "completion_tokens": 260,
            "total_tokens": 427,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bxe7",
            "d3",
            "O-O",
            "Nc3",
            "Bg4",
            "f3",
            "Be6",
            "Be3",
            "c5",
            "d4",
            "cxd4",
            "Bxd4",
            "Rab8",
            "b3",
            "Rfc8",
            "Qd2",
            "Nd7",
            "Nd5",
            "Bxd5",
            "exd5",
            "Bf6",
            "Bxf6",
            "Nxf6",
            "c4",
            "Re8",
            "Rfe1",
            "Rxe1+",
            "Rxe1",
            "Re8",
            "Rxe8+",
            "Nxe8",
            "Qa5",
            "Kf8",
            "Qxa6",
            "Ke7",
            "Qb7+",
            "Kf6",
            "Qd7",
            "Kg6",
            "Qxe8",
            "Kf6",
            "Qd7",
            "Kg6",
            "Qxd6+",
            "f6",
            "c5",
            "Kf7",
            "c6",
            "Kg6",
            "c7",
            "Kf7",
            "c8=Q",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qdd7+",
            "Kf8",
            "Qgxg7#",
            "e5",
            "d5",
            "Nc6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nxe7",
          "positional_uci": "c6e7",
          "positional_response": "8. Nxe7 Bxe7 9. d3 O-O 10. Nc3 c5 11. Bg5 h6 12. Bxf6 Bxf6 13. Nd5 Bxb2 14. Rb1 Bd4 15. c3 Be5 16. f4 Bf6 17. Nxf6+ gxf6 18. Qh5 Kh7 19. Rf3 Rg8 20. Qxf7+ Rg7 21. Qxf6 Bg4 22. Rg3 Rag8 23. h3 Rg6 24. Qf7+ R8g7 25. Qd5 Be6 26. Rxg6 Bxd5 27. Rxg7+ Kxg7 28. exd5 Kf6 29. Rb6 Kf5 30. Rxd6 Kxf4 31. Rxh6 Ke3 32. d6 Kxd3 33. d7 Kxc3 34. d8=Q Kb2 35. Qd2+ Ka3 36. Rxa6# e4 d5 Qxd5",
          "positional_tokens": {
            "prompt_tokens": 167,
            "completion_tokens": 256,
            "total_tokens": 423,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bxe7",
            "d3",
            "O-O",
            "Nc3",
            "c5",
            "Bg5",
            "h6",
            "Bxf6",
            "Bxf6",
            "Nd5",
            "Bxb2",
            "Rb1",
            "Bd4",
            "c3",
            "Be5",
            "f4",
            "Bf6",
            "Nxf6+",
            "gxf6",
            "Qh5",
            "Kh7",
            "Rf3",
            "Rg8",
            "Qxf7+",
            "Rg7",
            "Qxf6",
            "Bg4",
            "Rg3",
            "Rag8",
            "h3",
            "Rg6",
            "Qf7+",
            "R8g7",
            "Qd5",
            "Be6",
            "Rxg6",
            "Bxd5",
            "Rxg7+",
            "Kxg7",
            "exd5",
            "Kf6",
            "Rb6",
            "Kf5",
            "Rxd6",
            "Kxf4",
            "Rxh6",
            "Ke3",
            "d6",
            "Kxd3",
            "d7",
            "Kxc3",
            "d8=Q",
            "Kb2",
            "Qd2+",
            "Ka3",
            "Rxa6#",
            "e4",
            "d5",
            "Qxd5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nxe7",
          "neutral_uci": "c6e7",
          "neutral_response": "8. Nxe7 Bxe7 9. d3 O-O 10. Bg5 h6 11. Bxf6 Bxf6 12. Nc3 c6 13. d4 Re8 14. Re1 Rb8 15. b3 c5 16. Nd5 Bxd4 17. c3 Be5 18. f4 Bf6 19. Nxf6+ gxf6 20. Qxd6 Be6 21. f5 Rbd8 22. Qg3+ Kh7 23. fxe6 Rxe6 24. Rad1 Rg8 25. Qf4 Rg5 26. Rd7 Kg7 27. h4 Rge5 28. Qg4+ Kf8 29. Rd8+ Ke7 30. Red1 f5 31. R1d7+ Kf6 32. Qh5 Rxe4 33. Qxf7+ Ke5 34. Rd5+ Kf4 35. Qxf5+ Ke3 36. Qf3# e4 Qg6 Qxg6",
          "neutral_tokens": {
            "prompt_tokens": 166,
            "completion_tokens": 250,
            "total_tokens": 416,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bxe7",
            "d3",
            "O-O",
            "Bg5",
            "h6",
            "Bxf6",
            "Bxf6",
            "Nc3",
            "c6",
            "d4",
            "Re8",
            "Re1",
            "Rb8",
            "b3",
            "c5",
            "Nd5",
            "Bxd4",
            "c3",
            "Be5",
            "f4",
            "Bf6",
            "Nxf6+",
            "gxf6",
            "Qxd6",
            "Be6",
            "f5",
            "Rbd8",
            "Qg3+",
            "Kh7",
            "fxe6",
            "Rxe6",
            "Rad1",
            "Rg8",
            "Qf4",
            "Rg5",
            "Rd7",
            "Kg7",
            "h4",
            "Rge5",
            "Qg4+",
            "Kf8",
            "Rd8+",
            "Ke7",
            "Red1",
            "f5",
            "R1d7+",
            "Kf6",
            "Qh5",
            "Rxe4",
            "Qxf7+",
            "Ke5",
            "Rd5+",
            "Kf4",
            "Qxf5+",
            "Ke3",
            "Qf3#",
            "e4",
            "Qg6",
            "Qxg6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "c6e7",
          "positional_uci": "c6e7",
          "neutral_uci": "c6e7",
          "consensus_move": "c6e7",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bxe7",
              "d3",
              "O-O",
              "Nc3",
              "Bg4",
              "f3",
              "Be6",
              "Be3",
              "c5",
              "d4",
              "cxd4",
              "Bxd4",
              "Rab8",
              "b3",
              "Rfc8",
              "Qd2",
              "Nd7",
              "Nd5",
              "Bxd5",
              "exd5",
              "Bf6",
              "Bxf6",
              "Nxf6",
              "c4",
              "Re8",
              "Rfe1",
              "Rxe1+",
              "Rxe1",
              "Re8",
              "Rxe8+",
              "Nxe8",
              "Qa5",
              "Kf8",
              "Qxa6",
              "Ke7",
              "Qb7+",
              "Kf6",
              "Qd7",
              "Kg6",
              "Qxe8",
              "Kf6",
              "Qd7",
              "Kg6",
              "Qxd6+",
              "f6",
              "c5",
              "Kf7",
              "c6",
              "Kg6",
              "c7",
              "Kf7",
              "c8=Q",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qdd7+",
              "Kf8",
              "Qgxg7#",
              "e5",
              "d5",
              "Nc6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bxe7",
              "d3",
              "O-O",
              "Nc3",
              "c5",
              "Bg5",
              "h6",
              "Bxf6",
              "Bxf6",
              "Nd5",
              "Bxb2",
              "Rb1",
              "Bd4",
              "c3",
              "Be5",
              "f4",
              "Bf6",
              "Nxf6+",
              "gxf6",
              "Qh5",
              "Kh7",
              "Rf3",
              "Rg8",
              "Qxf7+",
              "Rg7",
              "Qxf6",
              "Bg4",
              "Rg3",
              "Rag8",
              "h3",
              "Rg6",
              "Qf7+",
              "R8g7",
              "Qd5",
              "Be6",
              "Rxg6",
              "Bxd5",
              "Rxg7+",
              "Kxg7",
              "exd5",
              "Kf6",
              "Rb6",
              "Kf5",
              "Rxd6",
              "Kxf4",
              "Rxh6",
              "Ke3",
              "d6",
              "Kxd3",
              "d7",
              "Kxc3",
              "d8=Q",
              "Kb2",
              "Qd2+",
              "Ka3",
              "Rxa6#",
              "e4",
              "d5",
              "Qxd5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bxe7",
              "d3",
              "O-O",
              "Bg5",
              "h6",
              "Bxf6",
              "Bxf6",
              "Nc3",
              "c6",
              "d4",
              "Re8",
              "Re1",
              "Rb8",
              "b3",
              "c5",
              "Nd5",
              "Bxd4",
              "c3",
              "Be5",
              "f4",
              "Bf6",
              "Nxf6+",
              "gxf6",
              "Qxd6",
              "Be6",
              "f5",
              "Rbd8",
              "Qg3+",
              "Kh7",
              "fxe6",
              "Rxe6",
              "Rad1",
              "Rg8",
              "Qf4",
              "Rg5",
              "Rd7",
              "Kg7",
              "h4",
              "Rge5",
              "Qg4+",
              "Kf8",
              "Rd8+",
              "Ke7",
              "Red1",
              "f5",
              "R1d7+",
              "Kf6",
              "Qh5",
              "Rxe4",
              "Qxf7+",
              "Ke5",
              "Rd5+",
              "Kf4",
              "Qxf5+",
              "Ke3",
              "Qf3#",
              "e4",
              "Qg6",
              "Qxg6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 167,
            "completion_tokens": 260,
            "total_tokens": 427,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 167,
            "completion_tokens": 256,
            "total_tokens": 423,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 166,
            "completion_tokens": 250,
            "total_tokens": 416,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 500,
          "total_completion_tokens": 766,
          "total_tokens": 1266
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 167,
            "completion_tokens": 260,
            "total_tokens": 427,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 167,
            "completion_tokens": 256,
            "total_tokens": 423,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 166,
            "completion_tokens": 250,
            "total_tokens": 416,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1b1kb1r/2p1qppp/p1Np1n2/8/4P3/8/PPPP1PPP/RNBQ1RK1 w kq - 1 8"
    },
    {
      "turn": 17,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": "d2d4",
          "aggressive_response": "9. d4 Nxe4 10. Re1 d5 11. f3 Bf5 12. fxe4 Bxe4 13. Nc3 f5 14. Nxd5+ Kd6 15. Nc3 Re8 16. Nxe4+ fxe4 17. Qg4 Kd5 18. Qd7+ Bd6 19. Bf4 Rd8 20. Qf5+ Kxd4 21. Rad1+ Kc4 22. Qd5+ Kb4 23. Rd4# d3 Qc5 Qxc5",
          "aggressive_tokens": {
            "prompt_tokens": 176,
            "completion_tokens": 136,
            "total_tokens": 312,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxe4",
            "Re1",
            "d5",
            "f3",
            "Bf5",
            "fxe4",
            "Bxe4",
            "Nc3",
            "f5",
            "Nxd5+",
            "Kd6",
            "Nc3",
            "Re8",
            "Nxe4+",
            "fxe4",
            "Qg4",
            "Kd5",
            "Qd7+",
            "Bd6",
            "Bf4",
            "Rd8",
            "Qf5+",
            "Kxd4",
            "Rad1+",
            "Kc4",
            "Qd5+",
            "Kb4",
            "Rd4#",
            "d3",
            "Qc5",
            "Qxc5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": "d2d4",
          "positional_response": "9. d4 Nxe4 10. Re1 d5 11. f3 f5 12. fxe4 fxe4 13. Bg5+ Kd6 14. Nc3 h6 15. Bf4+ Kc6 16. Qh5 Be6 17. Qe5 Bd6 18. Qxe6 Rae8 19. Qxd5+ Kd7 20. Bxd6 cxd6 21. Nxe4 Re6 22. Nc5+ Ke7 23. Qxe6+ Kf8 24. Nd7# d5 25. Qe8# Kg8 26. Nf6# gxf6 27. Qg6+ Kf8 28. Re8# Kg8 29. Rae1# d4 30. R1e7# d3 31. Rg7# Kf8 32. Qf7# d2 33. Qe7# Kg8 34. Qe8#",
          "positional_tokens": {
            "prompt_tokens": 176,
            "completion_tokens": 227,
            "total_tokens": 403,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "R1e7#",
            "d3",
            "Rg7#",
            "Kf8",
            "Qf7#",
            "d2",
            "Qe7#",
            "Kg8",
            "Qe8#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": "d2d4",
          "neutral_response": "9. d4 Nxe4 10. Re1 d5 11. f3 Bf5 12. fxe4 Bxe4 13. Nc3 f5 14. Nxd5+ Kd6 15. Nc3 Re8 16. Nxe4+ Rxe4 17. Rxe4 fxe4 18. Qg4 Be7 19. Qxe4 Re8 20. Bf4+ Kd7 21. Qd5+ Kc8 22. Qc6 Bd6 23. Bxd6 Re6 24. Qxc7# e4 dxe4 Qd4+ Kd7 Qxa8 e3 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+ Kd7 Qxa6+ Kd8 Qxa8+",
          "neutral_tokens": {
            "prompt_tokens": 175,
            "completion_tokens": 640,
            "total_tokens": 815,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nxe4",
            "Re1",
            "d5",
            "f3",
            "Bf5",
            "fxe4",
            "Bxe4",
            "Nc3",
            "f5",
            "Nxd5+",
            "Kd6",
            "Nc3",
            "Re8",
            "Nxe4+",
            "Rxe4",
            "Rxe4",
            "fxe4",
            "Qg4",
            "Be7",
            "Qxe4",
            "Re8",
            "Bf4+",
            "Kd7",
            "Qd5+",
            "Kc8",
            "Qc6",
            "Bd6",
            "Bxd6",
            "Re6",
            "Qxc7#",
            "e4",
            "dxe4",
            "Qd4+",
            "Kd7",
            "Qxa8",
            "e3",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+",
            "Kd7",
            "Qxa6+",
            "Kd8",
            "Qxa8+"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d2d4",
          "positional_uci": "d2d4",
          "neutral_uci": "d2d4",
          "consensus_move": "d2d4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nxe4",
              "Re1",
              "d5",
              "f3",
              "Bf5",
              "fxe4",
              "Bxe4",
              "Nc3",
              "f5",
              "Nxd5+",
              "Kd6",
              "Nc3",
              "Re8",
              "Nxe4+",
              "fxe4",
              "Qg4",
              "Kd5",
              "Qd7+",
              "Bd6",
              "Bf4",
              "Rd8",
              "Qf5+",
              "Kxd4",
              "Rad1+",
              "Kc4",
              "Qd5+",
              "Kb4",
              "Rd4#",
              "d3",
              "Qc5",
              "Qxc5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "R1e7#",
              "d3",
              "Rg7#",
              "Kf8",
              "Qf7#",
              "d2",
              "Qe7#",
              "Kg8",
              "Qe8#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nxe4",
              "Re1",
              "d5",
              "f3",
              "Bf5",
              "fxe4",
              "Bxe4",
              "Nc3",
              "f5",
              "Nxd5+",
              "Kd6",
              "Nc3",
              "Re8",
              "Nxe4+",
              "Rxe4",
              "Rxe4",
              "fxe4",
              "Qg4",
              "Be7",
              "Qxe4",
              "Re8",
              "Bf4+",
              "Kd7",
              "Qd5+",
              "Kc8",
              "Qc6",
              "Bd6",
              "Bxd6",
              "Re6",
              "Qxc7#",
              "e4",
              "dxe4",
              "Qd4+",
              "Kd7",
              "Qxa8",
              "e3",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+",
              "Kd7",
              "Qxa6+",
              "Kd8",
              "Qxa8+"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 176,
            "completion_tokens": 136,
            "total_tokens": 312,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 176,
            "completion_tokens": 227,
            "total_tokens": 403,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 175,
            "completion_tokens": 640,
            "total_tokens": 815,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 527,
          "total_completion_tokens": 1003,
          "total_tokens": 1530
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 176,
            "completion_tokens": 136,
            "total_tokens": 312,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 176,
            "completion_tokens": 227,
            "total_tokens": 403,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 175,
            "completion_tokens": 640,
            "total_tokens": 815,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1b2b1r/2p1kppp/p2p1n2/8/4P3/8/PPPP1PPP/RNBQ1RK1 w - - 0 9"
    },
    {
      "turn": 19,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "exd5",
          "aggressive_uci": "e4d5",
          "aggressive_response": "10. exd5 Nxd5 11. Re1+ Kd8 12. Qf3 c6 13. Qxf7 Be7 14. Qxg7 Re8 15. Bg5 Bxg5 16.\nQxg5+ Kd7 17. Qg7+ Kd8 18. Rxe8+ Kxe8 19. Nc3 Nxc3 20. bxc3 Rb8 21. Re1+ Kd8 22.\nQe7# Qxe7 23. Rxe7 Kxe7 24. f4 Rb2 25. g4 Rxc2 26. h4 Rxa2 27. f5 Rc2 28. g5 Rxc3\n29. f6+ Kf7 30. h5 Rg3+ 31. Kf2 Rxg5 32. Kf3 Rxh5 33. Kf4 Kxf6 34. Ke4 a5 35.\nKd3 a4 36. Kc4 a3 37. Kb3 Rh3+ 38. Ka2 Be6+ 39. Ka1 a2 40. Kb2 Rh2+ 41. Ka1 Rh1+\n42. Kb2 a1=Q+ 43. Kc2 Qa3 44. Kd2 Rh2+ 45. Ke1 Qa1# d4 e5 Nf3 Nc6 Bb5 a6 Bxc6\nbxc6 Nxe5 Nf6 O-O d6 Nxc6 Qe7 Nxe7 Kxe7 d4 d5 exd5 Nxd5 Re1+ Kd8 Qf3 c6 Qxf7 Be7 Qxg7 Re8 Bg5 Bxg5 Qxg5+ Kd7 Qg7+ Kd8 Rxe8+ Kxe8 Nc3 Nxc3 bxc3 Rb8 Re1+ Kd8 Qe7# Qxe7 Rxe7 Kxe7 f4 Rb2 g4 Rxc2 h4 Rxa2 f5 Rg2+ Kf1 Rxg4 Kf2 Rxh4 Kf3 Kxf6 Ke3 Kxf5 Kd3 a5 Kc3 a4 Kb2 Rh3 Ka2 a3+ Ka1 Be6 Kb1 a2+ Ka1 Rh1+ Kb2 a1=Q+ Kc2 Qa3 Kd2 Rh2+ Ke1 Qa1#",
          "aggressive_tokens": {
            "prompt_tokens": 183,
            "completion_tokens": 550,
            "total_tokens": 733,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxd5",
            "Re1+",
            "Kd8",
            "Qf3",
            "c6",
            "Qxf7",
            "Be7",
            "Qxg7",
            "Re8",
            "Bg5",
            "Bxg5",
            "Qxg5+",
            "Kd7",
            "Qg7+",
            "Kd8",
            "Rxe8+",
            "Kxe8",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Rb8",
            "Re1+",
            "Kd8",
            "Qe7#",
            "Qxe7",
            "Rxe7",
            "Kxe7",
            "f4",
            "Rb2",
            "g4",
            "Rxc2",
            "h4",
            "Rxa2",
            "f5",
            "Rg2+",
            "Kf1",
            "Rxg4",
            "Kf2",
            "Rxh4",
            "Kf3",
            "Kxf6",
            "Ke3",
            "Kxf5",
            "Kd3",
            "a5",
            "Kc3",
            "a4",
            "Kb2",
            "Rh3",
            "Ka2",
            "a3+",
            "Ka1",
            "Be6",
            "Kb1",
            "a2+",
            "Ka1",
            "Rh1+",
            "Kb2",
            "a1=Q+",
            "Kc2",
            "Qa3",
            "Kd2",
            "Rh2+",
            "Ke1",
            "Qa1#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "exd5",
          "positional_uci": "e4d5",
          "positional_response": "10. exd5 Nxd5 11. Re1+ Kd6 12. c4 Nb4 13. Bf4+ Kc6 14. Qf3+ Kb6 15. Qxa8 Bb7 16.\nQd8 Nc2 17. Qxc7+ Ka7 18. Qb8+ Kb6 19. Bc7+ Kc6 20. d5+ Kc5 21. Qxb7 Nxe1 22.\nQb6+ Kxc4 23. Nd2+ Kxd5 24. Rxe1 Bc5 25. Re5+ Kd4 26. Qxc5+ Kd3 27. Qc3# Nf6\n28. Qd3# Nxd3 29. Rd5+ Ke2 30. Nf3 Nxb2 31. Rd2# Kxd2 32. Nxd2 Kxd2 33. Be5 Nd3\n34. Bxg7 Re8 35. h3 Re1+ 36. Kh2 Nxf2 37. Bd4 Ne4 38. Bb6 Ra1 39. Ba5+ Ke2 40.\nBb4 Rxa2 41. Be7 a5 42. Bd8 a4 43. Be7 a3 44. Bxa3 Rxa3 45. h4 Kf2 46. g4 Rg3\n47. g5 Rg4 48. Kh3 Kf3 49. h5 Nxg5+ 50. Kh2 Kf2 51. h6 Rh4# Nf3+ 52. Kh1 Rxh6#\n53. gxf3 Kxf3 54. Kg1 Kg3 55. Kf1 Rh2 56. Ke1 Kf3 57. Kd1 Ke3 58. Kc1 Kd3 59.\nKb1 Kc3 60. Ka1 Kb3 61. Kb1 Rh1# Nf3 62. Kc1 Rxe1# Nxe1 63. Kd1 Nf3# Nxd3 64.\nKd2 Kc4 65. Ke3 Kd5 66. Kf3 Ke5 67. Kg4 Rf1 68. Kg5 Rg1+ 69. Kh6 Kf5 70. Kh7\nKf6 71. Kh8 Kf7 72. Kh7 Rh1# Nf3 73. Kh6 Kf6 74. Kh5 Kf5 75. Kh6 Rg1 76. Kh7\nKf6 77. Kh8 Kf7 78. Kh7 Rh1# Nf3 79. Kh6 Kf6 80. Kh5 Kf5 81. Kh6 Rg1 82. Kh7\nKf6",
          "positional_tokens": {
            "prompt_tokens": 183,
            "completion_tokens": 640,
            "total_tokens": 823,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nxd5",
            "Re1+",
            "Kd6",
            "c4",
            "Nb4",
            "Bf4+",
            "Kc6",
            "Qf3+",
            "Kb6",
            "Qxa8",
            "Bb7",
            "Qd8",
            "Nc2",
            "Qxc7+",
            "Ka7",
            "Qb8+",
            "Kb6",
            "Bc7+",
            "Kc6",
            "d5+",
            "Kc5",
            "Qxb7",
            "Nxe1",
            "Qb6+",
            "Kxc4",
            "Nd2+",
            "Kxd5",
            "Rxe1",
            "Bc5",
            "Re5+",
            "Kd4",
            "Qxc5+",
            "Kd3",
            "Qc3#",
            "Nf6",
            "Qd3#",
            "Nxd3",
            "Rd5+",
            "Ke2",
            "Nf3",
            "Nxb2",
            "Rd2#",
            "Kxd2",
            "Nxd2",
            "Kxd2",
            "Be5",
            "Nd3",
            "Bxg7",
            "Re8",
            "h3",
            "Re1+",
            "Kh2",
            "Nxf2",
            "Bd4",
            "Ne4",
            "Bb6",
            "Ra1",
            "Ba5+",
            "Ke2",
            "Bb4",
            "Rxa2",
            "Be7",
            "a5",
            "Bd8",
            "a4",
            "Be7",
            "a3",
            "Bxa3",
            "Rxa3",
            "h4",
            "Kf2",
            "g4",
            "Rg3",
            "g5",
            "Rg4",
            "Kh3",
            "Kf3",
            "h5",
            "Nxg5+",
            "Kh2",
            "Kf2",
            "h6",
            "Rh4#",
            "Nf3+",
            "Kh1",
            "Rxh6#",
            "gxf3",
            "Kxf3",
            "Kg1",
            "Kg3",
            "Kf1",
            "Rh2",
            "Ke1",
            "Kf3",
            "Kd1",
            "Ke3",
            "Kc1",
            "Kd3",
            "Kb1",
            "Kc3",
            "Ka1",
            "Kb3",
            "Kb1",
            "Rh1#",
            "Nf3",
            "Kc1",
            "Rxe1#",
            "Nxe1",
            "Kd1",
            "Nf3#",
            "Nxd3",
            "Kd2",
            "Kc4",
            "Ke3",
            "Kd5",
            "Kf3",
            "Ke5",
            "Kg4",
            "Rf1",
            "Kg5",
            "Rg1+",
            "Kh6",
            "Kf5",
            "Kh7",
            "Kf6",
            "Kh8",
            "Kf7",
            "Kh7",
            "Rh1#",
            "Nf3",
            "Kh6",
            "Kf6",
            "Kh5",
            "Kf5",
            "Kh6",
            "Rg1",
            "Kh7",
            "Kf6",
            "Kh8",
            "Kf7",
            "Kh7",
            "Rh1#",
            "Nf3",
            "Kh6",
            "Kf6",
            "Kh5",
            "Kf5",
            "Kh6",
            "Rg1",
            "Kh7",
            "Kf6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "exd5",
          "neutral_uci": "e4d5",
          "neutral_response": "10. exd5 Nxd5 11. Re1+ Kd6 12. c4 Nb6 13. c5+ Kd5 14. Nc3+ Kc4 15. Qb3+ Kxd4 16.\nBe3+ Ke5 17. cxb6 cxb6 18. Qd5+ Kf6 19. Bd4+ Kg6 20. Qxa8 Bb4 21. Qe4+ Bf5 22.\nQc6+ Kg5 23. Be3+ Kh5 24. Qf3+ Kg6 25. Qg3+ Kf6 26. Nd5+ Ke6 27. Nxb4 a5 28.\nBxb6+ Kf6 29. Bd4# d5 Qg3+ Kf5",
          "neutral_tokens": {
            "prompt_tokens": 182,
            "completion_tokens": 185,
            "total_tokens": 367,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxd5",
            "Re1+",
            "Kd6",
            "c4",
            "Nb6",
            "c5+",
            "Kd5",
            "Nc3+",
            "Kc4",
            "Qb3+",
            "Kxd4",
            "Be3+",
            "Ke5",
            "cxb6",
            "cxb6",
            "Qd5+",
            "Kf6",
            "Bd4+",
            "Kg6",
            "Qxa8",
            "Bb4",
            "Qe4+",
            "Bf5",
            "Qc6+",
            "Kg5",
            "Be3+",
            "Kh5",
            "Qf3+",
            "Kg6",
            "Qg3+",
            "Kf6",
            "Nd5+",
            "Ke6",
            "Nxb4",
            "a5",
            "Bxb6+",
            "Kf6",
            "Bd4#",
            "d5",
            "Qg3+",
            "Kf5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e4d5",
          "positional_uci": "e4d5",
          "neutral_uci": "e4d5",
          "consensus_move": "e4d5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nxd5",
              "Re1+",
              "Kd8",
              "Qf3",
              "c6",
              "Qxf7",
              "Be7",
              "Qxg7",
              "Re8",
              "Bg5",
              "Bxg5",
              "Qxg5+",
              "Kd7",
              "Qg7+",
              "Kd8",
              "Rxe8+",
              "Kxe8",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Rb8",
              "Re1+",
              "Kd8",
              "Qe7#",
              "Qxe7",
              "Rxe7",
              "Kxe7",
              "f4",
              "Rb2",
              "g4",
              "Rxc2",
              "h4",
              "Rxa2",
              "f5",
              "Rg2+",
              "Kf1",
              "Rxg4",
              "Kf2",
              "Rxh4",
              "Kf3",
              "Kxf6",
              "Ke3",
              "Kxf5",
              "Kd3",
              "a5",
              "Kc3",
              "a4",
              "Kb2",
              "Rh3",
              "Ka2",
              "a3+",
              "Ka1",
              "Be6",
              "Kb1",
              "a2+",
              "Ka1",
              "Rh1+",
              "Kb2",
              "a1=Q+",
              "Kc2",
              "Qa3",
              "Kd2",
              "Rh2+",
              "Ke1",
              "Qa1#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nxd5",
              "Re1+",
              "Kd6",
              "c4",
              "Nb4",
              "Bf4+",
              "Kc6",
              "Qf3+",
              "Kb6",
              "Qxa8",
              "Bb7",
              "Qd8",
              "Nc2",
              "Qxc7+",
              "Ka7",
              "Qb8+",
              "Kb6",
              "Bc7+",
              "Kc6",
              "d5+",
              "Kc5",
              "Qxb7",
              "Nxe1",
              "Qb6+",
              "Kxc4",
              "Nd2+",
              "Kxd5",
              "Rxe1",
              "Bc5",
              "Re5+",
              "Kd4",
              "Qxc5+",
              "Kd3",
              "Qc3#",
              "Nf6",
              "Qd3#",
              "Nxd3",
              "Rd5+",
              "Ke2",
              "Nf3",
              "Nxb2",
              "Rd2#",
              "Kxd2",
              "Nxd2",
              "Kxd2",
              "Be5",
              "Nd3",
              "Bxg7",
              "Re8",
              "h3",
              "Re1+",
              "Kh2",
              "Nxf2",
              "Bd4",
              "Ne4",
              "Bb6",
              "Ra1",
              "Ba5+",
              "Ke2",
              "Bb4",
              "Rxa2",
              "Be7",
              "a5",
              "Bd8",
              "a4",
              "Be7",
              "a3",
              "Bxa3",
              "Rxa3",
              "h4",
              "Kf2",
              "g4",
              "Rg3",
              "g5",
              "Rg4",
              "Kh3",
              "Kf3",
              "h5",
              "Nxg5+",
              "Kh2",
              "Kf2",
              "h6",
              "Rh4#",
              "Nf3+",
              "Kh1",
              "Rxh6#",
              "gxf3",
              "Kxf3",
              "Kg1",
              "Kg3",
              "Kf1",
              "Rh2",
              "Ke1",
              "Kf3",
              "Kd1",
              "Ke3",
              "Kc1",
              "Kd3",
              "Kb1",
              "Kc3",
              "Ka1",
              "Kb3",
              "Kb1",
              "Rh1#",
              "Nf3",
              "Kc1",
              "Rxe1#",
              "Nxe1",
              "Kd1",
              "Nf3#",
              "Nxd3",
              "Kd2",
              "Kc4",
              "Ke3",
              "Kd5",
              "Kf3",
              "Ke5",
              "Kg4",
              "Rf1",
              "Kg5",
              "Rg1+",
              "Kh6",
              "Kf5",
              "Kh7",
              "Kf6",
              "Kh8",
              "Kf7",
              "Kh7",
              "Rh1#",
              "Nf3",
              "Kh6",
              "Kf6",
              "Kh5",
              "Kf5",
              "Kh6",
              "Rg1",
              "Kh7",
              "Kf6",
              "Kh8",
              "Kf7",
              "Kh7",
              "Rh1#",
              "Nf3",
              "Kh6",
              "Kf6",
              "Kh5",
              "Kf5",
              "Kh6",
              "Rg1",
              "Kh7",
              "Kf6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nxd5",
              "Re1+",
              "Kd6",
              "c4",
              "Nb6",
              "c5+",
              "Kd5",
              "Nc3+",
              "Kc4",
              "Qb3+",
              "Kxd4",
              "Be3+",
              "Ke5",
              "cxb6",
              "cxb6",
              "Qd5+",
              "Kf6",
              "Bd4+",
              "Kg6",
              "Qxa8",
              "Bb4",
              "Qe4+",
              "Bf5",
              "Qc6+",
              "Kg5",
              "Be3+",
              "Kh5",
              "Qf3+",
              "Kg6",
              "Qg3+",
              "Kf6",
              "Nd5+",
              "Ke6",
              "Nxb4",
              "a5",
              "Bxb6+",
              "Kf6",
              "Bd4#",
              "d5",
              "Qg3+",
              "Kf5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 183,
            "completion_tokens": 550,
            "total_tokens": 733,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 183,
            "completion_tokens": 640,
            "total_tokens": 823,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 182,
            "completion_tokens": 185,
            "total_tokens": 367,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 548,
          "total_completion_tokens": 1375,
          "total_tokens": 1923
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 18,
            "turn_number": 10,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 183,
            "completion_tokens": 550,
            "total_tokens": 733,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 18,
            "turn_number": 10,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 183,
            "completion_tokens": 640,
            "total_tokens": 823,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 18,
            "turn_number": 10,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 182,
            "completion_tokens": 185,
            "total_tokens": 367,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1b2b1r/2p1kppp/p4n2/3p4/3PP3/8/PPP2PPP/RNBQ1RK1 w - - 0 10"
    },
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e5",
          "aggressive_uci": "e7e5",
          "aggressive_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6\n4. Ba4 Nf6 5. O-O Be7 6. Re1 b5\n7. Bb3 O-O 8. c3 d5 9. exd5 Nxd5\n10. Nxe5 Nxe5 11. Rxe5 c6 12. d4 Bd6\n13. Re1 Qh4 14. g3 Qh3 15. Be3 Bg4\n16. Qd3 Rae8 17. Nd2 Re6 18. Qf1 Qh5\n19. a4 Rfe8 20. axb5 axb5\n21. Bxd5 Qxd5 22. Qg2 Qh5 23. Qxc6 Bxg3\n24. Qg2 Bf4 25. Ra8 Bb8 26. Kh1 Rg6\n27. Bf4 Rxe1+ 28. Nf1 Rxf1+ 29. Qxf1 Bf3+ 30. Qg2 Rxg2 31. Rxb8#",
          "aggressive_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 256,
            "total_tokens": 370,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "O-O",
            "c3",
            "d5",
            "exd5",
            "Nxd5",
            "Nxe5",
            "Nxe5",
            "Rxe5",
            "c6",
            "d4",
            "Bd6",
            "Re1",
            "Qh4",
            "g3",
            "Qh3",
            "Be3",
            "Bg4",
            "Qd3",
            "Rae8",
            "Nd2",
            "Re6",
            "Qf1",
            "Qh5",
            "a4",
            "Rfe8",
            "axb5",
            "axb5",
            "Bxd5",
            "Qxd5",
            "Qg2",
            "Qh5",
            "Qxc6",
            "Bxg3",
            "Qg2",
            "Bf4",
            "Ra8",
            "Bb8",
            "Kh1",
            "Rg6",
            "Bf4",
            "Rxe1+",
            "Nf1",
            "Rxf1+",
            "Qxf1",
            "Bf3+",
            "Qg2",
            "Rxg2",
            "Rxb8#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e5",
          "positional_uci": "e7e5",
          "positional_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7\n6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5\n11. d4 Qc7 12. Nbd2 cxd4 13. cxd4 Nc6 14. Nb3 a5\n15. Be3 a4 16. Nbd2 Bd7 17. Rc1 Qb7 18. Bb1 Rfc8\n19. Nf1 Bd8 20. Ng3 Bb6 21. Qd2 Ba5 22. Qe2 Bxe1\n23. Rxe1 Re8 24. Qd2 Rac8 25. Bg5 Kh8 26. Bxf6 gxf6\n27. Qh6 Rg8 28. Qxf6+ Rg7 29. Nh5 Rg8 30. Ng5 Be8\n31. Nxg7 Rxg7 32. dxe5 dxe5 33. Rd1 Qe7 34. Qxe7 Nxe7\n35. Rd8 Rg8 36. Nf3 f6 37. Bd3 Kg7 38. Rb8 Bc6\n39. Rxg8+ Kxg8 40. Kf1 Kf7 41. Ke2 Ke6 42. Ke3 Kd6\n43. Nd2 Kc5 44. a3 Bd7 45. Nb1 Be6 46. Nc3 Bc4\n47. Bxc4 Kxc4 48. Kd2 Nc6 49. Nd5 Kb3 50. Kc1 Nd4\n51. Nxf6 Ne2+ 52. Kb1 Nf4 53. Nxh7 Nxg2 54. Nf6 Nf4\n55. h4 Nd3 56. h5 Nxb2 57. h6 Nc4 58. h7 Nxa3+\n59. Kc1 Nc4 60. h8=Q a3 61. Qh3+ Ka4 62. Nd5 a2\n63. Qc3 b4 64. Qxb4# e5 Nf6 Nc6",
          "positional_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 528,
            "total_tokens": 642,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "Nc6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e5",
          "neutral_uci": "e7e5",
          "neutral_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6\n4. Ba4 Nf6 5. O-O Be7 6. Re1 b5\n7. Bb3 d6 8. c3 O-O 9. h3 Bb7\n10. d4 Re8 11. Nbd2 Bf8 12. a4 h6\n13. Bc2 exd4 14. cxd4 Nb4 15. Bb1 c5\n16. d5 Nd7 17. Ra3 f5 18. Rae3 fxe4\n19. Nxe4 Nxd5 20. Ba2 c4 21. Nxd6 Rxe3\n22. Nxb7 Rxe1+ 23. Nxe1 Qe7 24. Qxd5+ Kh8\n25. Nf3 Nf6 26. Qc6 Ra7 27. Na5 Qd8\n28. Bd2 Rd7 29. Bc3 Rd1+ 30. Kh2 Bd6+\n31. g3 Bc7 32. Nb7 Qd3 33. Bxf6 Qf1\n34. Qe8+ Kh7 35. Qe4+ Kh8 36. Qe8+ Kh7\n37. Qe4+ Kh8 38. Qe8+ Kh7 39. Qe4+ Kh8 40. Qe8+ Kh7 41. Qe4+ Kh8 42. Qe8+ Kh7 43. Qe4+ Kh8 44. Qe8+ Kh7 45. Qe4+ Kh8 46. Qe8+ Kh7 47. Qe4+ Kh8 48. Qe8+ Kh7 49. Qe4+ Kh8 50. Qe8+ Kh7 51. Qe4+ Kh8 52. Qe8+ Kh7 53. Qe4+ Kh8 54. Qe8+ Kh7 55. Qe4+ Kh8 56. Qe8+ Kh7 57. Qe4+ Kh8 58. Qe8+ Kh7 59. Qe4+ Kh8 60. Qe8+ Kh7 61. Qe4+ Kh8 62. Qe8+ Kh7 63. Qe4+ Kh8 64. Qe8+ Kh7 65. Qe4+ Kh8 66. Qe8+ Kh7 67. Qe4+ Kh8 68. Qe8+ Kh7 69. Qe4+ Kh8 70. Qe8+ Kh7 71. Qe4+ Kh8 72. Qe8+ Kh7 73. Qe4+ Kh8 74. Qe8+ Kh7 75. Qe4+ Kh",
          "neutral_tokens": {
            "prompt_tokens": 113,
            "completion_tokens": 640,
            "total_tokens": 753,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Bb7",
            "d4",
            "Re8",
            "Nbd2",
            "Bf8",
            "a4",
            "h6",
            "Bc2",
            "exd4",
            "cxd4",
            "Nb4",
            "Bb1",
            "c5",
            "d5",
            "Nd7",
            "Ra3",
            "f5",
            "Rae3",
            "fxe4",
            "Nxe4",
            "Nxd5",
            "Ba2",
            "c4",
            "Nxd6",
            "Rxe3",
            "Nxb7",
            "Rxe1+",
            "Nxe1",
            "Qe7",
            "Qxd5+",
            "Kh8",
            "Nf3",
            "Nf6",
            "Qc6",
            "Ra7",
            "Na5",
            "Qd8",
            "Bd2",
            "Rd7",
            "Bc3",
            "Rd1+",
            "Kh2",
            "Bd6+",
            "g3",
            "Bc7",
            "Nb7",
            "Qd3",
            "Bxf6",
            "Qf1",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e7e5",
          "positional_uci": "e7e5",
          "neutral_uci": "e7e5",
          "consensus_move": "e7e5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "O-O",
              "c3",
              "d5",
              "exd5",
              "Nxd5",
              "Nxe5",
              "Nxe5",
              "Rxe5",
              "c6",
              "d4",
              "Bd6",
              "Re1",
              "Qh4",
              "g3",
              "Qh3",
              "Be3",
              "Bg4",
              "Qd3",
              "Rae8",
              "Nd2",
              "Re6",
              "Qf1",
              "Qh5",
              "a4",
              "Rfe8",
              "axb5",
              "axb5",
              "Bxd5",
              "Qxd5",
              "Qg2",
              "Qh5",
              "Qxc6",
              "Bxg3",
              "Qg2",
              "Bf4",
              "Ra8",
              "Bb8",
              "Kh1",
              "Rg6",
              "Bf4",
              "Rxe1+",
              "Nf1",
              "Rxf1+",
              "Qxf1",
              "Bf3+",
              "Qg2",
              "Rxg2",
              "Rxb8#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf6",
              "Nc6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Bb7",
              "d4",
              "Re8",
              "Nbd2",
              "Bf8",
              "a4",
              "h6",
              "Bc2",
              "exd4",
              "cxd4",
              "Nb4",
              "Bb1",
              "c5",
              "d5",
              "Nd7",
              "Ra3",
              "f5",
              "Rae3",
              "fxe4",
              "Nxe4",
              "Nxd5",
              "Ba2",
              "c4",
              "Nxd6",
              "Rxe3",
              "Nxb7",
              "Rxe1+",
              "Nxe1",
              "Qe7",
              "Qxd5+",
              "Kh8",
              "Nf3",
              "Nf6",
              "Qc6",
              "Ra7",
              "Na5",
              "Qd8",
              "Bd2",
              "Rd7",
              "Bc3",
              "Rd1+",
              "Kh2",
              "Bd6+",
              "g3",
              "Bc7",
              "Nb7",
              "Qd3",
              "Bxf6",
              "Qf1",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 114,
            "completion_tokens": 256,
            "total_tokens": 370,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 114,
            "completion_tokens": 528,
            "total_tokens": 642,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 113,
            "completion_tokens": 640,
            "total_tokens": 753,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 341,
          "total_completion_tokens": 1424,
          "total_tokens": 1765
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 114,
            "completion_tokens": 256,
            "total_tokens": 370,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 114,
            "completion_tokens": 528,
            "total_tokens": 642,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 113,
            "completion_tokens": 640,
            "total_tokens": 753,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": "d2d4",
          "aggressive_response": "d4 d5\nd4 d5 Nf3 Nf6 Bf4 Bf5 e3 e6 c4 c6 Nc3 Nbd7 Bd3 Bxd3 Qxd3 Be7 O-O O-O Rac1 Rc8 c5 b6 b4 b5 a4 a6 axb5 axb5 Ra1 Ra8 Ra2 Rxa2 Nxa2 Qa8 Nc3 Qa3 Qc2 Ra8 Rb1 Nh5 Bc7 Bd8 Bxd8 Rxd8 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+ Rb1 Qa3 Rb3 Qa1+",
          "aggressive_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Nf3",
            "Nf6",
            "Bf4",
            "Bf5",
            "e3",
            "e6",
            "c4",
            "c6",
            "Nc3",
            "Nbd7",
            "Bd3",
            "Bxd3",
            "Qxd3",
            "Be7",
            "O-O",
            "O-O",
            "Rac1",
            "Rc8",
            "c5",
            "b6",
            "b4",
            "b5",
            "a4",
            "a6",
            "axb5",
            "axb5",
            "Ra1",
            "Ra8",
            "Ra2",
            "Rxa2",
            "Nxa2",
            "Qa8",
            "Nc3",
            "Qa3",
            "Qc2",
            "Ra8",
            "Rb1",
            "Nh5",
            "Bc7",
            "Bd8",
            "Bxd8",
            "Rxd8",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+",
            "Rb1",
            "Qa3",
            "Rb3",
            "Qa1+"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 Bc5 4. O-O Nge7 5. Nxe5 Nxe5 6. d4 Bxd4 7. Qxd4 N7c6 8. Bxc6 Nxc6 9. Qxg7 Rf8 10. Bh6 Qe7 11. Nc3 d6 12. Nd5 Qe5 13. Qxf8+ Kd7 14. Qxf7+ Kd8 15. Qxc7+ Ke8 16. Bg7 Qe6 17. Nf6+ Qxf6 18. Bxf6 Be6 19. Qxb7 Rc8 20. Rad1 Bd7 21. Rxd6 Nb8 22. Rfd1 Rxc2 23. Rxd7 Nxd7 24. Qxd7+ Kf8 25. Qe7+ Kg8 26. Qg7# 1-0 e4 e5 Nf3 Nc6 Bb5 Bc5 O-O Nge7 Nxe5 Nxe5 d4 Bxd4 Qxd4 N7c6 Bxc6 Nxc6 Qxg7 Rf8 Bh6 Qe7 Nc3 d6 Nd5 Qe5 Qxf8+ Kd7 Qxf7+ Kd8 Bg7 Qe6 Nf6 Qxf6 Bxf6 Be6 Qxb7 Rc8 Rad1 Bd7 Rxd6 Nb8 Qxd7#",
          "positional_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 345,
            "total_tokens": 455,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bb5",
            "Bc5",
            "O-O",
            "Nge7",
            "Nxe5",
            "Nxe5",
            "d4",
            "Bxd4",
            "Qxd4",
            "N7c6",
            "Bxc6",
            "Nxc6",
            "Qxg7",
            "Rf8",
            "Bh6",
            "Qe7",
            "Nc3",
            "d6",
            "Nd5",
            "Qe5",
            "Qxf8+",
            "Kd7",
            "Qxf7+",
            "Kd8",
            "Bg7",
            "Qe6",
            "Nf6",
            "Qxf6",
            "Bxf6",
            "Be6",
            "Qxb7",
            "Rc8",
            "Rad1",
            "Bd7",
            "Rxd6",
            "Nb8",
            "Qxd7#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": "d2d4",
          "neutral_response": "d4 d5 2. Nf3 Nf6 3. Bg5 Bf5 4. e3 e6 5. Bd3 Bxd3 6. Qxd3 Be7\n7. O-O O-O 8. Nbd2 Nbd7 9. c4 c6 10. Rac1 Rc8 11. cxd5 cxd5\n12. Nb3 Ne4 13. Bxe7 Qxe7 14. Nfd2 Ndf6 15. Nxe4 Nxe4\n16. f3 Nd6 17. Nc5 b6 18. Nb3 Nc4 19. Rc2 Rc7 20. Rfc1 Rfc8\n21. Nd2 Nxd2 22. Qxd2 Qd7 23. Rxc7 Rxc7 24. Rxc7 Qxc7\n25. Qc3 Qxc3 26. bxc3 Kf8 27. Kf2 Ke7 28. Ke2 Kd6\n29. Kd3 Kc6 30. c4 dxc4+ 31. Kxc4 b5+ 32. Kb4 Kb6\n33. e4 a5+ 34. Kc3 Kc6 35. a3 Kd6 36. f4 f5 37. exf5 exf5\n38. Kd3 Kd5 39. h3 b4 40. axb4 axb4 41. g4 fxg4\n42. hxg4 b3 43. Kc3 b2 44. Kxb2 Kxd4 45. Kc2 Ke4\n46. f5 Kf4 47. Kd3 Kxg4 48. Ke4 h5 49. Ke5 h4\n50. f6 gxf6+ 51. Kxf6 h3 52. Ke5 h2 53. Ke4 h1=Q+\n54. Ke3 Qf3+ 55. Kd4 Kf4 56. Kc5 Qe4 57. Kd6 Qe5+\n58. Kc6 Kf5 59. Kd7 Qe6+ 60. Kc7 Kf6 61. Kb7 Qd6\n62. Kc8 Qe7 63. Kb8 Ke6 64. Kc8 Kd6 65. Kb8 Kc6\n66. Ka8 Qb7# e4 d5 2. Nf3 Nf6 3. Bg5 Bf5 4. e3 e6 5. Bd3 Bxd3 6. Qxd3 Be7\n7. O-O O-O 8. Nbd2 Nbd7 9. c4 c6 10. Rac1 Rc8 11. cxd5 cxd5\n12. Nb3 Ne4",
          "neutral_tokens": {
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Nf3",
            "Nf6",
            "Bg5",
            "Bf5",
            "e3",
            "e6",
            "Bd3",
            "Bxd3",
            "Qxd3",
            "Be7",
            "O-O",
            "O-O",
            "Nbd2",
            "Nbd7",
            "c4",
            "c6",
            "Rac1",
            "Rc8",
            "cxd5",
            "cxd5",
            "Nb3",
            "Ne4",
            "Bxe7",
            "Qxe7",
            "Nfd2",
            "Ndf6",
            "Nxe4",
            "Nxe4",
            "f3",
            "Nd6",
            "Nc5",
            "b6",
            "Nb3",
            "Nc4",
            "Rc2",
            "Rc7",
            "Rfc1",
            "Rfc8",
            "Nd2",
            "Nxd2",
            "Qxd2",
            "Qd7",
            "Rxc7",
            "Rxc7",
            "Rxc7",
            "Qxc7",
            "Qc3",
            "Qxc3",
            "bxc3",
            "Kf8",
            "Kf2",
            "Ke7",
            "Ke2",
            "Kd6",
            "Kd3",
            "Kc6",
            "c4",
            "dxc4+",
            "Kxc4",
            "b5+",
            "Kb4",
            "Kb6",
            "e4",
            "a5+",
            "Kc3",
            "Kc6",
            "a3",
            "Kd6",
            "f4",
            "f5",
            "exf5",
            "exf5",
            "Kd3",
            "Kd5",
            "h3",
            "b4",
            "axb4",
            "axb4",
            "g4",
            "fxg4",
            "hxg4",
            "b3",
            "Kc3",
            "b2",
            "Kxb2",
            "Kxd4",
            "Kc2",
            "Ke4",
            "f5",
            "Kf4",
            "Kd3",
            "Kxg4",
            "Ke4",
            "h5",
            "Ke5",
            "h4",
            "f6",
            "gxf6+",
            "Kxf6",
            "h3",
            "Ke5",
            "h2",
            "Ke4",
            "h1=Q+",
            "Ke3",
            "Qf3+",
            "Kd4",
            "Kf4",
            "Kc5",
            "Qe4",
            "Kd6",
            "Qe5+",
            "Kc6",
            "Kf5",
            "Kd7",
            "Qe6+",
            "Kc7",
            "Kf6",
            "Kb7",
            "Qd6",
            "Kc8",
            "Qe7",
            "Kb8",
            "Ke6",
            "Kc8",
            "Kd6",
            "Kb8",
            "Kc6",
            "Ka8",
            "Qb7#",
            "e4",
            "d5",
            "Nf3",
            "Nf6",
            "Bg5",
            "Bf5",
            "e3",
            "e6",
            "Bd3",
            "Bxd3",
            "Qxd3",
            "Be7",
            "O-O",
            "O-O",
            "Nbd2",
            "Nbd7",
            "c4",
            "c6",
            "Rac1",
            "Rc8",
            "cxd5",
            "cxd5",
            "Nb3",
            "Ne4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d2d4",
          "positional_uci": "e2e4",
          "neutral_uci": "d2d4",
          "consensus_move": "d2d4",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d5",
              "Nf3",
              "Nf6",
              "Bf4",
              "Bf5",
              "e3",
              "e6",
              "c4",
              "c6",
              "Nc3",
              "Nbd7",
              "Bd3",
              "Bxd3",
              "Qxd3",
              "Be7",
              "O-O",
              "O-O",
              "Rac1",
              "Rc8",
              "c5",
              "b6",
              "b4",
              "b5",
              "a4",
              "a6",
              "axb5",
              "axb5",
              "Ra1",
              "Ra8",
              "Ra2",
              "Rxa2",
              "Nxa2",
              "Qa8",
              "Nc3",
              "Qa3",
              "Qc2",
              "Ra8",
              "Rb1",
              "Nh5",
              "Bc7",
              "Bd8",
              "Bxd8",
              "Rxd8",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+",
              "Rb1",
              "Qa3",
              "Rb3",
              "Qa1+"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bb5",
              "Bc5",
              "O-O",
              "Nge7",
              "Nxe5",
              "Nxe5",
              "d4",
              "Bxd4",
              "Qxd4",
              "N7c6",
              "Bxc6",
              "Nxc6",
              "Qxg7",
              "Rf8",
              "Bh6",
              "Qe7",
              "Nc3",
              "d6",
              "Nd5",
              "Qe5",
              "Qxf8+",
              "Kd7",
              "Qxf7+",
              "Kd8",
              "Bg7",
              "Qe6",
              "Nf6",
              "Qxf6",
              "Bxf6",
              "Be6",
              "Qxb7",
              "Rc8",
              "Rad1",
              "Bd7",
              "Rxd6",
              "Nb8",
              "Qxd7#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "d5",
              "Nf3",
              "Nf6",
              "Bg5",
              "Bf5",
              "e3",
              "e6",
              "Bd3",
              "Bxd3",
              "Qxd3",
              "Be7",
              "O-O",
              "O-O",
              "Nbd2",
              "Nbd7",
              "c4",
              "c6",
              "Rac1",
              "Rc8",
              "cxd5",
              "cxd5",
              "Nb3",
              "Ne4",
              "Bxe7",
              "Qxe7",
              "Nfd2",
              "Ndf6",
              "Nxe4",
              "Nxe4",
              "f3",
              "Nd6",
              "Nc5",
              "b6",
              "Nb3",
              "Nc4",
              "Rc2",
              "Rc7",
              "Rfc1",
              "Rfc8",
              "Nd2",
              "Nxd2",
              "Qxd2",
              "Qd7",
              "Rxc7",
              "Rxc7",
              "Rxc7",
              "Qxc7",
              "Qc3",
              "Qxc3",
              "bxc3",
              "Kf8",
              "Kf2",
              "Ke7",
              "Ke2",
              "Kd6",
              "Kd3",
              "Kc6",
              "c4",
              "dxc4+",
              "Kxc4",
              "b5+",
              "Kb4",
              "Kb6",
              "e4",
              "a5+",
              "Kc3",
              "Kc6",
              "a3",
              "Kd6",
              "f4",
              "f5",
              "exf5",
              "exf5",
              "Kd3",
              "Kd5",
              "h3",
              "b4",
              "axb4",
              "axb4",
              "g4",
              "fxg4",
              "hxg4",
              "b3",
              "Kc3",
              "b2",
              "Kxb2",
              "Kxd4",
              "Kc2",
              "Ke4",
              "f5",
              "Kf4",
              "Kd3",
              "Kxg4",
              "Ke4",
              "h5",
              "Ke5",
              "h4",
              "f6",
              "gxf6+",
              "Kxf6",
              "h3",
              "Ke5",
              "h2",
              "Ke4",
              "h1=Q+",
              "Ke3",
              "Qf3+",
              "Kd4",
              "Kf4",
              "Kc5",
              "Qe4",
              "Kd6",
              "Qe5+",
              "Kc6",
              "Kf5",
              "Kd7",
              "Qe6+",
              "Kc7",
              "Kf6",
              "Kb7",
              "Qd6",
              "Kc8",
              "Qe7",
              "Kb8",
              "Ke6",
              "Kc8",
              "Kd6",
              "Kb8",
              "Kc6",
              "Ka8",
              "Qb7#",
              "e4",
              "d5",
              "Nf3",
              "Nf6",
              "Bg5",
              "Bf5",
              "e3",
              "e6",
              "Bd3",
              "Bxd3",
              "Qxd3",
              "Be7",
              "O-O",
              "O-O",
              "Nbd2",
              "Nbd7",
              "c4",
              "c6",
              "Rac1",
              "Rc8",
              "cxd5",
              "cxd5",
              "Nb3",
              "Ne4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 110,
            "completion_tokens": 345,
            "total_tokens": 455,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 329,
          "total_completion_tokens": 1625,
          "total_tokens": 1954
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 110,
            "completion_tokens": 345,
            "total_tokens": 455,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 3,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "c4",
          "aggressive_uci": "c2c4",
          "aggressive_response": "2. c4 c6 3. Nf3 Nf6 4. Nc3 dxc4 5. a4 Bf5 6. e3 e6 7. Bxc4 Bb4 8. O-O O-O 9. Qe2 Nbd7 10. e4 Bg6 11. Bd3 Bh5 12. e5 Nd5 13. Nxd5 cxd5 14. Qe3 Bg6 15. Bxg6 hxg6 16. Ng5 Be7 17. Qg3 Re8 18. Ra3 Bxa3 19. Qh4 Nf8 20. bxa3 f6 21. exf6 gxf6 22. Nf3 Qe7 23. Re1 Rac8 24. Bd2 Rc4 25. Bb4 Qg7 26. Qg4 f5 27. Qf4 Nd7 28. Qd6 Rc6 29. Qf4 Nf6 30. Ne5 Rc2 31. Qg5 Ne4 32. Qxg6 Qxg6 33. Nxg6 Nxf2 34. Nf4 Kf7 35. h3 Ne4 36. Nd3 Rg8 37. Nf4 Rf2 38. Rxe4 Rgxg2+ 39. Nxg2 Rxg2+ 40. Kxg2 dxe4 41. Kg3 Kg6 42. Kf4 Kh5 43. Be1 Kg6 44. Ke5 Kf7 45. h4 b6 46. h5 a6 47. h6 b5 48. axb5 axb5 49. h7 Kg7 50. Kxe6 f4 51. Ke5 e3 52. Kxf4 e2 53. Ke3 Kxh7 54. Kxe2 Kg6 55. Kd3 Kf5 56. Kc3 Ke4 57. Bf2 Kd5 58. Kb4 Kc6 59. Bg3 Kb6 60. d5 Ka6 61. Kc5 Ka5 62. d6 Ka4 63. d7 Kxa3 64. Kxb5 Kb3 65. d8=Q Kc3 66. Qd5 Kc2 67. Qd4 Kb3 68. Qc4+ Kb2 69. Kb4 Kb1 70. Qe2 Kc1 71. Kb3 Kb1 72. Qb2# 1-0 d4 d5 c4 c6 Nf3 Nf6 Nc3 dxc4 a4 Bf5 e3 e6 Bxc4 Bb4 O-O O-O Qe2 Nbd7 e4 Bg6 Bd3 Bh5 e",
          "aggressive_tokens": {
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "c6",
            "Nf3",
            "Nf6",
            "Nc3",
            "dxc4",
            "a4",
            "Bf5",
            "e3",
            "e6",
            "Bxc4",
            "Bb4",
            "O-O",
            "O-O",
            "Qe2",
            "Nbd7",
            "e4",
            "Bg6",
            "Bd3",
            "Bh5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf3",
          "positional_uci": "g1f3",
          "positional_response": "2. Nf3 Nf6 3. c4 c6 4. Nc3 dxc4 5. a4 Bf5 6. e3 e6 7. Bxc4 Bb4 8. O-O O-O 9. Qe2 Bg6 10. Ne5 Nbd7 11. Nxg6 hxg6 12. Rd1 Qa5 13. Bd2 e5 14. dxe5 Nxe5 15. Bb3 Rad8 16. Be1 Rxd1 17. Rxd1 Rd8 18. Rxd8+ Qxd8 19. Qd1 Qxd1 20. Bxd1 Nd3 21. Kf1 Nxb2 22. Bb3 Nd3 23. Bd2 Nc5 24. Bc2 Bxc3 25. Bxc3 Nd5 26. Bd4 b6 27. a5 Kf8 28. axb6 axb6 29. Ke2 Ne6 30. Bb2 Ke7 31. g3 Kd6 32. e4 Nb4 33. Bb3 c5 34. f4 b5 35. f5 gxf5 36. exf5 Nd4+ 37. Bxd4 cxd4 38. Bxf7 Ke5 39. Be8 Kxf5 40. Bxb5 Kg4 41. Bd7+ Kg5 42. Kf3 Nd5 43. h4+ Kf6 44. Ke4 Nb6 45. Bb5 g5 46. h5 g4 47. Kxd4 Kg5 48. Be8 Nc8 49. Ke5 Ne7 50. Ke6 Nf5 51. h6 Nxh6 52. Ke5 Nf5 53. Bd7 Nxg3 54. Bxg4 Kxg4 55. Kd4 Kf4 56. Kd3 Kf3 57. Kd4 Ne4 58. Kd3 Kf4 59. Kd4 Nf6 60. Kd3 Ke5 61. Ke3 Nd5+ 62. Kd3 Nb6 63. Ke3 Kd5 64. Kd3 Nc4 65. Kc3 Kc5 66. Kd3 Kd5 67. Kc3 Ne5 68. Kb4 Kd4 69. Kb5 Kd5 70. Kb4 Nc6+ 71. Kc3 Kc5 72. Kd3 Kd5 73. Kc3 Ne5 74. Kb4 Kd4 75. Kb5 Kd5 76. Kb4 Nc6+ 77. Kc3 Kc5 78. Kd3 Kd5 79. K",
          "positional_tokens": {
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf6",
            "c4",
            "c6",
            "Nc3",
            "dxc4",
            "a4",
            "Bf5",
            "e3",
            "e6",
            "Bxc4",
            "Bb4",
            "O-O",
            "O-O",
            "Qe2",
            "Bg6",
            "Ne5",
            "Nbd7",
            "Nxg6",
            "hxg6",
            "Rd1",
            "Qa5",
            "Bd2",
            "e5",
            "dxe5",
            "Nxe5",
            "Bb3",
            "Rad8",
            "Be1",
            "Rxd1",
            "Rxd1",
            "Rd8",
            "Rxd8+",
            "Qxd8",
            "Qd1",
            "Qxd1",
            "Bxd1",
            "Nd3",
            "Kf1",
            "Nxb2",
            "Bb3",
            "Nd3",
            "Bd2",
            "Nc5",
            "Bc2",
            "Bxc3",
            "Bxc3",
            "Nd5",
            "Bd4",
            "b6",
            "a5",
            "Kf8",
            "axb6",
            "axb6",
            "Ke2",
            "Ne6",
            "Bb2",
            "Ke7",
            "g3",
            "Kd6",
            "e4",
            "Nb4",
            "Bb3",
            "c5",
            "f4",
            "b5",
            "f5",
            "gxf5",
            "exf5",
            "Nd4+",
            "Bxd4",
            "cxd4",
            "Bxf7",
            "Ke5",
            "Be8",
            "Kxf5",
            "Bxb5",
            "Kg4",
            "Bd7+",
            "Kg5",
            "Kf3",
            "Nd5",
            "h4+",
            "Kf6",
            "Ke4",
            "Nb6",
            "Bb5",
            "g5",
            "h5",
            "g4",
            "Kxd4",
            "Kg5",
            "Be8",
            "Nc8",
            "Ke5",
            "Ne7",
            "Ke6",
            "Nf5",
            "h6",
            "Nxh6",
            "Ke5",
            "Nf5",
            "Bd7",
            "Nxg3",
            "Bxg4",
            "Kxg4",
            "Kd4",
            "Kf4",
            "Kd3",
            "Kf3",
            "Kd4",
            "Ne4",
            "Kd3",
            "Kf4",
            "Kd4",
            "Nf6",
            "Kd3",
            "Ke5",
            "Ke3",
            "Nd5+",
            "Kd3",
            "Nb6",
            "Ke3",
            "Kd5",
            "Kd3",
            "Nc4",
            "Kc3",
            "Kc5",
            "Kd3",
            "Kd5",
            "Kc3",
            "Ne5",
            "Kb4",
            "Kd4",
            "Kb5",
            "Kd5",
            "Kb4",
            "Nc6+",
            "Kc3",
            "Kc5",
            "Kd3",
            "Kd5",
            "Kc3",
            "Ne5",
            "Kb4",
            "Kd4",
            "Kb5",
            "Kd5",
            "Kb4",
            "Nc6+",
            "Kc3",
            "Kc5",
            "Kd3",
            "Kd5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "c4",
          "neutral_uci": "c2c4",
          "neutral_response": "2. c4 c6 3. Nc3 Nf6 4. Nf3 e6 *\n5. Bg5 Be7 6. e3 O-O 7. Bd3 Nbd7 *\n8. O-O dxc4 9. Bxc4 Nd5 10. Bxe7 Qxe7 *\n11. e4 Nxc3 12. bxc3 e5 13. Re1 Nb6 *\n14. Bb3 Bg4 15. h3 Bxf3 16. Qxf3 Rad8 *\n17. Rad1 Rd6 18. Qg3 Nd7 19. f4 exf4 *\n20. Qxf4 Rf6 21. Qc7 Qd8 22. Qxb7 Qc8 *\n23. Qxa7 c5 24. e5 Ra6 25. Qxa6 Qxa6 *\n26. e6 c4 27. exd7 cxb3 28. Re8 Qd6 *\n29. Rxf8+ Kxf8 30. axb3 Qxd7 31. c4 *\nQb7 32. Rd3 Qe4 33. Rd1 Qe3+ *\n34. Kh1 Qxb3 35. Rc1 Qe3 36. Rd1 Qe2 *\n37. Rc1 Qd2 38. Rb1 Qxd4 39. Rb8+ Ke7 *\n40. Rb7+ Kf6 41. Rc7 Qd1+ 42. Kh2 Qd6+ *\n43. Kh1 Qxc7 44. c5 Qxc5 45. Kh2 Qf2 *\n46. Kh1 Kg5 47. Kh2 Kh4 48. Kh1 Kg3 *\n49. h4 Qxg2# 0-1\nd4 d5 c4 c6 Nc3 Nf6 Nf3 e6 Bg5 Be7 e3 O-O Bd3 Nbd7 O-O dxc4 Bxc4 Nd5 Bxe7 Qxe7 e4 Nxc3 bxc3 e5 Re1 Nb6 Bb3 Bg4 h3 Bxf3 Qxf3 Rad8 Rad1 Rd6 Qg3 Nd7 f4 exf4 Qxf4 Rf6 Qc7 Qd8 Qxb7 Qc8 Qxa7 c5 e5 Ra6 e6 c4 exd7 cxb3 dxc8=Q Rxa7 Qxf8+ Kxf8 axb3 Qxb3 Re8# 0-1",
          "neutral_tokens": {
            "prompt_tokens": 115,
            "completion_tokens": 575,
            "total_tokens": 690,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "exd7",
            "cxb3",
            "dxc8=Q",
            "Rxa7",
            "Qxf8+",
            "Kxf8",
            "axb3",
            "Qxb3",
            "Re8#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "c2c4",
          "positional_uci": "g1f3",
          "neutral_uci": "c2c4",
          "consensus_move": "c2c4",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "c6",
              "Nf3",
              "Nf6",
              "Nc3",
              "dxc4",
              "a4",
              "Bf5",
              "e3",
              "e6",
              "Bxc4",
              "Bb4",
              "O-O",
              "O-O",
              "Qe2",
              "Nbd7",
              "e4",
              "Bg6",
              "Bd3",
              "Bh5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf6",
              "c4",
              "c6",
              "Nc3",
              "dxc4",
              "a4",
              "Bf5",
              "e3",
              "e6",
              "Bxc4",
              "Bb4",
              "O-O",
              "O-O",
              "Qe2",
              "Bg6",
              "Ne5",
              "Nbd7",
              "Nxg6",
              "hxg6",
              "Rd1",
              "Qa5",
              "Bd2",
              "e5",
              "dxe5",
              "Nxe5",
              "Bb3",
              "Rad8",
              "Be1",
              "Rxd1",
              "Rxd1",
              "Rd8",
              "Rxd8+",
              "Qxd8",
              "Qd1",
              "Qxd1",
              "Bxd1",
              "Nd3",
              "Kf1",
              "Nxb2",
              "Bb3",
              "Nd3",
              "Bd2",
              "Nc5",
              "Bc2",
              "Bxc3",
              "Bxc3",
              "Nd5",
              "Bd4",
              "b6",
              "a5",
              "Kf8",
              "axb6",
              "axb6",
              "Ke2",
              "Ne6",
              "Bb2",
              "Ke7",
              "g3",
              "Kd6",
              "e4",
              "Nb4",
              "Bb3",
              "c5",
              "f4",
              "b5",
              "f5",
              "gxf5",
              "exf5",
              "Nd4+",
              "Bxd4",
              "cxd4",
              "Bxf7",
              "Ke5",
              "Be8",
              "Kxf5",
              "Bxb5",
              "Kg4",
              "Bd7+",
              "Kg5",
              "Kf3",
              "Nd5",
              "h4+",
              "Kf6",
              "Ke4",
              "Nb6",
              "Bb5",
              "g5",
              "h5",
              "g4",
              "Kxd4",
              "Kg5",
              "Be8",
              "Nc8",
              "Ke5",
              "Ne7",
              "Ke6",
              "Nf5",
              "h6",
              "Nxh6",
              "Ke5",
              "Nf5",
              "Bd7",
              "Nxg3",
              "Bxg4",
              "Kxg4",
              "Kd4",
              "Kf4",
              "Kd3",
              "Kf3",
              "Kd4",
              "Ne4",
              "Kd3",
              "Kf4",
              "Kd4",
              "Nf6",
              "Kd3",
              "Ke5",
              "Ke3",
              "Nd5+",
              "Kd3",
              "Nb6",
              "Ke3",
              "Kd5",
              "Kd3",
              "Nc4",
              "Kc3",
              "Kc5",
              "Kd3",
              "Kd5",
              "Kc3",
              "Ne5",
              "Kb4",
              "Kd4",
              "Kb5",
              "Kd5",
              "Kb4",
              "Nc6+",
              "Kc3",
              "Kc5",
              "Kd3",
              "Kd5",
              "Kc3",
              "Ne5",
              "Kb4",
              "Kd4",
              "Kb5",
              "Kd5",
              "Kb4",
              "Nc6+",
              "Kc3",
              "Kc5",
              "Kd3",
              "Kd5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "exd7",
              "cxb3",
              "dxc8=Q",
              "Rxa7",
              "Qxf8+",
              "Kxf8",
              "axb3",
              "Qxb3",
              "Re8#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 115,
            "completion_tokens": 575,
            "total_tokens": 690,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 347,
          "total_completion_tokens": 1855,
          "total_tokens": 2202
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 115,
            "completion_tokens": 575,
            "total_tokens": 690,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/3p4/3P4/8/PPP1PPPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 5,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf3",
          "aggressive_uci": "g1f3",
          "aggressive_response": "3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. dxc5 Qxd1 8. Rxd1 Bxc5 9. b3 b5 10. Be2 Bb7 11. Bb2 Nbd7 12. Nbd2 Ke7 13. Rac1 Rhc8 14. a4 bxa4 15. bxa4 Bb4 16. Nc4 Bd5 17. Nfe5 Rab8 18. Nxd7 Nxd7 19. Bxg7 f6 20. Bh6 Nc5 21. Bf4 e5 22. Rxd5 exf4 23. exf4 Nxa4 24. Bg4 Rc7 25. Rh5 Kf8 26. Rc2 Re8 27. g3 Re4 28. Ne3 Rxc2 29. Nxc2 Kg7 30. Bf5 Rc4 31. Rxh7+ Kg8 32. Ra7 Nc5 33. Ne3 Rc1+ 34. Kg2 a5 35. Nd5 Bc3 36. Nxc3 Rxc3 37. Rxa5 Kg7 38. Ra7+ Kg8 39. h4 Nb3 40. Be6+ Kh8 41. Bxb3 Rxb3 42. Rf7 Rb6 43. g4 Kg8 44. Ra7 Rb3 45. f3 Rb1 46. g5 f5 47. h5 Rb2+ 48. Kg3 Rb1 49. h6 Rg1+ 50. Kf2 Rh1 51. Rg7+ Kh8 52. Rf7 Kg8 53. Rxf5 Kh7 54. Rf7+ Kg6 55. Rg7+ Kf5 56. h7 Kxf4 57. g6 Rh2+ 58. Kg1 Rh6 59. Rg8 Kxf3 60. h8=Q Rxh8 61. Rxh8 Kg4 62. g7 Kf5 63. g8=Q Ke5 64. Rh4 Kf5 65. Qg4+ Ke5 66. Rh5+ Kf6 67. Qg5+ Ke6 68. Rh6+ Kf7 69. Qg6+ Ke7 70. Rh7+ Kf8 71. Qg7+ Ke8 72. Qg8# e4 e5 e6",
          "aggressive_tokens": {
            "prompt_tokens": 124,
            "completion_tokens": 585,
            "total_tokens": 709,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "e3",
            "e6",
            "Bxc4",
            "c5",
            "O-O",
            "a6",
            "dxc5",
            "Qxd1",
            "Rxd1",
            "Bxc5",
            "b3",
            "b5",
            "Be2",
            "Bb7",
            "Bb2",
            "Nbd7",
            "Nbd2",
            "Ke7",
            "Rac1",
            "Rhc8",
            "a4",
            "bxa4",
            "bxa4",
            "Bb4",
            "Nc4",
            "Bd5",
            "Nfe5",
            "Rab8",
            "Nxd7",
            "Nxd7",
            "Bxg7",
            "f6",
            "Bh6",
            "Nc5",
            "Bf4",
            "e5",
            "Rxd5",
            "exf4",
            "exf4",
            "Nxa4",
            "Bg4",
            "Rc7",
            "Rh5",
            "Kf8",
            "Rc2",
            "Re8",
            "g3",
            "Re4",
            "Ne3",
            "Rxc2",
            "Nxc2",
            "Kg7",
            "Bf5",
            "Rc4",
            "Rxh7+",
            "Kg8",
            "Ra7",
            "Nc5",
            "Ne3",
            "Rc1+",
            "Kg2",
            "a5",
            "Nd5",
            "Bc3",
            "Nxc3",
            "Rxc3",
            "Rxa5",
            "Kg7",
            "Ra7+",
            "Kg8",
            "h4",
            "Nb3",
            "Be6+",
            "Kh8",
            "Bxb3",
            "Rxb3",
            "Rf7",
            "Rb6",
            "g4",
            "Kg8",
            "Ra7",
            "Rb3",
            "f3",
            "Rb1",
            "g5",
            "f5",
            "h5",
            "Rb2+",
            "Kg3",
            "Rb1",
            "h6",
            "Rg1+",
            "Kf2",
            "Rh1",
            "Rg7+",
            "Kh8",
            "Rf7",
            "Kg8",
            "Rxf5",
            "Kh7",
            "Rf7+",
            "Kg6",
            "Rg7+",
            "Kf5",
            "h7",
            "Kxf4",
            "g6",
            "Rh2+",
            "Kg1",
            "Rh6",
            "Rg8",
            "Kxf3",
            "h8=Q",
            "Rxh8",
            "Rxh8",
            "Kg4",
            "g7",
            "Kf5",
            "g8=Q",
            "Ke5",
            "Rh4",
            "Kf5",
            "Qg4+",
            "Ke5",
            "Rh5+",
            "Kf6",
            "Qg5+",
            "Ke6",
            "Rh6+",
            "Kf7",
            "Qg6+",
            "Ke7",
            "Rh7+",
            "Kf8",
            "Qg7+",
            "Ke8",
            "Qg8#",
            "e4",
            "e5",
            "e6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf3",
          "positional_uci": "g1f3",
          "positional_response": "3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. dxc5 Qxd1 8. Rxd1 Bxc5 9. Nbd2 O-O 10. Nb3 Be7 11. Be2 Nc6 12. Bd2 e5 13. Bc3 e4 14. Nfd4 Nxd4 15. Bxd4 Be6 16. Nc5 Bxc5 17. Bxc5 Rfd8 18. Rxd8+ Rxd8 19. Bd4 Rc8 20. Bc3 Nd5 21. Bd4 Nb4 22. Bc3 Nxa2 23. Bd4 Nb4 24. Bc3 Nd5 25. Bd4 Nb4 26. Bc3 Nd5 27. Bd4 Nb4 28. Bc3 Nd5 29. Bd4 Nb4 30. Bc3 Nd5 31. Bd4 Nb4 32. Bc3 Nd5 33. Bd4 Nb4 34. Bc3 Nd5 35. Bd4 Nb4 36. Bc3 Nd5 37. Bd4 Nb4 38. Bc3 Nd5 39. Bd4 Nb4 40. Bc3 Nd5 41. Bd4 Nb4 42. Bc3 Nd5 43. Bd4 Nb4 44. Bc3 Nd5 45. Bd4 Nb4 46. Bc3 Nd5 47. Bd4 Nb4 48. Bc3 Nd5 49. Bd4 Nb4 50. Bc3 Nd5 51. Bd4 Nb4 52. Bc3 Nd5 53. Bd4 Nb4 54. Bc3 Nd5 55. Bd4 Nb4 56. Bc3 Nd5 57. Bd4 Nb4 58. Bc3 Nd5 59. Bd4 Nb4 60. Bc3 Nd5 61. Bd4 Nb4 62. Bc3 Nd5 63. Bd4 Nb4 64. Bc3 Nd5 65. Bd4 Nb4 66. Bc3 Nd5 67. Bd4 Nb4 68. Bc3 Nd5 69. Bd4 Nb4 70. Bc3 Nd5 71. Bd4 Nb4 72. Bc3 Nd5 73. Bd4 Nb4 74. Bc3 Nd5 75. Bd4 Nb4 76. Bc3 Nd5 77. Bd4 Nb4 78. Bc3 Nd5 79. Bd4 Nb4 80. Bc3 Nd5 81. Bd4 Nb4 82. Bc3 Nd5 83. Bd4 Nb4 84. Bc3 Nd5 85. Bd4 Nb4 86. Bc",
          "positional_tokens": {
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf6",
            "e3",
            "e6",
            "Bxc4",
            "c5",
            "O-O",
            "a6",
            "dxc5",
            "Qxd1",
            "Rxd1",
            "Bxc5",
            "Nbd2",
            "O-O",
            "Nb3",
            "Be7",
            "Be2",
            "Nc6",
            "Bd2",
            "e5",
            "Bc3",
            "e4",
            "Nfd4",
            "Nxd4",
            "Bxd4",
            "Be6",
            "Nc5",
            "Bxc5",
            "Bxc5",
            "Rfd8",
            "Rxd8+",
            "Rxd8",
            "Bd4",
            "Rc8",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nxa2",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4",
            "Bc3",
            "Nd5",
            "Bd4",
            "Nb4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf3",
          "neutral_uci": "g1f3",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. dxc5 Qxd1 8. Rxd1 Bxc5 9. Nbd2 b5 10. Be2 Bb7 11. Nb3 Bb6 12. a4 bxa4 13. Rxa4 O-O 14. Bd2 Nc6 15. Bc3 Nd5 16. Be1 Rfd8 17. Rda1 a5 18. Nfd2 Ndb4 19. Nc4 Nc2 20. Rc1 Nxe1 21. Nxb6 Nd3 22. Bxd3 Rxd3 23. Nxa8 Rxb3 24. Nc7 Rxb2 25. h3 g6 26. Rc5 Rb1+ 27. Kh2 Rb2 28. Kg3 Ne7 29. Raxa5 Nf5+ 30. Kh2 Rxf2 31. Kg1 Rxg2+ 32. Kf1 Nxe3+ 33. Ke1 Bf3 34. Ra8+ Kg7 35. Ne8+ Kh6 36. Ra2 Rxa2 37. Rc2 Rxc2 38. Nf6 Re2# 1-0\n1. d4 d5 2. c4 dxc4 3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. dxc5 Qxd1 8. Rxd1 Bxc5 9. Nbd2 b5 10. Be2 Bb7 11. Nb3 Bb6 12. a4 bxa4 13. Rxa4 O-O 14. Bd2 Nc6 15. Bc3 Nd5 16. Be1 Rfd8 17. Rda1 a5 18. Nfd2 Ndb4 19. Nc4 Nc2 20. Rc1 Nxe1 21. Nxb6 Nd3 22. Bxd3 Rxd3 23. Nxa8 Rxb3 24. Nc7 Rxb2 25. h3 g6 26. Rc5 Rb1+ 27. Kh2 Rb2 28. Kg3 Ne7 29. Raxa5 Nf5+ 30. Kh2 Rxf2 31. Kg1 Rxg2+ 32. Kf1 Nxe3+ 33. Ke1 Bf3 34. Ra8+ Kg7 35. Ne8+ Kh6 36. Ra2 Rxa2 37. Rc2 Rxc2 38. Nf6 Re2# 1-0 e5",
          "neutral_tokens": {
            "prompt_tokens": 123,
            "completion_tokens": 640,
            "total_tokens": 763,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf6",
            "e3",
            "e6",
            "Bxc4",
            "c5",
            "O-O",
            "a6",
            "dxc5",
            "Qxd1",
            "Rxd1",
            "Bxc5",
            "Nbd2",
            "b5",
            "Be2",
            "Bb7",
            "Nb3",
            "Bb6",
            "a4",
            "bxa4",
            "Rxa4",
            "O-O",
            "Bd2",
            "Nc6",
            "Bc3",
            "Nd5",
            "Be1",
            "Rfd8",
            "Rda1",
            "a5",
            "Nfd2",
            "Ndb4",
            "Nc4",
            "Nc2",
            "Rc1",
            "Nxe1",
            "Nxb6",
            "Nd3",
            "Bxd3",
            "Rxd3",
            "Nxa8",
            "Rxb3",
            "Nc7",
            "Rxb2",
            "h3",
            "g6",
            "Rc5",
            "Rb1+",
            "Kh2",
            "Rb2",
            "Kg3",
            "Ne7",
            "Raxa5",
            "Nf5+",
            "Kh2",
            "Rxf2",
            "Kg1",
            "Rxg2+",
            "Kf1",
            "Nxe3+",
            "Ke1",
            "Bf3",
            "Ra8+",
            "Kg7",
            "Ne8+",
            "Kh6",
            "Ra2",
            "Rxa2",
            "Rc2",
            "Rxc2",
            "Nf6",
            "Re2#",
            "e5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g1f3",
          "positional_uci": "g1f3",
          "neutral_uci": "g1f3",
          "consensus_move": "g1f3",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf6",
              "e3",
              "e6",
              "Bxc4",
              "c5",
              "O-O",
              "a6",
              "dxc5",
              "Qxd1",
              "Rxd1",
              "Bxc5",
              "b3",
              "b5",
              "Be2",
              "Bb7",
              "Bb2",
              "Nbd7",
              "Nbd2",
              "Ke7",
              "Rac1",
              "Rhc8",
              "a4",
              "bxa4",
              "bxa4",
              "Bb4",
              "Nc4",
              "Bd5",
              "Nfe5",
              "Rab8",
              "Nxd7",
              "Nxd7",
              "Bxg7",
              "f6",
              "Bh6",
              "Nc5",
              "Bf4",
              "e5",
              "Rxd5",
              "exf4",
              "exf4",
              "Nxa4",
              "Bg4",
              "Rc7",
              "Rh5",
              "Kf8",
              "Rc2",
              "Re8",
              "g3",
              "Re4",
              "Ne3",
              "Rxc2",
              "Nxc2",
              "Kg7",
              "Bf5",
              "Rc4",
              "Rxh7+",
              "Kg8",
              "Ra7",
              "Nc5",
              "Ne3",
              "Rc1+",
              "Kg2",
              "a5",
              "Nd5",
              "Bc3",
              "Nxc3",
              "Rxc3",
              "Rxa5",
              "Kg7",
              "Ra7+",
              "Kg8",
              "h4",
              "Nb3",
              "Be6+",
              "Kh8",
              "Bxb3",
              "Rxb3",
              "Rf7",
              "Rb6",
              "g4",
              "Kg8",
              "Ra7",
              "Rb3",
              "f3",
              "Rb1",
              "g5",
              "f5",
              "h5",
              "Rb2+",
              "Kg3",
              "Rb1",
              "h6",
              "Rg1+",
              "Kf2",
              "Rh1",
              "Rg7+",
              "Kh8",
              "Rf7",
              "Kg8",
              "Rxf5",
              "Kh7",
              "Rf7+",
              "Kg6",
              "Rg7+",
              "Kf5",
              "h7",
              "Kxf4",
              "g6",
              "Rh2+",
              "Kg1",
              "Rh6",
              "Rg8",
              "Kxf3",
              "h8=Q",
              "Rxh8",
              "Rxh8",
              "Kg4",
              "g7",
              "Kf5",
              "g8=Q",
              "Ke5",
              "Rh4",
              "Kf5",
              "Qg4+",
              "Ke5",
              "Rh5+",
              "Kf6",
              "Qg5+",
              "Ke6",
              "Rh6+",
              "Kf7",
              "Qg6+",
              "Ke7",
              "Rh7+",
              "Kf8",
              "Qg7+",
              "Ke8",
              "Qg8#",
              "e4",
              "e5",
              "e6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf6",
              "e3",
              "e6",
              "Bxc4",
              "c5",
              "O-O",
              "a6",
              "dxc5",
              "Qxd1",
              "Rxd1",
              "Bxc5",
              "Nbd2",
              "O-O",
              "Nb3",
              "Be7",
              "Be2",
              "Nc6",
              "Bd2",
              "e5",
              "Bc3",
              "e4",
              "Nfd4",
              "Nxd4",
              "Bxd4",
              "Be6",
              "Nc5",
              "Bxc5",
              "Bxc5",
              "Rfd8",
              "Rxd8+",
              "Rxd8",
              "Bd4",
              "Rc8",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nxa2",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4",
              "Bc3",
              "Nd5",
              "Bd4",
              "Nb4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf6",
              "e3",
              "e6",
              "Bxc4",
              "c5",
              "O-O",
              "a6",
              "dxc5",
              "Qxd1",
              "Rxd1",
              "Bxc5",
              "Nbd2",
              "b5",
              "Be2",
              "Bb7",
              "Nb3",
              "Bb6",
              "a4",
              "bxa4",
              "Rxa4",
              "O-O",
              "Bd2",
              "Nc6",
              "Bc3",
              "Nd5",
              "Be1",
              "Rfd8",
              "Rda1",
              "a5",
              "Nfd2",
              "Ndb4",
              "Nc4",
              "Nc2",
              "Rc1",
              "Nxe1",
              "Nxb6",
              "Nd3",
              "Bxd3",
              "Rxd3",
              "Nxa8",
              "Rxb3",
              "Nc7",
              "Rxb2",
              "h3",
              "g6",
              "Rc5",
              "Rb1+",
              "Kh2",
              "Rb2",
              "Kg3",
              "Ne7",
              "Raxa5",
              "Nf5+",
              "Kh2",
              "Rxf2",
              "Kg1",
              "Rxg2+",
              "Kf1",
              "Nxe3+",
              "Ke1",
              "Bf3",
              "Ra8+",
              "Kg7",
              "Ne8+",
              "Kh6",
              "Ra2",
              "Rxa2",
              "Rc2",
              "Rxc2",
              "Nf6",
              "Re2#",
              "e5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 124,
            "completion_tokens": 585,
            "total_tokens": 709,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 123,
            "completion_tokens": 640,
            "total_tokens": 763,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 371,
          "total_completion_tokens": 1865,
          "total_tokens": 2236
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 124,
            "completion_tokens": 585,
            "total_tokens": 709,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 123,
            "completion_tokens": 640,
            "total_tokens": 763,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/8/2pP4/8/PP2PPPP/RNBQKBNR w KQkq - 0 3"
    },
    {
      "turn": 7,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc3",
          "aggressive_uci": "b1c3",
          "aggressive_response": "4. Nc3 Nf6 5. e4 Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O Bxf3 10. gxf3 Nh5 11. Kh1 Qh4 12. Rg1 Rad8 13. Rg4 Qh3 14. Qe2 Nxd4 15. Bxd4 Rxd4 16. Rag1 Rfd8 17. Qe3 Rxc4 18. Ne2 Bd2 19. Qb3 Rb4 20. Qc2 Qxf3+ 21. R1g2 Qxe2 22. Qxc7 Rbd4 23. Qe5 Qf1+ 24. Rg1 Qh3 25. Rxg7+ Kf8 26. Qc5+ Rd6 27. Rg8+ Ke7 28. Rxd8 Qf3+ 29. Rg2 Qd1+ 30. Rg1 Qf3+ 31. Rg2 Nf4 32. Qxd6+ Kf6 33. Qd4+ Ke7 34. Rd7+ Ke8 35. Rd8+ Ke7 36. Qd6+ Kf6 37. Qd4+ Ke7 38. Rd7+ Ke8 39. Rd8+ Ke7 40. Rd7+ Ke8 41. Rd8+ Ke7 42. Rd7+ Ke8 43. Rd8+ Ke7 44. Rd7+ Ke8 45. Rd8+ Ke7 46. Rd7+ Ke8 47. Rd8+ Ke7 48. Rd7+ Ke8 49. Rd8+ Ke7 50. Rd7+ Ke8 51. Rd8+ Ke7 52. Rd7+ Ke8 53. Rd8+ Ke7 54. Rd7+ Ke8 55. Rd8+ Ke7 56. Rd7+ Ke8 57. Rd8+ Ke7 58. Rd7+ Ke8 59. Rd8+ Ke7 60. Rd7+ Ke8 61. Rd8+ Ke7 62. Rd7+ Ke8 63. Rd8+ Ke7 64. Rd7+ Ke8 65. Rd8+ Ke7 66. Rd7+ Ke8 67. Rd8+ Ke7 68. Rd7+ Ke8 69. Rd8+ Ke7 70. Rd7+ Ke8 71. Rd8+ Ke7 72. Rd7+ Ke8 73. Rd8+ Ke7 74. Rd7+ Ke8 75. Rd8+ Ke7 76. Rd7+ Ke8 77. Rd8+ Ke7 78. Rd7+ Ke8 79. Rd8+ Ke7 80",
          "aggressive_tokens": {
            "prompt_tokens": 133,
            "completion_tokens": 640,
            "total_tokens": 773,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf6",
            "e4",
            "Bg4",
            "Be3",
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O",
            "O-O",
            "Bxf3",
            "gxf3",
            "Nh5",
            "Kh1",
            "Qh4",
            "Rg1",
            "Rad8",
            "Rg4",
            "Qh3",
            "Qe2",
            "Nxd4",
            "Bxd4",
            "Rxd4",
            "Rag1",
            "Rfd8",
            "Qe3",
            "Rxc4",
            "Ne2",
            "Bd2",
            "Qb3",
            "Rb4",
            "Qc2",
            "Qxf3+",
            "R1g2",
            "Qxe2",
            "Qxc7",
            "Rbd4",
            "Qe5",
            "Qf1+",
            "Rg1",
            "Qh3",
            "Rxg7+",
            "Kf8",
            "Qc5+",
            "Rd6",
            "Rg8+",
            "Ke7",
            "Rxd8",
            "Qf3+",
            "Rg2",
            "Qd1+",
            "Rg1",
            "Qf3+",
            "Rg2",
            "Nf4",
            "Qxd6+",
            "Kf6",
            "Qd4+",
            "Ke7",
            "Rd7+",
            "Ke8",
            "Rd8+",
            "Ke7",
            "Qd6+",
            "Kf6",
            "Qd4+",
            "Ke7",
            "Rd7+",
            "Ke8",
            "Rd8+",
            "Ke7",
            "Rd7+",
            "Ke8",
            "Rd8+",
            "Ke7",
            "Rd7+",
            "Ke8",
            "Rd8+",
            "Ke7",
            "Rd7+",
            "Ke8",
            "Rd8+",
            "Ke7",
            "Rd7+",
            "Ke8",
            "Rd8+",
            "Ke7",
            "Rd7+",
            "Ke8",
            "Rd8+",
            "Ke7",
            "Rd7+",
            "Ke8",
            "Rd8+",
            "Ke7",
            "Rd7+",
            "Ke8",
            "Rd8+",
            "Ke7",
            "Rd7+",
            "Ke8",
            "Rd8+",
            "Ke7",
            "Rd7+",
            "Ke8",
            "Rd8+",
            "Ke7",
            "Rd7+",
            "Ke8",
            "Rd8+",
            "Ke7",
            "Rd7+",
            "Ke8",
            "Rd8+",
            "Ke7",
            "Rd7+",
            "Ke8",
            "Rd8+",
            "Ke7",
            "Rd7+",
            "Ke8",
            "Rd8+",
            "Ke7",
            "Rd7+",
            "Ke8",
            "Rd8+",
            "Ke7",
            "Rd7+",
            "Ke8",
            "Rd8+",
            "Ke7",
            "Rd7+",
            "Ke8",
            "Rd8+",
            "Ke7",
            "Rd7+",
            "Ke8",
            "Rd8+",
            "Ke7",
            "Rd7+",
            "Ke8",
            "Rd8+",
            "Ke7",
            "Rd7+",
            "Ke8",
            "Rd8+",
            "Ke7",
            "Rd7+",
            "Ke8",
            "Rd8+",
            "Ke7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e3",
          "positional_uci": "e2e3",
          "positional_response": "4. e3 e5 5. Bxc4 exd4 6. exd4 Bg4 7. O-O Nf6 8. Re1+ Be7 9. d5 Na5 10. Qa4+ c6 11. dxc6 Nxc6 12. Ne5 Bd7 13. Nxf7 Qa5 14. Qxa5 Nxa5 15. Nxh8 Nxc4 16. b3 Nb6 17. Ba3 Nbd5 18. Nc3 Kf8 19. Nxd5 Nxd5 20. Rad1 Bxa3 21. Rxd5 Bc6 22. Rf5+ Kg8 23. Nf7 Rf8 24. Nh6+ gxh6 25. Rxf8+ Kxf8 26. f3 Bc5+ 27. Kh1 Kf7 28. Rc1 Bb6 29. Rc4 Kf6 30. Rh4 Kg5 31. Rg4+ Kf5 32. Rh4 Kg6 33. Rg4+ Kf6 34. Rh4 Kg7 35. Rg4+ Kf7 36. Rh4 Kg6 37. Rg4+ Kf5 38. Rh4 Kg5 39. Rg4+ Kf6 40. Rh4 Kg7 41. Rg4+ Kf7 42. Rh4 Kg6 43. Rg4+ Kf5 44. Rh4 Kg5 45. Rg4+ Kf6 46. Rh4 Kg7 47. Rg4+ Kf7 48. Rh4 Kg6 49. Rg4+ Kf5 50. Rh4 Kg5 51. Rg4+ Kf6 52. Rh4 Kg7 53. Rg4+ Kf7 54. Rh4 Kg6 55. Rg4+ Kf5 56. Rh4 Kg5 57. Rg4+ Kf6 58. Rh4 Kg7 59. Rg4+ Kf7 60. Rh4 Kg6 61. Rg4+ Kf5 62. Rh4 Kg5 63. Rg4+ Kf6 64. Rh4 Kg7 65. Rg4+ Kf7 66. Rh4 Kg6 67. Rg4+ Kf5 68. Rh4 Kg5 69. Rg4+ Kf6 70. Rh4 Kg7 71. Rg4+ Kf7 72. Rh4 Kg6 73. Rg4+ Kf5 74. Rh4 Kg5 75. Rg4+ Kf6 76. Rh4 Kg7 77. Rg4+ Kf7 78. Rh4 Kg6 79. Rg",
          "positional_tokens": {
            "prompt_tokens": 133,
            "completion_tokens": 640,
            "total_tokens": 773,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "e5",
            "Bxc4",
            "exd4",
            "exd4",
            "Bg4",
            "O-O",
            "Nf6",
            "Re1+",
            "Be7",
            "d5",
            "Na5",
            "Qa4+",
            "c6",
            "dxc6",
            "Nxc6",
            "Ne5",
            "Bd7",
            "Nxf7",
            "Qa5",
            "Qxa5",
            "Nxa5",
            "Nxh8",
            "Nxc4",
            "b3",
            "Nb6",
            "Ba3",
            "Nbd5",
            "Nc3",
            "Kf8",
            "Nxd5",
            "Nxd5",
            "Rad1",
            "Bxa3",
            "Rxd5",
            "Bc6",
            "Rf5+",
            "Kg8",
            "Nf7",
            "Rf8",
            "Nh6+",
            "gxh6",
            "Rxf8+",
            "Kxf8",
            "f3",
            "Bc5+",
            "Kh1",
            "Kf7",
            "Rc1",
            "Bb6",
            "Rc4",
            "Kf6",
            "Rh4",
            "Kg5",
            "Rg4+",
            "Kf5",
            "Rh4",
            "Kg6",
            "Rg4+",
            "Kf6",
            "Rh4",
            "Kg7",
            "Rg4+",
            "Kf7",
            "Rh4",
            "Kg6",
            "Rg4+",
            "Kf5",
            "Rh4",
            "Kg5",
            "Rg4+",
            "Kf6",
            "Rh4",
            "Kg7",
            "Rg4+",
            "Kf7",
            "Rh4",
            "Kg6",
            "Rg4+",
            "Kf5",
            "Rh4",
            "Kg5",
            "Rg4+",
            "Kf6",
            "Rh4",
            "Kg7",
            "Rg4+",
            "Kf7",
            "Rh4",
            "Kg6",
            "Rg4+",
            "Kf5",
            "Rh4",
            "Kg5",
            "Rg4+",
            "Kf6",
            "Rh4",
            "Kg7",
            "Rg4+",
            "Kf7",
            "Rh4",
            "Kg6",
            "Rg4+",
            "Kf5",
            "Rh4",
            "Kg5",
            "Rg4+",
            "Kf6",
            "Rh4",
            "Kg7",
            "Rg4+",
            "Kf7",
            "Rh4",
            "Kg6",
            "Rg4+",
            "Kf5",
            "Rh4",
            "Kg5",
            "Rg4+",
            "Kf6",
            "Rh4",
            "Kg7",
            "Rg4+",
            "Kf7",
            "Rh4",
            "Kg6",
            "Rg4+",
            "Kf5",
            "Rh4",
            "Kg5",
            "Rg4+",
            "Kf6",
            "Rh4",
            "Kg7",
            "Rg4+",
            "Kf7",
            "Rh4",
            "Kg6",
            "Rg4+",
            "Kf5",
            "Rh4",
            "Kg5",
            "Rg4+",
            "Kf6",
            "Rh4",
            "Kg7",
            "Rg4+",
            "Kf7",
            "Rh4",
            "Kg6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e3",
          "neutral_uci": "e2e3",
          "neutral_response": "4. e3 e5 5. Bxc4 exd4 6. exd4 Bg4 7. O-O Nf6 8. Re1+ Be7 9. d5 Na5 10. Qa4+ c6 11. dxc6 Nxc6 12. Ne5 Bd7 13. Nxf7 Qc7 14. Nxh8 Ne5 15. Bb5 O-O-O 16. Bxd7+ Rxd7 17. Bf4 Bd6 18. Rc1 Nc6 19. Bxd6 Qxd6 20. Nc3 Qf8 21. Nb5 a6 22. Na7+ Kb8 23. Nxc6+ bxc6 24. Qxc6 Qxh8 25. Qxa6 Qd8 26. Qb5+ Rb7 27. Qe5+ Ka8 28. Rc3 Ra7 29. Rac1 Qd7 30. Rc8+ Kb7 31. R8c7+ Kb6 32. Rxd7 Nxd7 33. Qd6+ Kb5 34. Qc6+ Kb4 35. Rc4+ Ka5 36. Ra4# N 1-0",
          "neutral_tokens": {
            "prompt_tokens": 132,
            "completion_tokens": 285,
            "total_tokens": 417,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Bxc4",
            "exd4",
            "exd4",
            "Bg4",
            "O-O",
            "Nf6",
            "Re1+",
            "Be7",
            "d5",
            "Na5",
            "Qa4+",
            "c6",
            "dxc6",
            "Nxc6",
            "Ne5",
            "Bd7",
            "Nxf7",
            "Qc7",
            "Nxh8",
            "Ne5",
            "Bb5",
            "O-O-O",
            "Bxd7+",
            "Rxd7",
            "Bf4",
            "Bd6",
            "Rc1",
            "Nc6",
            "Bxd6",
            "Qxd6",
            "Nc3",
            "Qf8",
            "Nb5",
            "a6",
            "Na7+",
            "Kb8",
            "Nxc6+",
            "bxc6",
            "Qxc6",
            "Qxh8",
            "Qxa6",
            "Qd8",
            "Qb5+",
            "Rb7",
            "Qe5+",
            "Ka8",
            "Rc3",
            "Ra7",
            "Rac1",
            "Qd7",
            "Rc8+",
            "Kb7",
            "R8c7+",
            "Kb6",
            "Rxd7",
            "Nxd7",
            "Qd6+",
            "Kb5",
            "Qc6+",
            "Kb4",
            "Rc4+",
            "Ka5",
            "Ra4#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b1c3",
          "positional_uci": "e2e3",
          "neutral_uci": "e2e3",
          "consensus_move": "e2e3",
          "source_agents": [
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf6",
              "e4",
              "Bg4",
              "Be3",
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O",
              "O-O",
              "Bxf3",
              "gxf3",
              "Nh5",
              "Kh1",
              "Qh4",
              "Rg1",
              "Rad8",
              "Rg4",
              "Qh3",
              "Qe2",
              "Nxd4",
              "Bxd4",
              "Rxd4",
              "Rag1",
              "Rfd8",
              "Qe3",
              "Rxc4",
              "Ne2",
              "Bd2",
              "Qb3",
              "Rb4",
              "Qc2",
              "Qxf3+",
              "R1g2",
              "Qxe2",
              "Qxc7",
              "Rbd4",
              "Qe5",
              "Qf1+",
              "Rg1",
              "Qh3",
              "Rxg7+",
              "Kf8",
              "Qc5+",
              "Rd6",
              "Rg8+",
              "Ke7",
              "Rxd8",
              "Qf3+",
              "Rg2",
              "Qd1+",
              "Rg1",
              "Qf3+",
              "Rg2",
              "Nf4",
              "Qxd6+",
              "Kf6",
              "Qd4+",
              "Ke7",
              "Rd7+",
              "Ke8",
              "Rd8+",
              "Ke7",
              "Qd6+",
              "Kf6",
              "Qd4+",
              "Ke7",
              "Rd7+",
              "Ke8",
              "Rd8+",
              "Ke7",
              "Rd7+",
              "Ke8",
              "Rd8+",
              "Ke7",
              "Rd7+",
              "Ke8",
              "Rd8+",
              "Ke7",
              "Rd7+",
              "Ke8",
              "Rd8+",
              "Ke7",
              "Rd7+",
              "Ke8",
              "Rd8+",
              "Ke7",
              "Rd7+",
              "Ke8",
              "Rd8+",
              "Ke7",
              "Rd7+",
              "Ke8",
              "Rd8+",
              "Ke7",
              "Rd7+",
              "Ke8",
              "Rd8+",
              "Ke7",
              "Rd7+",
              "Ke8",
              "Rd8+",
              "Ke7",
              "Rd7+",
              "Ke8",
              "Rd8+",
              "Ke7",
              "Rd7+",
              "Ke8",
              "Rd8+",
              "Ke7",
              "Rd7+",
              "Ke8",
              "Rd8+",
              "Ke7",
              "Rd7+",
              "Ke8",
              "Rd8+",
              "Ke7",
              "Rd7+",
              "Ke8",
              "Rd8+",
              "Ke7",
              "Rd7+",
              "Ke8",
              "Rd8+",
              "Ke7",
              "Rd7+",
              "Ke8",
              "Rd8+",
              "Ke7",
              "Rd7+",
              "Ke8",
              "Rd8+",
              "Ke7",
              "Rd7+",
              "Ke8",
              "Rd8+",
              "Ke7",
              "Rd7+",
              "Ke8",
              "Rd8+",
              "Ke7",
              "Rd7+",
              "Ke8",
              "Rd8+",
              "Ke7",
              "Rd7+",
              "Ke8",
              "Rd8+",
              "Ke7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Bxc4",
              "exd4",
              "exd4",
              "Bg4",
              "O-O",
              "Nf6",
              "Re1+",
              "Be7",
              "d5",
              "Na5",
              "Qa4+",
              "c6",
              "dxc6",
              "Nxc6",
              "Ne5",
              "Bd7",
              "Nxf7",
              "Qa5",
              "Qxa5",
              "Nxa5",
              "Nxh8",
              "Nxc4",
              "b3",
              "Nb6",
              "Ba3",
              "Nbd5",
              "Nc3",
              "Kf8",
              "Nxd5",
              "Nxd5",
              "Rad1",
              "Bxa3",
              "Rxd5",
              "Bc6",
              "Rf5+",
              "Kg8",
              "Nf7",
              "Rf8",
              "Nh6+",
              "gxh6",
              "Rxf8+",
              "Kxf8",
              "f3",
              "Bc5+",
              "Kh1",
              "Kf7",
              "Rc1",
              "Bb6",
              "Rc4",
              "Kf6",
              "Rh4",
              "Kg5",
              "Rg4+",
              "Kf5",
              "Rh4",
              "Kg6",
              "Rg4+",
              "Kf6",
              "Rh4",
              "Kg7",
              "Rg4+",
              "Kf7",
              "Rh4",
              "Kg6",
              "Rg4+",
              "Kf5",
              "Rh4",
              "Kg5",
              "Rg4+",
              "Kf6",
              "Rh4",
              "Kg7",
              "Rg4+",
              "Kf7",
              "Rh4",
              "Kg6",
              "Rg4+",
              "Kf5",
              "Rh4",
              "Kg5",
              "Rg4+",
              "Kf6",
              "Rh4",
              "Kg7",
              "Rg4+",
              "Kf7",
              "Rh4",
              "Kg6",
              "Rg4+",
              "Kf5",
              "Rh4",
              "Kg5",
              "Rg4+",
              "Kf6",
              "Rh4",
              "Kg7",
              "Rg4+",
              "Kf7",
              "Rh4",
              "Kg6",
              "Rg4+",
              "Kf5",
              "Rh4",
              "Kg5",
              "Rg4+",
              "Kf6",
              "Rh4",
              "Kg7",
              "Rg4+",
              "Kf7",
              "Rh4",
              "Kg6",
              "Rg4+",
              "Kf5",
              "Rh4",
              "Kg5",
              "Rg4+",
              "Kf6",
              "Rh4",
              "Kg7",
              "Rg4+",
              "Kf7",
              "Rh4",
              "Kg6",
              "Rg4+",
              "Kf5",
              "Rh4",
              "Kg5",
              "Rg4+",
              "Kf6",
              "Rh4",
              "Kg7",
              "Rg4+",
              "Kf7",
              "Rh4",
              "Kg6",
              "Rg4+",
              "Kf5",
              "Rh4",
              "Kg5",
              "Rg4+",
              "Kf6",
              "Rh4",
              "Kg7",
              "Rg4+",
              "Kf7",
              "Rh4",
              "Kg6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5",
              "Bxc4",
              "exd4",
              "exd4",
              "Bg4",
              "O-O",
              "Nf6",
              "Re1+",
              "Be7",
              "d5",
              "Na5",
              "Qa4+",
              "c6",
              "dxc6",
              "Nxc6",
              "Ne5",
              "Bd7",
              "Nxf7",
              "Qc7",
              "Nxh8",
              "Ne5",
              "Bb5",
              "O-O-O",
              "Bxd7+",
              "Rxd7",
              "Bf4",
              "Bd6",
              "Rc1",
              "Nc6",
              "Bxd6",
              "Qxd6",
              "Nc3",
              "Qf8",
              "Nb5",
              "a6",
              "Na7+",
              "Kb8",
              "Nxc6+",
              "bxc6",
              "Qxc6",
              "Qxh8",
              "Qxa6",
              "Qd8",
              "Qb5+",
              "Rb7",
              "Qe5+",
              "Ka8",
              "Rc3",
              "Ra7",
              "Rac1",
              "Qd7",
              "Rc8+",
              "Kb7",
              "R8c7+",
              "Kb6",
              "Rxd7",
              "Nxd7",
              "Qd6+",
              "Kb5",
              "Qc6+",
              "Kb4",
              "Rc4+",
              "Ka5",
              "Ra4#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 133,
            "completion_tokens": 640,
            "total_tokens": 773,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 133,
            "completion_tokens": 640,
            "total_tokens": 773,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 132,
            "completion_tokens": 285,
            "total_tokens": 417,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 398,
          "total_completion_tokens": 1565,
          "total_tokens": 1963
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 133,
            "completion_tokens": 640,
            "total_tokens": 773,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 133,
            "completion_tokens": 640,
            "total_tokens": 773,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 132,
            "completion_tokens": 285,
            "total_tokens": 417,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkbnr/ppp1pppp/2n5/8/2pP4/5N2/PP2PPPP/RNBQKB1R w KQkq - 2 4"
    },
    {
      "turn": 9,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bxc4",
          "aggressive_uci": "f1c4",
          "aggressive_response": "5. Bxc4 Bg4 6. O-O e6 7. Nc3 Bb4 8. a3 Bxc3 9. bxc3 O-O 10. h3 Bh5 11. Be2 Ne4 12. Qc2 Bg6 13. Bd3 f5 14. Nd2 Nxd2 15. Bxd2 Qd6 16. c4 Rad8 17. c5 Qd7 18. Bc4 Kh8 19. Qb3 Bf7 20. Qxb7 Rb8 21. Qa6 e5 22. Bxf7 Rxf7 23. Bc3 exd4 24. exd4 Nxd4 25. Rad1 Qb5 26. Qxb5 Nxb5 27. Rb1 Rff8 28. Be5 Nxa3 29. Rxb8 Rxb8 30. Bxc7 Rc8 31. Bd6 Nb5 32. Rd1 Kg8 33. Rd5 g6 34. Be5 Kf7 35. Rd7+ Ke6 36. Rb7 Rxc5 37. Bb8 a5 38. Rxh7 a4 39. Rh6 Kf7 40. Rh7+ Ke6 41. Rh6 Kf7 42. Rh7+ Ke6 43. Rh6 Kf7 44. Rh7+ Ke6 45. Rh6 Kf7 46. Rh7+ Ke6 47. Rh6 Kf7 48. Rh7+ Ke6 49. Rh6 Kf7 50. Rh7+ Ke6 51. Rh6 Kf7 52. Rh7+ Ke6 53. Rh6 Kf7 54. Rh7+ Ke6 55. Rh6 Kf7 56. Rh7+ Ke6 57. Rh6 Kf7 58. Rh7+ Ke6 59. Rh6 Kf7 60. Rh7+ Ke6 61. Rh6 Kf7 62. Rh7+ Ke6 63. Rh6 Kf7 64. Rh7+ Ke6 65. Rh6 Kf7 66. Rh7+ Ke6 67. Rh6 Kf7 68. Rh7+ Ke6 69. Rh6 Kf7 70. Rh7+ Ke6 71. Rh6 Kf7 72. Rh7+ Ke6 73. Rh6 Kf7 74. Rh7+ Ke6 75. Rh6 Kf7 76. Rh7+ Ke6 77. Rh6 Kf7 78. Rh7+ Ke6 79. Rh6 Kf7 80. Rh7+ Ke6 81. Rh6 Kf7 82. Rh7+ Ke6 83. Rh6 Kf7 84.",
          "aggressive_tokens": {
            "prompt_tokens": 141,
            "completion_tokens": 640,
            "total_tokens": 781,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bg4",
            "O-O",
            "e6",
            "Nc3",
            "Bb4",
            "a3",
            "Bxc3",
            "bxc3",
            "O-O",
            "h3",
            "Bh5",
            "Be2",
            "Ne4",
            "Qc2",
            "Bg6",
            "Bd3",
            "f5",
            "Nd2",
            "Nxd2",
            "Bxd2",
            "Qd6",
            "c4",
            "Rad8",
            "c5",
            "Qd7",
            "Bc4",
            "Kh8",
            "Qb3",
            "Bf7",
            "Qxb7",
            "Rb8",
            "Qa6",
            "e5",
            "Bxf7",
            "Rxf7",
            "Bc3",
            "exd4",
            "exd4",
            "Nxd4",
            "Rad1",
            "Qb5",
            "Qxb5",
            "Nxb5",
            "Rb1",
            "Rff8",
            "Be5",
            "Nxa3",
            "Rxb8",
            "Rxb8",
            "Bxc7",
            "Rc8",
            "Bd6",
            "Nb5",
            "Rd1",
            "Kg8",
            "Rd5",
            "g6",
            "Be5",
            "Kf7",
            "Rd7+",
            "Ke6",
            "Rb7",
            "Rxc5",
            "Bb8",
            "a5",
            "Rxh7",
            "a4",
            "Rh6",
            "Kf7",
            "Rh7+",
            "Ke6",
            "Rh6",
            "Kf7",
            "Rh7+",
            "Ke6",
            "Rh6",
            "Kf7",
            "Rh7+",
            "Ke6",
            "Rh6",
            "Kf7",
            "Rh7+",
            "Ke6",
            "Rh6",
            "Kf7",
            "Rh7+",
            "Ke6",
            "Rh6",
            "Kf7",
            "Rh7+",
            "Ke6",
            "Rh6",
            "Kf7",
            "Rh7+",
            "Ke6",
            "Rh6",
            "Kf7",
            "Rh7+",
            "Ke6",
            "Rh6",
            "Kf7",
            "Rh7+",
            "Ke6",
            "Rh6",
            "Kf7",
            "Rh7+",
            "Ke6",
            "Rh6",
            "Kf7",
            "Rh7+",
            "Ke6",
            "Rh6",
            "Kf7",
            "Rh7+",
            "Ke6",
            "Rh6",
            "Kf7",
            "Rh7+",
            "Ke6",
            "Rh6",
            "Kf7",
            "Rh7+",
            "Ke6",
            "Rh6",
            "Kf7",
            "Rh7+",
            "Ke6",
            "Rh6",
            "Kf7",
            "Rh7+",
            "Ke6",
            "Rh6",
            "Kf7",
            "Rh7+",
            "Ke6",
            "Rh6",
            "Kf7",
            "Rh7+",
            "Ke6",
            "Rh6",
            "Kf7",
            "Rh7+",
            "Ke6",
            "Rh6",
            "Kf7",
            "Rh7+",
            "Ke6",
            "Rh6",
            "Kf7",
            "Rh7+",
            "Ke6",
            "Rh6",
            "Kf7",
            "Rh7+",
            "Ke6",
            "Rh6",
            "Kf7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bxc4",
          "positional_uci": "f1c4",
          "positional_response": "5. Bxc4 Bg4 6. O-O e6 7. Nc3 Bd6 8. h3 Bh5 9. Be2 O-O 10. a3 a6 11. b4 b5 12. Bb2 Nd5 13. Rc1 Nce7 14. Ne4 f5 15. Nc5 Bxc5 16. dxc5 c6 17. Qd4 Nf6 18. Qe5 Qd5 19. Rfd1 Qxe5 20. Bxe5 Ned5 21. Nd4 Bxe2 22. Nxe2 Nd7 23. Bd6 Rf7 24. Nd4 e5 25. Nxc6 N5f6 26. Nxe5 Nxe5 27. Bxe5 Ne4 28. c6 Rc8 29. c7 Re7 30. Rd8+ Re8 31. Rxe8+ Rxe8 32. c8=Q Rxc8 33. Rxc8+ Kf7 34. Rc7+ Ke6 35. Bxg7 Nd6 36. Rc6 Kd5 37. Rxa6 Nc4 38. Bd4 Ke4 39. a4 bxa4 40. Rxa4 Kd3 41. b5 Nd6 42. b6 Nb7 43. Ra7 Nd6 44. b7 Nxb7 45. Rxb7 Ke4 46. Rxh7 f4 47. Rh4 Kf5 48. Rxf4+ Kg5 49. h4+ Kh5 50. g3 Kg6 51. g4 Kh6 52. Rf6+ Kg7 53. h5 Kh7 54. g5 Kg7 55. h6+ Kh7 56. Rf7+ Kg6 57. Rg7+ Kh5 58. h7 Kg4 59. h8=Q Kf3 60. Qh5+ Ke4 61. Re7+ Kd3 62. Qd1+ Kc4 63. Rc7+ Kd5 64. Bf6+ Ke4 65. Qd4+ Kf5 66. Qf4+ Kg6 67. Rg7+ Kh5 68. Qh2+ Kg4 69. Qg3+ Kf5 70. Qf4+ Ke6 71. Re7+ Kd5 72. Qd4+ Kc6 73. Re6+ Kb5 74. Rb6+ Ka5 75. Qb4# Nxe5+ Kc5",
          "positional_tokens": {
            "prompt_tokens": 141,
            "completion_tokens": 597,
            "total_tokens": 738,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bg4",
            "O-O",
            "e6",
            "Nc3",
            "Bd6",
            "h3",
            "Bh5",
            "Be2",
            "O-O",
            "a3",
            "a6",
            "b4",
            "b5",
            "Bb2",
            "Nd5",
            "Rc1",
            "Nce7",
            "Ne4",
            "f5",
            "Nc5",
            "Bxc5",
            "dxc5",
            "c6",
            "Qd4",
            "Nf6",
            "Qe5",
            "Qd5",
            "Rfd1",
            "Qxe5",
            "Bxe5",
            "Ned5",
            "Nd4",
            "Bxe2",
            "Nxe2",
            "Nd7",
            "Bd6",
            "Rf7",
            "Nd4",
            "e5",
            "Nxc6",
            "N5f6",
            "Nxe5",
            "Nxe5",
            "Bxe5",
            "Ne4",
            "c6",
            "Rc8",
            "c7",
            "Re7",
            "Rd8+",
            "Re8",
            "Rxe8+",
            "Rxe8",
            "c8=Q",
            "Rxc8",
            "Rxc8+",
            "Kf7",
            "Rc7+",
            "Ke6",
            "Bxg7",
            "Nd6",
            "Rc6",
            "Kd5",
            "Rxa6",
            "Nc4",
            "Bd4",
            "Ke4",
            "a4",
            "bxa4",
            "Rxa4",
            "Kd3",
            "b5",
            "Nd6",
            "b6",
            "Nb7",
            "Ra7",
            "Nd6",
            "b7",
            "Nxb7",
            "Rxb7",
            "Ke4",
            "Rxh7",
            "f4",
            "Rh4",
            "Kf5",
            "Rxf4+",
            "Kg5",
            "h4+",
            "Kh5",
            "g3",
            "Kg6",
            "g4",
            "Kh6",
            "Rf6+",
            "Kg7",
            "h5",
            "Kh7",
            "g5",
            "Kg7",
            "h6+",
            "Kh7",
            "Rf7+",
            "Kg6",
            "Rg7+",
            "Kh5",
            "h7",
            "Kg4",
            "h8=Q",
            "Kf3",
            "Qh5+",
            "Ke4",
            "Re7+",
            "Kd3",
            "Qd1+",
            "Kc4",
            "Rc7+",
            "Kd5",
            "Bf6+",
            "Ke4",
            "Qd4+",
            "Kf5",
            "Qf4+",
            "Kg6",
            "Rg7+",
            "Kh5",
            "Qh2+",
            "Kg4",
            "Qg3+",
            "Kf5",
            "Qf4+",
            "Ke6",
            "Re7+",
            "Kd5",
            "Qd4+",
            "Kc6",
            "Re6+",
            "Kb5",
            "Rb6+",
            "Ka5",
            "Qb4#",
            "Nxe5+",
            "Kc5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bxc4",
          "neutral_uci": "f1c4",
          "neutral_response": "5. Bxc4 Bg4 6. O-O e6 7. Nc3 Bd6 8. h3 Bh5 9. Be2 O-O 10. a3 a6 11. b4 b5 12. Bb2 Nd5 13. Rc1 Nce7 14. Ne4 Bg6 15. Nc5 Bxc5 16. dxc5 c6 17. Ne5 Qc7 18. Nxg6 Nxg6 19. Bf3 Rad8 20. Qc2 Ne5 21. Be2 Ng6 22. Rfd1 Rd7 23. Rd2 Rfd8 24. Rcd1 Nde7 25. Rxd7 Rxd7 26. Rxd7 Qxd7 27. Qd3 Qxd3 28. Bxd3 Nd5 29. g3 f6 30. e4 Nde7 31. f4 Kf7 32. Kf2 Nf8 33. Ke3 Nd7 34. Bc2 g6 35. Bb3 f5 36. exf5 Nxf5+ 37. Kf3 Nf6 38. g4 Ne7 39. Be5 Nfd5 40. Ke4 h6 41. h4 Nf6+ 42. Bxf6 Kxf6 43. g5+ hxg5 44. hxg5+ Kf7 45. Ke5 Nd5 46. Bxd5 exd5 47. f5 gxf5 48. Kxf5 Kg7 49. g6 Kg8 50. Kf6 Kf8 51. g7+ Kg8 52. Kg6 d4 53. Kf6 d3 54. Ke6 d2 55. Kd6 d1=Q+ 56. Kxc6 Qd8 57. Kb7 Qd7+ 58. Kxa6 Qc6+ 59. Ka5 Kxg7 60. a4 bxa4 61. b5 Qxc5 62. Kxa4 Qb6 63. Kb4 Kf6 64. Kc4 Ke5 65. Kb4 Kd5 66. Ka4 Kc5 67. Ka3 Qxb5 68. Ka2 Kc4 69. Ka1 Kc3 70. Ka2 Qb2# e4 d5 c4 dxc4 Nf3 Nc6 e3 Nf6 Bxc4 Bg4 O-O e6 Nc3 Bd6 h3 Bh5 Be2 O-O a3 a6 b4 b5 Bb2 Nd5 Rc1 Nce7 Ne4 Bg6 Nc5 Bxc5 bxc5 c6 Ne5 Qc7 Nxg6 Nxg6 Bf3 Rad8 Qc2 Ne5",
          "neutral_tokens": {
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bg4",
            "O-O",
            "e6",
            "Nc3",
            "Bd6",
            "h3",
            "Bh5",
            "Be2",
            "O-O",
            "a3",
            "a6",
            "b4",
            "b5",
            "Bb2",
            "Nd5",
            "Rc1",
            "Nce7",
            "Ne4",
            "Bg6",
            "Nc5",
            "Bxc5",
            "bxc5",
            "c6",
            "Ne5",
            "Qc7",
            "Nxg6",
            "Nxg6",
            "Bf3",
            "Rad8",
            "Qc2",
            "Ne5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f1c4",
          "positional_uci": "f1c4",
          "neutral_uci": "f1c4",
          "consensus_move": "f1c4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bg4",
              "O-O",
              "e6",
              "Nc3",
              "Bb4",
              "a3",
              "Bxc3",
              "bxc3",
              "O-O",
              "h3",
              "Bh5",
              "Be2",
              "Ne4",
              "Qc2",
              "Bg6",
              "Bd3",
              "f5",
              "Nd2",
              "Nxd2",
              "Bxd2",
              "Qd6",
              "c4",
              "Rad8",
              "c5",
              "Qd7",
              "Bc4",
              "Kh8",
              "Qb3",
              "Bf7",
              "Qxb7",
              "Rb8",
              "Qa6",
              "e5",
              "Bxf7",
              "Rxf7",
              "Bc3",
              "exd4",
              "exd4",
              "Nxd4",
              "Rad1",
              "Qb5",
              "Qxb5",
              "Nxb5",
              "Rb1",
              "Rff8",
              "Be5",
              "Nxa3",
              "Rxb8",
              "Rxb8",
              "Bxc7",
              "Rc8",
              "Bd6",
              "Nb5",
              "Rd1",
              "Kg8",
              "Rd5",
              "g6",
              "Be5",
              "Kf7",
              "Rd7+",
              "Ke6",
              "Rb7",
              "Rxc5",
              "Bb8",
              "a5",
              "Rxh7",
              "a4",
              "Rh6",
              "Kf7",
              "Rh7+",
              "Ke6",
              "Rh6",
              "Kf7",
              "Rh7+",
              "Ke6",
              "Rh6",
              "Kf7",
              "Rh7+",
              "Ke6",
              "Rh6",
              "Kf7",
              "Rh7+",
              "Ke6",
              "Rh6",
              "Kf7",
              "Rh7+",
              "Ke6",
              "Rh6",
              "Kf7",
              "Rh7+",
              "Ke6",
              "Rh6",
              "Kf7",
              "Rh7+",
              "Ke6",
              "Rh6",
              "Kf7",
              "Rh7+",
              "Ke6",
              "Rh6",
              "Kf7",
              "Rh7+",
              "Ke6",
              "Rh6",
              "Kf7",
              "Rh7+",
              "Ke6",
              "Rh6",
              "Kf7",
              "Rh7+",
              "Ke6",
              "Rh6",
              "Kf7",
              "Rh7+",
              "Ke6",
              "Rh6",
              "Kf7",
              "Rh7+",
              "Ke6",
              "Rh6",
              "Kf7",
              "Rh7+",
              "Ke6",
              "Rh6",
              "Kf7",
              "Rh7+",
              "Ke6",
              "Rh6",
              "Kf7",
              "Rh7+",
              "Ke6",
              "Rh6",
              "Kf7",
              "Rh7+",
              "Ke6",
              "Rh6",
              "Kf7",
              "Rh7+",
              "Ke6",
              "Rh6",
              "Kf7",
              "Rh7+",
              "Ke6",
              "Rh6",
              "Kf7",
              "Rh7+",
              "Ke6",
              "Rh6",
              "Kf7",
              "Rh7+",
              "Ke6",
              "Rh6",
              "Kf7",
              "Rh7+",
              "Ke6",
              "Rh6",
              "Kf7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bg4",
              "O-O",
              "e6",
              "Nc3",
              "Bd6",
              "h3",
              "Bh5",
              "Be2",
              "O-O",
              "a3",
              "a6",
              "b4",
              "b5",
              "Bb2",
              "Nd5",
              "Rc1",
              "Nce7",
              "Ne4",
              "f5",
              "Nc5",
              "Bxc5",
              "dxc5",
              "c6",
              "Qd4",
              "Nf6",
              "Qe5",
              "Qd5",
              "Rfd1",
              "Qxe5",
              "Bxe5",
              "Ned5",
              "Nd4",
              "Bxe2",
              "Nxe2",
              "Nd7",
              "Bd6",
              "Rf7",
              "Nd4",
              "e5",
              "Nxc6",
              "N5f6",
              "Nxe5",
              "Nxe5",
              "Bxe5",
              "Ne4",
              "c6",
              "Rc8",
              "c7",
              "Re7",
              "Rd8+",
              "Re8",
              "Rxe8+",
              "Rxe8",
              "c8=Q",
              "Rxc8",
              "Rxc8+",
              "Kf7",
              "Rc7+",
              "Ke6",
              "Bxg7",
              "Nd6",
              "Rc6",
              "Kd5",
              "Rxa6",
              "Nc4",
              "Bd4",
              "Ke4",
              "a4",
              "bxa4",
              "Rxa4",
              "Kd3",
              "b5",
              "Nd6",
              "b6",
              "Nb7",
              "Ra7",
              "Nd6",
              "b7",
              "Nxb7",
              "Rxb7",
              "Ke4",
              "Rxh7",
              "f4",
              "Rh4",
              "Kf5",
              "Rxf4+",
              "Kg5",
              "h4+",
              "Kh5",
              "g3",
              "Kg6",
              "g4",
              "Kh6",
              "Rf6+",
              "Kg7",
              "h5",
              "Kh7",
              "g5",
              "Kg7",
              "h6+",
              "Kh7",
              "Rf7+",
              "Kg6",
              "Rg7+",
              "Kh5",
              "h7",
              "Kg4",
              "h8=Q",
              "Kf3",
              "Qh5+",
              "Ke4",
              "Re7+",
              "Kd3",
              "Qd1+",
              "Kc4",
              "Rc7+",
              "Kd5",
              "Bf6+",
              "Ke4",
              "Qd4+",
              "Kf5",
              "Qf4+",
              "Kg6",
              "Rg7+",
              "Kh5",
              "Qh2+",
              "Kg4",
              "Qg3+",
              "Kf5",
              "Qf4+",
              "Ke6",
              "Re7+",
              "Kd5",
              "Qd4+",
              "Kc6",
              "Re6+",
              "Kb5",
              "Rb6+",
              "Ka5",
              "Qb4#",
              "Nxe5+",
              "Kc5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bg4",
              "O-O",
              "e6",
              "Nc3",
              "Bd6",
              "h3",
              "Bh5",
              "Be2",
              "O-O",
              "a3",
              "a6",
              "b4",
              "b5",
              "Bb2",
              "Nd5",
              "Rc1",
              "Nce7",
              "Ne4",
              "Bg6",
              "Nc5",
              "Bxc5",
              "bxc5",
              "c6",
              "Ne5",
              "Qc7",
              "Nxg6",
              "Nxg6",
              "Bf3",
              "Rad8",
              "Qc2",
              "Ne5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 141,
            "completion_tokens": 640,
            "total_tokens": 781,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 141,
            "completion_tokens": 597,
            "total_tokens": 738,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 422,
          "total_completion_tokens": 1877,
          "total_tokens": 2299
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 141,
            "completion_tokens": 640,
            "total_tokens": 781,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 141,
            "completion_tokens": 597,
            "total_tokens": 738,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp1pppp/2n2n2/8/2pP4/4PN2/PP3PPP/RNBQKB1R w KQkq - 1 5"
    },
    {
      "turn": 11,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "O-O",
          "aggressive_uci": "e1g1",
          "aggressive_response": "6. O-O Bd6 7. Nc3 O-O 8. e4 e5 9. d5 Ne7 10. Bg5 Ng6 11. Rc1 h6 12. Be3 a6 13. a4 Bg4 14. h3 Bd7 15. Qb3 b6 16. Rfd1 Nh5 17. Ne2 Qf6 18. Nh2 Nhf4 19. Nxf4 Nxf4 20. Bxf4 Qxf4 21. Qf3 Qg5 22. b3 f5 23. exf5 Bxf5 24. Qe3 Qg6 25. Bd3 e4 26. Be2 Rae8 27. Bg4 Bxg4 28. Nxg4 Bf4 29. Qc3 Bxc1 30. Rxc1 Re7 31. Ne5 Qf6 32. Nc6 Qxf2+ 33. Kh1 Ref7 34. Ne5 Rf5 35. Ng6 R8f7 36. Ne5 Re7 37. Ng6 Rd7 38. Ne5 Rxd5 39. Ng4 Qf4 40. Qc4 Kh7 41. Re1 h5 42. Ne3 Rc5 43. Qe2 Rg5 44. Rf1 Qg3 45. Rf7 Rc1+ 46. Nf1 Re1 47. Qc2 Kg8 48. Rf2 e3 49. Qc4+ Kh7 50. Qd3+ Kh6 51. Rf6+ gxf6 52. Qh7+ Kxh7 53. h4 Qxg2# 0-1\n54. Kh2 Qxg2#",
          "aggressive_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 408,
            "total_tokens": 557,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e4",
            "e5",
            "d5",
            "Ne7",
            "Bg5",
            "Ng6",
            "Rc1",
            "h6",
            "Be3",
            "a6",
            "a4",
            "Bg4",
            "h3",
            "Bd7",
            "Qb3",
            "b6",
            "Rfd1",
            "Nh5",
            "Ne2",
            "Qf6",
            "Nh2",
            "Nhf4",
            "Nxf4",
            "Nxf4",
            "Bxf4",
            "Qxf4",
            "Qf3",
            "Qg5",
            "b3",
            "f5",
            "exf5",
            "Bxf5",
            "Qe3",
            "Qg6",
            "Bd3",
            "e4",
            "Be2",
            "Rae8",
            "Bg4",
            "Bxg4",
            "Nxg4",
            "Bf4",
            "Qc3",
            "Bxc1",
            "Rxc1",
            "Re7",
            "Ne5",
            "Qf6",
            "Nc6",
            "Qxf2+",
            "Kh1",
            "Ref7",
            "Ne5",
            "Rf5",
            "Ng6",
            "R8f7",
            "Ne5",
            "Re7",
            "Ng6",
            "Rd7",
            "Ne5",
            "Rxd5",
            "Ng4",
            "Qf4",
            "Qc4",
            "Kh7",
            "Re1",
            "h5",
            "Ne3",
            "Rc5",
            "Qe2",
            "Rg5",
            "Rf1",
            "Qg3",
            "Rf7",
            "Rc1+",
            "Nf1",
            "Re1",
            "Qc2",
            "Kg8",
            "Rf2",
            "e3",
            "Qc4+",
            "Kh7",
            "Qd3+",
            "Kh6",
            "Rf6+",
            "gxf6",
            "Qh7+",
            "Kxh7",
            "h4",
            "Qxg2#",
            "Kh2",
            "Qxg2#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "O-O",
          "positional_uci": "e1g1",
          "positional_response": "6. O-O Bd6 7. Nc3 O-O 8. e4 e5 9. d5 Ne7 10. Bg5 Ng6 11. Rc1 h6 12. Be3 a6 13. h3 b5 14. Bd3 Bd7 15. a3 Qc8 16. Kh2 Nh5 17. Ne2 f5 18. exf5 Bxf5 19. Bxf5 Qxf5 20. Ng3 Nxg3 21. fxg3 e4 22. Nd2 Qe5 23. Qg4 Ne7 24. Bxh6 Nf5 25. Bf4 Qxd5 26. Bxd6 Nxd6 27. Rxc7 Qe5 28. Rxf8+ Rxf8 29. Rc6 e3 30. Nf3 Qd5 31. Rc7 Nf5 32. Nh4 Qe5 33. Nxf5 Rxf5 34. Rc8+ Kf7 35. Qe2 Rf2 36. Qd3 Rd2 37. Qb3+ Kg6 38. Rc6+ Kh7 39. Re6 Qxb2 40. Qxe3 Rxg2+ 41. Kh1 Rh2+ 42. Kg1 Qg2# 0-1\ne4 d5 Nc3 dxc4 Nf3 Nc6 e5 Nf6 Bxc4 e6 O-O Bd6 Nc3 O-O e4 e5 d5 Ne7 Bg5 Ng6 Rc1 h6 Be3 a6 h3 b5 Bd3 Bd7 a3 Qc8 Kh2 Nh5 Ne2 f5 exf5 Bxf5 Bxf5 Qxf5 Ng3 Nxg3 fxg3 e4 Nd2 Qe5 Qg4 Ne7 Bxh6 Nf5 Bf4 Qxd5 Rxc7 e3 Nf3 Qd3 Rfc1 e2 Ne1 Qd5 Qxe2 Rae8 Qg4 Re4 Rc8 Rxc8 Rxc8 Qe2 Nd4 Qd3 Rd8 Nf3 Nxf3+ Qxf3 Qxf3 gxf3 Rd3 Kg2 Rb3 Be5 Kf7 f4 g6 h4 Ke6 Kh3 Kf5 Kg2 Kg4 Kf2 Rxg3 f5 Rf3+ Ke2 Rxf5 Bc3 Kxh4 Ke3 g5 Ke4 Rf4+ Ke3 Rf7 Be1+ Kh3 Ke4 g4 Ke5 g3 Ke6 Rf1 Ke7 g2 Bf2 Rxf2 Kd5 g1=Q Kc6 Qg6+ Kd5 Qd6+ Ke4 Qe6+ Kd3 Qd6+ Kc2 Qc5+ Kb1 Qc2+ Ka1",
          "positional_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "e4",
            "e5",
            "d5",
            "Ne7",
            "Bg5",
            "Ng6",
            "Rc1",
            "h6",
            "Be3",
            "a6",
            "h3",
            "b5",
            "Bd3",
            "Bd7",
            "a3",
            "Qc8",
            "Kh2",
            "Nh5",
            "Ne2",
            "f5",
            "exf5",
            "Bxf5",
            "Bxf5",
            "Qxf5",
            "Ng3",
            "Nxg3",
            "fxg3",
            "e4",
            "Nd2",
            "Qe5",
            "Qg4",
            "Ne7",
            "Bxh6",
            "Nf5",
            "Bf4",
            "Qxd5",
            "Rxc7",
            "e3",
            "Nf3",
            "Qd3",
            "Rfc1",
            "e2",
            "Ne1",
            "Qd5",
            "Qxe2",
            "Rae8",
            "Qg4",
            "Re4",
            "Rc8",
            "Rxc8",
            "Rxc8",
            "Qe2",
            "Nd4",
            "Qd3",
            "Rd8",
            "Nf3",
            "Nxf3+",
            "Qxf3",
            "Qxf3",
            "gxf3",
            "Rd3",
            "Kg2",
            "Rb3",
            "Be5",
            "Kf7",
            "f4",
            "g6",
            "h4",
            "Ke6",
            "Kh3",
            "Kf5",
            "Kg2",
            "Kg4",
            "Kf2",
            "Rxg3",
            "f5",
            "Rf3+",
            "Ke2",
            "Rxf5",
            "Bc3",
            "Kxh4",
            "Ke3",
            "g5",
            "Ke4",
            "Rf4+",
            "Ke3",
            "Rf7",
            "Be1+",
            "Kh3",
            "Ke4",
            "g4",
            "Ke5",
            "g3",
            "Ke6",
            "Rf1",
            "Ke7",
            "g2",
            "Bf2",
            "Rxf2",
            "Kd5",
            "g1=Q",
            "Kc6",
            "Qg6+",
            "Kd5",
            "Qd6+",
            "Ke4",
            "Qe6+",
            "Kd3",
            "Qd6+",
            "Kc2",
            "Qc5+",
            "Kb1",
            "Qc2+",
            "Ka1"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "O-O",
          "neutral_uci": "e1g1",
          "neutral_response": "6. O-O Bd6 7. Nc3 O-O 8. e4 e5 9. d5 Ne7 10. Bg5 Ng6 11. Rc1 h6 12. Be3 a6 13. a4 Bg4 14. h3 Bd7 15. Qb3 b6 16. Rfd1 Nh5 17. Ne2 Qf6 18. Bd2 Nhf4 19. Nxf4 Nxf4 20. Bxf4 Qxf4 21. Qd3 Bxa4 22. b3 Bd7 23. Bxa6 f5 24. Nd2 fxe4 25. Nxe4 Bf5 26. f3 Bxe4 27. Qxe4 Rxa6 28. Qxf4 Rxf4 29. Ra1 Rxa1 30. Rxa1 Rd4 31. Ra8+ Kf7 32. Kf2 Rxd5 33. Ke3 Rb5 34. Ke4 Rxb3 35. Kf5 Rb2 36. g4 Rb4 37. g5 Rf4# 0-1\n38. g6+ Ke7 39. Rg8 Rxf3+ 40. Ke4 Rxh3 41. Rxg7+ Kf6 42. Rg8 Rg3 43. Kd5 Rxg6 44. Rh8 Kg5 45. Ke4 h5 46. Kf3 h4 47. Kg2 Kg4 48. Kh1 h3 49. Ra8 e4 50. Ra4 Kf3 51. Ra3+ e3 52. Ra2 e2 53. Ra3+ Kf2 54. Rf3+ Kxf3#",
          "neutral_tokens": {
            "prompt_tokens": 148,
            "completion_tokens": 403,
            "total_tokens": 551,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e4",
            "e5",
            "d5",
            "Ne7",
            "Bg5",
            "Ng6",
            "Rc1",
            "h6",
            "Be3",
            "a6",
            "a4",
            "Bg4",
            "h3",
            "Bd7",
            "Qb3",
            "b6",
            "Rfd1",
            "Nh5",
            "Ne2",
            "Qf6",
            "Bd2",
            "Nhf4",
            "Nxf4",
            "Nxf4",
            "Bxf4",
            "Qxf4",
            "Qd3",
            "Bxa4",
            "b3",
            "Bd7",
            "Bxa6",
            "f5",
            "Nd2",
            "fxe4",
            "Nxe4",
            "Bf5",
            "f3",
            "Bxe4",
            "Qxe4",
            "Rxa6",
            "Qxf4",
            "Rxf4",
            "Ra1",
            "Rxa1",
            "Rxa1",
            "Rd4",
            "Ra8+",
            "Kf7",
            "Kf2",
            "Rxd5",
            "Ke3",
            "Rb5",
            "Ke4",
            "Rxb3",
            "Kf5",
            "Rb2",
            "g4",
            "Rb4",
            "g5",
            "Rf4#",
            "g6+",
            "Ke7",
            "Rg8",
            "Rxf3+",
            "Ke4",
            "Rxh3",
            "Rxg7+",
            "Kf6",
            "Rg8",
            "Rg3",
            "Kd5",
            "Rxg6",
            "Rh8",
            "Kg5",
            "Ke4",
            "h5",
            "Kf3",
            "h4",
            "Kg2",
            "Kg4",
            "Kh1",
            "h3",
            "Ra8",
            "e4",
            "Ra4",
            "Kf3",
            "Ra3+",
            "e3",
            "Ra2",
            "e2",
            "Ra3+",
            "Kf2",
            "Rf3+",
            "Kxf3#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e1g1",
          "positional_uci": "e1g1",
          "neutral_uci": "e1g1",
          "consensus_move": "e1g1",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "e4",
              "e5",
              "d5",
              "Ne7",
              "Bg5",
              "Ng6",
              "Rc1",
              "h6",
              "Be3",
              "a6",
              "a4",
              "Bg4",
              "h3",
              "Bd7",
              "Qb3",
              "b6",
              "Rfd1",
              "Nh5",
              "Ne2",
              "Qf6",
              "Nh2",
              "Nhf4",
              "Nxf4",
              "Nxf4",
              "Bxf4",
              "Qxf4",
              "Qf3",
              "Qg5",
              "b3",
              "f5",
              "exf5",
              "Bxf5",
              "Qe3",
              "Qg6",
              "Bd3",
              "e4",
              "Be2",
              "Rae8",
              "Bg4",
              "Bxg4",
              "Nxg4",
              "Bf4",
              "Qc3",
              "Bxc1",
              "Rxc1",
              "Re7",
              "Ne5",
              "Qf6",
              "Nc6",
              "Qxf2+",
              "Kh1",
              "Ref7",
              "Ne5",
              "Rf5",
              "Ng6",
              "R8f7",
              "Ne5",
              "Re7",
              "Ng6",
              "Rd7",
              "Ne5",
              "Rxd5",
              "Ng4",
              "Qf4",
              "Qc4",
              "Kh7",
              "Re1",
              "h5",
              "Ne3",
              "Rc5",
              "Qe2",
              "Rg5",
              "Rf1",
              "Qg3",
              "Rf7",
              "Rc1+",
              "Nf1",
              "Re1",
              "Qc2",
              "Kg8",
              "Rf2",
              "e3",
              "Qc4+",
              "Kh7",
              "Qd3+",
              "Kh6",
              "Rf6+",
              "gxf6",
              "Qh7+",
              "Kxh7",
              "h4",
              "Qxg2#",
              "Kh2",
              "Qxg2#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e4",
              "e5",
              "d5",
              "Ne7",
              "Bg5",
              "Ng6",
              "Rc1",
              "h6",
              "Be3",
              "a6",
              "h3",
              "b5",
              "Bd3",
              "Bd7",
              "a3",
              "Qc8",
              "Kh2",
              "Nh5",
              "Ne2",
              "f5",
              "exf5",
              "Bxf5",
              "Bxf5",
              "Qxf5",
              "Ng3",
              "Nxg3",
              "fxg3",
              "e4",
              "Nd2",
              "Qe5",
              "Qg4",
              "Ne7",
              "Bxh6",
              "Nf5",
              "Bf4",
              "Qxd5",
              "Rxc7",
              "e3",
              "Nf3",
              "Qd3",
              "Rfc1",
              "e2",
              "Ne1",
              "Qd5",
              "Qxe2",
              "Rae8",
              "Qg4",
              "Re4",
              "Rc8",
              "Rxc8",
              "Rxc8",
              "Qe2",
              "Nd4",
              "Qd3",
              "Rd8",
              "Nf3",
              "Nxf3+",
              "Qxf3",
              "Qxf3",
              "gxf3",
              "Rd3",
              "Kg2",
              "Rb3",
              "Be5",
              "Kf7",
              "f4",
              "g6",
              "h4",
              "Ke6",
              "Kh3",
              "Kf5",
              "Kg2",
              "Kg4",
              "Kf2",
              "Rxg3",
              "f5",
              "Rf3+",
              "Ke2",
              "Rxf5",
              "Bc3",
              "Kxh4",
              "Ke3",
              "g5",
              "Ke4",
              "Rf4+",
              "Ke3",
              "Rf7",
              "Be1+",
              "Kh3",
              "Ke4",
              "g4",
              "Ke5",
              "g3",
              "Ke6",
              "Rf1",
              "Ke7",
              "g2",
              "Bf2",
              "Rxf2",
              "Kd5",
              "g1=Q",
              "Kc6",
              "Qg6+",
              "Kd5",
              "Qd6+",
              "Ke4",
              "Qe6+",
              "Kd3",
              "Qd6+",
              "Kc2",
              "Qc5+",
              "Kb1",
              "Qc2+",
              "Ka1"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e4",
              "e5",
              "d5",
              "Ne7",
              "Bg5",
              "Ng6",
              "Rc1",
              "h6",
              "Be3",
              "a6",
              "a4",
              "Bg4",
              "h3",
              "Bd7",
              "Qb3",
              "b6",
              "Rfd1",
              "Nh5",
              "Ne2",
              "Qf6",
              "Bd2",
              "Nhf4",
              "Nxf4",
              "Nxf4",
              "Bxf4",
              "Qxf4",
              "Qd3",
              "Bxa4",
              "b3",
              "Bd7",
              "Bxa6",
              "f5",
              "Nd2",
              "fxe4",
              "Nxe4",
              "Bf5",
              "f3",
              "Bxe4",
              "Qxe4",
              "Rxa6",
              "Qxf4",
              "Rxf4",
              "Ra1",
              "Rxa1",
              "Rxa1",
              "Rd4",
              "Ra8+",
              "Kf7",
              "Kf2",
              "Rxd5",
              "Ke3",
              "Rb5",
              "Ke4",
              "Rxb3",
              "Kf5",
              "Rb2",
              "g4",
              "Rb4",
              "g5",
              "Rf4#",
              "g6+",
              "Ke7",
              "Rg8",
              "Rxf3+",
              "Ke4",
              "Rxh3",
              "Rxg7+",
              "Kf6",
              "Rg8",
              "Rg3",
              "Kd5",
              "Rxg6",
              "Rh8",
              "Kg5",
              "Ke4",
              "h5",
              "Kf3",
              "h4",
              "Kg2",
              "Kg4",
              "Kh1",
              "h3",
              "Ra8",
              "e4",
              "Ra4",
              "Kf3",
              "Ra3+",
              "e3",
              "Ra2",
              "e2",
              "Ra3+",
              "Kf2",
              "Rf3+",
              "Kxf3#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 149,
            "completion_tokens": 408,
            "total_tokens": 557,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 148,
            "completion_tokens": 403,
            "total_tokens": 551,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 446,
          "total_completion_tokens": 1451,
          "total_tokens": 1897
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 149,
            "completion_tokens": 408,
            "total_tokens": 557,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 148,
            "completion_tokens": 403,
            "total_tokens": 551,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n1pn2/8/2BP4/4PN2/PP3PPP/RNBQK2R w KQkq - 0 6"
    },
    {
      "turn": 13,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc3",
          "aggressive_uci": "b1c3",
          "aggressive_response": "7. Nc3 O-O 8. e4 a6 9. a4 Na5 10. Ba2 c5 11. d5 exd5 12. exd5 Bg4 13. h3 Bh5 14. Bf4 Bd6 15. Bxd6 Qxd6 16. Re1 Rfe8 17. Qd3 Bg6 18. Qd2 b6 19. Rxe8+ Rxe8 20. Re1 Rxe1+ 21. Qxe1 h6 22. Qe5 Qxe5 23. Nxe5 Bc2 24. d6 Nb7 25. d7 Kf8 26. Bxf7 Ke7 27. Nd5+ Nxd5 28. Bxd5 Nd8 29. b3 Kd6 30. Nf7+ Kxd7 31. Nxd8 Kxd8 32. Kf1 Kd7 33. Ke2 Kd6 34. Bc4 a5 35. Kd2 Be4 36. g3 Ke5 37. Ke3 Bg2 38. h4 g5 39. hxg5 hxg5 40. f4+ gxf4+ 41. gxf4+ Kf5 42. Bd3+ Kf6 43. Be4 Bf1 44. Bd3 Bg2 45. Be4 Bf1 46. Bd3 Bg2 47. Be4 Bf1 48. Bd3 Bg2 49. Be4 Bf1 50. Bd3 Bg2 51. Be4 Bf1 52. Bd3 Bg2 53. Be4 Bf1 54. Bd3 Bg2 55. Be4 Bf1 56. Bd3 Bg2 57. Be4 Bf1 58. Bd3 Bg2 59. Be4 Bf1 60. Bd3 Bg2 61. Be4 Bf1 62. Bd3 Bg2 63. Be4 Bf1 64. Bd3 Bg2 65. Be4 Bf1 66. Bd3 Bg2 67. Be4 Bf1 68. Bd3 Bg2 69. Be4 Bf1 70. Bd3 Bg2 71. Be4 Bf1 72. Bd3 Bg2 73. Be4 Bf1 74. Bd3 Bg2 75. Be4 Bf1 76. Bd3 Bg2 77. Be4 Bf1 78. Bd3 Bg2 79. Be4 Bf1 80. Bd3 Bg2 81. Be4 Bf1 82. Bd3 Bg2 83. Be4 Bf1 84. Bd3 Bg2 85. Be",
          "aggressive_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "O-O",
            "e4",
            "a6",
            "a4",
            "Na5",
            "Ba2",
            "c5",
            "d5",
            "exd5",
            "exd5",
            "Bg4",
            "h3",
            "Bh5",
            "Bf4",
            "Bd6",
            "Bxd6",
            "Qxd6",
            "Re1",
            "Rfe8",
            "Qd3",
            "Bg6",
            "Qd2",
            "b6",
            "Rxe8+",
            "Rxe8",
            "Re1",
            "Rxe1+",
            "Qxe1",
            "h6",
            "Qe5",
            "Qxe5",
            "Nxe5",
            "Bc2",
            "d6",
            "Nb7",
            "d7",
            "Kf8",
            "Bxf7",
            "Ke7",
            "Nd5+",
            "Nxd5",
            "Bxd5",
            "Nd8",
            "b3",
            "Kd6",
            "Nf7+",
            "Kxd7",
            "Nxd8",
            "Kxd8",
            "Kf1",
            "Kd7",
            "Ke2",
            "Kd6",
            "Bc4",
            "a5",
            "Kd2",
            "Be4",
            "g3",
            "Ke5",
            "Ke3",
            "Bg2",
            "h4",
            "g5",
            "hxg5",
            "hxg5",
            "f4+",
            "gxf4+",
            "gxf4+",
            "Kf5",
            "Bd3+",
            "Kf6",
            "Be4",
            "Bf1",
            "Bd3",
            "Bg2",
            "Be4",
            "Bf1",
            "Bd3",
            "Bg2",
            "Be4",
            "Bf1",
            "Bd3",
            "Bg2",
            "Be4",
            "Bf1",
            "Bd3",
            "Bg2",
            "Be4",
            "Bf1",
            "Bd3",
            "Bg2",
            "Be4",
            "Bf1",
            "Bd3",
            "Bg2",
            "Be4",
            "Bf1",
            "Bd3",
            "Bg2",
            "Be4",
            "Bf1",
            "Bd3",
            "Bg2",
            "Be4",
            "Bf1",
            "Bd3",
            "Bg2",
            "Be4",
            "Bf1",
            "Bd3",
            "Bg2",
            "Be4",
            "Bf1",
            "Bd3",
            "Bg2",
            "Be4",
            "Bf1",
            "Bd3",
            "Bg2",
            "Be4",
            "Bf1",
            "Bd3",
            "Bg2",
            "Be4",
            "Bf1",
            "Bd3",
            "Bg2",
            "Be4",
            "Bf1",
            "Bd3",
            "Bg2",
            "Be4",
            "Bf1",
            "Bd3",
            "Bg2",
            "Be4",
            "Bf1",
            "Bd3",
            "Bg2",
            "Be4",
            "Bf1",
            "Bd3",
            "Bg2",
            "Be4",
            "Bf1",
            "Bd3",
            "Bg2",
            "Be4",
            "Bf1",
            "Bd3",
            "Bg2",
            "Be4",
            "Bf1",
            "Bd3",
            "Bg2"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nc3",
          "positional_uci": "b1c3",
          "positional_response": "7. Nc3 O-O 8. e4 a6 9. a4 Na5 10. Ba2 c5 11. d5 exd5 12. exd5 Bg4 13. h3 Bh5 14. Re1 Bd6 15. Bg5 h6 16. Bh4 g5 17. Bg3 Bxg3 18. fxg3 Qd6 19. g4 Bg6 20. Ne5 Rae8 21. Nxg6 fxg6 22. Qd3 Kg7 23. Re6 Rxe6 24. Rd1 Re5 25. Bb1 Nd7 26. Ne4 Qb6 27. Kh1 c4 28. Qc3 Qb3 29. Qd4 Rf4 30. Re1 Qb6 31. Qc3 Nb3 32. g3 Rf8 33. Kg2 Qd4 34. Qb4 Qxb2+ 35. Kh1 Rxe4 36. Bxe4 Rf2 37. Qe7+ Rf7 38. Qe6 Qf6 39. Qe8 Nbc5 40. Bg2 Nd3 41. Re6 Qa1+ 42. Kh2 N3e5 43. d6 Qb2 44. Kh1 Qc1+ 45. Kh2 Qd2 46. Kh1 Qe1+ 47. Kh2 Nf3+ 48. Bxf3 Qxe8 49. Rxe8 Rxf3 50. Re7+ Rf7 51. Re4 b5 52. axb5 axb5 53. Kg2 c3 54. Re3 b4 55. Re4 c2 56. Rc4 b3 57. Rc3 b2 58. Rxc2 b1=Q 59. Rc7 Qf1+ 60. Kh2 Rf2# 0-1 d4 d5 c4 dxc4 Nf3 Nc6 e3 Nf6 Bxc4 e6 O-O Be7 Nc3 O-O e4 a6 a4 Na5 Ba2 c5 d5 exd5 exd5 Bg4 h3 Bh5 Re1 Bd6 Bg5 h6 Bh4 g5 Bg3 Bxg3 fxg3 Qd6 g4 Bg6 Ne5 Rae8 Nxg6 fxg6 Qd3 Kg7 Re6 Rxe6 Rd1 Re5 Bb1 Nd7 Ne4 Qb6 Kh1 c4 Qc3 Qb3 Qd4 Rf4 30. Re1 Qb6 31. Qc3 Nb3 32. g3 Rf8 33. Kg2 Qd4 34. Qb4 Qxb2+ 35. Kh1 Rxe4",
          "positional_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "O-O",
            "e4",
            "a6",
            "a4",
            "Na5",
            "Ba2",
            "c5",
            "d5",
            "exd5",
            "exd5",
            "Bg4",
            "h3",
            "Bh5",
            "Re1",
            "Bd6",
            "Bg5",
            "h6",
            "Bh4",
            "g5",
            "Bg3",
            "Bxg3",
            "fxg3",
            "Qd6",
            "g4",
            "Bg6",
            "Ne5",
            "Rae8",
            "Nxg6",
            "fxg6",
            "Qd3",
            "Kg7",
            "Re6",
            "Rxe6",
            "Rd1",
            "Re5",
            "Bb1",
            "Nd7",
            "Ne4",
            "Qb6",
            "Kh1",
            "c4",
            "Qc3",
            "Qb3",
            "Qd4",
            "Rf4",
            "Re1",
            "Qb6",
            "Qc3",
            "Nb3",
            "g3",
            "Rf8",
            "Kg2",
            "Qd4",
            "Qb4",
            "Qxb2+",
            "Kh1",
            "Rxe4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc3",
          "neutral_uci": "b1c3",
          "neutral_response": "7. Nc3 O-O 8. e4 a6 9. a4 Na5 10. Ba2 c5 11. d5 exd5 12. exd5 Bg4 13. h3 Bh5 14. Bf4 Bd6 15. Bxd6 Qxd6 16. Qd3 Bg6 17. Qd2 b6 18. Rfe1 Rfe8 19. Rxe8+ Rxe8 20. Re1 Rxe1+ 21. Qxe1 h6 22. Qe2 Bh5 23. g4 Bg6 24. Qxa6 Qf4 25. Qe2 c4 26. Qe3 Qd6 27. Ne5 Nxd5 28. Nxd5 Qxd5 29. Nxg6 fxg6 30. Qxb6 Kh7 31. Qb5 Qd1+ 32. Kg2 Nb3 33. Qxc4 Nd2 34. Qg8# 1-0\n\nNf6 Bxf6 gxf6",
          "neutral_tokens": {
            "prompt_tokens": 155,
            "completion_tokens": 245,
            "total_tokens": 400,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "O-O",
            "e4",
            "a6",
            "a4",
            "Na5",
            "Ba2",
            "c5",
            "d5",
            "exd5",
            "exd5",
            "Bg4",
            "h3",
            "Bh5",
            "Bf4",
            "Bd6",
            "Bxd6",
            "Qxd6",
            "Qd3",
            "Bg6",
            "Qd2",
            "b6",
            "Rfe1",
            "Rfe8",
            "Rxe8+",
            "Rxe8",
            "Re1",
            "Rxe1+",
            "Qxe1",
            "h6",
            "Qe2",
            "Bh5",
            "g4",
            "Bg6",
            "Qxa6",
            "Qf4",
            "Qe2",
            "c4",
            "Qe3",
            "Qd6",
            "Ne5",
            "Nxd5",
            "Nxd5",
            "Qxd5",
            "Nxg6",
            "fxg6",
            "Qxb6",
            "Kh7",
            "Qb5",
            "Qd1+",
            "Kg2",
            "Nb3",
            "Qxc4",
            "Nd2",
            "Qg8#",
            "Nf6",
            "Bxf6",
            "gxf6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b1c3",
          "positional_uci": "b1c3",
          "neutral_uci": "b1c3",
          "consensus_move": "b1c3",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "O-O",
              "e4",
              "a6",
              "a4",
              "Na5",
              "Ba2",
              "c5",
              "d5",
              "exd5",
              "exd5",
              "Bg4",
              "h3",
              "Bh5",
              "Bf4",
              "Bd6",
              "Bxd6",
              "Qxd6",
              "Re1",
              "Rfe8",
              "Qd3",
              "Bg6",
              "Qd2",
              "b6",
              "Rxe8+",
              "Rxe8",
              "Re1",
              "Rxe1+",
              "Qxe1",
              "h6",
              "Qe5",
              "Qxe5",
              "Nxe5",
              "Bc2",
              "d6",
              "Nb7",
              "d7",
              "Kf8",
              "Bxf7",
              "Ke7",
              "Nd5+",
              "Nxd5",
              "Bxd5",
              "Nd8",
              "b3",
              "Kd6",
              "Nf7+",
              "Kxd7",
              "Nxd8",
              "Kxd8",
              "Kf1",
              "Kd7",
              "Ke2",
              "Kd6",
              "Bc4",
              "a5",
              "Kd2",
              "Be4",
              "g3",
              "Ke5",
              "Ke3",
              "Bg2",
              "h4",
              "g5",
              "hxg5",
              "hxg5",
              "f4+",
              "gxf4+",
              "gxf4+",
              "Kf5",
              "Bd3+",
              "Kf6",
              "Be4",
              "Bf1",
              "Bd3",
              "Bg2",
              "Be4",
              "Bf1",
              "Bd3",
              "Bg2",
              "Be4",
              "Bf1",
              "Bd3",
              "Bg2",
              "Be4",
              "Bf1",
              "Bd3",
              "Bg2",
              "Be4",
              "Bf1",
              "Bd3",
              "Bg2",
              "Be4",
              "Bf1",
              "Bd3",
              "Bg2",
              "Be4",
              "Bf1",
              "Bd3",
              "Bg2",
              "Be4",
              "Bf1",
              "Bd3",
              "Bg2",
              "Be4",
              "Bf1",
              "Bd3",
              "Bg2",
              "Be4",
              "Bf1",
              "Bd3",
              "Bg2",
              "Be4",
              "Bf1",
              "Bd3",
              "Bg2",
              "Be4",
              "Bf1",
              "Bd3",
              "Bg2",
              "Be4",
              "Bf1",
              "Bd3",
              "Bg2",
              "Be4",
              "Bf1",
              "Bd3",
              "Bg2",
              "Be4",
              "Bf1",
              "Bd3",
              "Bg2",
              "Be4",
              "Bf1",
              "Bd3",
              "Bg2",
              "Be4",
              "Bf1",
              "Bd3",
              "Bg2",
              "Be4",
              "Bf1",
              "Bd3",
              "Bg2",
              "Be4",
              "Bf1",
              "Bd3",
              "Bg2",
              "Be4",
              "Bf1",
              "Bd3",
              "Bg2",
              "Be4",
              "Bf1",
              "Bd3",
              "Bg2"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "O-O",
              "e4",
              "a6",
              "a4",
              "Na5",
              "Ba2",
              "c5",
              "d5",
              "exd5",
              "exd5",
              "Bg4",
              "h3",
              "Bh5",
              "Re1",
              "Bd6",
              "Bg5",
              "h6",
              "Bh4",
              "g5",
              "Bg3",
              "Bxg3",
              "fxg3",
              "Qd6",
              "g4",
              "Bg6",
              "Ne5",
              "Rae8",
              "Nxg6",
              "fxg6",
              "Qd3",
              "Kg7",
              "Re6",
              "Rxe6",
              "Rd1",
              "Re5",
              "Bb1",
              "Nd7",
              "Ne4",
              "Qb6",
              "Kh1",
              "c4",
              "Qc3",
              "Qb3",
              "Qd4",
              "Rf4",
              "Re1",
              "Qb6",
              "Qc3",
              "Nb3",
              "g3",
              "Rf8",
              "Kg2",
              "Qd4",
              "Qb4",
              "Qxb2+",
              "Kh1",
              "Rxe4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "O-O",
              "e4",
              "a6",
              "a4",
              "Na5",
              "Ba2",
              "c5",
              "d5",
              "exd5",
              "exd5",
              "Bg4",
              "h3",
              "Bh5",
              "Bf4",
              "Bd6",
              "Bxd6",
              "Qxd6",
              "Qd3",
              "Bg6",
              "Qd2",
              "b6",
              "Rfe1",
              "Rfe8",
              "Rxe8+",
              "Rxe8",
              "Re1",
              "Rxe1+",
              "Qxe1",
              "h6",
              "Qe2",
              "Bh5",
              "g4",
              "Bg6",
              "Qxa6",
              "Qf4",
              "Qe2",
              "c4",
              "Qe3",
              "Qd6",
              "Ne5",
              "Nxd5",
              "Nxd5",
              "Qxd5",
              "Nxg6",
              "fxg6",
              "Qxb6",
              "Kh7",
              "Qb5",
              "Qd1+",
              "Kg2",
              "Nb3",
              "Qxc4",
              "Nd2",
              "Qg8#",
              "Nf6",
              "Bxf6",
              "gxf6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 155,
            "completion_tokens": 245,
            "total_tokens": 400,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 467,
          "total_completion_tokens": 1525,
          "total_tokens": 1992
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 155,
            "completion_tokens": 245,
            "total_tokens": 400,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqk2r/ppp1bppp/2n1pn2/8/2BP4/4PN2/PP3PPP/RNBQ1RK1 w kq - 2 7"
    },
    {
      "turn": 15,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bb5",
          "aggressive_uci": "c4b5",
          "aggressive_response": "8. Bb5 Bb7 9. Ne5 Qd6 10. Qa4 O-O 11. Bxc6 Bxc6 12. Qxc6 Qxc6 13. Nxc6 Bd6 14. e4 Nd7 15. e5 Bxe5 16. dxe5 Nc5 17. Be3 Nd3 18. b3 a5 19. Rfd1 Nb4 20. Nxb4 axb4 21. Nb5 c5 22. Rd6 Rab8 23. Rad1 h6 24. f4 Kh7 25. g4 Kg6 26. h4 h5 27. f5+ Kh7 28. fxe6 fxe6 29. Rxe6 hxg4 30. Rdd6 Rf3 31. Bg5 Rbf8 32. Rd1 Rf2 33. Rxb6 Rxa2 34. Rf1 Re8 35. e6 Rb2 36. e7 Rxb3 37. Rf8 Rxe7 38. Bxe7 c4 39. Rxb4 Rxb4 40. Bxb4 Kg6 41. Kg2 Kh5 42. Kg3 g5 43. Rh8+ Kg6 44. Kxg4 gxh4 45. Rxh4 Kf6 46. Rh5 Ke6 47. Kf4 Kd7 48. Rh6 Kc7 49. Ke5 Kd7 50. Kd5 Kc7 51. Kxc4 Kd7 52. Kd5 Kc7 53. Ba5+ Kd7 54. Rh7+ Ke8 55. Ke6 Kf8 56. Ra7 Kg8 57. Kf6 Kh8 58. Kg6 Kg8 59. Ra8# Bb7 Bc8 Bb7",
          "aggressive_tokens": {
            "prompt_tokens": 164,
            "completion_tokens": 429,
            "total_tokens": 593,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bb7",
            "Ne5",
            "Qd6",
            "Qa4",
            "O-O",
            "Bxc6",
            "Bxc6",
            "Qxc6",
            "Qxc6",
            "Nxc6",
            "Bd6",
            "e4",
            "Nd7",
            "e5",
            "Bxe5",
            "dxe5",
            "Nc5",
            "Be3",
            "Nd3",
            "b3",
            "a5",
            "Rfd1",
            "Nb4",
            "Nxb4",
            "axb4",
            "Nb5",
            "c5",
            "Rd6",
            "Rab8",
            "Rad1",
            "h6",
            "f4",
            "Kh7",
            "g4",
            "Kg6",
            "h4",
            "h5",
            "f5+",
            "Kh7",
            "fxe6",
            "fxe6",
            "Rxe6",
            "hxg4",
            "Rdd6",
            "Rf3",
            "Bg5",
            "Rbf8",
            "Rd1",
            "Rf2",
            "Rxb6",
            "Rxa2",
            "Rf1",
            "Re8",
            "e6",
            "Rb2",
            "e7",
            "Rxb3",
            "Rf8",
            "Rxe7",
            "Bxe7",
            "c4",
            "Rxb4",
            "Rxb4",
            "Bxb4",
            "Kg6",
            "Kg2",
            "Kh5",
            "Kg3",
            "g5",
            "Rh8+",
            "Kg6",
            "Kxg4",
            "gxh4",
            "Rxh4",
            "Kf6",
            "Rh5",
            "Ke6",
            "Kf4",
            "Kd7",
            "Rh6",
            "Kc7",
            "Ke5",
            "Kd7",
            "Kd5",
            "Kc7",
            "Kxc4",
            "Kd7",
            "Kd5",
            "Kc7",
            "Ba5+",
            "Kd7",
            "Rh7+",
            "Ke8",
            "Ke6",
            "Kf8",
            "Ra7",
            "Kg8",
            "Kf6",
            "Kh8",
            "Kg6",
            "Kg8",
            "Ra8#",
            "Bb7",
            "Bc8",
            "Bb7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bb5",
          "positional_uci": "c4b5",
          "positional_response": "8. Bb5 Bb7 9. Ne5 Qd6 10. Qa4 O-O 11. Bxc6 Bxc6 12. Qxc6 Qxc6 13. Nxc6 Bd6 14. e4 Nd7 15. e5 Bb4 16. Nxb4 c5 17. dxc5 Nxc5 18. Be3 Nd7 19. f4 a5 20. Nc6 Kh8 21. Rad1 Nc5 22. Bxc5 bxc5 23. Rd7 Rac8 24. Nxa5 Ra8 25. Nc4 Rab8 26. Rfd1 h6 27. Rc7 Rbc8 28. Rdd7 Rxc7 29. Rxc7 Kg8 30. Rxc5 Rd8 31. Nd6 Rb8 32. Rc8+ Rxc8 33. Nxc8 Kf8 34. a4 Ke8 35. a5 Kd8 36. Nd6 Kc7 37. b4 Kc6 38. b5+ Kc7 39. b6+ Kb8 40. a6 Ka8 41. Ncb5 Kb8 42. a7+ Ka8 43. Nc7# Bb7 Nxb7",
          "positional_tokens": {
            "prompt_tokens": 164,
            "completion_tokens": 296,
            "total_tokens": 460,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bb7",
            "Ne5",
            "Qd6",
            "Qa4",
            "O-O",
            "Bxc6",
            "Bxc6",
            "Qxc6",
            "Qxc6",
            "Nxc6",
            "Bd6",
            "e4",
            "Nd7",
            "e5",
            "Bb4",
            "Nxb4",
            "c5",
            "dxc5",
            "Nxc5",
            "Be3",
            "Nd7",
            "f4",
            "a5",
            "Nc6",
            "Kh8",
            "Rad1",
            "Nc5",
            "Bxc5",
            "bxc5",
            "Rd7",
            "Rac8",
            "Nxa5",
            "Ra8",
            "Nc4",
            "Rab8",
            "Rfd1",
            "h6",
            "Rc7",
            "Rbc8",
            "Rdd7",
            "Rxc7",
            "Rxc7",
            "Kg8",
            "Rxc5",
            "Rd8",
            "Nd6",
            "Rb8",
            "Rc8+",
            "Rxc8",
            "Nxc8",
            "Kf8",
            "a4",
            "Ke8",
            "a5",
            "Kd8",
            "Nd6",
            "Kc7",
            "b4",
            "Kc6",
            "b5+",
            "Kc7",
            "b6+",
            "Kb8",
            "a6",
            "Ka8",
            "Ncb5",
            "Kb8",
            "a7+",
            "Ka8",
            "Nc7#",
            "Bb7",
            "Nxb7"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bb5",
          "neutral_uci": "c4b5",
          "neutral_response": "8. Bb5 Bb7 9. Ne5 Qd6 10. Qa4 O-O 11. Bxc6 Bxc6 12. Qxc6 Qxc6 13. Nxc6 Bd6 14. e4 Nd7 15. e5 Bb4 16. Nxb4 c5 17. dxc5 Nxc5 18. Be3 Nd7 19. f4 Rac8 20. Rad1 Nc5 21. Bxc5 Rxc5 22. Rd7 a5 23. Nd3 Rc4 24. Rd1 h6 25. g3 Rb8 26. Nf2 b5 27. Rd8+ Rxd8 28. Rxd8+ Kh7 29. Nxb5 Rc2 30. Nd3 Rd2 31. Rd7 Kg6 32. Nc5 Rxb2 33. Nd6 Rxa2 34. Rxf7 a4 35. Nxe6 a3 36. Rxg7+ Kh5 37. Nf5 Ra1+ 38. Kg2 Ra2+ 39. Kh3 Rxh2+ 40. Kxh2 a2 41. g4# Bg1+ 42. Kh3 a1=Q 43. Ng3# Bf2 44. f5# Bxg3 45. Nf4# Bxf4",
          "neutral_tokens": {
            "prompt_tokens": 163,
            "completion_tokens": 316,
            "total_tokens": 479,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bb7",
            "Ne5",
            "Qd6",
            "Qa4",
            "O-O",
            "Bxc6",
            "Bxc6",
            "Qxc6",
            "Qxc6",
            "Nxc6",
            "Bd6",
            "e4",
            "Nd7",
            "e5",
            "Bb4",
            "Nxb4",
            "c5",
            "dxc5",
            "Nxc5",
            "Be3",
            "Nd7",
            "f4",
            "Rac8",
            "Rad1",
            "Nc5",
            "Bxc5",
            "Rxc5",
            "Rd7",
            "a5",
            "Nd3",
            "Rc4",
            "Rd1",
            "h6",
            "g3",
            "Rb8",
            "Nf2",
            "b5",
            "Rd8+",
            "Rxd8",
            "Rxd8+",
            "Kh7",
            "Nxb5",
            "Rc2",
            "Nd3",
            "Rd2",
            "Rd7",
            "Kg6",
            "Nc5",
            "Rxb2",
            "Nd6",
            "Rxa2",
            "Rxf7",
            "a4",
            "Nxe6",
            "a3",
            "Rxg7+",
            "Kh5",
            "Nf5",
            "Ra1+",
            "Kg2",
            "Ra2+",
            "Kh3",
            "Rxh2+",
            "Kxh2",
            "a2",
            "g4#",
            "Bg1+",
            "Kh3",
            "a1=Q",
            "Ng3#",
            "Bf2",
            "f5#",
            "Bxg3",
            "Nf4#",
            "Bxf4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "c4b5",
          "positional_uci": "c4b5",
          "neutral_uci": "c4b5",
          "consensus_move": "c4b5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bb7",
              "Ne5",
              "Qd6",
              "Qa4",
              "O-O",
              "Bxc6",
              "Bxc6",
              "Qxc6",
              "Qxc6",
              "Nxc6",
              "Bd6",
              "e4",
              "Nd7",
              "e5",
              "Bxe5",
              "dxe5",
              "Nc5",
              "Be3",
              "Nd3",
              "b3",
              "a5",
              "Rfd1",
              "Nb4",
              "Nxb4",
              "axb4",
              "Nb5",
              "c5",
              "Rd6",
              "Rab8",
              "Rad1",
              "h6",
              "f4",
              "Kh7",
              "g4",
              "Kg6",
              "h4",
              "h5",
              "f5+",
              "Kh7",
              "fxe6",
              "fxe6",
              "Rxe6",
              "hxg4",
              "Rdd6",
              "Rf3",
              "Bg5",
              "Rbf8",
              "Rd1",
              "Rf2",
              "Rxb6",
              "Rxa2",
              "Rf1",
              "Re8",
              "e6",
              "Rb2",
              "e7",
              "Rxb3",
              "Rf8",
              "Rxe7",
              "Bxe7",
              "c4",
              "Rxb4",
              "Rxb4",
              "Bxb4",
              "Kg6",
              "Kg2",
              "Kh5",
              "Kg3",
              "g5",
              "Rh8+",
              "Kg6",
              "Kxg4",
              "gxh4",
              "Rxh4",
              "Kf6",
              "Rh5",
              "Ke6",
              "Kf4",
              "Kd7",
              "Rh6",
              "Kc7",
              "Ke5",
              "Kd7",
              "Kd5",
              "Kc7",
              "Kxc4",
              "Kd7",
              "Kd5",
              "Kc7",
              "Ba5+",
              "Kd7",
              "Rh7+",
              "Ke8",
              "Ke6",
              "Kf8",
              "Ra7",
              "Kg8",
              "Kf6",
              "Kh8",
              "Kg6",
              "Kg8",
              "Ra8#",
              "Bb7",
              "Bc8",
              "Bb7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bb7",
              "Ne5",
              "Qd6",
              "Qa4",
              "O-O",
              "Bxc6",
              "Bxc6",
              "Qxc6",
              "Qxc6",
              "Nxc6",
              "Bd6",
              "e4",
              "Nd7",
              "e5",
              "Bb4",
              "Nxb4",
              "c5",
              "dxc5",
              "Nxc5",
              "Be3",
              "Nd7",
              "f4",
              "a5",
              "Nc6",
              "Kh8",
              "Rad1",
              "Nc5",
              "Bxc5",
              "bxc5",
              "Rd7",
              "Rac8",
              "Nxa5",
              "Ra8",
              "Nc4",
              "Rab8",
              "Rfd1",
              "h6",
              "Rc7",
              "Rbc8",
              "Rdd7",
              "Rxc7",
              "Rxc7",
              "Kg8",
              "Rxc5",
              "Rd8",
              "Nd6",
              "Rb8",
              "Rc8+",
              "Rxc8",
              "Nxc8",
              "Kf8",
              "a4",
              "Ke8",
              "a5",
              "Kd8",
              "Nd6",
              "Kc7",
              "b4",
              "Kc6",
              "b5+",
              "Kc7",
              "b6+",
              "Kb8",
              "a6",
              "Ka8",
              "Ncb5",
              "Kb8",
              "a7+",
              "Ka8",
              "Nc7#",
              "Bb7",
              "Nxb7"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bb7",
              "Ne5",
              "Qd6",
              "Qa4",
              "O-O",
              "Bxc6",
              "Bxc6",
              "Qxc6",
              "Qxc6",
              "Nxc6",
              "Bd6",
              "e4",
              "Nd7",
              "e5",
              "Bb4",
              "Nxb4",
              "c5",
              "dxc5",
              "Nxc5",
              "Be3",
              "Nd7",
              "f4",
              "Rac8",
              "Rad1",
              "Nc5",
              "Bxc5",
              "Rxc5",
              "Rd7",
              "a5",
              "Nd3",
              "Rc4",
              "Rd1",
              "h6",
              "g3",
              "Rb8",
              "Nf2",
              "b5",
              "Rd8+",
              "Rxd8",
              "Rxd8+",
              "Kh7",
              "Nxb5",
              "Rc2",
              "Nd3",
              "Rd2",
              "Rd7",
              "Kg6",
              "Nc5",
              "Rxb2",
              "Nd6",
              "Rxa2",
              "Rxf7",
              "a4",
              "Nxe6",
              "a3",
              "Rxg7+",
              "Kh5",
              "Nf5",
              "Ra1+",
              "Kg2",
              "Ra2+",
              "Kh3",
              "Rxh2+",
              "Kxh2",
              "a2",
              "g4#",
              "Bg1+",
              "Kh3",
              "a1=Q",
              "Ng3#",
              "Bf2",
              "f5#",
              "Bxg3",
              "Nf4#",
              "Bxf4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 164,
            "completion_tokens": 429,
            "total_tokens": 593,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 164,
            "completion_tokens": 296,
            "total_tokens": 460,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 163,
            "completion_tokens": 316,
            "total_tokens": 479,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 491,
          "total_completion_tokens": 1041,
          "total_tokens": 1532
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 164,
            "completion_tokens": 429,
            "total_tokens": 593,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 164,
            "completion_tokens": 296,
            "total_tokens": 460,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 163,
            "completion_tokens": 316,
            "total_tokens": 479,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqk2r/p1p1bppp/1pn1pn2/8/2BP4/2N1PN2/PP3PPP/R1BQ1RK1 w kq - 0 8"
    },
    {
      "turn": 17,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Ne5",
          "aggressive_uci": "f3e5",
          "aggressive_response": "9. Ne5 Qd6 10. Qa4 O-O 11. Bxc6 Bxc6 12. Qxc6 Qxc6 13. Nxc6 Bd6 14. e4 Nd7 15.\ne5 Nb8 16. Nxb8 Raxb8 17. exd6 cxd6 18. Bf4 Rfd8 19. Nb5 Rbc8 20. Nxa7 Rc4 21.\nRac1 Rxd4 22. Nc6 Rxf4 23. Nxd8 h6 24. Rc8 Kh7 25. Rc6 d5 26. Rxb6 d4 27. Rd1\ne5 28. Rb5 f6 29. Ne6 Re4 30. Rb7 Kg6 31. Rxg7+ Kf5 32. Nc5 Re2 33. g4+ Kf4\n34. Nd3+ Ke4 35. Kf1 Rc2 36. Nb4 Rxb2 37. Re1+ Kf4 38. Nd3+ Kf3 39. Nxb2 e4\n40. Rf7 Kxg4 41. Rxf6 Kg5 42. Rf8 d3 43. Rxe4 d2 44. Rd4 d1=Q+ 45. Rxd1 h5\n46. Rd5+ Kg4 47. Rg8+ Kh4 48. Rd4+ Kh3 49. Rg3+ Kxh2 50. Rh4# Bb7 Bc8 Bb7",
          "aggressive_tokens": {
            "prompt_tokens": 174,
            "completion_tokens": 359,
            "total_tokens": 533,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qd6",
            "Qa4",
            "O-O",
            "Bxc6",
            "Bxc6",
            "Qxc6",
            "Qxc6",
            "Nxc6",
            "Bd6",
            "e4",
            "Nd7",
            "e5",
            "Nb8",
            "Nxb8",
            "Raxb8",
            "exd6",
            "cxd6",
            "Bf4",
            "Rfd8",
            "Nb5",
            "Rbc8",
            "Nxa7",
            "Rc4",
            "Rac1",
            "Rxd4",
            "Nc6",
            "Rxf4",
            "Nxd8",
            "h6",
            "Rc8",
            "Kh7",
            "Rc6",
            "d5",
            "Rxb6",
            "d4",
            "Rd1",
            "e5",
            "Rb5",
            "f6",
            "Ne6",
            "Re4",
            "Rb7",
            "Kg6",
            "Rxg7+",
            "Kf5",
            "Nc5",
            "Re2",
            "g4+",
            "Kf4",
            "Nd3+",
            "Ke4",
            "Kf1",
            "Rc2",
            "Nb4",
            "Rxb2",
            "Re1+",
            "Kf4",
            "Nd3+",
            "Kf3",
            "Nxb2",
            "e4",
            "Rf7",
            "Kxg4",
            "Rxf6",
            "Kg5",
            "Rf8",
            "d3",
            "Rxe4",
            "d2",
            "Rd4",
            "d1=Q+",
            "Rxd1",
            "h5",
            "Rd5+",
            "Kg4",
            "Rg8+",
            "Kh4",
            "Rd4+",
            "Kh3",
            "Rg3+",
            "Kxh2",
            "Rh4#",
            "Bb7",
            "Bc8",
            "Bb7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Ne5",
          "positional_uci": "f3e5",
          "positional_response": "9. Ne5 Qd6 10. Qa4 O-O 11. Bxc6 Bxc6 12. Qxc6 Qxc6 13. Nxc6 Bd6 14. e4 Nd7 15.\ne5 Nb8 16. Nxb8 Raxb8 17. exd6 cxd6 18. Bf4 Rfd8 19. Nb5 Rbc8 20. Nxa7 Rc2 21.\nRac1 Rxb2 22. Nc6 Rd7 23. Ne7+ Kf8 24. Nc8 Rxa2 25. Nxb6 Rd8 26. Rc8 Rxc8 27.\nNxc8 d5 28. Bd6+ Ke8 29. Rc1 Kd7 30. Bf8 g6 31. Nb6+ Ke8 32. Rc8# d4 e5 d3\nd2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1# d4 e5 d3 d2 d1=Q+ Rxd1#",
          "positional_tokens": {
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Qd6",
            "Qa4",
            "O-O",
            "Bxc6",
            "Bxc6",
            "Qxc6",
            "Qxc6",
            "Nxc6",
            "Bd6",
            "e4",
            "Nd7",
            "e5",
            "Nb8",
            "Nxb8",
            "Raxb8",
            "exd6",
            "cxd6",
            "Bf4",
            "Rfd8",
            "Nb5",
            "Rbc8",
            "Nxa7",
            "Rc2",
            "Rac1",
            "Rxb2",
            "Nc6",
            "Rd7",
            "Ne7+",
            "Kf8",
            "Nc8",
            "Rxa2",
            "Nxb6",
            "Rd8",
            "Rc8",
            "Rxc8",
            "Nxc8",
            "d5",
            "Bd6+",
            "Ke8",
            "Rc1",
            "Kd7",
            "Bf8",
            "g6",
            "Nb6+",
            "Ke8",
            "Rc8#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#",
            "d4",
            "e5",
            "d3",
            "d2",
            "d1=Q+",
            "Rxd1#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Ne5",
          "neutral_uci": "f3e5",
          "neutral_response": "9. Ne5 Qd6 10. Qa4 O-O 11. Bxc6 Bxc6 12. Qxc6 Qxc6 13. Nxc6 Bd6 14. e4 Nd7 15.\ne5 Nb8 16. Nxb8 Raxb8 17. exd6 cxd6 18. Bf4 Rfd8 19. Nb5 Rbc8 20. Nxa7 Rc2 21.\nRac1 Rxb2 22. Nc6 Rd7 23. Ne7+ Kf8 24. Nc8 Rxa2 25. Nxb6 Rd8 26. Rc8 Rxc8 27.\nNxc8 d5 28. Bd6+ Ke8 29. Nb6 Ra6 30. Bc5 Kd8 31. Rb1 Kc7 32. g3 Kc6 33. Nc8\nKc7 34. Ne7 Kd7 35. Rb7+ Kd8 36. Bb6+ Ke8 37. Nc6 Ra8 38. Re7+ Kf8 39. Bc5\nKg8 40. Ra7 Rxa7 41. Nxa7 f6 42. Nc6 Kf7 43. Kg2 Kg6 44. Kf3 Kf5 45. Ne7+ Kg5\n46. h4+ Kh5 47. Kf4 g5+ 48. hxg5 fxg5+ 49. Ke5 Kg4 50. Kxe6 Kf3 51. Kxd5 Kxf2\n52. Ke5 Kxg3 53. d5 h5 54. d6 h4 55. d7 h3 56. d8=Q h2 57. Qd3+ Kg2 58. Qe2+\nKg3 59. Nf5+ Kh3 60. Qf3# Kd2 Kc1 Qc2#",
          "neutral_tokens": {
            "prompt_tokens": 173,
            "completion_tokens": 449,
            "total_tokens": 622,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qd6",
            "Qa4",
            "O-O",
            "Bxc6",
            "Bxc6",
            "Qxc6",
            "Qxc6",
            "Nxc6",
            "Bd6",
            "e4",
            "Nd7",
            "e5",
            "Nb8",
            "Nxb8",
            "Raxb8",
            "exd6",
            "cxd6",
            "Bf4",
            "Rfd8",
            "Nb5",
            "Rbc8",
            "Nxa7",
            "Rc2",
            "Rac1",
            "Rxb2",
            "Nc6",
            "Rd7",
            "Ne7+",
            "Kf8",
            "Nc8",
            "Rxa2",
            "Nxb6",
            "Rd8",
            "Rc8",
            "Rxc8",
            "Nxc8",
            "d5",
            "Bd6+",
            "Ke8",
            "Nb6",
            "Ra6",
            "Bc5",
            "Kd8",
            "Rb1",
            "Kc7",
            "g3",
            "Kc6",
            "Nc8",
            "Kc7",
            "Ne7",
            "Kd7",
            "Rb7+",
            "Kd8",
            "Bb6+",
            "Ke8",
            "Nc6",
            "Ra8",
            "Re7+",
            "Kf8",
            "Bc5",
            "Kg8",
            "Ra7",
            "Rxa7",
            "Nxa7",
            "f6",
            "Nc6",
            "Kf7",
            "Kg2",
            "Kg6",
            "Kf3",
            "Kf5",
            "Ne7+",
            "Kg5",
            "h4+",
            "Kh5",
            "Kf4",
            "g5+",
            "hxg5",
            "fxg5+",
            "Ke5",
            "Kg4",
            "Kxe6",
            "Kf3",
            "Kxd5",
            "Kxf2",
            "Ke5",
            "Kxg3",
            "d5",
            "h5",
            "d6",
            "h4",
            "d7",
            "h3",
            "d8=Q",
            "h2",
            "Qd3+",
            "Kg2",
            "Qe2+",
            "Kg3",
            "Nf5+",
            "Kh3",
            "Qf3#",
            "Kd2",
            "Kc1",
            "Qc2#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f3e5",
          "positional_uci": "f3e5",
          "neutral_uci": "f3e5",
          "consensus_move": "f3e5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qd6",
              "Qa4",
              "O-O",
              "Bxc6",
              "Bxc6",
              "Qxc6",
              "Qxc6",
              "Nxc6",
              "Bd6",
              "e4",
              "Nd7",
              "e5",
              "Nb8",
              "Nxb8",
              "Raxb8",
              "exd6",
              "cxd6",
              "Bf4",
              "Rfd8",
              "Nb5",
              "Rbc8",
              "Nxa7",
              "Rc4",
              "Rac1",
              "Rxd4",
              "Nc6",
              "Rxf4",
              "Nxd8",
              "h6",
              "Rc8",
              "Kh7",
              "Rc6",
              "d5",
              "Rxb6",
              "d4",
              "Rd1",
              "e5",
              "Rb5",
              "f6",
              "Ne6",
              "Re4",
              "Rb7",
              "Kg6",
              "Rxg7+",
              "Kf5",
              "Nc5",
              "Re2",
              "g4+",
              "Kf4",
              "Nd3+",
              "Ke4",
              "Kf1",
              "Rc2",
              "Nb4",
              "Rxb2",
              "Re1+",
              "Kf4",
              "Nd3+",
              "Kf3",
              "Nxb2",
              "e4",
              "Rf7",
              "Kxg4",
              "Rxf6",
              "Kg5",
              "Rf8",
              "d3",
              "Rxe4",
              "d2",
              "Rd4",
              "d1=Q+",
              "Rxd1",
              "h5",
              "Rd5+",
              "Kg4",
              "Rg8+",
              "Kh4",
              "Rd4+",
              "Kh3",
              "Rg3+",
              "Kxh2",
              "Rh4#",
              "Bb7",
              "Bc8",
              "Bb7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qd6",
              "Qa4",
              "O-O",
              "Bxc6",
              "Bxc6",
              "Qxc6",
              "Qxc6",
              "Nxc6",
              "Bd6",
              "e4",
              "Nd7",
              "e5",
              "Nb8",
              "Nxb8",
              "Raxb8",
              "exd6",
              "cxd6",
              "Bf4",
              "Rfd8",
              "Nb5",
              "Rbc8",
              "Nxa7",
              "Rc2",
              "Rac1",
              "Rxb2",
              "Nc6",
              "Rd7",
              "Ne7+",
              "Kf8",
              "Nc8",
              "Rxa2",
              "Nxb6",
              "Rd8",
              "Rc8",
              "Rxc8",
              "Nxc8",
              "d5",
              "Bd6+",
              "Ke8",
              "Rc1",
              "Kd7",
              "Bf8",
              "g6",
              "Nb6+",
              "Ke8",
              "Rc8#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#",
              "d4",
              "e5",
              "d3",
              "d2",
              "d1=Q+",
              "Rxd1#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qd6",
              "Qa4",
              "O-O",
              "Bxc6",
              "Bxc6",
              "Qxc6",
              "Qxc6",
              "Nxc6",
              "Bd6",
              "e4",
              "Nd7",
              "e5",
              "Nb8",
              "Nxb8",
              "Raxb8",
              "exd6",
              "cxd6",
              "Bf4",
              "Rfd8",
              "Nb5",
              "Rbc8",
              "Nxa7",
              "Rc2",
              "Rac1",
              "Rxb2",
              "Nc6",
              "Rd7",
              "Ne7+",
              "Kf8",
              "Nc8",
              "Rxa2",
              "Nxb6",
              "Rd8",
              "Rc8",
              "Rxc8",
              "Nxc8",
              "d5",
              "Bd6+",
              "Ke8",
              "Nb6",
              "Ra6",
              "Bc5",
              "Kd8",
              "Rb1",
              "Kc7",
              "g3",
              "Kc6",
              "Nc8",
              "Kc7",
              "Ne7",
              "Kd7",
              "Rb7+",
              "Kd8",
              "Bb6+",
              "Ke8",
              "Nc6",
              "Ra8",
              "Re7+",
              "Kf8",
              "Bc5",
              "Kg8",
              "Ra7",
              "Rxa7",
              "Nxa7",
              "f6",
              "Nc6",
              "Kf7",
              "Kg2",
              "Kg6",
              "Kf3",
              "Kf5",
              "Ne7+",
              "Kg5",
              "h4+",
              "Kh5",
              "Kf4",
              "g5+",
              "hxg5",
              "fxg5+",
              "Ke5",
              "Kg4",
              "Kxe6",
              "Kf3",
              "Kxd5",
              "Kxf2",
              "Ke5",
              "Kxg3",
              "d5",
              "h5",
              "d6",
              "h4",
              "d7",
              "h3",
              "d8=Q",
              "h2",
              "Qd3+",
              "Kg2",
              "Qe2+",
              "Kg3",
              "Nf5+",
              "Kh3",
              "Qf3#",
              "Kd2",
              "Kc1",
              "Qc2#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 174,
            "completion_tokens": 359,
            "total_tokens": 533,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 173,
            "completion_tokens": 449,
            "total_tokens": 622,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 521,
          "total_completion_tokens": 1448,
          "total_tokens": 1969
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 174,
            "completion_tokens": 359,
            "total_tokens": 533,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 173,
            "completion_tokens": 449,
            "total_tokens": 622,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qk2r/pbp1bppp/1pn1pn2/1B6/3P4/2N1PN2/PP3PPP/R1BQ1RK1 w kq - 2 9"
    },
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e5",
          "aggressive_uci": "e7e5",
          "aggressive_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7\n6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5\n11. d4 Qc7 12. Nbd2 cxd4 13. cxd4 Nc6 14. Nb3 a5\n15. Be3 a4 16. Nbd2 Bd7 17. Rc1 Qb7 18. Bb1 Rfc8\n19. Nf1 Bd8 20. Ng3 Bb6 21. Qd2 Ba5 22. Qe2 Bxe1\n23. Rxe1 Re8 24. Qd2 Rac8 25. Bg5 Kh8 26. Bxf6 gxf6\n27. Qh6 Rg8 28. Qxf6+ Rg7 29. Nh5 Rg8 30. Ng5 Be8\n31. Nxg7 Rxg7 32. dxe5 dxe5 33. Rd1 Qe7 34. Qxe7 Nxe7\n35. Rd8 Rg8 36. Nf3 f6 37. Bd3 Kg7 38. Rb8 Kf7\n39. Bxb5 Bxb5 40. Rxb5 Rc8 41. Rb4 Rc1+ 42. Kh2 Rc2\n43. Kg3 Ng6 44. Ne1 Re2 45. Nd3 Rd2 46. Nc5 Nf4\n47. Nxa4 Nd3 48. Rb7+ Kg6 49. f3 Nf4 50. Kh4 Nxg2+\n51. Kg3 Nf4 52. Nc3 Rg2+ 53. Kh4 Rg5 54. Nd5 Ng2# 1-0\n1... e5 2. Nf3 Nc6 3. Bb5 a6",
          "aggressive_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 462,
            "total_tokens": 576,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e5",
          "positional_uci": "e7e5",
          "positional_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7\n6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5\n11. d4 Qc7 12. Nbd2 cxd4 13. cxd4 Nc6 14. Nb3 a5\n15. Be3 a4 16. Nbd2 Bd7 17. Rc1 Qb7 18. Bb1 Rfc8\n19. Nf1 Bd8 20. Ng3 Bb6 21. Qd2 Ba5 22. Qe2 Bxe1\n23. Rxe1 Re8 24. Qd2 Rac8 25. Bg5 Kh8 26. Bxf6 gxf6\n27. Qh6 Rg8 28. Qxf6+ Rg7 29. Nh5 Rg8 30. Ng5 Be8\n31. Nxg7 Rxg7 32. dxe5 dxe5 33. Rd1 Qe7 34. Qxe7 Nxe7\n35. Rd8 Rg8 36. Nf3 f6 37. Bd3 Kg7 38. Rb8 Bc6\n39. Rxg8+ Kxg8 40. Kf1 Kf7 41. Ke2 Ke6 42. Ke3 Kd6\n43. Nd2 Kc5 44. a3 Bd7 45. Nb1 Be6 46. Nc3 Bc4\n47. Bxc4 Kxc4 48. Kd2 Nc6 49. Nd5 Kb3 50. Kc1 Nd4\n51. Nxf6 Ne2+ 52. Kb1 Nf4 53. Nxh7 Nxg2 54. Nf6 Nf4\n55. h4 Nd3 56. h5 Nxb2 57. h6 Nc4 58. h7 Nxa3+\n59. Kc1 Nc4 60. h8=Q a3 61. Qh3+ Ka4 62. Nd5 a2\n63. Qc3 b4 64. Qxb4# Nxb4 65. Nc3+ Kb3 66. Nxa2 Nxa2+\n67. Kd1 Kc3 68. Ke2 Kd4 69. f3 Nc3+ 70. Kf2 Nxe4+\n71. fxe4 Kxe4 72. Ke2 Kd4 73. Kd2 e4 74. Ke2 e3\n75. Ke1 Kd3 76. Kd1 e2+ 77. Ke1 Ke3 78. 1-0 e",
          "positional_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Qc7",
            "Nbd2",
            "cxd4",
            "cxd4",
            "Nc6",
            "Nb3",
            "a5",
            "Be3",
            "a4",
            "Nbd2",
            "Bd7",
            "Rc1",
            "Qb7",
            "Bb1",
            "Rfc8",
            "Nf1",
            "Bd8",
            "Ng3",
            "Bb6",
            "Qd2",
            "Ba5",
            "Qe2",
            "Bxe1",
            "Rxe1",
            "Re8",
            "Qd2",
            "Rac8",
            "Bg5",
            "Kh8",
            "Bxf6",
            "gxf6",
            "Qh6",
            "Rg8",
            "Qxf6+",
            "Rg7",
            "Nh5",
            "Rg8",
            "Ng5",
            "Be8",
            "Nxg7",
            "Rxg7",
            "dxe5",
            "dxe5",
            "Rd1",
            "Qe7",
            "Qxe7",
            "Nxe7",
            "Rd8",
            "Rg8",
            "Nf3",
            "f6",
            "Bd3",
            "Kg7",
            "Rb8",
            "Bc6",
            "Rxg8+",
            "Kxg8",
            "Kf1",
            "Kf7",
            "Ke2",
            "Ke6",
            "Ke3",
            "Kd6",
            "Nd2",
            "Kc5",
            "a3",
            "Bd7",
            "Nb1",
            "Be6",
            "Nc3",
            "Bc4",
            "Bxc4",
            "Kxc4",
            "Kd2",
            "Nc6",
            "Nd5",
            "Kb3",
            "Kc1",
            "Nd4",
            "Nxf6",
            "Ne2+",
            "Kb1",
            "Nf4",
            "Nxh7",
            "Nxg2",
            "Nf6",
            "Nf4",
            "h4",
            "Nd3",
            "h5",
            "Nxb2",
            "h6",
            "Nc4",
            "h7",
            "Nxa3+",
            "Kc1",
            "Nc4",
            "h8=Q",
            "a3",
            "Qh3+",
            "Ka4",
            "Nd5",
            "a2",
            "Qc3",
            "b4",
            "Qxb4#",
            "Nxb4",
            "Nc3+",
            "Kb3",
            "Nxa2",
            "Nxa2+",
            "Kd1",
            "Kc3",
            "Ke2",
            "Kd4",
            "f3",
            "Nc3+",
            "Kf2",
            "Nxe4+",
            "fxe4",
            "Kxe4",
            "Ke2",
            "Kd4",
            "Kd2",
            "e4",
            "Ke2",
            "e3",
            "Ke1",
            "Kd3",
            "Kd1",
            "e2+",
            "Ke1",
            "Ke3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e5",
          "neutral_uci": "e7e5",
          "neutral_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6\n4. Ba4 Nf6 5. O-O Be7 6. Re1 b5\n7. Bb3 d6 8. c3 O-O 9. h3 Bb7\n10. d4 Re8 11. Nbd2 Bf8 12. a4 h6\n13. Bc2 exd4 14. cxd4 Nb4 15. Bb1 c5\n16. d5 Nd7 17. Ra3 c4 18. Nd4 Qf6\n19. N2f3 Nc5 20. axb5 Nbd3 21. Bxd3 Nxd3\n22. Re2 axb5 23. Nxb5 Rxa3 24. Nxa3 Ba6\n25. Qa4 Ra8 26. Be3 Nxb2\n27. Qc6 Qd8 28. Rxb2 c3 29. Qxc3 Rc8",
          "neutral_tokens": {
            "prompt_tokens": 113,
            "completion_tokens": 231,
            "total_tokens": 344,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Bb7",
            "d4",
            "Re8",
            "Nbd2",
            "Bf8",
            "a4",
            "h6",
            "Bc2",
            "exd4",
            "cxd4",
            "Nb4",
            "Bb1",
            "c5",
            "d5",
            "Nd7",
            "Ra3",
            "c4",
            "Nd4",
            "Qf6",
            "N2f3",
            "Nc5",
            "axb5",
            "Nbd3",
            "Bxd3",
            "Nxd3",
            "Re2",
            "axb5",
            "Nxb5",
            "Rxa3",
            "Nxa3",
            "Ba6",
            "Qa4",
            "Ra8",
            "Be3",
            "Nxb2",
            "Qc6",
            "Qd8",
            "Rxb2",
            "c3",
            "Qxc3",
            "Rc8"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e7e5",
          "positional_uci": "e7e5",
          "neutral_uci": "e7e5",
          "consensus_move": "e7e5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Qc7",
              "Nbd2",
              "cxd4",
              "cxd4",
              "Nc6",
              "Nb3",
              "a5",
              "Be3",
              "a4",
              "Nbd2",
              "Bd7",
              "Rc1",
              "Qb7",
              "Bb1",
              "Rfc8",
              "Nf1",
              "Bd8",
              "Ng3",
              "Bb6",
              "Qd2",
              "Ba5",
              "Qe2",
              "Bxe1",
              "Rxe1",
              "Re8",
              "Qd2",
              "Rac8",
              "Bg5",
              "Kh8",
              "Bxf6",
              "gxf6",
              "Qh6",
              "Rg8",
              "Qxf6+",
              "Rg7",
              "Nh5",
              "Rg8",
              "Ng5",
              "Be8",
              "Nxg7",
              "Rxg7",
              "dxe5",
              "dxe5",
              "Rd1",
              "Qe7",
              "Qxe7",
              "Nxe7",
              "Rd8",
              "Rg8",
              "Nf3",
              "f6",
              "Bd3",
              "Kg7",
              "Rb8",
              "Bc6",
              "Rxg8+",
              "Kxg8",
              "Kf1",
              "Kf7",
              "Ke2",
              "Ke6",
              "Ke3",
              "Kd6",
              "Nd2",
              "Kc5",
              "a3",
              "Bd7",
              "Nb1",
              "Be6",
              "Nc3",
              "Bc4",
              "Bxc4",
              "Kxc4",
              "Kd2",
              "Nc6",
              "Nd5",
              "Kb3",
              "Kc1",
              "Nd4",
              "Nxf6",
              "Ne2+",
              "Kb1",
              "Nf4",
              "Nxh7",
              "Nxg2",
              "Nf6",
              "Nf4",
              "h4",
              "Nd3",
              "h5",
              "Nxb2",
              "h6",
              "Nc4",
              "h7",
              "Nxa3+",
              "Kc1",
              "Nc4",
              "h8=Q",
              "a3",
              "Qh3+",
              "Ka4",
              "Nd5",
              "a2",
              "Qc3",
              "b4",
              "Qxb4#",
              "Nxb4",
              "Nc3+",
              "Kb3",
              "Nxa2",
              "Nxa2+",
              "Kd1",
              "Kc3",
              "Ke2",
              "Kd4",
              "f3",
              "Nc3+",
              "Kf2",
              "Nxe4+",
              "fxe4",
              "Kxe4",
              "Ke2",
              "Kd4",
              "Kd2",
              "e4",
              "Ke2",
              "e3",
              "Ke1",
              "Kd3",
              "Kd1",
              "e2+",
              "Ke1",
              "Ke3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Bb7",
              "d4",
              "Re8",
              "Nbd2",
              "Bf8",
              "a4",
              "h6",
              "Bc2",
              "exd4",
              "cxd4",
              "Nb4",
              "Bb1",
              "c5",
              "d5",
              "Nd7",
              "Ra3",
              "c4",
              "Nd4",
              "Qf6",
              "N2f3",
              "Nc5",
              "axb5",
              "Nbd3",
              "Bxd3",
              "Nxd3",
              "Re2",
              "axb5",
              "Nxb5",
              "Rxa3",
              "Nxa3",
              "Ba6",
              "Qa4",
              "Ra8",
              "Be3",
              "Nxb2",
              "Qc6",
              "Qd8",
              "Rxb2",
              "c3",
              "Qxc3",
              "Rc8"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 114,
            "completion_tokens": 462,
            "total_tokens": 576,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 113,
            "completion_tokens": 231,
            "total_tokens": 344,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 341,
          "total_completion_tokens": 1333,
          "total_tokens": 1674
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 114,
            "completion_tokens": 462,
            "total_tokens": 576,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 113,
            "completion_tokens": 231,
            "total_tokens": 344,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1"
    }
  ],
  "white_errors": [],
  "black_errors": [],
  "white_plan_log": [],
  "black_plan_log": [],
  "timestamp": "2025-11-30T00:54:21.678302",
  "game_id": "meta-llama_llama-3.3-70b-instruct_vs_gpt-3.5-turbo-instruct_SC_8",
  "configuration": "SC"
}