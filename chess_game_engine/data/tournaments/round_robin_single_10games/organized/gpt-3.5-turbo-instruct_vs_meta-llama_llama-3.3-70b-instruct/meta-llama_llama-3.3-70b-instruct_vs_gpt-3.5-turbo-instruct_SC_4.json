{
  "result": "gpt-3.5-turbo-instruct wins by disqualification (meta-llama/llama-3.3-70b-instruct failed to produce a legal move)",
  "white_player": "meta-llama/llama-3.3-70b-instruct",
  "black_player": "gpt-3.5-turbo-instruct",
  "white_score": 0.0,
  "black_score": 1.0,
  "total_moves": 2,
  "moves": [
    "e2e4",
    "e7e5"
  ],
  "game_history": [
    {
      "move_number": 1,
      "player": "meta-llama/llama-3.3-70b-instruct",
      "color": "white",
      "move_uci": "e2e4",
      "move_san": "e4",
      "fen_after": "rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "move_number": 2,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "e7e5",
      "move_san": "e5",
      "fen_after": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    }
  ],
  "final_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2",
  "white_tokens": {
    "prompt_tokens": 9630,
    "completion_tokens": 2077,
    "total_tokens": 11707
  },
  "black_tokens": {
    "prompt_tokens": 8308,
    "completion_tokens": 31699,
    "total_tokens": 40007
  },
  "white_token_log": [
    {
      "turn": 2,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 14,
          "total_tokens": 162,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 446,
        "total_completion_tokens": 28,
        "total_tokens": 474
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 14,
          "total_tokens": 162,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 4,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 21,
          "total_tokens": 177,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 18,
          "total_tokens": 174,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 18,
          "total_tokens": 173,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 467,
        "total_completion_tokens": 57,
        "total_tokens": 524
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 21,
          "total_tokens": 177,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 18,
          "total_tokens": 174,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 18,
          "total_tokens": 173,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 6,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 165,
          "completion_tokens": 28,
          "total_tokens": 193,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 165,
          "completion_tokens": 30,
          "total_tokens": 195,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 164,
          "completion_tokens": 27,
          "total_tokens": 191,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 494,
        "total_completion_tokens": 85,
        "total_tokens": 579
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 165,
          "completion_tokens": 28,
          "total_tokens": 193,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 165,
          "completion_tokens": 30,
          "total_tokens": 195,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 164,
          "completion_tokens": 27,
          "total_tokens": 191,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 8,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 174,
          "completion_tokens": 36,
          "total_tokens": 210,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 174,
          "completion_tokens": 36,
          "total_tokens": 210,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 173,
          "completion_tokens": 36,
          "total_tokens": 209,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 521,
        "total_completion_tokens": 108,
        "total_tokens": 629
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 174,
          "completion_tokens": 36,
          "total_tokens": 210,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 174,
          "completion_tokens": 36,
          "total_tokens": 210,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 173,
          "completion_tokens": 36,
          "total_tokens": 209,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 10,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 182,
          "completion_tokens": 47,
          "total_tokens": 229,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 182,
          "completion_tokens": 48,
          "total_tokens": 230,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 181,
          "completion_tokens": 48,
          "total_tokens": 229,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 545,
        "total_completion_tokens": 143,
        "total_tokens": 688
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 182,
          "completion_tokens": 47,
          "total_tokens": 229,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 182,
          "completion_tokens": 48,
          "total_tokens": 230,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 181,
          "completion_tokens": 48,
          "total_tokens": 229,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 12,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 190,
          "completion_tokens": 53,
          "total_tokens": 243,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 190,
          "completion_tokens": 53,
          "total_tokens": 243,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 189,
          "completion_tokens": 53,
          "total_tokens": 242,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 569,
        "total_completion_tokens": 159,
        "total_tokens": 728
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 190,
          "completion_tokens": 53,
          "total_tokens": 243,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 190,
          "completion_tokens": 53,
          "total_tokens": 243,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 189,
          "completion_tokens": 53,
          "total_tokens": 242,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 14,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 199,
          "completion_tokens": 65,
          "total_tokens": 264,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 199,
          "completion_tokens": 63,
          "total_tokens": 262,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 198,
          "completion_tokens": 65,
          "total_tokens": 263,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 596,
        "total_completion_tokens": 193,
        "total_tokens": 789
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 199,
          "completion_tokens": 65,
          "total_tokens": 264,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 199,
          "completion_tokens": 63,
          "total_tokens": 262,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 198,
          "completion_tokens": 65,
          "total_tokens": 263,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 16,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 208,
          "completion_tokens": 72,
          "total_tokens": 280,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 208,
          "completion_tokens": 72,
          "total_tokens": 280,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 207,
          "completion_tokens": 72,
          "total_tokens": 279,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 623,
        "total_completion_tokens": 216,
        "total_tokens": 839
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 208,
          "completion_tokens": 72,
          "total_tokens": 280,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 208,
          "completion_tokens": 72,
          "total_tokens": 280,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 207,
          "completion_tokens": 72,
          "total_tokens": 279,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 184,
          "completion_tokens": 16,
          "total_tokens": 200,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 554,
        "total_completion_tokens": 48,
        "total_tokens": 602
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 184,
          "completion_tokens": 16,
          "total_tokens": 200,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 2,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 14,
          "total_tokens": 162,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 446,
        "total_completion_tokens": 28,
        "total_tokens": 474
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 14,
          "total_tokens": 162,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 4,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 21,
          "total_tokens": 177,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 18,
          "total_tokens": 174,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 18,
          "total_tokens": 173,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 467,
        "total_completion_tokens": 57,
        "total_tokens": 524
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 21,
          "total_tokens": 177,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 18,
          "total_tokens": 174,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 18,
          "total_tokens": 173,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 6,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 165,
          "completion_tokens": 28,
          "total_tokens": 193,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 165,
          "completion_tokens": 30,
          "total_tokens": 195,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 164,
          "completion_tokens": 27,
          "total_tokens": 191,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 494,
        "total_completion_tokens": 85,
        "total_tokens": 579
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 165,
          "completion_tokens": 28,
          "total_tokens": 193,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 165,
          "completion_tokens": 30,
          "total_tokens": 195,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 164,
          "completion_tokens": 27,
          "total_tokens": 191,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 8,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 174,
          "completion_tokens": 36,
          "total_tokens": 210,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 174,
          "completion_tokens": 38,
          "total_tokens": 212,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 173,
          "completion_tokens": 36,
          "total_tokens": 209,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 521,
        "total_completion_tokens": 110,
        "total_tokens": 631
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 174,
          "completion_tokens": 36,
          "total_tokens": 210,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 174,
          "completion_tokens": 38,
          "total_tokens": 212,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 173,
          "completion_tokens": 36,
          "total_tokens": 209,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 10,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 182,
          "completion_tokens": 47,
          "total_tokens": 229,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 182,
          "completion_tokens": 48,
          "total_tokens": 230,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 181,
          "completion_tokens": 48,
          "total_tokens": 229,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 545,
        "total_completion_tokens": 143,
        "total_tokens": 688
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 182,
          "completion_tokens": 47,
          "total_tokens": 229,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 182,
          "completion_tokens": 48,
          "total_tokens": 230,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 181,
          "completion_tokens": 48,
          "total_tokens": 229,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 12,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 190,
          "completion_tokens": 54,
          "total_tokens": 244,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 190,
          "completion_tokens": 53,
          "total_tokens": 243,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 189,
          "completion_tokens": 53,
          "total_tokens": 242,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 569,
        "total_completion_tokens": 160,
        "total_tokens": 729
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 190,
          "completion_tokens": 54,
          "total_tokens": 244,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 190,
          "completion_tokens": 53,
          "total_tokens": 243,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 189,
          "completion_tokens": 53,
          "total_tokens": 242,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 14,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 199,
          "completion_tokens": 65,
          "total_tokens": 264,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 199,
          "completion_tokens": 63,
          "total_tokens": 262,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 198,
          "completion_tokens": 65,
          "total_tokens": 263,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 596,
        "total_completion_tokens": 193,
        "total_tokens": 789
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 199,
          "completion_tokens": 65,
          "total_tokens": 264,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 199,
          "completion_tokens": 63,
          "total_tokens": 262,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 198,
          "completion_tokens": 65,
          "total_tokens": 263,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 16,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 208,
          "completion_tokens": 72,
          "total_tokens": 280,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 208,
          "completion_tokens": 72,
          "total_tokens": 280,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 207,
          "completion_tokens": 72,
          "total_tokens": 279,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 623,
        "total_completion_tokens": 216,
        "total_tokens": 839
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 208,
          "completion_tokens": 72,
          "total_tokens": 280,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 208,
          "completion_tokens": 72,
          "total_tokens": 280,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 207,
          "completion_tokens": 72,
          "total_tokens": 279,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 184,
          "completion_tokens": 16,
          "total_tokens": 200,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 554,
        "total_completion_tokens": 48,
        "total_tokens": 602
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 184,
          "completion_tokens": 16,
          "total_tokens": 200,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    }
  ],
  "black_token_log": [
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 547,
          "total_tokens": 657,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 640,
          "total_tokens": 749,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 329,
        "total_completion_tokens": 1827,
        "total_tokens": 2156
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 547,
          "total_tokens": 657,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 640,
          "total_tokens": 749,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 3,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 116,
          "completion_tokens": 640,
          "total_tokens": 756,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 116,
          "completion_tokens": 640,
          "total_tokens": 756,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 115,
          "completion_tokens": 640,
          "total_tokens": 755,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 347,
        "total_completion_tokens": 1920,
        "total_tokens": 2267
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 116,
          "completion_tokens": 640,
          "total_tokens": 756,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 116,
          "completion_tokens": 640,
          "total_tokens": 756,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 115,
          "completion_tokens": 640,
          "total_tokens": 755,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 5,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 124,
          "completion_tokens": 640,
          "total_tokens": 764,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 124,
          "completion_tokens": 640,
          "total_tokens": 764,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 123,
          "completion_tokens": 640,
          "total_tokens": 763,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 371,
        "total_completion_tokens": 1920,
        "total_tokens": 2291
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 124,
          "completion_tokens": 640,
          "total_tokens": 764,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 124,
          "completion_tokens": 640,
          "total_tokens": 764,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 123,
          "completion_tokens": 640,
          "total_tokens": 763,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 7,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 133,
          "completion_tokens": 519,
          "total_tokens": 652,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 133,
          "completion_tokens": 640,
          "total_tokens": 773,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 132,
          "completion_tokens": 254,
          "total_tokens": 386,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 398,
        "total_completion_tokens": 1413,
        "total_tokens": 1811
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 133,
          "completion_tokens": 519,
          "total_tokens": 652,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 133,
          "completion_tokens": 640,
          "total_tokens": 773,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 132,
          "completion_tokens": 254,
          "total_tokens": 386,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 9,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 142,
          "completion_tokens": 640,
          "total_tokens": 782,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 142,
          "completion_tokens": 640,
          "total_tokens": 782,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 141,
          "completion_tokens": 180,
          "total_tokens": 321,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 425,
        "total_completion_tokens": 1460,
        "total_tokens": 1885
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 142,
          "completion_tokens": 640,
          "total_tokens": 782,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 142,
          "completion_tokens": 640,
          "total_tokens": 782,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 141,
          "completion_tokens": 180,
          "total_tokens": 321,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 11,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 150,
          "completion_tokens": 291,
          "total_tokens": 441,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 150,
          "completion_tokens": 640,
          "total_tokens": 790,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 149,
          "completion_tokens": 640,
          "total_tokens": 789,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 449,
        "total_completion_tokens": 1571,
        "total_tokens": 2020
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 150,
          "completion_tokens": 291,
          "total_tokens": 441,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 150,
          "completion_tokens": 640,
          "total_tokens": 790,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 149,
          "completion_tokens": 640,
          "total_tokens": 789,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 13,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 157,
          "completion_tokens": 446,
          "total_tokens": 603,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 157,
          "completion_tokens": 640,
          "total_tokens": 797,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 156,
          "completion_tokens": 460,
          "total_tokens": 616,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 470,
        "total_completion_tokens": 1546,
        "total_tokens": 2016
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 157,
          "completion_tokens": 446,
          "total_tokens": 603,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 157,
          "completion_tokens": 640,
          "total_tokens": 797,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 156,
          "completion_tokens": 460,
          "total_tokens": 616,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 15,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 167,
          "completion_tokens": 640,
          "total_tokens": 807,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 167,
          "completion_tokens": 640,
          "total_tokens": 807,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 166,
          "completion_tokens": 409,
          "total_tokens": 575,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 500,
        "total_completion_tokens": 1689,
        "total_tokens": 2189
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 167,
          "completion_tokens": 640,
          "total_tokens": 807,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 167,
          "completion_tokens": 640,
          "total_tokens": 807,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 166,
          "completion_tokens": 409,
          "total_tokens": 575,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 17,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 175,
          "completion_tokens": 288,
          "total_tokens": 463,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 175,
          "completion_tokens": 640,
          "total_tokens": 815,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 174,
          "completion_tokens": 640,
          "total_tokens": 814,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 524,
        "total_completion_tokens": 1568,
        "total_tokens": 2092
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 175,
          "completion_tokens": 288,
          "total_tokens": 463,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 175,
          "completion_tokens": 640,
          "total_tokens": 815,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 174,
          "completion_tokens": 640,
          "total_tokens": 814,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 2,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 236,
          "total_tokens": 350,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 622,
          "total_tokens": 736,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 230,
          "total_tokens": 343,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 341,
        "total_completion_tokens": 1088,
        "total_tokens": 1429
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 236,
          "total_tokens": 350,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 622,
          "total_tokens": 736,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 230,
          "total_tokens": 343,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 492,
          "total_tokens": 602,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 568,
          "total_tokens": 677,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 329,
        "total_completion_tokens": 1700,
        "total_tokens": 2029
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 492,
          "total_tokens": 602,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 568,
          "total_tokens": 677,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 3,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 116,
          "completion_tokens": 525,
          "total_tokens": 641,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 116,
          "completion_tokens": 636,
          "total_tokens": 752,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 115,
          "completion_tokens": 640,
          "total_tokens": 755,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 347,
        "total_completion_tokens": 1801,
        "total_tokens": 2148
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 116,
          "completion_tokens": 525,
          "total_tokens": 641,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 116,
          "completion_tokens": 636,
          "total_tokens": 752,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 115,
          "completion_tokens": 640,
          "total_tokens": 755,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 5,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 124,
          "completion_tokens": 601,
          "total_tokens": 725,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 124,
          "completion_tokens": 508,
          "total_tokens": 632,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 123,
          "completion_tokens": 640,
          "total_tokens": 763,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 371,
        "total_completion_tokens": 1749,
        "total_tokens": 2120
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 124,
          "completion_tokens": 601,
          "total_tokens": 725,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 124,
          "completion_tokens": 508,
          "total_tokens": 632,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 123,
          "completion_tokens": 640,
          "total_tokens": 763,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 7,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 133,
          "completion_tokens": 291,
          "total_tokens": 424,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 133,
          "completion_tokens": 595,
          "total_tokens": 728,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 132,
          "completion_tokens": 323,
          "total_tokens": 455,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 398,
        "total_completion_tokens": 1209,
        "total_tokens": 1607
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 133,
          "completion_tokens": 291,
          "total_tokens": 424,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 133,
          "completion_tokens": 595,
          "total_tokens": 728,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 132,
          "completion_tokens": 323,
          "total_tokens": 455,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 9,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 142,
          "completion_tokens": 640,
          "total_tokens": 782,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 142,
          "completion_tokens": 640,
          "total_tokens": 782,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 141,
          "completion_tokens": 365,
          "total_tokens": 506,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 425,
        "total_completion_tokens": 1645,
        "total_tokens": 2070
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 142,
          "completion_tokens": 640,
          "total_tokens": 782,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 142,
          "completion_tokens": 640,
          "total_tokens": 782,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 141,
          "completion_tokens": 365,
          "total_tokens": 506,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 11,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 150,
          "completion_tokens": 338,
          "total_tokens": 488,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 150,
          "completion_tokens": 640,
          "total_tokens": 790,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 149,
          "completion_tokens": 522,
          "total_tokens": 671,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 449,
        "total_completion_tokens": 1500,
        "total_tokens": 1949
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 150,
          "completion_tokens": 338,
          "total_tokens": 488,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 150,
          "completion_tokens": 640,
          "total_tokens": 790,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 149,
          "completion_tokens": 522,
          "total_tokens": 671,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 13,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 157,
          "completion_tokens": 640,
          "total_tokens": 797,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 157,
          "completion_tokens": 640,
          "total_tokens": 797,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 156,
          "completion_tokens": 640,
          "total_tokens": 796,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 470,
        "total_completion_tokens": 1920,
        "total_tokens": 2390
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 157,
          "completion_tokens": 640,
          "total_tokens": 797,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 157,
          "completion_tokens": 640,
          "total_tokens": 797,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 156,
          "completion_tokens": 640,
          "total_tokens": 796,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 15,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 167,
          "completion_tokens": 114,
          "total_tokens": 281,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 167,
          "completion_tokens": 426,
          "total_tokens": 593,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 166,
          "completion_tokens": 640,
          "total_tokens": 806,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 500,
        "total_completion_tokens": 1180,
        "total_tokens": 1680
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 167,
          "completion_tokens": 114,
          "total_tokens": 281,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 167,
          "completion_tokens": 426,
          "total_tokens": 593,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 166,
          "completion_tokens": 640,
          "total_tokens": 806,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 17,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 175,
          "completion_tokens": 416,
          "total_tokens": 591,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 175,
          "completion_tokens": 572,
          "total_tokens": 747,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 174,
          "completion_tokens": 545,
          "total_tokens": 719,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 524,
        "total_completion_tokens": 1533,
        "total_tokens": 2057
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 175,
          "completion_tokens": 416,
          "total_tokens": 591,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 175,
          "completion_tokens": 572,
          "total_tokens": 747,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 174,
          "completion_tokens": 545,
          "total_tokens": 719,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 2,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 590,
          "total_tokens": 704,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 230,
          "total_tokens": 343,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 341,
        "total_completion_tokens": 1460,
        "total_tokens": 1801
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 590,
          "total_tokens": 704,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 230,
          "total_tokens": 343,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    }
  ],
  "white_responses": [
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 \nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4 \nWhite",
          "positional_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d5",
          "neutral_uci": "d7d5",
          "neutral_response": "1. d4 \nd5 Nf3 Nc6",
          "neutral_tokens": {
            "prompt_tokens": 148,
            "completion_tokens": 14,
            "total_tokens": 162,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": "d7d5",
          "consensus_move": "d7d5",
          "source_agents": [
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf3",
              "Nc6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 148,
            "completion_tokens": 14,
            "total_tokens": 162,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 446,
          "total_completion_tokens": 28,
          "total_tokens": 474
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 148,
            "completion_tokens": 14,
            "total_tokens": 162,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 4,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d6",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 \nd6 Nf3 Nc6",
          "aggressive_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 21,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "dxc4",
          "positional_uci": "d5c4",
          "positional_response": "1. d4 d5 2. c4 \ndxc4 b4",
          "positional_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 18,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "dxc4",
          "neutral_uci": "d5c4",
          "neutral_response": "1. d4 d5 2. c4 \ndxc4 b3",
          "neutral_tokens": {
            "prompt_tokens": 155,
            "completion_tokens": 18,
            "total_tokens": 173,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": "d5c4",
          "neutral_uci": "d5c4",
          "consensus_move": "d5c4",
          "source_agents": [
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf3",
              "Nc6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "b4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "b3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 156,
            "completion_tokens": 21,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 156,
            "completion_tokens": 18,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 155,
            "completion_tokens": 18,
            "total_tokens": 173,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 467,
          "total_completion_tokens": 57,
          "total_tokens": 524
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 156,
            "completion_tokens": 21,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 156,
            "completion_tokens": 18,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 155,
            "completion_tokens": 18,
            "total_tokens": 173,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/3p4/2PP4/8/PP2PPPP/RNBQKBNR b KQkq - 0 2"
    },
    {
      "turn": 6,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc6",
          "aggressive_uci": "b8c6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 \nNc6 Bg5",
          "aggressive_tokens": {
            "prompt_tokens": 165,
            "completion_tokens": 28,
            "total_tokens": 193,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bg5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nc6",
          "positional_uci": "b8c6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 \nNc6 e6 Nbd2",
          "positional_tokens": {
            "prompt_tokens": 165,
            "completion_tokens": 30,
            "total_tokens": 195,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e6",
            "Nbd2"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc6",
          "neutral_uci": "b8c6",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 \nNc6 e6",
          "neutral_tokens": {
            "prompt_tokens": 164,
            "completion_tokens": 27,
            "total_tokens": 191,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b8c6",
          "positional_uci": "b8c6",
          "neutral_uci": "b8c6",
          "consensus_move": "b8c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bg5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e6",
              "Nbd2"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 165,
            "completion_tokens": 28,
            "total_tokens": 193,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 165,
            "completion_tokens": 30,
            "total_tokens": 195,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 164,
            "completion_tokens": 27,
            "total_tokens": 191,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 494,
          "total_completion_tokens": 85,
          "total_tokens": 579
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 165,
            "completion_tokens": 28,
            "total_tokens": 193,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 165,
            "completion_tokens": 30,
            "total_tokens": 195,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 164,
            "completion_tokens": 27,
            "total_tokens": 191,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/8/2pP4/5N2/PP2PPPP/RNBQKB1R b KQkq - 1 3"
    },
    {
      "turn": 8,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf6",
          "aggressive_uci": "g8f6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 \nNf6 e3",
          "aggressive_tokens": {
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf6",
          "positional_uci": "g8f6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 \nNf6 e3",
          "positional_tokens": {
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf6",
          "neutral_uci": "g8f6",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 \nNf6 e3",
          "neutral_tokens": {
            "prompt_tokens": 173,
            "completion_tokens": 36,
            "total_tokens": 209,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g8f6",
          "positional_uci": "g8f6",
          "neutral_uci": "g8f6",
          "consensus_move": "g8f6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "e3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 173,
            "completion_tokens": 36,
            "total_tokens": 209,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 521,
          "total_completion_tokens": 108,
          "total_tokens": 629
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 173,
            "completion_tokens": 36,
            "total_tokens": 209,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkbnr/ppp1pppp/2n5/8/2pP4/2N2N2/PP2PPPP/R1BQKB1R b KQkq - 3 4"
    },
    {
      "turn": 10,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e5",
          "aggressive_uci": "e7e5",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 \ne5 Nxe4 Nxd4",
          "aggressive_tokens": {
            "prompt_tokens": 182,
            "completion_tokens": 47,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxe4",
            "Nxd4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bg4",
          "positional_uci": "c8g4",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 Nc2 a6 b3",
          "positional_tokens": {
            "prompt_tokens": 182,
            "completion_tokens": 48,
            "total_tokens": 230,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc2",
            "a6",
            "b3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bg4",
          "neutral_uci": "c8g4",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 Nc2 a6 b3",
          "neutral_tokens": {
            "prompt_tokens": 181,
            "completion_tokens": 48,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc2",
            "a6",
            "b3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e7e5",
          "positional_uci": "c8g4",
          "neutral_uci": "c8g4",
          "consensus_move": "c8g4",
          "source_agents": [
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nxe4",
              "Nxd4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nc2",
              "a6",
              "b3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc2",
              "a6",
              "b3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 182,
            "completion_tokens": 47,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 182,
            "completion_tokens": 48,
            "total_tokens": 230,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 181,
            "completion_tokens": 48,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 545,
          "total_completion_tokens": 143,
          "total_tokens": 688
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 182,
            "completion_tokens": 47,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 182,
            "completion_tokens": 48,
            "total_tokens": 230,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 181,
            "completion_tokens": 48,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp1pppp/2n2n2/8/2pPP3/2N2N2/PP3PPP/R1BQKB1R b KQkq - 0 5"
    },
    {
      "turn": 12,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Na5",
          "aggressive_uci": "c6a5",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 Nc2 e6",
          "aggressive_tokens": {
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc2",
            "e6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Na5",
          "positional_uci": "c6a5",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 Nc2 h6",
          "positional_tokens": {
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc2",
            "h6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Na5",
          "neutral_uci": "c6a5",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 Nc2 e6",
          "neutral_tokens": {
            "prompt_tokens": 189,
            "completion_tokens": 53,
            "total_tokens": 242,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc2",
            "e6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "c6a5",
          "positional_uci": "c6a5",
          "neutral_uci": "c6a5",
          "consensus_move": "c6a5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nc2",
              "e6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nc2",
              "h6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc2",
              "e6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 189,
            "completion_tokens": 53,
            "total_tokens": 242,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 569,
          "total_completion_tokens": 159,
          "total_tokens": 728
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 189,
            "completion_tokens": 53,
            "total_tokens": 242,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/2n2n2/3P4/2p1P1b1/2N2N2/PP3PPP/R1BQKB1R b KQkq - 0 6"
    },
    {
      "turn": 14,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc6",
          "aggressive_uci": "a5c6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 b5 Qxb5 a6",
          "aggressive_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 65,
            "total_tokens": 264,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b5",
            "Qxb5",
            "a6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nc6",
          "positional_uci": "a5c6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 b5 Qxb5",
          "positional_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 63,
            "total_tokens": 262,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b5",
            "Qxb5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc6",
          "neutral_uci": "a5c6",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 b5 Qxb5 a6",
          "neutral_tokens": {
            "prompt_tokens": 198,
            "completion_tokens": 65,
            "total_tokens": 263,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b5",
            "Qxb5",
            "a6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "a5c6",
          "positional_uci": "a5c6",
          "neutral_uci": "a5c6",
          "consensus_move": "a5c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "b5",
              "Qxb5",
              "a6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "b5",
              "Qxb5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "b5",
              "Qxb5",
              "a6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 199,
            "completion_tokens": 65,
            "total_tokens": 264,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 199,
            "completion_tokens": 63,
            "total_tokens": 262,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 198,
            "completion_tokens": 65,
            "total_tokens": 263,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 596,
          "total_completion_tokens": 193,
          "total_tokens": 789
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 199,
            "completion_tokens": 65,
            "total_tokens": 264,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 199,
            "completion_tokens": 63,
            "total_tokens": 262,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 198,
            "completion_tokens": 65,
            "total_tokens": 263,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/5n2/n2P4/Q1p1P1b1/2N2N2/PP3PPP/R1B1KB1R b KQkq - 2 7"
    },
    {
      "turn": 16,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "b6",
          "aggressive_uci": "b7b6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 Bb4 Qe7",
          "aggressive_tokens": {
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bb4",
            "Qe7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "b6",
          "positional_uci": "b7b6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 Bc5 Qe7",
          "positional_tokens": {
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "Qe7"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "b6",
          "neutral_uci": "b7b6",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 Bb4+ Bd7",
          "neutral_tokens": {
            "prompt_tokens": 207,
            "completion_tokens": 72,
            "total_tokens": 279,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bb4+",
            "Bd7"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b7b6",
          "positional_uci": "b7b6",
          "neutral_uci": "b7b6",
          "consensus_move": "b7b6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bb4",
              "Qe7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bc5",
              "Qe7"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bb4+",
              "Bd7"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 207,
            "completion_tokens": 72,
            "total_tokens": 279,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 623,
          "total_completion_tokens": 216,
          "total_tokens": 839
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 207,
            "completion_tokens": 72,
            "total_tokens": 279,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/2P2n2/8/Q1p1P1b1/2N2N2/PP3PPP/R1B1KB1R b KQkq - 0 8"
    },
    {
      "turn": 18,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qe7",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6 b4",
          "aggressive_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6",
            "b4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qe7",
          "positional_uci": null,
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 Nd7 Bb5",
          "positional_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "Nd7",
            "Bb5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qe7",
          "neutral_uci": null,
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6",
          "neutral_tokens": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qe2",
              "a6",
              "b4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qe2",
              "Nd7",
              "Bb5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qe2",
              "a6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 647,
          "total_completion_tokens": 245,
          "total_tokens": 892
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/p1p1pppp/1pP2n2/8/Q1B1P1b1/2N2N2/PP3PPP/R1B1K2R b KQkq - 0 9"
    },
    {
      "turn": 18,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qe7",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 Nd7 b4",
          "aggressive_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "Nd7",
            "b4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qe7",
          "positional_uci": null,
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 Nd7 Bb5",
          "positional_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "Nd7",
            "Bb5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qe7",
          "neutral_uci": null,
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6",
          "neutral_tokens": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qe2",
              "Nd7",
              "b4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qe2",
              "Nd7",
              "Bb5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qe2",
              "a6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 647,
          "total_completion_tokens": 245,
          "total_tokens": 892
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/p1p1pppp/1pP2n2/8/Q1B1P1b1/2N2N2/PP3PPP/R1B1K2R b KQkq - 0 9"
    },
    {
      "turn": 18,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qe7",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6 b4",
          "aggressive_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6",
            "b4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qe7",
          "positional_uci": null,
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 Nd7 Bb5",
          "positional_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "Nd7",
            "Bb5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qe7",
          "neutral_uci": null,
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6",
          "neutral_tokens": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qe2",
              "a6",
              "b4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qe2",
              "Nd7",
              "Bb5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qe2",
              "a6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 647,
          "total_completion_tokens": 245,
          "total_tokens": 892
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/p1p1pppp/1pP2n2/8/Q1B1P1b1/2N2N2/PP3PPP/R1B1K2R b KQkq - 0 9"
    },
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": "e2e4",
          "aggressive_response": "Move: e4\nPlan: e5 Nf3 Nc6",
          "aggressive_tokens": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "Move: e4 \nPlan: e5 Nf3 Nc6",
          "positional_tokens": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": "e2e4",
          "neutral_response": "Move: e4\nPlan: e5 Nf3 Nc6",
          "neutral_tokens": {
            "prompt_tokens": 184,
            "completion_tokens": 16,
            "total_tokens": 200,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e2e4",
          "positional_uci": "e2e4",
          "neutral_uci": "e2e4",
          "consensus_move": "e2e4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5",
              "Nf3",
              "Nc6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Nf3",
              "Nc6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5",
              "Nf3",
              "Nc6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 184,
            "completion_tokens": 16,
            "total_tokens": 200,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 554,
          "total_completion_tokens": 48,
          "total_tokens": 602
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 184,
            "completion_tokens": 16,
            "total_tokens": 200,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 3,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": null,
          "positional_response": "1. e4 e5\nWhite",
          "positional_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\nWhite",
          "neutral_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 452,
          "total_completion_tokens": 27,
          "total_tokens": 479
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 3,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": null,
          "positional_response": "1. e4 e5\nWhite",
          "positional_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\nWhite",
          "neutral_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 452,
          "total_completion_tokens": 27,
          "total_tokens": 479
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 3,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": null,
          "positional_response": "1. e4 e5\nWhite",
          "positional_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\nWhite",
          "neutral_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 452,
          "total_completion_tokens": 27,
          "total_tokens": 479
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 \nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4 \nWhite",
          "positional_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d5",
          "neutral_uci": "d7d5",
          "neutral_response": "1. d4 \nd5 Nf3 Nc6",
          "neutral_tokens": {
            "prompt_tokens": 148,
            "completion_tokens": 14,
            "total_tokens": 162,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": "d7d5",
          "consensus_move": "d7d5",
          "source_agents": [
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf3",
              "Nc6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 148,
            "completion_tokens": 14,
            "total_tokens": 162,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 446,
          "total_completion_tokens": 28,
          "total_tokens": 474
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 148,
            "completion_tokens": 14,
            "total_tokens": 162,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 4,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d6",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 \nd6 Nf3 Nc6",
          "aggressive_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 21,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "dxc4",
          "positional_uci": "d5c4",
          "positional_response": "1. d4 d5 2. c4 \ndxc4 b4",
          "positional_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 18,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "dxc4",
          "neutral_uci": "d5c4",
          "neutral_response": "1. d4 d5 2. c4 \ndxc4 b3",
          "neutral_tokens": {
            "prompt_tokens": 155,
            "completion_tokens": 18,
            "total_tokens": 173,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": "d5c4",
          "neutral_uci": "d5c4",
          "consensus_move": "d5c4",
          "source_agents": [
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf3",
              "Nc6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "b4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "b3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 156,
            "completion_tokens": 21,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 156,
            "completion_tokens": 18,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 155,
            "completion_tokens": 18,
            "total_tokens": 173,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 467,
          "total_completion_tokens": 57,
          "total_tokens": 524
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 156,
            "completion_tokens": 21,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 156,
            "completion_tokens": 18,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 155,
            "completion_tokens": 18,
            "total_tokens": 173,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/3p4/2PP4/8/PP2PPPP/RNBQKBNR b KQkq - 0 2"
    },
    {
      "turn": 6,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc6",
          "aggressive_uci": "b8c6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 \nNc6 Bg5",
          "aggressive_tokens": {
            "prompt_tokens": 165,
            "completion_tokens": 28,
            "total_tokens": 193,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bg5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nc6",
          "positional_uci": "b8c6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 \nNc6 e6 Nbd2",
          "positional_tokens": {
            "prompt_tokens": 165,
            "completion_tokens": 30,
            "total_tokens": 195,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e6",
            "Nbd2"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc6",
          "neutral_uci": "b8c6",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 \nNc6 e6",
          "neutral_tokens": {
            "prompt_tokens": 164,
            "completion_tokens": 27,
            "total_tokens": 191,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b8c6",
          "positional_uci": "b8c6",
          "neutral_uci": "b8c6",
          "consensus_move": "b8c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bg5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e6",
              "Nbd2"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 165,
            "completion_tokens": 28,
            "total_tokens": 193,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 165,
            "completion_tokens": 30,
            "total_tokens": 195,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 164,
            "completion_tokens": 27,
            "total_tokens": 191,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 494,
          "total_completion_tokens": 85,
          "total_tokens": 579
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 165,
            "completion_tokens": 28,
            "total_tokens": 193,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 165,
            "completion_tokens": 30,
            "total_tokens": 195,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 164,
            "completion_tokens": 27,
            "total_tokens": 191,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/8/2pP4/5N2/PP2PPPP/RNBQKB1R b KQkq - 1 3"
    },
    {
      "turn": 8,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf6",
          "aggressive_uci": "g8f6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 \nNf6 e3",
          "aggressive_tokens": {
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e6",
          "positional_uci": "e7e6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 e6 Nf6 Bg4",
          "positional_tokens": {
            "prompt_tokens": 174,
            "completion_tokens": 38,
            "total_tokens": 212,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "Bg4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf6",
          "neutral_uci": "g8f6",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 \nNf6 e3",
          "neutral_tokens": {
            "prompt_tokens": 173,
            "completion_tokens": 36,
            "total_tokens": 209,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g8f6",
          "positional_uci": "e7e6",
          "neutral_uci": "g8f6",
          "consensus_move": "g8f6",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "e3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf6",
              "Bg4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 174,
            "completion_tokens": 38,
            "total_tokens": 212,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 173,
            "completion_tokens": 36,
            "total_tokens": 209,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 521,
          "total_completion_tokens": 110,
          "total_tokens": 631
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 174,
            "completion_tokens": 38,
            "total_tokens": 212,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 173,
            "completion_tokens": 36,
            "total_tokens": 209,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkbnr/ppp1pppp/2n5/8/2pP4/2N2N2/PP2PPPP/R1BQKB1R b KQkq - 3 4"
    },
    {
      "turn": 10,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e5",
          "aggressive_uci": "e7e5",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 \ne5 Nxe4 Nxd4",
          "aggressive_tokens": {
            "prompt_tokens": 182,
            "completion_tokens": 47,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxe4",
            "Nxd4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bg4",
          "positional_uci": "c8g4",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 Nc2 a6 b3",
          "positional_tokens": {
            "prompt_tokens": 182,
            "completion_tokens": 48,
            "total_tokens": 230,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc2",
            "a6",
            "b3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bg4",
          "neutral_uci": "c8g4",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 Nc2 a6 b3",
          "neutral_tokens": {
            "prompt_tokens": 181,
            "completion_tokens": 48,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc2",
            "a6",
            "b3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e7e5",
          "positional_uci": "c8g4",
          "neutral_uci": "c8g4",
          "consensus_move": "c8g4",
          "source_agents": [
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nxe4",
              "Nxd4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nc2",
              "a6",
              "b3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc2",
              "a6",
              "b3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 182,
            "completion_tokens": 47,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 182,
            "completion_tokens": 48,
            "total_tokens": 230,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 181,
            "completion_tokens": 48,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 545,
          "total_completion_tokens": 143,
          "total_tokens": 688
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 182,
            "completion_tokens": 47,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 182,
            "completion_tokens": 48,
            "total_tokens": 230,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 181,
            "completion_tokens": 48,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp1pppp/2n2n2/8/2pPP3/2N2N2/PP3PPP/R1BQKB1R b KQkq - 0 5"
    },
    {
      "turn": 12,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Na5",
          "aggressive_uci": "c6a5",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 Nxd5 exd5",
          "aggressive_tokens": {
            "prompt_tokens": 190,
            "completion_tokens": 54,
            "total_tokens": 244,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxd5",
            "exd5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Na5",
          "positional_uci": "c6a5",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 Nc2 h6",
          "positional_tokens": {
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc2",
            "h6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Na5",
          "neutral_uci": "c6a5",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 Nc2 e6",
          "neutral_tokens": {
            "prompt_tokens": 189,
            "completion_tokens": 53,
            "total_tokens": 242,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc2",
            "e6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "c6a5",
          "positional_uci": "c6a5",
          "neutral_uci": "c6a5",
          "consensus_move": "c6a5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nxd5",
              "exd5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nc2",
              "h6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc2",
              "e6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 190,
            "completion_tokens": 54,
            "total_tokens": 244,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 189,
            "completion_tokens": 53,
            "total_tokens": 242,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 569,
          "total_completion_tokens": 160,
          "total_tokens": 729
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 190,
            "completion_tokens": 54,
            "total_tokens": 244,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 189,
            "completion_tokens": 53,
            "total_tokens": 242,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/2n2n2/3P4/2p1P1b1/2N2N2/PP3PPP/R1BQKB1R b KQkq - 0 6"
    },
    {
      "turn": 14,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc6",
          "aggressive_uci": "a5c6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 b5 Qxb5 a6",
          "aggressive_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 65,
            "total_tokens": 264,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b5",
            "Qxb5",
            "a6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nc6",
          "positional_uci": "a5c6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 b5 Qxb5",
          "positional_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 63,
            "total_tokens": 262,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b5",
            "Qxb5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc6",
          "neutral_uci": "a5c6",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 b5 Qxb5 a6",
          "neutral_tokens": {
            "prompt_tokens": 198,
            "completion_tokens": 65,
            "total_tokens": 263,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b5",
            "Qxb5",
            "a6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "a5c6",
          "positional_uci": "a5c6",
          "neutral_uci": "a5c6",
          "consensus_move": "a5c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "b5",
              "Qxb5",
              "a6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "b5",
              "Qxb5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "b5",
              "Qxb5",
              "a6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 199,
            "completion_tokens": 65,
            "total_tokens": 264,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 199,
            "completion_tokens": 63,
            "total_tokens": 262,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 198,
            "completion_tokens": 65,
            "total_tokens": 263,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 596,
          "total_completion_tokens": 193,
          "total_tokens": 789
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 199,
            "completion_tokens": 65,
            "total_tokens": 264,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 199,
            "completion_tokens": 63,
            "total_tokens": 262,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 198,
            "completion_tokens": 65,
            "total_tokens": 263,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/5n2/n2P4/Q1p1P1b1/2N2N2/PP3PPP/R1B1KB1R b KQkq - 2 7"
    },
    {
      "turn": 16,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "b6",
          "aggressive_uci": "b7b6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 Bb4 Qe7",
          "aggressive_tokens": {
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bb4",
            "Qe7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "b6",
          "positional_uci": "b7b6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 Bc5 Qe7",
          "positional_tokens": {
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "Qe7"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "b6",
          "neutral_uci": "b7b6",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 Bc5 Qe7",
          "neutral_tokens": {
            "prompt_tokens": 207,
            "completion_tokens": 72,
            "total_tokens": 279,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "Qe7"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b7b6",
          "positional_uci": "b7b6",
          "neutral_uci": "b7b6",
          "consensus_move": "b7b6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bb4",
              "Qe7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bc5",
              "Qe7"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bc5",
              "Qe7"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 207,
            "completion_tokens": 72,
            "total_tokens": 279,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 623,
          "total_completion_tokens": 216,
          "total_tokens": 839
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 207,
            "completion_tokens": 72,
            "total_tokens": 279,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/2P2n2/8/Q1p1P1b1/2N2N2/PP3PPP/R1B1KB1R b KQkq - 0 8"
    },
    {
      "turn": 18,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qe7",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6 b4",
          "aggressive_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6",
            "b4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qe7",
          "positional_uci": null,
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 Nd7 Bb5",
          "positional_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "Nd7",
            "Bb5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qe7",
          "neutral_uci": null,
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6",
          "neutral_tokens": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qe2",
              "a6",
              "b4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qe2",
              "Nd7",
              "Bb5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qe2",
              "a6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 647,
          "total_completion_tokens": 245,
          "total_tokens": 892
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/p1p1pppp/1pP2n2/8/Q1B1P1b1/2N2N2/PP3PPP/R1B1K2R b KQkq - 0 9"
    },
    {
      "turn": 18,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qe7",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6 b4",
          "aggressive_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6",
            "b4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qe7",
          "positional_uci": null,
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 Nd7 Bb5",
          "positional_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "Nd7",
            "Bb5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qe7",
          "neutral_uci": null,
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6",
          "neutral_tokens": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qe2",
              "a6",
              "b4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qe2",
              "Nd7",
              "Bb5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qe2",
              "a6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 647,
          "total_completion_tokens": 245,
          "total_tokens": 892
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/p1p1pppp/1pP2n2/8/Q1B1P1b1/2N2N2/PP3PPP/R1B1K2R b KQkq - 0 9"
    },
    {
      "turn": 18,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qe7",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6 b4",
          "aggressive_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6",
            "b4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qe7",
          "positional_uci": null,
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 Nd7 Bb5",
          "positional_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "Nd7",
            "Bb5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qe7",
          "neutral_uci": null,
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6",
          "neutral_tokens": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qe2",
              "a6",
              "b4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qe2",
              "Nd7",
              "Bb5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qe2",
              "a6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 647,
          "total_completion_tokens": 245,
          "total_tokens": 892
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/p1p1pppp/1pP2n2/8/Q1B1P1b1/2N2N2/PP3PPP/R1B1K2R b KQkq - 0 9"
    },
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": "e2e4",
          "aggressive_response": "Move: e4\nPlan: e5 Nf3 Nc6",
          "aggressive_tokens": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "Move: e4 \nPlan: e5 Nf3 Nc6",
          "positional_tokens": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": "e2e4",
          "neutral_response": "Move: e4\nPlan: e5 Nf3 Nc6",
          "neutral_tokens": {
            "prompt_tokens": 184,
            "completion_tokens": 16,
            "total_tokens": 200,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e2e4",
          "positional_uci": "e2e4",
          "neutral_uci": "e2e4",
          "consensus_move": "e2e4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5",
              "Nf3",
              "Nc6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Nf3",
              "Nc6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5",
              "Nf3",
              "Nc6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 184,
            "completion_tokens": 16,
            "total_tokens": 200,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 554,
          "total_completion_tokens": 48,
          "total_tokens": 602
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 184,
            "completion_tokens": 16,
            "total_tokens": 200,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 3,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": null,
          "positional_response": "1. e4 e5\nWhite",
          "positional_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\nWhite",
          "neutral_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 452,
          "total_completion_tokens": 27,
          "total_tokens": 479
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 3,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": null,
          "positional_response": "1. e4 e5\nWhite",
          "positional_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\nWhite",
          "neutral_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 452,
          "total_completion_tokens": 27,
          "total_tokens": 479
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 3,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": null,
          "positional_response": "1. e4 e5\nWhite",
          "positional_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\nWhite",
          "neutral_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 452,
          "total_completion_tokens": 27,
          "total_tokens": 479
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    }
  ],
  "black_responses": [
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": "d2d4",
          "aggressive_response": "d4 d5 2. Nf3 Nf6 3. Bg5 Bf5 4. e3 e6 5. Bd3 Bxd3 6. Qxd3 Be7 7. O-O O-O 8. Nbd2 Nbd7 9. c4 c6 10. Rac1 Rc8 11. cxd5 exd5 12. a3 Re8 13. b4 a6 14. Nb3 Ne4 15. Bxe7 Qxe7 16. Nc5 Ndxc5 17. bxc5 f5 18. Rb1 Rc7 19. Rb2 Qf6 20. Rfb1 Ree7 21. Ne5 Nxc5 22. dxc5 Qxe5 23. Rb4 Qf6 24. Rf4 g6 25. g4 Re4 26. Rxe4 fxe4 27. Qd4 Qxd4 28. exd4 Kf7 29. Kg2 Kf6 30. Kg3 Kg5 31. h4+ Kf6 32. Kf4 h6 33. g5+ hxg5+ 34. hxg5+ Ke6 35. Rh1 Rf7+ 36. Ke3 Kf5 37. Rg1 Rh7 38. f3 Rh3 39. Rf1 Rxf3+ 40. Rxf3+ exf3 41. Kxf3 Kxg5 42. Kg3 Kf5 43. Kf3 g5 44. a4 g4+ 45. Ke3 g3 46. Kf3 g2 47. Kxg2 Ke4 48. Kf2 Kxd4 49. Ke2 Kxc5 50. Kd3 Kb4 51. Kd4 Kxa4 52. Kc5 Kb3 53. Kb6 d4 54. Kxb7 d3 55. Kxc6 d2 56. Kb6 d1=Q 57. Kxa6 Qd5 58. Kb6 Kb4 59. Kc7 Qe6 60. Kb7 Kb5 61. Kc7 Qe7+ 62. Kc8 Kb6 63. Kb8 Qb7# d4 d5 2. Nf3 Nf6 3. Bg5 Bf5 4. e3 e6 5. Bd3 Bxd3 6. Qxd3 Be7 7. O-O O-O 8. Nbd2 Nbd7 9. c4 c6 10. Rac1 Rc8 11. cxd5 exd5 12. a3 Re8 13. b4 a6 14. Nb3 Ne4 15. Bxe7 Qxe7 16. Nc5 Ndxc",
          "aggressive_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Nf3",
            "Nf6",
            "Bg5",
            "Bf5",
            "e3",
            "e6",
            "Bd3",
            "Bxd3",
            "Qxd3",
            "Be7",
            "O-O",
            "O-O",
            "Nbd2",
            "Nbd7",
            "c4",
            "c6",
            "Rac1",
            "Rc8",
            "cxd5",
            "exd5",
            "a3",
            "Re8",
            "b4",
            "a6",
            "Nb3",
            "Ne4",
            "Bxe7",
            "Qxe7",
            "Nc5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 Bc5 4. O-O Nge7 5. Nxe5 Nxe5 6. d4 Bxd4 7. Qxd4 N7c6 8. Bxc6 Nxc6 9. Qxg7 Rf8 10. Bh6 Qe7 11. Nc3 d6 12. Nd5 Qe5 13. Qxf8+ Kd7 14. Qxf7+ Kd8 15. Qxc7+ Ke8 16. Qxh7 Be6 17. Nc7+ Kd8 18. Nxa8 Nd4 19. Qc7+ Ke8 20. Qxb7 Ne2+ 21. Kh1 Qh5 22. Nc7+ Kf7 23. Nxe6+ Kxe6 24. Qd5+ Qxd5 25. exd5+ Kxd5 26. Rad1+ Kc4 27. Rfe1 Nd4 28. Re4 d5 29. Rexd4+ Kc5 30. Rxd5+ Kc6 31. Rd6+ Kc5 32. R1d5+ Kc4 33. b3+ Kc3 34. Rc6+ Kb2 35. Bg7+ Kxa2 36. Ra5+ Kb1 37. Ra1# 1-0 e4 Nf6 d4 Nxd4 Nxe5 Nxe5 d4 Bxd4 Qxd4 N7c6 Bxc6 Nxc6 Qxg7 Rf8 Bh6 Qe7 Nc3 d6 Nd5 Qe5 Qxf8+ Kd7 Qxf7+ Kd8 Qxc7+ Ke8 Qxh7 Be6 Nc7+ Kd8 Nxa8 Nd4 Qc7+ Ke8 Qxb7 Ne2+ Kh1 Qh5 Nc7+ Kf7 Nxe6+ Kxe6 Qd5+ Qxd5 exd5+ Kxd5 Rad1+ Kc4 Rfe1 Nd4 Re4 d5 Rexd4+ Kc5 Rxd5+ Kc6 Rd6+ Kc5 R1d5+ Kc4 b3+ Kc3 Rc5+ Kb2 Rc2+ Kxc2 a5 Rc6+ Kb4 Rd4+ Ka3 Ra4#",
          "positional_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 547,
            "total_tokens": 657,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "d4",
            "Nxd4",
            "Nxe5",
            "Nxe5",
            "d4",
            "Bxd4",
            "Qxd4",
            "N7c6",
            "Bxc6",
            "Nxc6",
            "Qxg7",
            "Rf8",
            "Bh6",
            "Qe7",
            "Nc3",
            "d6",
            "Nd5",
            "Qe5",
            "Qxf8+",
            "Kd7",
            "Qxf7+",
            "Kd8",
            "Qxc7+",
            "Ke8",
            "Qxh7",
            "Be6",
            "Nc7+",
            "Kd8",
            "Nxa8",
            "Nd4",
            "Qc7+",
            "Ke8",
            "Qxb7",
            "Ne2+",
            "Kh1",
            "Qh5",
            "Nc7+",
            "Kf7",
            "Nxe6+",
            "Kxe6",
            "Qd5+",
            "Qxd5",
            "exd5+",
            "Kxd5",
            "Rad1+",
            "Kc4",
            "Rfe1",
            "Nd4",
            "Re4",
            "d5",
            "Rexd4+",
            "Kc5",
            "Rxd5+",
            "Kc6",
            "Rd6+",
            "Kc5",
            "R1d5+",
            "Kc4",
            "b3+",
            "Kc3",
            "Rc5+",
            "Kb2",
            "Rc2+",
            "Kxc2",
            "a5",
            "Rc6+",
            "Kb4",
            "Rd4+",
            "Ka3",
            "Ra4#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": "d2d4",
          "neutral_response": "d4 Nf6\nd4 Nf6 Nc3 d5 Bg5 e6 e4 dxe4 Nxe4 Be7 Nxf6+ Bxf6 Bxf6 Qxf6 Nf3 O-O Bd3 Nc6 O-O Rd8 c3 e5 dxe5 Nxe5 Nxe5 Qxe5 Re1 Qd6 Re3 Bf5 Qe2 Bxd3 Qe1 Bb5 a4 Bc6 b4 Re8 b5 Rxe3 Qxe3 Bd7 Re1 h6 c4 Re8 Qc1 Rxe1+ Qxe1 Qe6 Qc1 Qe4 h3 Be6 c5 Qxa4 b6 axb6 cxb6 cxb6 Qxb6 Qd1+ Kh2 Qd6+ g3 Qd5 Qb8+ Kh7 Qf4 b5 h4 Qc4 Qe3 b4 f4 b3 f5 b2 fxe6 b1=Q exf7 Qbc2+ Kh3 Qc8+ g4 Q8c3+ Qg3 Qxg3+ Kxg3 Qd3+ Kf4 Qd4+ Kg3 Qe3+ Kg2 Qe2+ Kg3 Qe1+ Kg2 Qxh4 f8=Q Qxg4+ Kf2 Qf5+ Qxf5+ Qxf5+ Kg3 Kg6 Kh4 h5 Kg3 Kg5 Kh3 h4 Kg2 Kg4 Kh2 h3 Kg1 Kg3 Kh1 h2+ Kg2 Kh4 Kxh2 g5 Kg2 g4 Kh1 g3 Kg2 Kg4 Kg1 Kf3 Kh2 Kf2 Kh3 g2 Kh4 g1=Q Kh5 Kf4 Kh6 Kf3 Kh4 Qg4#\nd4 Nf6 Nc3 d5 Bg5 e6 e4 dxe4 Nxe4 Be7 Nxf6+ Bxf6 Bxf6 Qxf6 Nf3 O-O Bd3 Nc6 O-O Rd8 c3 e5 dxe5 Nxe5 Nxe5 Qxe5 Re1 Qd6 Re3 Bf5 Qe2 Bxd3 Qe1 Bb5 a4 Bc6 b4 Re8 b5 Rxe3 Qxe3 Bd7 Re1 h6 c4 Re8 Qc1 Rxe1+ Qxe1 Qe6 Qc1 Qe4 h3 Be6 c5 Qxa4 b6 axb6 cxb6 cxb6 Qxb6 Qd1+ Kh2 Qd6+ g3 Qd5 Qb8+ Kh7 Qf4 b5 h4 Qc4 Qe3 b4 f4 b3 f5 Bxf5 Qf3 Qc2+ Kg1 b2 g4 b1=Q+ Qf1 Qxf1+ Kxf1 Bxg4 Kf2 Kg6",
          "neutral_tokens": {
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf6",
            "Nc3",
            "d5",
            "Bg5",
            "e6",
            "e4",
            "dxe4",
            "Nxe4",
            "Be7",
            "Nxf6+",
            "Bxf6",
            "Bxf6",
            "Qxf6",
            "Nf3",
            "O-O",
            "Bd3",
            "Nc6",
            "O-O",
            "Rd8",
            "c3",
            "e5",
            "dxe5",
            "Nxe5",
            "Nxe5",
            "Qxe5",
            "Re1",
            "Qd6",
            "Re3",
            "Bf5",
            "Qe2",
            "Bxd3",
            "Qe1",
            "Bb5",
            "a4",
            "Bc6",
            "b4",
            "Re8",
            "b5",
            "Rxe3",
            "Qxe3",
            "Bd7",
            "Re1",
            "h6",
            "c4",
            "Re8",
            "Qc1",
            "Rxe1+",
            "Qxe1",
            "Qe6",
            "Qc1",
            "Qe4",
            "h3",
            "Be6",
            "c5",
            "Qxa4",
            "b6",
            "axb6",
            "cxb6",
            "cxb6",
            "Qxb6",
            "Qd1+",
            "Kh2",
            "Qd6+",
            "g3",
            "Qd5",
            "Qb8+",
            "Kh7",
            "Qf4",
            "b5",
            "h4",
            "Qc4",
            "Qe3",
            "b4",
            "f4",
            "b3",
            "f5",
            "Bxf5",
            "Qf3",
            "Qc2+",
            "Kg1",
            "b2",
            "g4",
            "b1=Q+",
            "Qf1",
            "Qxf1+",
            "Kxf1",
            "Bxg4",
            "Kf2",
            "Kg6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d2d4",
          "positional_uci": "e2e4",
          "neutral_uci": "d2d4",
          "consensus_move": "d2d4",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d5",
              "Nf3",
              "Nf6",
              "Bg5",
              "Bf5",
              "e3",
              "e6",
              "Bd3",
              "Bxd3",
              "Qxd3",
              "Be7",
              "O-O",
              "O-O",
              "Nbd2",
              "Nbd7",
              "c4",
              "c6",
              "Rac1",
              "Rc8",
              "cxd5",
              "exd5",
              "a3",
              "Re8",
              "b4",
              "a6",
              "Nb3",
              "Ne4",
              "Bxe7",
              "Qxe7",
              "Nc5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf6",
              "d4",
              "Nxd4",
              "Nxe5",
              "Nxe5",
              "d4",
              "Bxd4",
              "Qxd4",
              "N7c6",
              "Bxc6",
              "Nxc6",
              "Qxg7",
              "Rf8",
              "Bh6",
              "Qe7",
              "Nc3",
              "d6",
              "Nd5",
              "Qe5",
              "Qxf8+",
              "Kd7",
              "Qxf7+",
              "Kd8",
              "Qxc7+",
              "Ke8",
              "Qxh7",
              "Be6",
              "Nc7+",
              "Kd8",
              "Nxa8",
              "Nd4",
              "Qc7+",
              "Ke8",
              "Qxb7",
              "Ne2+",
              "Kh1",
              "Qh5",
              "Nc7+",
              "Kf7",
              "Nxe6+",
              "Kxe6",
              "Qd5+",
              "Qxd5",
              "exd5+",
              "Kxd5",
              "Rad1+",
              "Kc4",
              "Rfe1",
              "Nd4",
              "Re4",
              "d5",
              "Rexd4+",
              "Kc5",
              "Rxd5+",
              "Kc6",
              "Rd6+",
              "Kc5",
              "R1d5+",
              "Kc4",
              "b3+",
              "Kc3",
              "Rc5+",
              "Kb2",
              "Rc2+",
              "Kxc2",
              "a5",
              "Rc6+",
              "Kb4",
              "Rd4+",
              "Ka3",
              "Ra4#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf6",
              "Nc3",
              "d5",
              "Bg5",
              "e6",
              "e4",
              "dxe4",
              "Nxe4",
              "Be7",
              "Nxf6+",
              "Bxf6",
              "Bxf6",
              "Qxf6",
              "Nf3",
              "O-O",
              "Bd3",
              "Nc6",
              "O-O",
              "Rd8",
              "c3",
              "e5",
              "dxe5",
              "Nxe5",
              "Nxe5",
              "Qxe5",
              "Re1",
              "Qd6",
              "Re3",
              "Bf5",
              "Qe2",
              "Bxd3",
              "Qe1",
              "Bb5",
              "a4",
              "Bc6",
              "b4",
              "Re8",
              "b5",
              "Rxe3",
              "Qxe3",
              "Bd7",
              "Re1",
              "h6",
              "c4",
              "Re8",
              "Qc1",
              "Rxe1+",
              "Qxe1",
              "Qe6",
              "Qc1",
              "Qe4",
              "h3",
              "Be6",
              "c5",
              "Qxa4",
              "b6",
              "axb6",
              "cxb6",
              "cxb6",
              "Qxb6",
              "Qd1+",
              "Kh2",
              "Qd6+",
              "g3",
              "Qd5",
              "Qb8+",
              "Kh7",
              "Qf4",
              "b5",
              "h4",
              "Qc4",
              "Qe3",
              "b4",
              "f4",
              "b3",
              "f5",
              "Bxf5",
              "Qf3",
              "Qc2+",
              "Kg1",
              "b2",
              "g4",
              "b1=Q+",
              "Qf1",
              "Qxf1+",
              "Kxf1",
              "Bxg4",
              "Kf2",
              "Kg6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 110,
            "completion_tokens": 547,
            "total_tokens": 657,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 329,
          "total_completion_tokens": 1827,
          "total_tokens": 2156
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 110,
            "completion_tokens": 547,
            "total_tokens": 657,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 3,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "c4",
          "aggressive_uci": "c2c4",
          "aggressive_response": "2. c4 c6 3. Nc3 Nf6 4. Nf3 dxc4 5. a4 Bf5 6. e3 e6 7. Bxc4 Bb4 8. O-O O-O 9. Qe2 Nbd7 10. e4 Bg6 11. Bd3 Bh5 12. e5 Nd5 13. Nxd5 cxd5 14. Qe3 Bg6 15. Bxg6 hxg6 16. Ng5 Re8 17. Qh3 Nf8 18. Rd1 Rc8 19. Qb3 Qb6 20. Qf3 Re7 21. Qd3 Rec7 22. Be3 Be7 23. Nf3 Qxb2 24. Rab1 Qc2 25. Qxc2 Rxc2 26. Rxb7 R2c7 27. Rdb1 Nd7 28. g3 Nb6 29. Rxc7 Rxc7 30. a5 Nc4 31. Rb8+ Kh7 32. Bg5 f6 33. Bf4 g5 34. exf6 gxf4 35. fxe7 Rxe7 36. a6 fxg3 37. hxg3 Kg6 38. Rb7 Kf6 39. Ne5 Nxe5 40. dxe5+ Kf7 41. f4 Ke8 42. Kf2 Kd8 43. Rb8+ Kc7 44. Rb7+ Kd8 45. Rb8+ Kc7 46. Rb7+ Kd8 47. Rb8+ Kc7 48. Rb7+ Kd8 49. Rb8+ Kc7 50. Rb7+ Kd8 51. Rb8+ Kc7 52. Rb7+ Kd8 53. Rb8+ Kc7 54. Rb7+ Kd8 55. Rb8+ Kc7 56. Rb7+ Kd8 57. Rb8+ Kc7 58. Rb7+ Kd8 59. Rb8+ Kc7 60. Rb7+ Kd8 61. Rb8+ Kc7 62. Rb7+ Kd8 63. Rb8+ Kc7 64. Rb7+ Kd8 65. Rb8+ Kc7 66. Rb7+ Kd8 67. Rb8+ Kc7 68. Rb7+ Kd8 69. Rb8+ Kc7 70. Rb7+ Kd8 71. Rb8+ Kc7 72. Rb7+ Kd8",
          "aggressive_tokens": {
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "c6",
            "Nc3",
            "Nf6",
            "Nf3",
            "dxc4",
            "a4",
            "Bf5",
            "e3",
            "e6",
            "Bxc4",
            "Bb4",
            "O-O",
            "O-O",
            "Qe2",
            "Nbd7",
            "e4",
            "Bg6",
            "Bd3",
            "Bh5",
            "e5",
            "Nd5",
            "Nxd5",
            "cxd5",
            "Qe3",
            "Bg6",
            "Bxg6",
            "hxg6",
            "Ng5",
            "Re8",
            "Qh3",
            "Nf8",
            "Rd1",
            "Rc8",
            "Qb3",
            "Qb6",
            "Qf3",
            "Re7",
            "Qd3",
            "Rec7",
            "Be3",
            "Be7",
            "Nf3",
            "Qxb2",
            "Rab1",
            "Qc2",
            "Qxc2",
            "Rxc2",
            "Rxb7",
            "R2c7",
            "Rdb1",
            "Nd7",
            "g3",
            "Nb6",
            "Rxc7",
            "Rxc7",
            "a5",
            "Nc4",
            "Rb8+",
            "Kh7",
            "Bg5",
            "f6",
            "Bf4",
            "g5",
            "exf6",
            "gxf4",
            "fxe7",
            "Rxe7",
            "a6",
            "fxg3",
            "hxg3",
            "Kg6",
            "Rb7",
            "Kf6",
            "Ne5",
            "Nxe5",
            "dxe5+",
            "Kf7",
            "f4",
            "Ke8",
            "Kf2",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf3",
          "positional_uci": "g1f3",
          "positional_response": "2. Nf3 Nf6 3. c4 c6 4. Nc3 dxc4 5. a4 Bf5 6. e3 e6 7. Bxc4 Bb4 8. O-O O-O 9. Qe2 Bg6 10. Ne5 Nbd7 11. Nxg6 hxg6 12. Rd1 Qa5 13. Bd2 e5 14. dxe5 Nxe5 15. Bb3 Rad8 16. Be1 Rxd1 17. Rxd1 Rd8 18. Rxd8+ Qxd8 19. Qd1 Qxd1 20. Bxd1 Nd3 21. Kf1 Nxb2 22. Bb3 Nd3 23. Bd2 Nc5 24. Bc2 Bxc3 25. Bxc3 Nd5 26. Bd4 b6 27. a5 Kf8 28. axb6 axb6 29. Ke2 Ne6 30. Bb2 Ke7 31. g3 Kd6 32. e4 Nb4 33. Bb3 c5 34. f4 b5 35. f5 gxf5 36. exf5 Nd4+ 37. Bxd4 cxd4 38. Bxf7 Ke5 39. Be8 Kxf5 40. Bxb5 Kg4 41. Bd7+ Kg5 42. h4+ Kf6 43. g4 Nd5 44. Kd3 Ke5 45. Bf5 Nf4+ 46. Kd2 Ng2 47. h5 Ne3 48. Bc8 Kf4 49. Kd3 Nxg4 50. Kxd4 Kg5 51. Ke4 Kxh5 52. Kf4 Nf6 53. Bb7 g5+ 54. Kg3 Kg6 55. Bc6 Kf5 56. Bb7 Ne4+ 57. Kg2 Kf4 58. Bc6 Nf6 59. Bb7 Ng4 60. Bc6 Ne3+ 61. Kg1 Kg3 62. Bb7 Nf5 63. Bc6 Nd4 64. Bb7 Nf3+ 65. Kf1 g4 66. Bc6 Kh2 67. Kf2 Ne5 68. Bd5 g3+ 69. Kf1 Ng4 70. Ke2 Nf6 71. Bc6 Nh5 72. Kf1 Nf4 73. Bf3 Nd3 74. Be4 Nf2 75. Bc6 Ng4 76. Ke2 Ne5 77. Be4 Kg1 78. Ke3 Kf1 79. Kf4 Kf",
          "positional_tokens": {
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Kf1",
            "g4",
            "Bc6",
            "Kh2",
            "Kf2",
            "Ne5",
            "Bd5",
            "g3+",
            "Kf1",
            "Ng4",
            "Ke2",
            "Nf6",
            "Bc6",
            "Nh5",
            "Kf1",
            "Nf4",
            "Bf3",
            "Nd3",
            "Be4",
            "Nf2",
            "Bc6",
            "Ng4",
            "Ke2",
            "Ne5",
            "Be4",
            "Kg1",
            "Ke3",
            "Kf1",
            "Kf4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "c4",
          "neutral_uci": "c2c4",
          "neutral_response": "2. c4 c6 *\n3. Nc3 Nf6 *\n4. Nf3 e6 *\n5. Bg5 Nbd7 *\n6. e3 Qa5 *\n7. Nd2 Bb4 *\n8. Qc2 O-O *\n9. Be2 dxc4 *\n10. Bxf6 Nxf6 *\n11. Nxc4 Qc7 *\n12. O-O b6 *\n13. Rac1 Bb7 *\n14. a3 Be7 *\n15. b4 Rac8 *\n16. Qb3 Qb8 *\n17. Rfd1 Rfd8 *\n18. Ne5 c5 *\n19. bxc5 bxc5 *\n20. Rb1 cxd4 *\n21. exd4 Ba8 *\n22. Qxb8 Rxb8 *\n23. Rxb8 Rxb8 *\n24. Nb5 a6 *\n25. Nc7 Bb7 *\n26. Rb1 Bd6 *\n27. Nxa6 Bxa6 *\n28. Rxb8+ Bxb8 *\n29. Bxa6 Bxe5 *\n30. dxe5 Nd7 *\n31. f4 Nc5 *\n32. Bb5 Kf8 *\n33. Kf2 Ke7 *\n34. Ke3 Kd8 *\n35. Kd4 Nb7 *\n36. a4 Kc7 *\n37. Be8 Nd8 *\n38. Kc5 Nb7+ *\n39. Kb5 Nd8 *\n40. a5 Kb7 *\n41. a6+ Ka7 *\n42. g4 h6 *\n43. h4 g6 *\n44. g5 hxg5 *\n45. hxg5 Ka8 *\n46. Kb6 Kb8 *\n47. a7+ Ka8 *\n48. Ka6 Nb7 *\n49. Bc6 f5 *\n50. Bxb7# f5 *\n51. exf6 e5 *\n52. f7 exf4 *\n53. f8=Q# f3 *\n54. Qc8# f2 *\n55. Qa8# f1=Q *\n56. Bxf1# f4 *\n57. Qb7# f3 *\n58. Qc8# f2 *\n59. Qb7# f1=Q *\n60. Bxf1# f4 *\n61. Qc8# f3 *\n62. Qb7# f2 *\n63. Qc8# f1=Q *\n64. Bxf1# f4 *\n65. Qc8# f3 *\n66. Qb7# f2 *\n67. Qc8# f1=Q *\n68. Bxf1# f4 *\n69. Qc8#",
          "neutral_tokens": {
            "prompt_tokens": 115,
            "completion_tokens": 640,
            "total_tokens": 755,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "c6",
            "Nc3",
            "Nf6",
            "Nf3",
            "e6",
            "Bg5",
            "Nbd7",
            "e3",
            "Qa5",
            "Nd2",
            "Bb4",
            "Qc2",
            "O-O",
            "Be2",
            "dxc4",
            "Bxf6",
            "Nxf6",
            "Nxc4",
            "Qc7",
            "O-O",
            "b6",
            "Rac1",
            "Bb7",
            "a3",
            "Be7",
            "b4",
            "Rac8",
            "Qb3",
            "Qb8",
            "Rfd1",
            "Rfd8",
            "Ne5",
            "c5",
            "bxc5",
            "bxc5",
            "Rb1",
            "cxd4",
            "exd4",
            "Ba8",
            "Qxb8",
            "Rxb8",
            "Rxb8",
            "Rxb8",
            "Nb5",
            "a6",
            "Nc7",
            "Bb7",
            "Rb1",
            "Bd6",
            "Nxa6",
            "Bxa6",
            "Rxb8+",
            "Bxb8",
            "Bxa6",
            "Bxe5",
            "dxe5",
            "Nd7",
            "f4",
            "Nc5",
            "Bb5",
            "Kf8",
            "Kf2",
            "Ke7",
            "Ke3",
            "Kd8",
            "Kd4",
            "Nb7",
            "a4",
            "Kc7",
            "Be8",
            "Nd8",
            "Kc5",
            "Nb7+",
            "Kb5",
            "Nd8",
            "a5",
            "Kb7",
            "a6+",
            "Ka7",
            "g4",
            "h6",
            "h4",
            "g6",
            "g5",
            "hxg5",
            "hxg5",
            "Ka8",
            "Kb6",
            "Kb8",
            "a7+",
            "Ka8",
            "Ka6",
            "Nb7",
            "Bc6",
            "f5",
            "Bxb7#",
            "f5",
            "exf6",
            "e5",
            "f7",
            "exf4",
            "f8=Q#",
            "f3",
            "Qc8#",
            "f2",
            "Qa8#",
            "f1=Q",
            "Bxf1#",
            "f4",
            "Qb7#",
            "f3",
            "Qc8#",
            "f2",
            "Qb7#",
            "f1=Q",
            "Bxf1#",
            "f4",
            "Qc8#",
            "f3",
            "Qb7#",
            "f2",
            "Qc8#",
            "f1=Q",
            "Bxf1#",
            "f4",
            "Qc8#",
            "f3",
            "Qb7#",
            "f2",
            "Qc8#",
            "f1=Q",
            "Bxf1#",
            "f4",
            "Qc8#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "c2c4",
          "positional_uci": "g1f3",
          "neutral_uci": "c2c4",
          "consensus_move": "c2c4",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "c6",
              "Nc3",
              "Nf6",
              "Nf3",
              "dxc4",
              "a4",
              "Bf5",
              "e3",
              "e6",
              "Bxc4",
              "Bb4",
              "O-O",
              "O-O",
              "Qe2",
              "Nbd7",
              "e4",
              "Bg6",
              "Bd3",
              "Bh5",
              "e5",
              "Nd5",
              "Nxd5",
              "cxd5",
              "Qe3",
              "Bg6",
              "Bxg6",
              "hxg6",
              "Ng5",
              "Re8",
              "Qh3",
              "Nf8",
              "Rd1",
              "Rc8",
              "Qb3",
              "Qb6",
              "Qf3",
              "Re7",
              "Qd3",
              "Rec7",
              "Be3",
              "Be7",
              "Nf3",
              "Qxb2",
              "Rab1",
              "Qc2",
              "Qxc2",
              "Rxc2",
              "Rxb7",
              "R2c7",
              "Rdb1",
              "Nd7",
              "g3",
              "Nb6",
              "Rxc7",
              "Rxc7",
              "a5",
              "Nc4",
              "Rb8+",
              "Kh7",
              "Bg5",
              "f6",
              "Bf4",
              "g5",
              "exf6",
              "gxf4",
              "fxe7",
              "Rxe7",
              "a6",
              "fxg3",
              "hxg3",
              "Kg6",
              "Rb7",
              "Kf6",
              "Ne5",
              "Nxe5",
              "dxe5+",
              "Kf7",
              "f4",
              "Ke8",
              "Kf2",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Kf1",
              "g4",
              "Bc6",
              "Kh2",
              "Kf2",
              "Ne5",
              "Bd5",
              "g3+",
              "Kf1",
              "Ng4",
              "Ke2",
              "Nf6",
              "Bc6",
              "Nh5",
              "Kf1",
              "Nf4",
              "Bf3",
              "Nd3",
              "Be4",
              "Nf2",
              "Bc6",
              "Ng4",
              "Ke2",
              "Ne5",
              "Be4",
              "Kg1",
              "Ke3",
              "Kf1",
              "Kf4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "c6",
              "Nc3",
              "Nf6",
              "Nf3",
              "e6",
              "Bg5",
              "Nbd7",
              "e3",
              "Qa5",
              "Nd2",
              "Bb4",
              "Qc2",
              "O-O",
              "Be2",
              "dxc4",
              "Bxf6",
              "Nxf6",
              "Nxc4",
              "Qc7",
              "O-O",
              "b6",
              "Rac1",
              "Bb7",
              "a3",
              "Be7",
              "b4",
              "Rac8",
              "Qb3",
              "Qb8",
              "Rfd1",
              "Rfd8",
              "Ne5",
              "c5",
              "bxc5",
              "bxc5",
              "Rb1",
              "cxd4",
              "exd4",
              "Ba8",
              "Qxb8",
              "Rxb8",
              "Rxb8",
              "Rxb8",
              "Nb5",
              "a6",
              "Nc7",
              "Bb7",
              "Rb1",
              "Bd6",
              "Nxa6",
              "Bxa6",
              "Rxb8+",
              "Bxb8",
              "Bxa6",
              "Bxe5",
              "dxe5",
              "Nd7",
              "f4",
              "Nc5",
              "Bb5",
              "Kf8",
              "Kf2",
              "Ke7",
              "Ke3",
              "Kd8",
              "Kd4",
              "Nb7",
              "a4",
              "Kc7",
              "Be8",
              "Nd8",
              "Kc5",
              "Nb7+",
              "Kb5",
              "Nd8",
              "a5",
              "Kb7",
              "a6+",
              "Ka7",
              "g4",
              "h6",
              "h4",
              "g6",
              "g5",
              "hxg5",
              "hxg5",
              "Ka8",
              "Kb6",
              "Kb8",
              "a7+",
              "Ka8",
              "Ka6",
              "Nb7",
              "Bc6",
              "f5",
              "Bxb7#",
              "f5",
              "exf6",
              "e5",
              "f7",
              "exf4",
              "f8=Q#",
              "f3",
              "Qc8#",
              "f2",
              "Qa8#",
              "f1=Q",
              "Bxf1#",
              "f4",
              "Qb7#",
              "f3",
              "Qc8#",
              "f2",
              "Qb7#",
              "f1=Q",
              "Bxf1#",
              "f4",
              "Qc8#",
              "f3",
              "Qb7#",
              "f2",
              "Qc8#",
              "f1=Q",
              "Bxf1#",
              "f4",
              "Qc8#",
              "f3",
              "Qb7#",
              "f2",
              "Qc8#",
              "f1=Q",
              "Bxf1#",
              "f4",
              "Qc8#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 115,
            "completion_tokens": 640,
            "total_tokens": 755,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 347,
          "total_completion_tokens": 1920,
          "total_tokens": 2267
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 115,
            "completion_tokens": 640,
            "total_tokens": 755,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/3p4/3P4/8/PPP1PPPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 5,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf3",
          "aggressive_uci": "g1f3",
          "aggressive_response": "3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. dxc5 Qxd1 8. Rxd1 Bxc5 9. Nbd2 O-O 10. Nb3 Be7 11. Be2 Nc6 12. Bd2 e5 13. Bc3 e4 14. Nfd4 Nxd4 15. Bxd4 Be6 16. Nc5 Bxc5 17. Bxc5 Rfd8 18. Bd4 Rac8 19. Bc3 Nd5 20. Ba5 b6 21. Be1 a5 22. Rac1 Rxc1 23. Rxc1 Rc8 24. Rxc8+ Bxc8 25. Bc4 Be6 26. Bxd5 Bxd5 27. a3 f5 28. Bc3 Kf7 29. Bd4 b5 30. Bb6 a4 31. Bd4 g6 32. Kf1 Ke6 33. Ke1 Bc4 34. Kd2 Kd5 35. Kc3 Bf1 36. g3 Bc4 37. Kb4 Bf1 38. Ka5 Kc4 39. Kb6 Kb3 40. Kc6 Kc2 41. Kd6 Kd2 42. Ke6 Ke2 43. Kf6 Kxf2 44. Kg7 Kg2 45. Kxh7 Kxh2 46. Kxg6 Bh3 47. Be5 Kg2 48. Kf6 Kf3 49. Bf4 Ke2 50. Ke5 Kd3 51. Kd5 Kc2 52. Be5 Kd3 53. Bd4 Kc2 54. Kc5 Bf1 55. Kd5 Kd3 56. Ke5 Bh3 57. Kf4 Kc2 58. Kg5 Kd3 59. Kh4 Bg4 60. Kg5 Kc2 61. Kf4 Kd3 62. Ke5 Kc2 63. Kd5 Kd3 64. Kc5 Bh3 65. Kxb5 Bf1 66. Kxa4 Kc4 67. b3+ Kd5 68. Kb4 Kc6 69. a4 Bd3 70. Kc3 Be2 71. b4 Bd1 72. b5+ Kb7 73. Kb4 Be2 74. a5 Bd3 75. a6+ Ka8 76. Ka5 Bc4 77. b6 Bd5 78. b7+ Bxb7 79. axb7+ Kxb7 80. Kb5 Kc7 81. K",
          "aggressive_tokens": {
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf6",
            "e3",
            "e6",
            "Bxc4",
            "c5",
            "O-O",
            "a6",
            "dxc5",
            "Qxd1",
            "Rxd1",
            "Bxc5",
            "Nbd2",
            "O-O",
            "Nb3",
            "Be7",
            "Be2",
            "Nc6",
            "Bd2",
            "e5",
            "Bc3",
            "e4",
            "Nfd4",
            "Nxd4",
            "Bxd4",
            "Be6",
            "Nc5",
            "Bxc5",
            "Bxc5",
            "Rfd8",
            "Bd4",
            "Rac8",
            "Bc3",
            "Nd5",
            "Ba5",
            "b6",
            "Be1",
            "a5",
            "Rac1",
            "Rxc1",
            "Rxc1",
            "Rc8",
            "Rxc8+",
            "Bxc8",
            "Bc4",
            "Be6",
            "Bxd5",
            "Bxd5",
            "a3",
            "f5",
            "Bc3",
            "Kf7",
            "Bd4",
            "b5",
            "Bb6",
            "a4",
            "Bd4",
            "g6",
            "Kf1",
            "Ke6",
            "Ke1",
            "Bc4",
            "Kd2",
            "Kd5",
            "Kc3",
            "Bf1",
            "g3",
            "Bc4",
            "Kb4",
            "Bf1",
            "Ka5",
            "Kc4",
            "Kb6",
            "Kb3",
            "Kc6",
            "Kc2",
            "Kd6",
            "Kd2",
            "Ke6",
            "Ke2",
            "Kf6",
            "Kxf2",
            "Kg7",
            "Kg2",
            "Kxh7",
            "Kxh2",
            "Kxg6",
            "Bh3",
            "Be5",
            "Kg2",
            "Kf6",
            "Kf3",
            "Bf4",
            "Ke2",
            "Ke5",
            "Kd3",
            "Kd5",
            "Kc2",
            "Be5",
            "Kd3",
            "Bd4",
            "Kc2",
            "Kc5",
            "Bf1",
            "Kd5",
            "Kd3",
            "Ke5",
            "Bh3",
            "Kf4",
            "Kc2",
            "Kg5",
            "Kd3",
            "Kh4",
            "Bg4",
            "Kg5",
            "Kc2",
            "Kf4",
            "Kd3",
            "Ke5",
            "Kc2",
            "Kd5",
            "Kd3",
            "Kc5",
            "Bh3",
            "Kxb5",
            "Bf1",
            "Kxa4",
            "Kc4",
            "b3+",
            "Kd5",
            "Kb4",
            "Kc6",
            "a4",
            "Bd3",
            "Kc3",
            "Be2",
            "b4",
            "Bd1",
            "b5+",
            "Kb7",
            "Kb4",
            "Be2",
            "a5",
            "Bd3",
            "a6+",
            "Ka8",
            "Ka5",
            "Bc4",
            "b6",
            "Bd5",
            "b7+",
            "Bxb7",
            "axb7+",
            "Kxb7",
            "Kb5",
            "Kc7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf3",
          "positional_uci": "g1f3",
          "positional_response": "3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. dxc5 Qxd1 8. Rxd1 Bxc5 9. b3 O-O 10. Bb2 b5 11. Be2 Bb7 12. Nbd2 Nbd7 13. Rac1 Rac8 14. a4 bxa4 15. bxa4 Nb6 16. a5 Na4 17. Bxf6 gxf6 18. Nb3 Bb4 19. Rxc8 Rxc8 20. Rd7 Bd5 21. Nfd4 Nc5 22. Nxc5 Rxc5 23. h3 Rxa5 24. Bh5 e5 25. Nf5 Be6 26. Rd8+ Bf8 27. Ne7+ Kg7 28. Nc6 Ra1+ 29. Kh2 a5 30. Ra8 a4 31. Nd8 Bd5 32. Ra7 a3 33. Nxf7 Bxf7 34. Bxf7 Kh6 35. Bg8 Bg7 36. g4 Kg6 37. h4 h6 38. f4 exf4 39. exf4 f5 40. h5+ Kf6 41. Rf7+ Ke6 42. Rxg7+ Kd6 43. gxf5 a2 44. Ra7 Rf1 45. Rxa2 Rxf4 46. Be6 Ke5 47. Kg3 Rb4 48. Ra5+ Kf6 49. Ra8 Kg5 50. Rg8+ Kxh5 51. Bf7# 1-0 1. d4 d5 2. c4 dxc4 3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. dxc5 Qxd1 8. Rxd1 Bxc5 9. b3 O-O 10. Bb2 b5 11. Be2 Bb7 12. Nbd2 Nbd7 13. Rac1 Rac8 14. a4 bxa4 15. bxa4 Nb6 16. a5 Na4 17. Bxf6 gxf6 18. Nb3 Bb4 19. Rxc8 Rxc8 20. Rd7 Bd5 21. Nfd4 Nc5 22. Nxc5 Rxc5 23. h3 Rxa5 24. Bh5 e5 25. Nf5 Be6 26. Rd8+ Bf8 27. Ne7+ Kg7 28. Nc6 Ra1+ 29. Kh2 a5 30. Ra8 a",
          "positional_tokens": {
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf6",
            "e3",
            "e6",
            "Bxc4",
            "c5",
            "O-O",
            "a6",
            "dxc5",
            "Qxd1",
            "Rxd1",
            "Bxc5",
            "b3",
            "O-O",
            "Bb2",
            "b5",
            "Be2",
            "Bb7",
            "Nbd2",
            "Nbd7",
            "Rac1",
            "Rac8",
            "a4",
            "bxa4",
            "bxa4",
            "Nb6",
            "a5",
            "Na4",
            "Bxf6",
            "gxf6",
            "Nb3",
            "Bb4",
            "Rxc8",
            "Rxc8",
            "Rd7",
            "Bd5",
            "Nfd4",
            "Nc5",
            "Nxc5",
            "Rxc5",
            "h3",
            "Rxa5",
            "Bh5",
            "e5",
            "Nf5",
            "Be6",
            "Rd8+",
            "Bf8",
            "Ne7+",
            "Kg7",
            "Nc6",
            "Ra1+",
            "Kh2",
            "a5",
            "Ra8"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf3",
          "neutral_uci": "g1f3",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. Qe2 b5 8. Bb3 Bb7 9. Rd1 Nbd7 10. Nc3 Qb8 11. d5 c4 12. dxe6 fxe6 13. Bc2 Bd6 14. h3 O-O 15. Ng5 Nc5 16. b4 cxb3 17. axb3 h6 18. b4 hxg5 19. bxc5 Bxc5 20. Bb3 Qe5 21. Bb2 Bd6 22. g3 Qf5 23. Rxd6 Qxh3 24. Bxe6+ Qxe6 25. Rxe6 Kf7 26. Rxf6+ gxf6 27. Qh5+ Kg7 28. Rd1 Rf7 29. Nd5 Rh8 30. Qxg5+ Kf8 31. Ba3+ Ke8 32. Nxf6+ Rxf6 33. Qe5+ Kf7 34. Rd7+ Kg6 35. Rxb7 Rh5 36. Qe4+ Rhf5 37. g4 1-0 3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. Qe2 b5 8. Bb3 Bb7 9. Rd1 Nbd7 10. Nc3 Qb8 11. d5 c4 12. dxe6 fxe6 13. Bc2 Bd6 14. h3 O-O 15. Ng5 Nc5 16. b4 cxb3 17. axb3 h6 18. b4 hxg5 19. bxc5 Bxc5 20. Bb3 Qe5 21. Bb2 Bd6 22. g3 Qf5 23. Rxd6 Qxh3 24. Bxe6+ Qxe6 25. Rxe6 Kf7 26. Rxf6+ gxf6 27. Qh5+ Kg7 28. Rd1 Rf7 29. Nd5 Rh8 30. Qxg5+ Kf8 31. Ba3+ Ke8 32. Nxf6+ Rxf6 33. Qe5+ Kf7 34. Rd7+ Kg6 35. Rxb7 Rh5 36. Qe4+ Rhf5 37. g4 1-0 3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6.",
          "neutral_tokens": {
            "prompt_tokens": 123,
            "completion_tokens": 640,
            "total_tokens": 763,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf6",
            "e3",
            "e6",
            "Bxc4",
            "c5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g1f3",
          "positional_uci": "g1f3",
          "neutral_uci": "g1f3",
          "consensus_move": "g1f3",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf6",
              "e3",
              "e6",
              "Bxc4",
              "c5",
              "O-O",
              "a6",
              "dxc5",
              "Qxd1",
              "Rxd1",
              "Bxc5",
              "Nbd2",
              "O-O",
              "Nb3",
              "Be7",
              "Be2",
              "Nc6",
              "Bd2",
              "e5",
              "Bc3",
              "e4",
              "Nfd4",
              "Nxd4",
              "Bxd4",
              "Be6",
              "Nc5",
              "Bxc5",
              "Bxc5",
              "Rfd8",
              "Bd4",
              "Rac8",
              "Bc3",
              "Nd5",
              "Ba5",
              "b6",
              "Be1",
              "a5",
              "Rac1",
              "Rxc1",
              "Rxc1",
              "Rc8",
              "Rxc8+",
              "Bxc8",
              "Bc4",
              "Be6",
              "Bxd5",
              "Bxd5",
              "a3",
              "f5",
              "Bc3",
              "Kf7",
              "Bd4",
              "b5",
              "Bb6",
              "a4",
              "Bd4",
              "g6",
              "Kf1",
              "Ke6",
              "Ke1",
              "Bc4",
              "Kd2",
              "Kd5",
              "Kc3",
              "Bf1",
              "g3",
              "Bc4",
              "Kb4",
              "Bf1",
              "Ka5",
              "Kc4",
              "Kb6",
              "Kb3",
              "Kc6",
              "Kc2",
              "Kd6",
              "Kd2",
              "Ke6",
              "Ke2",
              "Kf6",
              "Kxf2",
              "Kg7",
              "Kg2",
              "Kxh7",
              "Kxh2",
              "Kxg6",
              "Bh3",
              "Be5",
              "Kg2",
              "Kf6",
              "Kf3",
              "Bf4",
              "Ke2",
              "Ke5",
              "Kd3",
              "Kd5",
              "Kc2",
              "Be5",
              "Kd3",
              "Bd4",
              "Kc2",
              "Kc5",
              "Bf1",
              "Kd5",
              "Kd3",
              "Ke5",
              "Bh3",
              "Kf4",
              "Kc2",
              "Kg5",
              "Kd3",
              "Kh4",
              "Bg4",
              "Kg5",
              "Kc2",
              "Kf4",
              "Kd3",
              "Ke5",
              "Kc2",
              "Kd5",
              "Kd3",
              "Kc5",
              "Bh3",
              "Kxb5",
              "Bf1",
              "Kxa4",
              "Kc4",
              "b3+",
              "Kd5",
              "Kb4",
              "Kc6",
              "a4",
              "Bd3",
              "Kc3",
              "Be2",
              "b4",
              "Bd1",
              "b5+",
              "Kb7",
              "Kb4",
              "Be2",
              "a5",
              "Bd3",
              "a6+",
              "Ka8",
              "Ka5",
              "Bc4",
              "b6",
              "Bd5",
              "b7+",
              "Bxb7",
              "axb7+",
              "Kxb7",
              "Kb5",
              "Kc7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf6",
              "e3",
              "e6",
              "Bxc4",
              "c5",
              "O-O",
              "a6",
              "dxc5",
              "Qxd1",
              "Rxd1",
              "Bxc5",
              "b3",
              "O-O",
              "Bb2",
              "b5",
              "Be2",
              "Bb7",
              "Nbd2",
              "Nbd7",
              "Rac1",
              "Rac8",
              "a4",
              "bxa4",
              "bxa4",
              "Nb6",
              "a5",
              "Na4",
              "Bxf6",
              "gxf6",
              "Nb3",
              "Bb4",
              "Rxc8",
              "Rxc8",
              "Rd7",
              "Bd5",
              "Nfd4",
              "Nc5",
              "Nxc5",
              "Rxc5",
              "h3",
              "Rxa5",
              "Bh5",
              "e5",
              "Nf5",
              "Be6",
              "Rd8+",
              "Bf8",
              "Ne7+",
              "Kg7",
              "Nc6",
              "Ra1+",
              "Kh2",
              "a5",
              "Ra8"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf6",
              "e3",
              "e6",
              "Bxc4",
              "c5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 123,
            "completion_tokens": 640,
            "total_tokens": 763,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 371,
          "total_completion_tokens": 1920,
          "total_tokens": 2291
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 123,
            "completion_tokens": 640,
            "total_tokens": 763,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/8/2pP4/8/PP2PPPP/RNBQKBNR w KQkq - 0 3"
    },
    {
      "turn": 7,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc3",
          "aggressive_uci": "b1c3",
          "aggressive_response": "4. Nc3 Nf6 5. e4 Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Ne5 14. Qe2 Nc6 15. e5 Nd5 16. Nxd5 exd5 17. f4 f5 18. Nf3 Na5 19. Bc2 Nc4 20. Ng5 Qe7 21. h4 h6 22. h5 Be8 23. gxf5 hxg5 24. fxg5 Nxe3 25. Qxe3 c5 26. f6 Qc7 27. Qd3 gxf6 28. gxf6 Kh8 29. Rdg1 c4 30. Qg3 Qf7 31. Qg7+ Qxg7 32. fxg7+ Kg8 33. Bh7+ Kxh7 34. gxf8=Q Bxf8 35. Rf1 Bh6+ 36. Kc2 Ra7 37. Rf6 Bg7 38. Rd6 Bf7 39. h6 Bf8 40. Rf6 Bg6+ 41. Kc3 b4+ 42. Kd2 Be7 43. Re6 Bg5+ 44. Ke2 Be4 45. Rg1 Bxh6 46. Rg3 Rg7 47. Rh3 Rg6 48. Re7+ Kg8 49. Re8+ Kf7 50. Rh8 Bg7 51. R8h7 Rg2+ 52. Kf1 Rxb2 53. Rg3 Bxh7 54. Rf3+ Ke6 55. Rh3 Bd3+ 56. Ke1 Rxa2 57. Rg3 Bh6 58. Rh3 Bd2+ 59. Kd1 Bc3 60. Rh6+ Kf5 61. Rf6+ Ke4 62. Rf4+ Kxf4 63. e6 Ra1# e4 d5 Nc6",
          "aggressive_tokens": {
            "prompt_tokens": 133,
            "completion_tokens": 519,
            "total_tokens": 652,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "e4",
            "Bg4",
            "Be3",
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O",
            "O-O-O",
            "a6",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Nd2",
            "b5",
            "Bb3",
            "Ne5",
            "Qe2",
            "Nc6",
            "e5",
            "Nd5",
            "Nxd5",
            "exd5",
            "f4",
            "f5",
            "Nf3",
            "Na5",
            "Bc2",
            "Nc4",
            "Ng5",
            "Qe7",
            "h4",
            "h6",
            "h5",
            "Be8",
            "gxf5",
            "hxg5",
            "fxg5",
            "Nxe3",
            "Qxe3",
            "c5",
            "f6",
            "Qc7",
            "Qd3",
            "gxf6",
            "gxf6",
            "Kh8",
            "Rdg1",
            "c4",
            "Qg3",
            "Qf7",
            "Qg7+",
            "Qxg7",
            "fxg7+",
            "Kg8",
            "Bh7+",
            "Kxh7",
            "gxf8=Q",
            "Bxf8",
            "Rf1",
            "Bh6+",
            "Kc2",
            "Ra7",
            "Rf6",
            "Bg7",
            "Rd6",
            "Bf7",
            "h6",
            "Bf8",
            "Rf6",
            "Bg6+",
            "Kc3",
            "b4+",
            "Kd2",
            "Be7",
            "Re6",
            "Bg5+",
            "Ke2",
            "Be4",
            "Rg1",
            "Bxh6",
            "Rg3",
            "Rg7",
            "Rh3",
            "Rg6",
            "Re7+",
            "Kg8",
            "Re8+",
            "Kf7",
            "Rh8",
            "Bg7",
            "R8h7",
            "Rg2+",
            "Kf1",
            "Rxb2",
            "Rg3",
            "Bxh7",
            "Rf3+",
            "Ke6",
            "Rh3",
            "Bd3+",
            "Ke1",
            "Rxa2",
            "Rg3",
            "Bh6",
            "Rh3",
            "Bd2+",
            "Kd1",
            "Bc3",
            "Rh6+",
            "Kf5",
            "Rf6+",
            "Ke4",
            "Rf4+",
            "Kxf4",
            "e6",
            "Ra1#",
            "e4",
            "d5",
            "Nc6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e3",
          "positional_uci": "e2e3",
          "positional_response": "4. e3 e5 5. Bxc4 exd4 6. exd4 Bb4+ 7. Nc3 Nf6 8. O-O O-O 9. Bg5 Bg4 10. Nd5 Be7 11. Nxe7+ Qxe7 12. Re1 Qd6 13. d5 Ne5 14. Be2 Bxf3 15. Bxf3 Nxf3+ 16. Qxf3 Qxd5 17. Qg3 Ne4 18. Rxe4 Qxe4 19. Bh6 Qg6 20. Qxg6 hxg6 21. Bf4 c6 22. Rd1 Rad8 23. Re1 Rfe8 24. Be3 f5 25. g3 g5 26. Rc1 g4 27. Bxa7 Ra8 28. Bd4 Rxa2 29. Rc5 g6 30. Rc3 Rxb2 31. Ra3 Rd2 32. Be3 Rd1+ 33. Kg2 b5 34. Ra6 Rc8 35. Ra7 c5 36. Bh6 c4 37. Rg7+ Kh8 38. Rxg6 c3 39. Rb6 c2 40. Rxb5 c1=Q 41. Bxc1 Rcxc1 42. Rxf5 Rg1# 0-1 d4 d5 c4 dxc4 Nf3 Nc6 e3 e5 Bxc4 exd4 exd4 Bb4+ Nc3 Nf6 O-O O-O Bg5 Bg4 Nd5 Be7 Nxe7+ Qxe7 Re1 Qd6 d5 Ne5 Be2 Bxf3 Bxf3 Nxf3+ Qxf3 Qxd5 Qg3 Ne4 Bf4 Nxg3 hxg3 Qd4 Be3 Qxb2 Rab1 Qxa2 Rxb7 Rab8 Rxc7 Rb1 Rxb1 Qxb1+ Kh2 Qf5 Bxa7 Qh5+ Kg1 Rd8 f3 Rd1+ Kf2 Qa5 Be3 Qe1# 0-1 d4 d5 c4 dxc4 Nf3 Nc6 e3 e5 Bxc4 exd4 exd4 Bb4+ Nc3 Nf6 O-O O-O Bg5 Bg4 Nd5 Be7 Nxe7+ Qxe7 Re1 Qd6 d5 Ne5 Be2 Bxf3 Bxf3 Nxf3+ Qxf3 Qxd5 Qg3 Ne4 Bf4 Nxg3 hxg3 Qd4 Be3 Qxb2 Rab1 Qxa2 Rxb7 Rab8 Rxc7 Rb1 Rxb1 Qxb1+ Kh2 Qf5 Bxa7 Qh5+ Kg1 Rd8 f3 Rd1",
          "positional_tokens": {
            "prompt_tokens": 133,
            "completion_tokens": 640,
            "total_tokens": 773,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "e5",
            "Bxc4",
            "exd4",
            "exd4",
            "Bb4+",
            "Nc3",
            "Nf6",
            "O-O",
            "O-O",
            "Bg5",
            "Bg4",
            "Nd5",
            "Be7",
            "Nxe7+",
            "Qxe7",
            "Re1",
            "Qd6",
            "d5",
            "Ne5",
            "Be2",
            "Bxf3",
            "Bxf3",
            "Nxf3+",
            "Qxf3",
            "Qxd5",
            "Qg3",
            "Ne4",
            "Bf4",
            "Nxg3",
            "hxg3",
            "Qd4",
            "Be3",
            "Qxb2",
            "Rab1",
            "Qxa2",
            "Rxb7",
            "Rab8",
            "Rxc7",
            "Rb1",
            "Rxb1",
            "Qxb1+",
            "Kh2",
            "Qf5",
            "Bxa7",
            "Qh5+",
            "Kg1",
            "Rd8",
            "f3",
            "Rd1"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc3",
          "neutral_uci": "b1c3",
          "neutral_response": "4. Nc3 Nf6 5. e4 Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Ne5 14. Qc2 Nc6 15. f4 Nxd4 16. Bxd4 Qxd4 17. f5 exf5 18. exf5 Bxf5 19. gxf5 c5 20. Nde4 Qe5 21. Nxf6+ Qxf6 22. Nd5 Qh6+ 23. Kb1 c4 24. Nxb4 cxb3 25. Qxb3 a5 26. Nd5 a4 27. Qxb5 Rfb8 28. Qc5 a3 29. b3 Qh4 30. Qd4 Qg3 31. Rhg1 Qxh3 32. Qxg7# 1-0 Nf6 Nxe4 Qxe4",
          "neutral_tokens": {
            "prompt_tokens": 132,
            "completion_tokens": 254,
            "total_tokens": 386,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "e4",
            "Bg4",
            "Be3",
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O",
            "O-O-O",
            "a6",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Nd2",
            "b5",
            "Bb3",
            "Ne5",
            "Qc2",
            "Nc6",
            "f4",
            "Nxd4",
            "Bxd4",
            "Qxd4",
            "f5",
            "exf5",
            "exf5",
            "Bxf5",
            "gxf5",
            "c5",
            "Nde4",
            "Qe5",
            "Nxf6+",
            "Qxf6",
            "Nd5",
            "Qh6+",
            "Kb1",
            "c4",
            "Nxb4",
            "cxb3",
            "Qxb3",
            "a5",
            "Nd5",
            "a4",
            "Qxb5",
            "Rfb8",
            "Qc5",
            "a3",
            "b3",
            "Qh4",
            "Qd4",
            "Qg3",
            "Rhg1",
            "Qxh3",
            "Qxg7#",
            "Nf6",
            "Nxe4",
            "Qxe4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b1c3",
          "positional_uci": "e2e3",
          "neutral_uci": "b1c3",
          "consensus_move": "b1c3",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf6",
              "e4",
              "Bg4",
              "Be3",
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O",
              "O-O-O",
              "a6",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Nd2",
              "b5",
              "Bb3",
              "Ne5",
              "Qe2",
              "Nc6",
              "e5",
              "Nd5",
              "Nxd5",
              "exd5",
              "f4",
              "f5",
              "Nf3",
              "Na5",
              "Bc2",
              "Nc4",
              "Ng5",
              "Qe7",
              "h4",
              "h6",
              "h5",
              "Be8",
              "gxf5",
              "hxg5",
              "fxg5",
              "Nxe3",
              "Qxe3",
              "c5",
              "f6",
              "Qc7",
              "Qd3",
              "gxf6",
              "gxf6",
              "Kh8",
              "Rdg1",
              "c4",
              "Qg3",
              "Qf7",
              "Qg7+",
              "Qxg7",
              "fxg7+",
              "Kg8",
              "Bh7+",
              "Kxh7",
              "gxf8=Q",
              "Bxf8",
              "Rf1",
              "Bh6+",
              "Kc2",
              "Ra7",
              "Rf6",
              "Bg7",
              "Rd6",
              "Bf7",
              "h6",
              "Bf8",
              "Rf6",
              "Bg6+",
              "Kc3",
              "b4+",
              "Kd2",
              "Be7",
              "Re6",
              "Bg5+",
              "Ke2",
              "Be4",
              "Rg1",
              "Bxh6",
              "Rg3",
              "Rg7",
              "Rh3",
              "Rg6",
              "Re7+",
              "Kg8",
              "Re8+",
              "Kf7",
              "Rh8",
              "Bg7",
              "R8h7",
              "Rg2+",
              "Kf1",
              "Rxb2",
              "Rg3",
              "Bxh7",
              "Rf3+",
              "Ke6",
              "Rh3",
              "Bd3+",
              "Ke1",
              "Rxa2",
              "Rg3",
              "Bh6",
              "Rh3",
              "Bd2+",
              "Kd1",
              "Bc3",
              "Rh6+",
              "Kf5",
              "Rf6+",
              "Ke4",
              "Rf4+",
              "Kxf4",
              "e6",
              "Ra1#",
              "e4",
              "d5",
              "Nc6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Bxc4",
              "exd4",
              "exd4",
              "Bb4+",
              "Nc3",
              "Nf6",
              "O-O",
              "O-O",
              "Bg5",
              "Bg4",
              "Nd5",
              "Be7",
              "Nxe7+",
              "Qxe7",
              "Re1",
              "Qd6",
              "d5",
              "Ne5",
              "Be2",
              "Bxf3",
              "Bxf3",
              "Nxf3+",
              "Qxf3",
              "Qxd5",
              "Qg3",
              "Ne4",
              "Bf4",
              "Nxg3",
              "hxg3",
              "Qd4",
              "Be3",
              "Qxb2",
              "Rab1",
              "Qxa2",
              "Rxb7",
              "Rab8",
              "Rxc7",
              "Rb1",
              "Rxb1",
              "Qxb1+",
              "Kh2",
              "Qf5",
              "Bxa7",
              "Qh5+",
              "Kg1",
              "Rd8",
              "f3",
              "Rd1"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf6",
              "e4",
              "Bg4",
              "Be3",
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O",
              "O-O-O",
              "a6",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Nd2",
              "b5",
              "Bb3",
              "Ne5",
              "Qc2",
              "Nc6",
              "f4",
              "Nxd4",
              "Bxd4",
              "Qxd4",
              "f5",
              "exf5",
              "exf5",
              "Bxf5",
              "gxf5",
              "c5",
              "Nde4",
              "Qe5",
              "Nxf6+",
              "Qxf6",
              "Nd5",
              "Qh6+",
              "Kb1",
              "c4",
              "Nxb4",
              "cxb3",
              "Qxb3",
              "a5",
              "Nd5",
              "a4",
              "Qxb5",
              "Rfb8",
              "Qc5",
              "a3",
              "b3",
              "Qh4",
              "Qd4",
              "Qg3",
              "Rhg1",
              "Qxh3",
              "Qxg7#",
              "Nf6",
              "Nxe4",
              "Qxe4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 133,
            "completion_tokens": 519,
            "total_tokens": 652,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 133,
            "completion_tokens": 640,
            "total_tokens": 773,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 132,
            "completion_tokens": 254,
            "total_tokens": 386,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 398,
          "total_completion_tokens": 1413,
          "total_tokens": 1811
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 133,
            "completion_tokens": 519,
            "total_tokens": 652,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 133,
            "completion_tokens": 640,
            "total_tokens": 773,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 132,
            "completion_tokens": 254,
            "total_tokens": 386,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkbnr/ppp1pppp/2n5/8/2pP4/5N2/PP2PPPP/RNBQKB1R w KQkq - 2 4"
    },
    {
      "turn": 9,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": "e2e4",
          "aggressive_response": "5. e4 Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Na5 14. Bc2 c5 15. dxc5 Qxd3 16. Bxd3 Rfd8 17. Bc2 Bxc3 18. bxc3 Rac8 19. f3 Nd7 20. Nb3 Nc4 21. Bf2 Nde5 22. Rxd8+ Rxd8 23. Rd1 Rxd1+ 24. Bxd1 Nd3+ 25. Kc2 Nxf2 26. c6 Nd6 27. c7 Nc8 28. Nc5 Nxd1 29. Kxd1 Kf8 30. Nxa6 Ke7 31. Kd2 Kd6 32. Kd3 f5 33. gxf5 exf5 34. Kd4 fxe4 35. fxe4 Bf7 36. e5+ Kc6 37. Nb4+ Kxc7 38. Kc5 Bc4 39. a4 Bf1 40. axb5 Bxh3 41. Nd5+ Kb7 42. c4 Be6 43. Nf4 Bf5 44. Kd5 g5 45. Ne2 h5 46. c5 h4 47. Nd4 Ne7+ 48. Kd6 h3 49. Nf3 g4 50. Nh2 g3 51. Nf3 h2 52. Kxe7 h1=Q 53. Nd4 Qh7+ 54. Kd6 g2 55. c6+ Kb6 56. e6 g1=Q 57. Nxf5 Qxf5 58. e7 Qd4# e4 d5 c4 dxc4 Nf3 Nc6 Nc3 Nf6 e4 Bg4 Be3 e6 Bxc4 Bb4 Qd3 O-O-O h3 Bg6 g4 Bg6 Nd2 b5 Bb3 Na5 Bc2 c5 dxc5 Qxd3 Bxd3 Rfd8 Bc2 Bxc3 bxc3 Rac8 f3 Nd7 Nb3 Nc4 Bf2 Nde5 Rd1 Rxd1+ Bxd1 Nd3+ Kc2 Nxf2 c6 Nd6 Nxa6 Kd6 c7 Kxc7 Kd3 Kb6 Nb4 Kc5 a3 Nc4 Nc2 Ne5+ Ke3 Nc4+ Kd3 Ne5+ Ke3 Nc4+ Kd3 Ne5+ Ke3 Nc4+ K",
          "aggressive_tokens": {
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bg4",
            "Be3",
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O-O",
            "h3",
            "Bg6",
            "g4",
            "Bg6",
            "Nd2",
            "b5",
            "Bb3",
            "Na5",
            "Bc2",
            "c5",
            "dxc5",
            "Qxd3",
            "Bxd3",
            "Rfd8",
            "Bc2",
            "Bxc3",
            "bxc3",
            "Rac8",
            "f3",
            "Nd7",
            "Nb3",
            "Nc4",
            "Bf2",
            "Nde5",
            "Rd1",
            "Rxd1+",
            "Bxd1",
            "Nd3+",
            "Kc2",
            "Nxf2",
            "c6",
            "Nd6",
            "Nxa6",
            "Kd6",
            "c7",
            "Kxc7",
            "Kd3",
            "Kb6",
            "Nb4",
            "Kc5",
            "a3",
            "Nc4",
            "Nc2",
            "Ne5+",
            "Ke3",
            "Nc4+",
            "Kd3",
            "Ne5+",
            "Ke3",
            "Nc4+",
            "Kd3",
            "Ne5+",
            "Ke3",
            "Nc4+"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "5. e4 Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Na5 14. Bc2 c5 15. f4 c4 16. Qe2 Bxc3 17. bxc3 Qe7 18. f5 Qa3+ 19. Kb1 Qxc3 20. fxg6 Qb4+ 21. Ka1 Qc3+ 22. Kb1 Qb4+ 23. Ka1 Qc3+ 24. Kb1 Qb4+ 25. Ka1 Qc3+ 26. Kb1 Qb4+ 27. Ka1 Qc3+ 28. Kb1 Qb4+ 29. Ka1 Qc3+ 30. Kb1 Qb4+ 31. Ka1 Qc3+ 32. Kb1 Qb4+ 33. Ka1 Qc3+ 34. Kb1 Qb4+ 35. Ka1 Qc3+ 36. Kb1 Qb4+ 37. Ka1 Qc3+ 38. Kb1 Qb4+ 39. Ka1 Qc3+ 40. Kb1 Qb4+ 41. Ka1 Qc3+ 42. Kb1 Qb4+ 43. Ka1 Qc3+ 44. Kb1 Qb4+ 45. Ka1 Qc3+ 46. Kb1 Qb4+ 47. Ka1 Qc3+ 48. Kb1 Qb4+ 49. Ka1 Qc3+ 50. Kb1 Qb4+ 51. Ka1 Qc3+ 52. Kb1 Qb4+ 53. Ka1 Qc3+ 54. Kb1 Qb4+ 55. Ka1 Qc3+ 56. Kb1 Qb4+ 57. Ka1 Qc3+ 58. Kb1 Qb4+ 59. Ka1 Qc3+ 60. Kb1 Qb4+ 61. Ka1 Qc3+ 62. Kb1 Qb4+ 63. Ka1 Qc3+ 64. Kb1 Qb4+ 65. Ka1 Qc3+ 66. Kb1 Qb4+ 67. Ka1 Qc3+ 68. Kb1 Qb4+ 69. Ka1 Qc3+ 70. Kb1 Qb4+ 71. Ka1 Qc3+ 72. Kb1 Qb4+ 73. Ka1 Qc3+ 74. Kb1 Qb4+ 75. Ka1 Qc3+ 76. Kb1 Qb4+ 77. Ka1 Q",
          "positional_tokens": {
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bg4",
            "Be3",
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O",
            "O-O-O",
            "a6",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Nd2",
            "b5",
            "Bb3",
            "Na5",
            "Bc2",
            "c5",
            "f4",
            "c4",
            "Qe2",
            "Bxc3",
            "bxc3",
            "Qe7",
            "f5",
            "Qa3+",
            "Kb1",
            "Qxc3",
            "fxg6",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": "e2e4",
          "neutral_response": "5. e4 e6 6. Bxc4 Bb4 7. Qd3 O-O 8. O-O Re8 9. Bg5 h6 10. Bh4 g5 11. Bg3 g4 12. Ne5 Nxd4 13. Rad1 c5 14. Bh4 Kg7 15. Nxg4 Nxg4 16. Bxd8 Rxd8 17. Qg3 h5 18. h3 f5 19. hxg4 hxg4 20. Qe5+ Kg6 21. exf5+ exf5 22. Nd5 Be6 23. Qf6+ Kh5 24. Nf4# 1-0\n25. Qg6+ Kh4 26. Qh6#",
          "neutral_tokens": {
            "prompt_tokens": 141,
            "completion_tokens": 180,
            "total_tokens": 321,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O",
            "O-O",
            "Re8",
            "Bg5",
            "h6",
            "Bh4",
            "g5",
            "Bg3",
            "g4",
            "Ne5",
            "Nxd4",
            "Rad1",
            "c5",
            "Bh4",
            "Kg7",
            "Nxg4",
            "Nxg4",
            "Bxd8",
            "Rxd8",
            "Qg3",
            "h5",
            "h3",
            "f5",
            "hxg4",
            "hxg4",
            "Qe5+",
            "Kg6",
            "exf5+",
            "exf5",
            "Nd5",
            "Be6",
            "Qf6+",
            "Kh5",
            "Nf4#",
            "Qg6+",
            "Kh4",
            "Qh6#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e2e4",
          "positional_uci": "e2e4",
          "neutral_uci": "e2e4",
          "consensus_move": "e2e4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bg4",
              "Be3",
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O-O",
              "h3",
              "Bg6",
              "g4",
              "Bg6",
              "Nd2",
              "b5",
              "Bb3",
              "Na5",
              "Bc2",
              "c5",
              "dxc5",
              "Qxd3",
              "Bxd3",
              "Rfd8",
              "Bc2",
              "Bxc3",
              "bxc3",
              "Rac8",
              "f3",
              "Nd7",
              "Nb3",
              "Nc4",
              "Bf2",
              "Nde5",
              "Rd1",
              "Rxd1+",
              "Bxd1",
              "Nd3+",
              "Kc2",
              "Nxf2",
              "c6",
              "Nd6",
              "Nxa6",
              "Kd6",
              "c7",
              "Kxc7",
              "Kd3",
              "Kb6",
              "Nb4",
              "Kc5",
              "a3",
              "Nc4",
              "Nc2",
              "Ne5+",
              "Ke3",
              "Nc4+",
              "Kd3",
              "Ne5+",
              "Ke3",
              "Nc4+",
              "Kd3",
              "Ne5+",
              "Ke3",
              "Nc4+"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bg4",
              "Be3",
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O",
              "O-O-O",
              "a6",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Nd2",
              "b5",
              "Bb3",
              "Na5",
              "Bc2",
              "c5",
              "f4",
              "c4",
              "Qe2",
              "Bxc3",
              "bxc3",
              "Qe7",
              "f5",
              "Qa3+",
              "Kb1",
              "Qxc3",
              "fxg6",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O",
              "O-O",
              "Re8",
              "Bg5",
              "h6",
              "Bh4",
              "g5",
              "Bg3",
              "g4",
              "Ne5",
              "Nxd4",
              "Rad1",
              "c5",
              "Bh4",
              "Kg7",
              "Nxg4",
              "Nxg4",
              "Bxd8",
              "Rxd8",
              "Qg3",
              "h5",
              "h3",
              "f5",
              "hxg4",
              "hxg4",
              "Qe5+",
              "Kg6",
              "exf5+",
              "exf5",
              "Nd5",
              "Be6",
              "Qf6+",
              "Kh5",
              "Nf4#",
              "Qg6+",
              "Kh4",
              "Qh6#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 141,
            "completion_tokens": 180,
            "total_tokens": 321,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 425,
          "total_completion_tokens": 1460,
          "total_tokens": 1885
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 141,
            "completion_tokens": 180,
            "total_tokens": 321,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp1pppp/2n2n2/8/2pP4/2N2N2/PP2PPPP/R1BQKB1R w KQkq - 4 5"
    },
    {
      "turn": 11,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d5",
          "aggressive_uci": "d4d5",
          "aggressive_response": "6. d5 Ne5 7. Bf4 Nfd7 8. Bxe5 Nxe5 9. Bxc4 Nxc4 10. Qa4+ Qd7 11. Qxc4 Bxf3 12. gxf3 g6 13. Qd4 f6 14. O-O-O Bg7 15. f4 O-O 16. h4 f5 17. e5 c6 18. h5 cxd5 19. hxg6 hxg6 20. Nxd5 Rac8+ 21. Kb1 Qe6 22. Qxa7 Ra8 23. Qxb7 Rfb8 24. Qxe7 Qa6 25. Nb4 Qc4 26. Rd7 Qe4+ 27. Nc2 Qxh1+ 28. Ne1 Qxe1+ 29. Kc2 Qxf2+ 30. Kd3 Qf3+ 31. Kc2 Qe2+ 32. Kc3 Qxb2+ 33. Kd3 Ra3+ 34. Kc4 Qb5+ 35. Kd4 Rd3# Rb1 Qd3+ Ke1 Qe3+ Kd1 Qe2+ Kc1 Qc2#",
          "aggressive_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 291,
            "total_tokens": 441,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Ne5",
            "Bf4",
            "Nfd7",
            "Bxe5",
            "Nxe5",
            "Bxc4",
            "Nxc4",
            "Qa4+",
            "Qd7",
            "Qxc4",
            "Bxf3",
            "gxf3",
            "g6",
            "Qd4",
            "f6",
            "O-O-O",
            "Bg7",
            "f4",
            "O-O",
            "h4",
            "f5",
            "e5",
            "c6",
            "h5",
            "cxd5",
            "hxg6",
            "hxg6",
            "Nxd5",
            "Rac8+",
            "Kb1",
            "Qe6",
            "Qxa7",
            "Ra8",
            "Qxb7",
            "Rfb8",
            "Qxe7",
            "Qa6",
            "Nb4",
            "Qc4",
            "Rd7",
            "Qe4+",
            "Nc2",
            "Qxh1+",
            "Ne1",
            "Qxe1+",
            "Kc2",
            "Qxf2+",
            "Kd3",
            "Qf3+",
            "Kc2",
            "Qe2+",
            "Kc3",
            "Qxb2+",
            "Kd3",
            "Ra3+",
            "Kc4",
            "Qb5+",
            "Kd4",
            "Rd3#",
            "Rb1",
            "Qd3+",
            "Ke1",
            "Qe3+",
            "Kd1",
            "Qe2+",
            "Kc1",
            "Qc2#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d5",
          "positional_uci": "d4d5",
          "positional_response": "6. d5 Ne5 7. Bf4 Nfd7 8. Bxe5 Nxe5 9. Qa4+ Bd7 10. Qd1 Nxf3+ 11. Qxf3 b5 12. a4 a6 13. Be2 e6 14. O-O Bd6 15. dxe6 fxe6 16. axb5 axb5 17. Rxa8 Qxa8 18. Qh5+ g6 19. Qh6 Bf8 20. Qf4 Bd6 21. Qh6 Bf8 22. Qf4 Bd6 23. Qh6 Bf8 24. Qf4 Bd6 25. Qh6 Bf8 26. Qf4 Bd6 27. Qh6 Bf8 28. Qf4 Bd6 29. Qh6 Bf8 30. Qf4 Bd6 31. Qh6 Bf8 32. Qf4 Bd6 33. Qh6 Bf8 34. Qf4 Bd6 35. Qh6 Bf8 36. Qf4 Bd6 37. Qh6 Bf8 38. Qf4 Bd6 39. Qh6 Bf8 40. Qf4 Bd6 41. Qh6 Bf8 42. Qf4 Bd6 43. Qh6 Bf8 44. Qf4 Bd6 45. Qh6 Bf8 46. Qf4 Bd6 47. Qh6 Bf8 48. Qf4 Bd6 49. Qh6 Bf8 50. Qf4 Bd6 51. Qh6 Bf8 52. Qf4 Bd6 53. Qh6 Bf8 54. Qf4 Bd6 55. Qh6 Bf8 56. Qf4 Bd6 57. Qh6 Bf8 58. Qf4 Bd6 59. Qh6 Bf8 60. Qf4 Bd6 61. Qh6 Bf8 62. Qf4 Bd6 63. Qh6 Bf8 64. Qf4 Bd6 65. Qh6 Bf8 66. Qf4 Bd6 67. Qh6 Bf8 68. Qf4 Bd6 69. Qh6 Bf8 70. Qf4 Bd6 71. Qh6 Bf8 72. Qf4 Bd6 73. Qh6 Bf8 74. Qf4 Bd6 75. Qh6 Bf8 76. Qf4 Bd6 77. Qh6 Bf8 78. Qf4 Bd6 79. Qh6 Bf8 80. Qf4 Bd6 81. Q",
          "positional_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 640,
            "total_tokens": 790,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Ne5",
            "Bf4",
            "Nfd7",
            "Bxe5",
            "Nxe5",
            "Qa4+",
            "Bd7",
            "Qd1",
            "Nxf3+",
            "Qxf3",
            "b5",
            "a4",
            "a6",
            "Be2",
            "e6",
            "O-O",
            "Bd6",
            "dxe6",
            "fxe6",
            "axb5",
            "axb5",
            "Rxa8",
            "Qxa8",
            "Qh5+",
            "g6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d5",
          "neutral_uci": "d4d5",
          "neutral_response": "6. d5 Ne5 7. Bf4 Nfd7 8. Bxe5 Nxe5 9. Qa4+ Bd7 10. Qd1 Nxf3+ 11. Qxf3 b5 12. a4 a6 13. Be2 e6 14. O-O Bc5 15. dxe6 fxe6 16. Qh5+ g6 17. Qxc5 Qe7 18. Qxe7+ Kxe7 19. axb5 axb5 20. Rxa8 Rxa8 21. f4 c6 22. e5 c5 23. Ne4 Ra2 24. Rb1 Bc6 25. Nxc5 Bd5 26. Bf3 Bxf3 27. gxf3 b4 28. Ne4 Kd7 29. Nd6 c3 30. bxc3 bxc3 31. Rc1 c2 32. Kf2 Kc6 33. Ke2 Kd5 34. Kd2 Ra3 35. Rxc2 Rxf3 36. Rc4 Rf2+ 37. Ke3 Rxh2 38. Rd4+ Kc5 39. Ne4+ Kc6 40. Rd6+ Kc7 41. Rxe6 Rh3+ 42. Kd4 Rf3 43. Rf6 Kd7 44. Rf7+ Ke6 45. Ng5# Kd7 46. Nxf3 Ke6 47. Ng5# Kd7 48. Nxh7 Ke6 49. Ng5# Kd7 50. Rf6 Ke7 51. Rxg6 Kd7 52. Rd6+ Ke7 53. f5 Ke8 54. f6 Kf8 55. Rd8# Kxd8 56. e6 Ke8 57. Kd5 Kf8 58. Kd6 Ke8 59. f7+ Kf8 60. e7+ Kg7 61. f8=Q+ Kg6 62. e8=Q+ Kxg5 63. Qe5+ Kg4 64. Qff4+ Kh3 65. Qg3# Kxg3 66. Qf5 Kg2 67. Qf4 Kh3 68. Qg5 Kh2 69. Qg4 Kh1 70. Ke5 Kh2 71. Kf4 Kh1 72. Kf3 Kh2 73. Qg2# Kh1 74. Qg3# Kh1 75. Qg2# Kh1 76. Qg4# Kh2 77. Qg3+ Kh1 78. Qg2# Kh1 79. Qg4# Kh2 80. Qg3+ Kh1",
          "neutral_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Ne5",
            "Bf4",
            "Nfd7",
            "Bxe5",
            "Nxe5",
            "Qa4+",
            "Bd7",
            "Qd1",
            "Nxf3+",
            "Qxf3",
            "b5",
            "a4",
            "a6",
            "Be2",
            "e6",
            "O-O",
            "Bc5",
            "dxe6",
            "fxe6",
            "Qh5+",
            "g6",
            "Qxc5",
            "Qe7",
            "Qxe7+",
            "Kxe7",
            "axb5",
            "axb5",
            "Rxa8",
            "Rxa8",
            "f4",
            "c6",
            "e5",
            "c5",
            "Ne4",
            "Ra2",
            "Rb1",
            "Bc6",
            "Nxc5",
            "Bd5",
            "Bf3",
            "Bxf3",
            "gxf3",
            "b4",
            "Ne4",
            "Kd7",
            "Nd6",
            "c3",
            "bxc3",
            "bxc3",
            "Rc1",
            "c2",
            "Kf2",
            "Kc6",
            "Ke2",
            "Kd5",
            "Kd2",
            "Ra3",
            "Rxc2",
            "Rxf3",
            "Rc4",
            "Rf2+",
            "Ke3",
            "Rxh2",
            "Rd4+",
            "Kc5",
            "Ne4+",
            "Kc6",
            "Rd6+",
            "Kc7",
            "Rxe6",
            "Rh3+",
            "Kd4",
            "Rf3",
            "Rf6",
            "Kd7",
            "Rf7+",
            "Ke6",
            "Ng5#",
            "Kd7",
            "Nxf3",
            "Ke6",
            "Ng5#",
            "Kd7",
            "Nxh7",
            "Ke6",
            "Ng5#",
            "Kd7",
            "Rf6",
            "Ke7",
            "Rxg6",
            "Kd7",
            "Rd6+",
            "Ke7",
            "f5",
            "Ke8",
            "f6",
            "Kf8",
            "Rd8#",
            "Kxd8",
            "e6",
            "Ke8",
            "Kd5",
            "Kf8",
            "Kd6",
            "Ke8",
            "f7+",
            "Kf8",
            "e7+",
            "Kg7",
            "f8=Q+",
            "Kg6",
            "e8=Q+",
            "Kxg5",
            "Qe5+",
            "Kg4",
            "Qff4+",
            "Kh3",
            "Qg3#",
            "Kxg3",
            "Qf5",
            "Kg2",
            "Qf4",
            "Kh3",
            "Qg5",
            "Kh2",
            "Qg4",
            "Kh1",
            "Ke5",
            "Kh2",
            "Kf4",
            "Kh1",
            "Kf3",
            "Kh2",
            "Qg2#",
            "Kh1",
            "Qg3#",
            "Kh1",
            "Qg2#",
            "Kh1",
            "Qg4#",
            "Kh2",
            "Qg3+",
            "Kh1",
            "Qg2#",
            "Kh1",
            "Qg4#",
            "Kh2",
            "Qg3+",
            "Kh1"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d4d5",
          "positional_uci": "d4d5",
          "neutral_uci": "d4d5",
          "consensus_move": "d4d5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Ne5",
              "Bf4",
              "Nfd7",
              "Bxe5",
              "Nxe5",
              "Bxc4",
              "Nxc4",
              "Qa4+",
              "Qd7",
              "Qxc4",
              "Bxf3",
              "gxf3",
              "g6",
              "Qd4",
              "f6",
              "O-O-O",
              "Bg7",
              "f4",
              "O-O",
              "h4",
              "f5",
              "e5",
              "c6",
              "h5",
              "cxd5",
              "hxg6",
              "hxg6",
              "Nxd5",
              "Rac8+",
              "Kb1",
              "Qe6",
              "Qxa7",
              "Ra8",
              "Qxb7",
              "Rfb8",
              "Qxe7",
              "Qa6",
              "Nb4",
              "Qc4",
              "Rd7",
              "Qe4+",
              "Nc2",
              "Qxh1+",
              "Ne1",
              "Qxe1+",
              "Kc2",
              "Qxf2+",
              "Kd3",
              "Qf3+",
              "Kc2",
              "Qe2+",
              "Kc3",
              "Qxb2+",
              "Kd3",
              "Ra3+",
              "Kc4",
              "Qb5+",
              "Kd4",
              "Rd3#",
              "Rb1",
              "Qd3+",
              "Ke1",
              "Qe3+",
              "Kd1",
              "Qe2+",
              "Kc1",
              "Qc2#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Ne5",
              "Bf4",
              "Nfd7",
              "Bxe5",
              "Nxe5",
              "Qa4+",
              "Bd7",
              "Qd1",
              "Nxf3+",
              "Qxf3",
              "b5",
              "a4",
              "a6",
              "Be2",
              "e6",
              "O-O",
              "Bd6",
              "dxe6",
              "fxe6",
              "axb5",
              "axb5",
              "Rxa8",
              "Qxa8",
              "Qh5+",
              "g6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Ne5",
              "Bf4",
              "Nfd7",
              "Bxe5",
              "Nxe5",
              "Qa4+",
              "Bd7",
              "Qd1",
              "Nxf3+",
              "Qxf3",
              "b5",
              "a4",
              "a6",
              "Be2",
              "e6",
              "O-O",
              "Bc5",
              "dxe6",
              "fxe6",
              "Qh5+",
              "g6",
              "Qxc5",
              "Qe7",
              "Qxe7+",
              "Kxe7",
              "axb5",
              "axb5",
              "Rxa8",
              "Rxa8",
              "f4",
              "c6",
              "e5",
              "c5",
              "Ne4",
              "Ra2",
              "Rb1",
              "Bc6",
              "Nxc5",
              "Bd5",
              "Bf3",
              "Bxf3",
              "gxf3",
              "b4",
              "Ne4",
              "Kd7",
              "Nd6",
              "c3",
              "bxc3",
              "bxc3",
              "Rc1",
              "c2",
              "Kf2",
              "Kc6",
              "Ke2",
              "Kd5",
              "Kd2",
              "Ra3",
              "Rxc2",
              "Rxf3",
              "Rc4",
              "Rf2+",
              "Ke3",
              "Rxh2",
              "Rd4+",
              "Kc5",
              "Ne4+",
              "Kc6",
              "Rd6+",
              "Kc7",
              "Rxe6",
              "Rh3+",
              "Kd4",
              "Rf3",
              "Rf6",
              "Kd7",
              "Rf7+",
              "Ke6",
              "Ng5#",
              "Kd7",
              "Nxf3",
              "Ke6",
              "Ng5#",
              "Kd7",
              "Nxh7",
              "Ke6",
              "Ng5#",
              "Kd7",
              "Rf6",
              "Ke7",
              "Rxg6",
              "Kd7",
              "Rd6+",
              "Ke7",
              "f5",
              "Ke8",
              "f6",
              "Kf8",
              "Rd8#",
              "Kxd8",
              "e6",
              "Ke8",
              "Kd5",
              "Kf8",
              "Kd6",
              "Ke8",
              "f7+",
              "Kf8",
              "e7+",
              "Kg7",
              "f8=Q+",
              "Kg6",
              "e8=Q+",
              "Kxg5",
              "Qe5+",
              "Kg4",
              "Qff4+",
              "Kh3",
              "Qg3#",
              "Kxg3",
              "Qf5",
              "Kg2",
              "Qf4",
              "Kh3",
              "Qg5",
              "Kh2",
              "Qg4",
              "Kh1",
              "Ke5",
              "Kh2",
              "Kf4",
              "Kh1",
              "Kf3",
              "Kh2",
              "Qg2#",
              "Kh1",
              "Qg3#",
              "Kh1",
              "Qg2#",
              "Kh1",
              "Qg4#",
              "Kh2",
              "Qg3+",
              "Kh1",
              "Qg2#",
              "Kh1",
              "Qg4#",
              "Kh2",
              "Qg3+",
              "Kh1"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 150,
            "completion_tokens": 291,
            "total_tokens": 441,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 150,
            "completion_tokens": 640,
            "total_tokens": 790,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 449,
          "total_completion_tokens": 1571,
          "total_tokens": 2020
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 150,
            "completion_tokens": 291,
            "total_tokens": 441,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 150,
            "completion_tokens": 640,
            "total_tokens": 790,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/2n2n2/8/2pPP1b1/2N2N2/PP3PPP/R1BQKB1R w KQkq - 1 6"
    },
    {
      "turn": 13,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qa4+",
          "aggressive_uci": "d1a4",
          "aggressive_response": "7. Qa4+ c6 8. b4 cxb3 9. axb3 b6 10. dxc6 Qc7 11. Nb5 Qxc6 12. Nc7+ Kd8 13. Nxa8 Qxa4 14. Rxa4 Bd7 15. Rd4 Nxb3 16. Rd1 Nxc1 17. Rxc1 Nxe4 18. Ne5 Be6 19. Nc7 Nc5 20. Nxe6+ Nxe6 21. Nxf7+ Ke8 22. Nxh8 g6 23. Bb5+ Kd8 24. Nf7# c5 bxc5 bxc5 *\n25. Rxc5 Nxc5 26. O-O a6 27. Rd1+ Kc7 28. Rc1 Kb6 29. Bc4 e6 30. Ng5 Bh6 31. Rb1+ Kc6 32. Nxe6 Nxe6 33. Bxe6 a5 34. Bg8 a4 35. Bxh7 a3 36. Bxg6 Bg7 37. Bf7 Kc5 38. h4 Kd4 39. g4 Ke4 40. Kg2 Kf4 41. Rb4+ Ke5 42. f4+ Kf6 43. Ba2 Bf8 44. Rb6+ Kg7 45. h5 Bc5 46. Rb7+ Kh6 47. Kh3 Be3 48. Kh4 Bxf4 49. Bb1 Bg5+ 50. Kh3 Bf6 51. Rh7+ Kg5 52. Rf7 Bb2 53. Rf5+ Kh6 54. Kh4 Bc1 55. g5+ Kg7 56. h6+ Kh8 57. Rf8# g5+ Kh7 Kh5 *",
          "aggressive_tokens": {
            "prompt_tokens": 157,
            "completion_tokens": 446,
            "total_tokens": 603,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "c6",
            "b4",
            "cxb3",
            "axb3",
            "b6",
            "dxc6",
            "Qc7",
            "Nb5",
            "Qxc6",
            "Nc7+",
            "Kd8",
            "Nxa8",
            "Qxa4",
            "Rxa4",
            "Bd7",
            "Rd4",
            "Nxb3",
            "Rd1",
            "Nxc1",
            "Rxc1",
            "Nxe4",
            "Ne5",
            "Be6",
            "Nc7",
            "Nc5",
            "Nxe6+",
            "Nxe6",
            "Nxf7+",
            "Ke8",
            "Nxh8",
            "g6",
            "Bb5+",
            "Kd8",
            "Nf7#",
            "c5",
            "bxc5",
            "bxc5",
            "Rxc5",
            "Nxc5",
            "O-O",
            "a6",
            "Rd1+",
            "Kc7",
            "Rc1",
            "Kb6",
            "Bc4",
            "e6",
            "Ng5",
            "Bh6",
            "Rb1+",
            "Kc6",
            "Nxe6",
            "Nxe6",
            "Bxe6",
            "a5",
            "Bg8",
            "a4",
            "Bxh7",
            "a3",
            "Bxg6",
            "Bg7",
            "Bf7",
            "Kc5",
            "h4",
            "Kd4",
            "g4",
            "Ke4",
            "Kg2",
            "Kf4",
            "Rb4+",
            "Ke5",
            "f4+",
            "Kf6",
            "Ba2",
            "Bf8",
            "Rb6+",
            "Kg7",
            "h5",
            "Bc5",
            "Rb7+",
            "Kh6",
            "Kh3",
            "Be3",
            "Kh4",
            "Bxf4",
            "Bb1",
            "Bg5+",
            "Kh3",
            "Bf6",
            "Rh7+",
            "Kg5",
            "Rf7",
            "Bb2",
            "Rf5+",
            "Kh6",
            "Kh4",
            "Bc1",
            "g5+",
            "Kg7",
            "h6+",
            "Kh8",
            "Rf8#",
            "g5+",
            "Kh7",
            "Kh5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qa4+",
          "positional_uci": "d1a4",
          "positional_response": "7. Qa4+ c6 8. b4 cxb3 9. axb3 Bxf3 10. gxf3 e6 11. Qxa5 Qxa5 12. Rxa5 Bb4 13. Ra4 Bxc3+ 14. Bd2 Bxd2+ 15. Kxd2 exd5 16. exd5 Nxd5 17. Bc4 O-O-O 18. Rxa7 Nb6+ 19. Kc3 Nxc4 20. bxc4 Kc7 21. Rb1 Rb8 22. c5 Rhd8 23. Re1 Rd7 24. Re4 Rbd8 25. Rb4 Rd3+ 26. Kc2 Rd2+ 27. Kc1 Rd1+ 28. Kc2 R8d2+ 29. Kc3 Rd3+ 30. Kc2 R1d2+ 31. Kc1 Rd1+ 32. Kc2 R3d2+ 33. Kc3 Rd3+ 34. Kc2 R1d2+ 35. Kc1 Rd1+ 36. Kc2 R3d2+ 37. Kc3 Rd3+ 38. Kc2 R1d2+ 39. Kc1 Rd1+ 40. Kc2 R3d2+ 41. Kc3 Rd3+ 42. Kc2 R1d2+ 43. Kc1 Rd1+ 44. Kc2 R3d2+ 45. Kc3 Rd3+ 46. Kc2 R1d2+ 47. Kc1 Rd1+ 48. Kc2 R3d2+ 49. Kc3 Rd3+ 50. Kc2 R1d2+ 51. Kc1 Rd1+ 52. Kc2 R3d2+ 53. Kc3 Rd3+ 54. Kc2 R1d2+ 55. Kc1 Rd1+ 56. Kc2 R3d2+ 57. Kc3 Rd3+ 58. Kc2 R1d2+ 59. Kc1 Rd1+ 60. Kc2 R3d2+ 61. Kc3 Rd3+ 62. Kc2 R1d2+ 63. Kc1 Rd1+ 64. Kc2 R3d2+ 65. Kc3 Rd3+ 66. Kc2 R1d2+ 67. Kc1 Rd1+ 68. Kc2 R3d2+ 69. Kc3 Rd3+ 70. Kc2 R1d2+ 71. Kc1 Rd1+ 72. Kc2 R3d2+ 73. Kc3",
          "positional_tokens": {
            "prompt_tokens": 157,
            "completion_tokens": 640,
            "total_tokens": 797,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "c6",
            "b4",
            "cxb3",
            "axb3",
            "Bxf3",
            "gxf3",
            "e6",
            "Qxa5",
            "Qxa5",
            "Rxa5",
            "Bb4",
            "Ra4",
            "Bxc3+",
            "Bd2",
            "Bxd2+",
            "Kxd2",
            "exd5",
            "exd5",
            "Nxd5",
            "Bc4",
            "O-O-O",
            "Rxa7",
            "Nb6+",
            "Kc3",
            "Nxc4",
            "bxc4",
            "Kc7",
            "Rb1",
            "Rb8",
            "c5",
            "Rhd8",
            "Re1",
            "Rd7",
            "Re4",
            "Rbd8",
            "Rb4",
            "Rd3+",
            "Kc2",
            "Rd2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R8d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qa4+",
          "neutral_uci": "d1a4",
          "neutral_response": "7. Qa4+ c6 8. b4 cxb3 9. axb3 Bxf3 10. gxf3 b6 11. dxc6 Qc7 12. Nd5 Nxd5 13. exd5 e6 14. Bf4 Bd6 15. Bxd6 Qxd6 16. c7+ Ke7 17. Rc1 Rhc8 18. Ba6 Rxc7 19. Rxc7+ Qxc7 20. O-O Rd8 21. dxe6 fxe6 22. Qh4+ Kf7 23. Qxh7 Nxb3 24. Qh5+ Kg8 25. Re1 Nc5 26. Bc4 Qf7 27. Qg4 Rd6 28. Kh1 a5 29. Rg1 a4 30. f4 Ne4 31. Qf3 Nd2 32. Qa8+ Kh7 33. Qxa4 Qxf4 34. Qc2+ Kh8 35. Be2 Qe4+ 36. Qxe4 Nxe4 37. Kg2 Rd2 38. Kf3 Nc3 39. Bc4 Nd5 40. Rb1 Rd4 41. Bxd5 Rxd5 42. Rxb6 Re5 43. Kf4 Re2 44. Kf3 Re5 45. Kf4 Re2 46. Kf3 Re1 47. Rb4 Kh7 48. Re4 Rxe4 49. Kxe4 Kg6 50. Ke5 Kf7 51. f4 Ke7 52. h4 Kf7 53. h5 Ke7 54. f5 exf5 55. Kxf5 Kf7 56. Kg5 Kg8 57. Kg6 Kh8 58. h6 gxh6 59. Kxh6 * e5 60. Kg5 e4 61. Kf4 e3 62. Kxe3",
          "neutral_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 460,
            "total_tokens": 616,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "c6",
            "b4",
            "cxb3",
            "axb3",
            "Bxf3",
            "gxf3",
            "b6",
            "dxc6",
            "Qc7",
            "Nd5",
            "Nxd5",
            "exd5",
            "e6",
            "Bf4",
            "Bd6",
            "Bxd6",
            "Qxd6",
            "c7+",
            "Ke7",
            "Rc1",
            "Rhc8",
            "Ba6",
            "Rxc7",
            "Rxc7+",
            "Qxc7",
            "O-O",
            "Rd8",
            "dxe6",
            "fxe6",
            "Qh4+",
            "Kf7",
            "Qxh7",
            "Nxb3",
            "Qh5+",
            "Kg8",
            "Re1",
            "Nc5",
            "Bc4",
            "Qf7",
            "Qg4",
            "Rd6",
            "Kh1",
            "a5",
            "Rg1",
            "a4",
            "f4",
            "Ne4",
            "Qf3",
            "Nd2",
            "Qa8+",
            "Kh7",
            "Qxa4",
            "Qxf4",
            "Qc2+",
            "Kh8",
            "Be2",
            "Qe4+",
            "Qxe4",
            "Nxe4",
            "Kg2",
            "Rd2",
            "Kf3",
            "Nc3",
            "Bc4",
            "Nd5",
            "Rb1",
            "Rd4",
            "Bxd5",
            "Rxd5",
            "Rxb6",
            "Re5",
            "Kf4",
            "Re2",
            "Kf3",
            "Re5",
            "Kf4",
            "Re2",
            "Kf3",
            "Re1",
            "Rb4",
            "Kh7",
            "Re4",
            "Rxe4",
            "Kxe4",
            "Kg6",
            "Ke5",
            "Kf7",
            "f4",
            "Ke7",
            "h4",
            "Kf7",
            "h5",
            "Ke7",
            "f5",
            "exf5",
            "Kxf5",
            "Kf7",
            "Kg5",
            "Kg8",
            "Kg6",
            "Kh8",
            "h6",
            "gxh6",
            "Kxh6",
            "e5",
            "Kg5",
            "e4",
            "Kf4",
            "e3",
            "Kxe3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d1a4",
          "positional_uci": "d1a4",
          "neutral_uci": "d1a4",
          "consensus_move": "d1a4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "c6",
              "b4",
              "cxb3",
              "axb3",
              "b6",
              "dxc6",
              "Qc7",
              "Nb5",
              "Qxc6",
              "Nc7+",
              "Kd8",
              "Nxa8",
              "Qxa4",
              "Rxa4",
              "Bd7",
              "Rd4",
              "Nxb3",
              "Rd1",
              "Nxc1",
              "Rxc1",
              "Nxe4",
              "Ne5",
              "Be6",
              "Nc7",
              "Nc5",
              "Nxe6+",
              "Nxe6",
              "Nxf7+",
              "Ke8",
              "Nxh8",
              "g6",
              "Bb5+",
              "Kd8",
              "Nf7#",
              "c5",
              "bxc5",
              "bxc5",
              "Rxc5",
              "Nxc5",
              "O-O",
              "a6",
              "Rd1+",
              "Kc7",
              "Rc1",
              "Kb6",
              "Bc4",
              "e6",
              "Ng5",
              "Bh6",
              "Rb1+",
              "Kc6",
              "Nxe6",
              "Nxe6",
              "Bxe6",
              "a5",
              "Bg8",
              "a4",
              "Bxh7",
              "a3",
              "Bxg6",
              "Bg7",
              "Bf7",
              "Kc5",
              "h4",
              "Kd4",
              "g4",
              "Ke4",
              "Kg2",
              "Kf4",
              "Rb4+",
              "Ke5",
              "f4+",
              "Kf6",
              "Ba2",
              "Bf8",
              "Rb6+",
              "Kg7",
              "h5",
              "Bc5",
              "Rb7+",
              "Kh6",
              "Kh3",
              "Be3",
              "Kh4",
              "Bxf4",
              "Bb1",
              "Bg5+",
              "Kh3",
              "Bf6",
              "Rh7+",
              "Kg5",
              "Rf7",
              "Bb2",
              "Rf5+",
              "Kh6",
              "Kh4",
              "Bc1",
              "g5+",
              "Kg7",
              "h6+",
              "Kh8",
              "Rf8#",
              "g5+",
              "Kh7",
              "Kh5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "c6",
              "b4",
              "cxb3",
              "axb3",
              "Bxf3",
              "gxf3",
              "e6",
              "Qxa5",
              "Qxa5",
              "Rxa5",
              "Bb4",
              "Ra4",
              "Bxc3+",
              "Bd2",
              "Bxd2+",
              "Kxd2",
              "exd5",
              "exd5",
              "Nxd5",
              "Bc4",
              "O-O-O",
              "Rxa7",
              "Nb6+",
              "Kc3",
              "Nxc4",
              "bxc4",
              "Kc7",
              "Rb1",
              "Rb8",
              "c5",
              "Rhd8",
              "Re1",
              "Rd7",
              "Re4",
              "Rbd8",
              "Rb4",
              "Rd3+",
              "Kc2",
              "Rd2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R8d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "c6",
              "b4",
              "cxb3",
              "axb3",
              "Bxf3",
              "gxf3",
              "b6",
              "dxc6",
              "Qc7",
              "Nd5",
              "Nxd5",
              "exd5",
              "e6",
              "Bf4",
              "Bd6",
              "Bxd6",
              "Qxd6",
              "c7+",
              "Ke7",
              "Rc1",
              "Rhc8",
              "Ba6",
              "Rxc7",
              "Rxc7+",
              "Qxc7",
              "O-O",
              "Rd8",
              "dxe6",
              "fxe6",
              "Qh4+",
              "Kf7",
              "Qxh7",
              "Nxb3",
              "Qh5+",
              "Kg8",
              "Re1",
              "Nc5",
              "Bc4",
              "Qf7",
              "Qg4",
              "Rd6",
              "Kh1",
              "a5",
              "Rg1",
              "a4",
              "f4",
              "Ne4",
              "Qf3",
              "Nd2",
              "Qa8+",
              "Kh7",
              "Qxa4",
              "Qxf4",
              "Qc2+",
              "Kh8",
              "Be2",
              "Qe4+",
              "Qxe4",
              "Nxe4",
              "Kg2",
              "Rd2",
              "Kf3",
              "Nc3",
              "Bc4",
              "Nd5",
              "Rb1",
              "Rd4",
              "Bxd5",
              "Rxd5",
              "Rxb6",
              "Re5",
              "Kf4",
              "Re2",
              "Kf3",
              "Re5",
              "Kf4",
              "Re2",
              "Kf3",
              "Re1",
              "Rb4",
              "Kh7",
              "Re4",
              "Rxe4",
              "Kxe4",
              "Kg6",
              "Ke5",
              "Kf7",
              "f4",
              "Ke7",
              "h4",
              "Kf7",
              "h5",
              "Ke7",
              "f5",
              "exf5",
              "Kxf5",
              "Kf7",
              "Kg5",
              "Kg8",
              "Kg6",
              "Kh8",
              "h6",
              "gxh6",
              "Kxh6",
              "e5",
              "Kg5",
              "e4",
              "Kf4",
              "e3",
              "Kxe3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 157,
            "completion_tokens": 446,
            "total_tokens": 603,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 157,
            "completion_tokens": 640,
            "total_tokens": 797,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 156,
            "completion_tokens": 460,
            "total_tokens": 616,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 470,
          "total_completion_tokens": 1546,
          "total_tokens": 2016
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 157,
            "completion_tokens": 446,
            "total_tokens": 603,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 157,
            "completion_tokens": 640,
            "total_tokens": 797,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 156,
            "completion_tokens": 460,
            "total_tokens": 616,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/5n2/n2P4/2p1P1b1/2N2N2/PP3PPP/R1BQKB1R w KQkq - 1 7"
    },
    {
      "turn": 15,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "dxc6",
          "aggressive_uci": "d5c6",
          "aggressive_response": "8. dxc6 bxc6 9. Qxc6+ Nd7 10. Ne5 Be6 11. Bxc4 Bxc4 12. Nxc4 e6 13. Nb5 Bb4+ 14. Bd2 Bxd2+ 15. Nxd2 O-O 16. Qxc7 Qe7 17. O-O Rfc8 18. Qd6 Qe8 19. Nc7 Rxc7 20. Qxc7 Rc8 21. Qxa7 Rc2 22. Qd4 e5 23. Qd3 Rxb2 24. Rfb1 Rxb1+ 25. Rxb1 Nc5 26. Qb5 Qxb5 27. Rxb5 Nd7 28. Rd5 Nf8 29. Rxe5 f6 30. Re7 Ng6 31. Rb7 Ne5 32. a4 Nc6 33. Nc4 Nd4 34. a5 Nc6 35. a6 Kf8 36. a7 Nxa7 37. Rxa7 h6 38. Nd6 Kg8 39. Nf5 g6 40. Nxh6+ Kf8 41. Ng4 f5 42. exf5 gxf5 43. Nh6 f4 44. g3 fxg3 45. hxg3 Ke8 46. g4 Kf8 47. g5 Ke8 48. g6 Kf8 49. g7+ Ke8 50. g8=Q# e4 f5 g5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fx",
          "aggressive_tokens": {
            "prompt_tokens": 167,
            "completion_tokens": 640,
            "total_tokens": 807,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "bxc6",
            "Qxc6+",
            "Nd7",
            "Ne5",
            "Be6",
            "Bxc4",
            "Bxc4",
            "Nxc4",
            "e6",
            "Nb5",
            "Bb4+",
            "Bd2",
            "Bxd2+",
            "Nxd2",
            "O-O",
            "Qxc7",
            "Qe7",
            "O-O",
            "Rfc8",
            "Qd6",
            "Qe8",
            "Nc7",
            "Rxc7",
            "Qxc7",
            "Rc8",
            "Qxa7",
            "Rc2",
            "Qd4",
            "e5",
            "Qd3",
            "Rxb2",
            "Rfb1",
            "Rxb1+",
            "Rxb1",
            "Nc5",
            "Qb5",
            "Qxb5",
            "Rxb5",
            "Nd7",
            "Rd5",
            "Nf8",
            "Rxe5",
            "f6",
            "Re7",
            "Ng6",
            "Rb7",
            "Ne5",
            "a4",
            "Nc6",
            "Nc4",
            "Nd4",
            "a5",
            "Nc6",
            "a6",
            "Kf8",
            "a7",
            "Nxa7",
            "Rxa7",
            "h6",
            "Nd6",
            "Kg8",
            "Nf5",
            "g6",
            "Nxh6+",
            "Kf8",
            "Ng4",
            "f5",
            "exf5",
            "gxf5",
            "Nh6",
            "f4",
            "g3",
            "fxg3",
            "hxg3",
            "Ke8",
            "g4",
            "Kf8",
            "g5",
            "Ke8",
            "g6",
            "Kf8",
            "g7+",
            "Ke8",
            "g8=Q#",
            "e4",
            "f5",
            "g5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "dxc6",
          "positional_uci": "d5c6",
          "positional_response": "8. dxc6 bxc6 9. Qxc6+ Nd7 10. Ne5 Be6 11. Bxc4 Bxc4 12. Nxc4 e6 13. Nb5 Bb4+ 14. Bd2 Bxd2+ 15. Nxd2 O-O 16. Qxc7 Qxc7 17. Nxc7 Rac8 18. Nb5 Rc2 19. b3 a6 20. Nd4 Rc3 21. O-O Rd3 22. N2f3 e5 23. Nc6 f6 24. Rfd1 Rxd1+ 25. Rxd1 Nc5 26. Nd2 Kf7 27. f3 Rc8 28. Nb4 a5 29. Nd5 Nd3 30. Nc4 Nb2 31. Nxb2 Rc2 32. Nc4 Rxa2 33. Rd2 Ra1+ 34. Kf2 a4 35. bxa4 Rxa4 36. Nd6+ Kg6 37. Ne7+ Kh6 38. Ndf5+ Kg5 39. h4+ Kh5 40. g4# Nxe7 41. Nxe7 Kxh4 42. Nf5+ Kg5 43. Nxg7 Kg6 44. Nf5 h5 45. gxh5+ Kxh5 46. Rd6 Kg5 47. Kg3 Ra1 48. Rd8 Rg1+ 49. Kf2 Ra1 50. Rg8+ Kh5 51. Ng3+ Kh6 52. Rf8 Kg7 53. Rc8 Ra2+ 54. Ke3 Ra3+ 55. Kf2 Ra2+ 56. Ne2 Kg6 57. Rc6 Kg5 58. Ke3 Ra3+ 59. Rc3 Ra1 60. Ng3 Re1+ 61. Kf2 Ra1 62. Rc8 Ra2+ 63. Ne2 Kg6 64. Rg8+ Kf7 65. Rg4 Ke6 66. f4 exf4 67. Rxf4 Ke5 68. Kf3 Ra3+ 69. Kg4 Ra2 70. Ng3 Ra8 71. Rf5+ Ke6 72. Kf4 Ra1 73. Nh5 Rf1+ 74. Kg4 Rg1+ 75. Ng3 Re1 76. Rf4 Ke5 77. Rf5+ Ke6 78. Kf4 Rg1 79. Nh5 Rf1+ 80. Kg4 Rg1+ 81. Ng3 Re1 82. Rf4 Ke5 83. Rf5+ Ke6 84. Kf4 Rg1 85. Nh5 Rf",
          "positional_tokens": {
            "prompt_tokens": 167,
            "completion_tokens": 640,
            "total_tokens": 807,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "bxc6",
            "Qxc6+",
            "Nd7",
            "Ne5",
            "Be6",
            "Bxc4",
            "Bxc4",
            "Nxc4",
            "e6",
            "Nb5",
            "Bb4+",
            "Bd2",
            "Bxd2+",
            "Nxd2",
            "O-O",
            "Qxc7",
            "Qxc7",
            "Nxc7",
            "Rac8",
            "Nb5",
            "Rc2",
            "b3",
            "a6",
            "Nd4",
            "Rc3",
            "O-O",
            "Rd3",
            "N2f3",
            "e5",
            "Nc6",
            "f6",
            "Rfd1",
            "Rxd1+",
            "Rxd1",
            "Nc5",
            "Nd2",
            "Kf7",
            "f3",
            "Rc8",
            "Nb4",
            "a5",
            "Nd5",
            "Nd3",
            "Nc4",
            "Nb2",
            "Nxb2",
            "Rc2",
            "Nc4",
            "Rxa2",
            "Rd2",
            "Ra1+",
            "Kf2",
            "a4",
            "bxa4",
            "Rxa4",
            "Nd6+",
            "Kg6",
            "Ne7+",
            "Kh6",
            "Ndf5+",
            "Kg5",
            "h4+",
            "Kh5",
            "g4#",
            "Nxe7",
            "Nxe7",
            "Kxh4",
            "Nf5+",
            "Kg5",
            "Nxg7",
            "Kg6",
            "Nf5",
            "h5",
            "gxh5+",
            "Kxh5",
            "Rd6",
            "Kg5",
            "Kg3",
            "Ra1",
            "Rd8",
            "Rg1+",
            "Kf2",
            "Ra1",
            "Rg8+",
            "Kh5",
            "Ng3+",
            "Kh6",
            "Rf8",
            "Kg7",
            "Rc8",
            "Ra2+",
            "Ke3",
            "Ra3+",
            "Kf2",
            "Ra2+",
            "Ne2",
            "Kg6",
            "Rc6",
            "Kg5",
            "Ke3",
            "Ra3+",
            "Rc3",
            "Ra1",
            "Ng3",
            "Re1+",
            "Kf2",
            "Ra1",
            "Rc8",
            "Ra2+",
            "Ne2",
            "Kg6",
            "Rg8+",
            "Kf7",
            "Rg4",
            "Ke6",
            "f4",
            "exf4",
            "Rxf4",
            "Ke5",
            "Kf3",
            "Ra3+",
            "Kg4",
            "Ra2",
            "Ng3",
            "Ra8",
            "Rf5+",
            "Ke6",
            "Kf4",
            "Ra1",
            "Nh5",
            "Rf1+",
            "Kg4",
            "Rg1+",
            "Ng3",
            "Re1",
            "Rf4",
            "Ke5",
            "Rf5+",
            "Ke6",
            "Kf4",
            "Rg1",
            "Nh5",
            "Rf1+",
            "Kg4",
            "Rg1+",
            "Ng3",
            "Re1",
            "Rf4",
            "Ke5",
            "Rf5+",
            "Ke6",
            "Kf4",
            "Rg1",
            "Nh5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "dxc6",
          "neutral_uci": "d5c6",
          "neutral_response": "8. dxc6 bxc6 9. Qxc6+ Bd7 10. Qxc4 e6 11. Bg5 Be7 12. Rd1 O-O 13. Ne5 Bd6 14. Nxd7 Qxd7 15. e5 Nd5 16. exd6 cxd6 17. Nxd5 exd5 18. Qxd5 Rfe8+ 19. Be2 Qg4 20. Be3 Qb4+ 21. Qd2 Qb7 22. O-O Rad8 23. Bf3 Qa6 24. a3 Re5 25. Bd4 Rf5 26. Be4 Rh5 27. Qf4 d5 28. Qg4 Qh6 29. Be3 g5 30. Rxd5 Rxd5 31. Bxd5 Rxh2 32. Qxg5+ Qxg5 33. Bxg5 Rh5 34. f4 h6 35. Bf3 hxg5 36. Bxh5 gxf4 37. Rxf4 Kg7 38. Rxf7+ Kh6 39. g4 Kg5 40. Rxa7 Kh4 41. b4 Kg3 42. b5 Kf4 43. b6 Ke5 44. b7 Kd6 45. b8=Q+ Kc6 46. Rc7+ Kd6 47. Qd8+ Ke6 48. Re7+ Kf6 49. Qf8+ Kg5 50. Rg7+ Kh4 51. Qf2+ Kh3 52. Qh2# 1-0 8. dxc6 bxc6 9. Qxc6+ Bd7 10. Qxc4 e6",
          "neutral_tokens": {
            "prompt_tokens": 166,
            "completion_tokens": 409,
            "total_tokens": 575,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "bxc6",
            "Qxc6+",
            "Bd7",
            "Qxc4",
            "e6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d5c6",
          "positional_uci": "d5c6",
          "neutral_uci": "d5c6",
          "consensus_move": "d5c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "bxc6",
              "Qxc6+",
              "Nd7",
              "Ne5",
              "Be6",
              "Bxc4",
              "Bxc4",
              "Nxc4",
              "e6",
              "Nb5",
              "Bb4+",
              "Bd2",
              "Bxd2+",
              "Nxd2",
              "O-O",
              "Qxc7",
              "Qe7",
              "O-O",
              "Rfc8",
              "Qd6",
              "Qe8",
              "Nc7",
              "Rxc7",
              "Qxc7",
              "Rc8",
              "Qxa7",
              "Rc2",
              "Qd4",
              "e5",
              "Qd3",
              "Rxb2",
              "Rfb1",
              "Rxb1+",
              "Rxb1",
              "Nc5",
              "Qb5",
              "Qxb5",
              "Rxb5",
              "Nd7",
              "Rd5",
              "Nf8",
              "Rxe5",
              "f6",
              "Re7",
              "Ng6",
              "Rb7",
              "Ne5",
              "a4",
              "Nc6",
              "Nc4",
              "Nd4",
              "a5",
              "Nc6",
              "a6",
              "Kf8",
              "a7",
              "Nxa7",
              "Rxa7",
              "h6",
              "Nd6",
              "Kg8",
              "Nf5",
              "g6",
              "Nxh6+",
              "Kf8",
              "Ng4",
              "f5",
              "exf5",
              "gxf5",
              "Nh6",
              "f4",
              "g3",
              "fxg3",
              "hxg3",
              "Ke8",
              "g4",
              "Kf8",
              "g5",
              "Ke8",
              "g6",
              "Kf8",
              "g7+",
              "Ke8",
              "g8=Q#",
              "e4",
              "f5",
              "g5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "bxc6",
              "Qxc6+",
              "Nd7",
              "Ne5",
              "Be6",
              "Bxc4",
              "Bxc4",
              "Nxc4",
              "e6",
              "Nb5",
              "Bb4+",
              "Bd2",
              "Bxd2+",
              "Nxd2",
              "O-O",
              "Qxc7",
              "Qxc7",
              "Nxc7",
              "Rac8",
              "Nb5",
              "Rc2",
              "b3",
              "a6",
              "Nd4",
              "Rc3",
              "O-O",
              "Rd3",
              "N2f3",
              "e5",
              "Nc6",
              "f6",
              "Rfd1",
              "Rxd1+",
              "Rxd1",
              "Nc5",
              "Nd2",
              "Kf7",
              "f3",
              "Rc8",
              "Nb4",
              "a5",
              "Nd5",
              "Nd3",
              "Nc4",
              "Nb2",
              "Nxb2",
              "Rc2",
              "Nc4",
              "Rxa2",
              "Rd2",
              "Ra1+",
              "Kf2",
              "a4",
              "bxa4",
              "Rxa4",
              "Nd6+",
              "Kg6",
              "Ne7+",
              "Kh6",
              "Ndf5+",
              "Kg5",
              "h4+",
              "Kh5",
              "g4#",
              "Nxe7",
              "Nxe7",
              "Kxh4",
              "Nf5+",
              "Kg5",
              "Nxg7",
              "Kg6",
              "Nf5",
              "h5",
              "gxh5+",
              "Kxh5",
              "Rd6",
              "Kg5",
              "Kg3",
              "Ra1",
              "Rd8",
              "Rg1+",
              "Kf2",
              "Ra1",
              "Rg8+",
              "Kh5",
              "Ng3+",
              "Kh6",
              "Rf8",
              "Kg7",
              "Rc8",
              "Ra2+",
              "Ke3",
              "Ra3+",
              "Kf2",
              "Ra2+",
              "Ne2",
              "Kg6",
              "Rc6",
              "Kg5",
              "Ke3",
              "Ra3+",
              "Rc3",
              "Ra1",
              "Ng3",
              "Re1+",
              "Kf2",
              "Ra1",
              "Rc8",
              "Ra2+",
              "Ne2",
              "Kg6",
              "Rg8+",
              "Kf7",
              "Rg4",
              "Ke6",
              "f4",
              "exf4",
              "Rxf4",
              "Ke5",
              "Kf3",
              "Ra3+",
              "Kg4",
              "Ra2",
              "Ng3",
              "Ra8",
              "Rf5+",
              "Ke6",
              "Kf4",
              "Ra1",
              "Nh5",
              "Rf1+",
              "Kg4",
              "Rg1+",
              "Ng3",
              "Re1",
              "Rf4",
              "Ke5",
              "Rf5+",
              "Ke6",
              "Kf4",
              "Rg1",
              "Nh5",
              "Rf1+",
              "Kg4",
              "Rg1+",
              "Ng3",
              "Re1",
              "Rf4",
              "Ke5",
              "Rf5+",
              "Ke6",
              "Kf4",
              "Rg1",
              "Nh5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "bxc6",
              "Qxc6+",
              "Bd7",
              "Qxc4",
              "e6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 167,
            "completion_tokens": 640,
            "total_tokens": 807,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 167,
            "completion_tokens": 640,
            "total_tokens": 807,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 166,
            "completion_tokens": 409,
            "total_tokens": 575,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 500,
          "total_completion_tokens": 1689,
          "total_tokens": 2189
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 167,
            "completion_tokens": 640,
            "total_tokens": 807,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 167,
            "completion_tokens": 640,
            "total_tokens": 807,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 166,
            "completion_tokens": 409,
            "total_tokens": 575,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/2n2n2/3P4/Q1p1P1b1/2N2N2/PP3PPP/R1B1KB1R w KQkq - 3 8"
    },
    {
      "turn": 17,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bxc4",
          "aggressive_uci": "f1c4",
          "aggressive_response": "9. Bxc4 Bxf3 10. gxf3 e6 11. Bg5 Be7 12. Rd1 Qc8 13. Ba6 Qb8 14. Bb7 O-O 15.\nBxa8 Qxa8 16. e5 Nd5 17. Nxd5 Bxg5 18. Nxc7 Qb8 19. Rd7 Rd8 20. Na6 Qxe5+ 21.\nKf1 Rxd7 22. cxd7 Qd6 23. Kg2 h6 24. Rd1 Qe7 25. d8=Q+ Qxd8 26. Rxd8+ Bxd8 27.\nQe8+ Kh7 28. Qxd8 Kg6 29. Qd3+ f5 30. Nc7 Kf6 31. Qd6 Kg5 32. Nxe6+ Kh5 33.\nNxg7+ Kg5 34. f4+ Kg4 35. Qg6+ Kxf4 36. Qxf5# e4 Qg4+ Kf3 Qg6+ Ke2 Qg4+ Kd2 Qg6+ Kc3 Qg4+ Kb4 Qg6+ Ka5 Qg4+ b5#",
          "aggressive_tokens": {
            "prompt_tokens": 175,
            "completion_tokens": 288,
            "total_tokens": 463,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bxf3",
            "gxf3",
            "e6",
            "Bg5",
            "Be7",
            "Rd1",
            "Qc8",
            "Ba6",
            "Qb8",
            "Bb7",
            "O-O",
            "Bxa8",
            "Qxa8",
            "e5",
            "Nd5",
            "Nxd5",
            "Bxg5",
            "Nxc7",
            "Qb8",
            "Rd7",
            "Rd8",
            "Na6",
            "Qxe5+",
            "Kf1",
            "Rxd7",
            "cxd7",
            "Qd6",
            "Kg2",
            "h6",
            "Rd1",
            "Qe7",
            "d8=Q+",
            "Qxd8",
            "Rxd8+",
            "Bxd8",
            "Qe8+",
            "Kh7",
            "Qxd8",
            "Kg6",
            "Qd3+",
            "f5",
            "Nc7",
            "Kf6",
            "Qd6",
            "Kg5",
            "Nxe6+",
            "Kh5",
            "Nxg7+",
            "Kg5",
            "f4+",
            "Kg4",
            "Qg6+",
            "Kxf4",
            "Qxf5#",
            "e4",
            "Qg4+",
            "Kf3",
            "Qg6+",
            "Ke2",
            "Qg4+",
            "Kd2",
            "Qg6+",
            "Kc3",
            "Qg4+",
            "Kb4",
            "Qg6+",
            "Ka5",
            "Qg4+",
            "b5#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bxc4",
          "positional_uci": "f1c4",
          "positional_response": "9. Bxc4 Bxf3 10. gxf3 e6 11. Bg5 Be7 12. Rd1 Qc8 13. Ba6 Qb8 14. Bb7 O-O 15.\nBxa8 Qxa8 16. e5 Nd5 17. Nxd5 Bxg5 18. Nxc7 Qb8 19. Rd7 Rd8 20. Na6 Qxe5+ 21.\nKf1 Rc8 22. c7 h6 23. Qc6 Qxb2 24. Qb7 Rf8 25. c8=Q Rxc8 26. Qxc8+ Kh7 27. Rxf7\nQxa2 28. Qb7 Qa1+ 29. Kg2 Qe5 30. Qxa7 Bf6 31. Qxb6 Qg5+ 32. Kf1 Qc1+ 33. Ke2\nQxh1 34. Qxe6 Qa1 35. Qf5+ Kg8 36. Qd5 Kh7 37. Nc5 Qb2+ 38. Kf1 Qc1+ 39. Kg2\nQg5+ 40. Qxg5 hxg5 41. Ne4 Kg6 42. Rb7 Kf5 43. Rb5+ Kg6 44. Kg3 Kh6 45. Kg4 Kg6\n46. Nxg5 Kh6 47. Nf7+ Kg6 48. Nh8+ Kh7 49. Rh5+ Kg8 50. Kf5 Kf8 51. Ng6+ Kf7\n52. Rh8 Be7 53. Nxe7 Kxe7 54. Kg6 Ke6 55. Kxg7 Kf5 56. h4 Kf4 57. h5 Kxf3 58.\nh6 Kxf2 59. h7 Kf3 60. Rf8+ Kg4 61. h8=Q Kg5 62. Qh6+ Kg4 63. Rf4+ Kg3 64.\nQh4+ Kg2 65. Rf2+ Kg1 66. Qh2# Qd4+ 68. Kg6 Qg4+ 69. Kf6 Qd4+ 70. Ke6 Qe4+ 71.\nKd6 Qd4+ 72. Kc6 Qc4+ 73. Kb6 Qb4+ 74. Kc7 Qc5+ 75. Kd7 Qd5+ 76. Qd6 Qb7+\n77. Ke6 Qe4+ 78. Qe5 Qc6+ 79. Kf5 Qd7+ 80. Kg5 Qd8+ 81. Kg4 Qg8+ 82",
          "positional_tokens": {
            "prompt_tokens": 175,
            "completion_tokens": 640,
            "total_tokens": 815,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bxf3",
            "gxf3",
            "e6",
            "Bg5",
            "Be7",
            "Rd1",
            "Qc8",
            "Ba6",
            "Qb8",
            "Bb7",
            "O-O",
            "Bxa8",
            "Qxa8",
            "e5",
            "Nd5",
            "Nxd5",
            "Bxg5",
            "Nxc7",
            "Qb8",
            "Rd7",
            "Rd8",
            "Na6",
            "Qxe5+",
            "Kf1",
            "Rc8",
            "c7",
            "h6",
            "Qc6",
            "Qxb2",
            "Qb7",
            "Rf8",
            "c8=Q",
            "Rxc8",
            "Qxc8+",
            "Kh7",
            "Rxf7",
            "Qxa2",
            "Qb7",
            "Qa1+",
            "Kg2",
            "Qe5",
            "Qxa7",
            "Bf6",
            "Qxb6",
            "Qg5+",
            "Kf1",
            "Qc1+",
            "Ke2",
            "Qxh1",
            "Qxe6",
            "Qa1",
            "Qf5+",
            "Kg8",
            "Qd5",
            "Kh7",
            "Nc5",
            "Qb2+",
            "Kf1",
            "Qc1+",
            "Kg2",
            "Qg5+",
            "Qxg5",
            "hxg5",
            "Ne4",
            "Kg6",
            "Rb7",
            "Kf5",
            "Rb5+",
            "Kg6",
            "Kg3",
            "Kh6",
            "Kg4",
            "Kg6",
            "Nxg5",
            "Kh6",
            "Nf7+",
            "Kg6",
            "Nh8+",
            "Kh7",
            "Rh5+",
            "Kg8",
            "Kf5",
            "Kf8",
            "Ng6+",
            "Kf7",
            "Rh8",
            "Be7",
            "Nxe7",
            "Kxe7",
            "Kg6",
            "Ke6",
            "Kxg7",
            "Kf5",
            "h4",
            "Kf4",
            "h5",
            "Kxf3",
            "h6",
            "Kxf2",
            "h7",
            "Kf3",
            "Rf8+",
            "Kg4",
            "h8=Q",
            "Kg5",
            "Qh6+",
            "Kg4",
            "Rf4+",
            "Kg3",
            "Qh4+",
            "Kg2",
            "Rf2+",
            "Kg1",
            "Qh2#",
            "Qd4+",
            "Kg6",
            "Qg4+",
            "Kf6",
            "Qd4+",
            "Ke6",
            "Qe4+",
            "Kd6",
            "Qd4+",
            "Kc6",
            "Qc4+",
            "Kb6",
            "Qb4+",
            "Kc7",
            "Qc5+",
            "Kd7",
            "Qd5+",
            "Qd6",
            "Qb7+",
            "Ke6",
            "Qe4+",
            "Qe5",
            "Qc6+",
            "Kf5",
            "Qd7+",
            "Kg5",
            "Qd8+",
            "Kg4",
            "Qg8+"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bxc4",
          "neutral_uci": "f1c4",
          "neutral_response": "9. Bxc4 e6 10. Ne5 Bd6 11. Nxg4 Nxg4 12. h3 Ne5 13. O-O O-O 14. Be3 Qh4 15.\nBe2 f5 16. exf5 Qxa4 17. Nxa4 exf5 18. Rac1 f4 19. Bd4 f3 20. Bxe5 fxe2 21.\nRfe1 Bxe5 22. Rxe2 Rae8 23. Rce1 Rf5 24. g4 Rg5 25. Kg2 Kf7 26. f4 Bxf4 27.\nRxe8 Ra5 28. R8e7+ Kf6 29. R1e6+ Kg5 30. Rxg7+ Kh4 31. Rxh7+ Kg5 32. Rh5# Bg5\n33. Nc3 Rc5 34. Ne4+ Kf4 35. Nxc5 bxc5 36. Rg6 Be7 37. Rf5+ Ke4 38. Re6+ Kd4\n39. Rxe7 c4 40. Rxc7 c3 41. bxc3+ Kxc3 42. Rxa7 Kb2 43. c7 Ka1 44. c8=Q Kb2\n45. Rb7+ Ka1 46. Qc1+ Kxa2 47. Ra5# Bb6 48. Rxb6# Bb4 49. Rxb4# Bb6 50.\nQb2# Bb4 51. Qxb4# Bb6 52. Qb2# Bb4 53. Qxb4# Bb6 54. Qb2# Bb4 55. Qxb4# Bb6\n56. Qb2# Bb4 57. Qxb4# Bb6 58. Qb2# Bb4 59. Qxb4# Bb6 60. Qb2# Bb4 61. Qxb4# Bb6\n62. Qb2# Bb4 63. Qxb4# Bb6 64. Qb2# Bb4 65. Qxb4# Bb6 66. Qb2# Bb4 67. Qxb4# Bb6\n68. Qb2# Bb4 69. Qxb4# Bb6 70. Qb2# Bb4 71. Qxb4# Bb6 72. Qb2# Bb4 73. Qxb4# Bb6\n74. Qb2# Bb4 75. Qxb4# Bb6 76. Qb2# Bb4 77. Qxb4# Bb6 78. Qb2# Bb4",
          "neutral_tokens": {
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "e6",
            "Ne5",
            "Bd6",
            "Nxg4",
            "Nxg4",
            "h3",
            "Ne5",
            "O-O",
            "O-O",
            "Be3",
            "Qh4",
            "Be2",
            "f5",
            "exf5",
            "Qxa4",
            "Nxa4",
            "exf5",
            "Rac1",
            "f4",
            "Bd4",
            "f3",
            "Bxe5",
            "fxe2",
            "Rfe1",
            "Bxe5",
            "Rxe2",
            "Rae8",
            "Rce1",
            "Rf5",
            "g4",
            "Rg5",
            "Kg2",
            "Kf7",
            "f4",
            "Bxf4",
            "Rxe8",
            "Ra5",
            "R8e7+",
            "Kf6",
            "R1e6+",
            "Kg5",
            "Rxg7+",
            "Kh4",
            "Rxh7+",
            "Kg5",
            "Rh5#",
            "Bg5",
            "Nc3",
            "Rc5",
            "Ne4+",
            "Kf4",
            "Nxc5",
            "bxc5",
            "Rg6",
            "Be7",
            "Rf5+",
            "Ke4",
            "Re6+",
            "Kd4",
            "Rxe7",
            "c4",
            "Rxc7",
            "c3",
            "bxc3+",
            "Kxc3",
            "Rxa7",
            "Kb2",
            "c7",
            "Ka1",
            "c8=Q",
            "Kb2",
            "Rb7+",
            "Ka1",
            "Qc1+",
            "Kxa2",
            "Ra5#",
            "Bb6",
            "Rxb6#",
            "Bb4",
            "Rxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f1c4",
          "positional_uci": "f1c4",
          "neutral_uci": "f1c4",
          "consensus_move": "f1c4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bxf3",
              "gxf3",
              "e6",
              "Bg5",
              "Be7",
              "Rd1",
              "Qc8",
              "Ba6",
              "Qb8",
              "Bb7",
              "O-O",
              "Bxa8",
              "Qxa8",
              "e5",
              "Nd5",
              "Nxd5",
              "Bxg5",
              "Nxc7",
              "Qb8",
              "Rd7",
              "Rd8",
              "Na6",
              "Qxe5+",
              "Kf1",
              "Rxd7",
              "cxd7",
              "Qd6",
              "Kg2",
              "h6",
              "Rd1",
              "Qe7",
              "d8=Q+",
              "Qxd8",
              "Rxd8+",
              "Bxd8",
              "Qe8+",
              "Kh7",
              "Qxd8",
              "Kg6",
              "Qd3+",
              "f5",
              "Nc7",
              "Kf6",
              "Qd6",
              "Kg5",
              "Nxe6+",
              "Kh5",
              "Nxg7+",
              "Kg5",
              "f4+",
              "Kg4",
              "Qg6+",
              "Kxf4",
              "Qxf5#",
              "e4",
              "Qg4+",
              "Kf3",
              "Qg6+",
              "Ke2",
              "Qg4+",
              "Kd2",
              "Qg6+",
              "Kc3",
              "Qg4+",
              "Kb4",
              "Qg6+",
              "Ka5",
              "Qg4+",
              "b5#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bxf3",
              "gxf3",
              "e6",
              "Bg5",
              "Be7",
              "Rd1",
              "Qc8",
              "Ba6",
              "Qb8",
              "Bb7",
              "O-O",
              "Bxa8",
              "Qxa8",
              "e5",
              "Nd5",
              "Nxd5",
              "Bxg5",
              "Nxc7",
              "Qb8",
              "Rd7",
              "Rd8",
              "Na6",
              "Qxe5+",
              "Kf1",
              "Rc8",
              "c7",
              "h6",
              "Qc6",
              "Qxb2",
              "Qb7",
              "Rf8",
              "c8=Q",
              "Rxc8",
              "Qxc8+",
              "Kh7",
              "Rxf7",
              "Qxa2",
              "Qb7",
              "Qa1+",
              "Kg2",
              "Qe5",
              "Qxa7",
              "Bf6",
              "Qxb6",
              "Qg5+",
              "Kf1",
              "Qc1+",
              "Ke2",
              "Qxh1",
              "Qxe6",
              "Qa1",
              "Qf5+",
              "Kg8",
              "Qd5",
              "Kh7",
              "Nc5",
              "Qb2+",
              "Kf1",
              "Qc1+",
              "Kg2",
              "Qg5+",
              "Qxg5",
              "hxg5",
              "Ne4",
              "Kg6",
              "Rb7",
              "Kf5",
              "Rb5+",
              "Kg6",
              "Kg3",
              "Kh6",
              "Kg4",
              "Kg6",
              "Nxg5",
              "Kh6",
              "Nf7+",
              "Kg6",
              "Nh8+",
              "Kh7",
              "Rh5+",
              "Kg8",
              "Kf5",
              "Kf8",
              "Ng6+",
              "Kf7",
              "Rh8",
              "Be7",
              "Nxe7",
              "Kxe7",
              "Kg6",
              "Ke6",
              "Kxg7",
              "Kf5",
              "h4",
              "Kf4",
              "h5",
              "Kxf3",
              "h6",
              "Kxf2",
              "h7",
              "Kf3",
              "Rf8+",
              "Kg4",
              "h8=Q",
              "Kg5",
              "Qh6+",
              "Kg4",
              "Rf4+",
              "Kg3",
              "Qh4+",
              "Kg2",
              "Rf2+",
              "Kg1",
              "Qh2#",
              "Qd4+",
              "Kg6",
              "Qg4+",
              "Kf6",
              "Qd4+",
              "Ke6",
              "Qe4+",
              "Kd6",
              "Qd4+",
              "Kc6",
              "Qc4+",
              "Kb6",
              "Qb4+",
              "Kc7",
              "Qc5+",
              "Kd7",
              "Qd5+",
              "Qd6",
              "Qb7+",
              "Ke6",
              "Qe4+",
              "Qe5",
              "Qc6+",
              "Kf5",
              "Qd7+",
              "Kg5",
              "Qd8+",
              "Kg4",
              "Qg8+"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e6",
              "Ne5",
              "Bd6",
              "Nxg4",
              "Nxg4",
              "h3",
              "Ne5",
              "O-O",
              "O-O",
              "Be3",
              "Qh4",
              "Be2",
              "f5",
              "exf5",
              "Qxa4",
              "Nxa4",
              "exf5",
              "Rac1",
              "f4",
              "Bd4",
              "f3",
              "Bxe5",
              "fxe2",
              "Rfe1",
              "Bxe5",
              "Rxe2",
              "Rae8",
              "Rce1",
              "Rf5",
              "g4",
              "Rg5",
              "Kg2",
              "Kf7",
              "f4",
              "Bxf4",
              "Rxe8",
              "Ra5",
              "R8e7+",
              "Kf6",
              "R1e6+",
              "Kg5",
              "Rxg7+",
              "Kh4",
              "Rxh7+",
              "Kg5",
              "Rh5#",
              "Bg5",
              "Nc3",
              "Rc5",
              "Ne4+",
              "Kf4",
              "Nxc5",
              "bxc5",
              "Rg6",
              "Be7",
              "Rf5+",
              "Ke4",
              "Re6+",
              "Kd4",
              "Rxe7",
              "c4",
              "Rxc7",
              "c3",
              "bxc3+",
              "Kxc3",
              "Rxa7",
              "Kb2",
              "c7",
              "Ka1",
              "c8=Q",
              "Kb2",
              "Rb7+",
              "Ka1",
              "Qc1+",
              "Kxa2",
              "Ra5#",
              "Bb6",
              "Rxb6#",
              "Bb4",
              "Rxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 175,
            "completion_tokens": 288,
            "total_tokens": 463,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 175,
            "completion_tokens": 640,
            "total_tokens": 815,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 524,
          "total_completion_tokens": 1568,
          "total_tokens": 2092
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 175,
            "completion_tokens": 288,
            "total_tokens": 463,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 175,
            "completion_tokens": 640,
            "total_tokens": 815,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r2qkb1r/p1p1pppp/1pP2n2/8/Q1p1P1b1/2N2N2/PP3PPP/R1B1KB1R w KQkq - 0 9"
    },
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e5",
          "aggressive_uci": "e7e5",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6\n4. Ba4 Nf6 5. O-O Be7 6. Re1 b5\n7. Bb3 d6 8. c3 O-O 9. h3 Na5\n10. Bc2 c5 11. d4 Qc7 12. Nbd2 cxd4\n13. cxd4 Nc6 14. Nb3 a5 15. Be3 a4\n16. Nbd2 Bd7 17. Rc1 Qb7 18. Bb1 Rfc8\n19. Nf1 Bd8 20. Ng3 Bb6 21. Qd2 Ba5\n22. Qe2 Bxe1 23. Rxe1 h6 24. Qd2 Qa6\n25. Bxh6 gxh6 26. Qxh6 Nh7 27. Nh5 1-0\n27. Nh7 Nxd4 28. Nf6+ Kh8",
          "aggressive_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 236,
            "total_tokens": 350,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Qc7",
            "Nbd2",
            "cxd4",
            "cxd4",
            "Nc6",
            "Nb3",
            "a5",
            "Be3",
            "a4",
            "Nbd2",
            "Bd7",
            "Rc1",
            "Qb7",
            "Bb1",
            "Rfc8",
            "Nf1",
            "Bd8",
            "Ng3",
            "Bb6",
            "Qd2",
            "Ba5",
            "Qe2",
            "Bxe1",
            "Rxe1",
            "h6",
            "Qd2",
            "Qa6",
            "Bxh6",
            "gxh6",
            "Qxh6",
            "Nh7",
            "Nh5",
            "Nh7",
            "Nxd4",
            "Nf6+",
            "Kh8"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e5",
          "positional_uci": "e7e5",
          "positional_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7\n6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5\n11. d4 Qc7 12. Nbd2 cxd4 13. cxd4 Nc6 14. Nb3 a5\n15. Be3 a4 16. Nbd2 Bd7 17. Rc1 Qb7 18. Bb1 Rfc8\n19. Nf1 Bd8 20. Ng3 Bb6 21. Qd2 Ba5 22. Qe2 Bxe1\n23. Rxe1 Re8 24. Qd2 Rac8 25. Bg5 Kh8 26. Bxf6 gxf6\n27. Qh6 Rg8 28. Qxf6+ Rg7 29. Nh5 Rg8 30. Ng5 Be8\n31. Nxg7 Rxg7 32. dxe5 dxe5 33. Rd1 Qe7 34. Qxe7 Nxe7\n35. Rd8 Rg8 36. Nf3 f6 37. Bd3 Kg7 38. Rb8 Kf7\n39. Bxb5 Bxb5 40. Rxb5 Rc8 41. Rb4 Rc1+ 42. Kh2 Rc2\n43. Kg3 Ng6 44. Ne1 Re2 45. Nd3 Rd2 46. Nc5 Nf4\n47. Nxa4 Nd3 48. Rb7+ Kg6 49. f3 Nf4 50. Kh2 Rxg2+\n51. Kh1 Rf2 52. Nc3 Rxf3 53. a4 Rxh3+ 54. Kg1 Rd3\n55. a5 Rd6 56. Rb6 Rd7 57. a6 Ra7 58. Nb5 Ra8\n59. a7 Nd3 60. Rb8 Rxa7 61. Nxa7 Nc5 62. b4 Nxe4\n63. b5 Nd6 64. b6 Kf5 65. Rd8 Nb7 66. Rd7 Nc5\n67. Rxh7 Ke6 68. b7 Nxb7 69. Rxb7 Kd5 70. Kf2 f5\n71. Rb5+ Ke4 72. Nc6 Kf4 73. Rxe5 Kg4 74. Nd4 f4\n75. Rf5 Kh4 76. Kf3 Kh3 77. Rh5# e5 d5 Nf6",
          "positional_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 622,
            "total_tokens": 736,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d5",
            "Nf6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e5",
          "neutral_uci": "e7e5",
          "neutral_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6\n4. Ba4 Nf6 5. O-O Be7 6. Re1 b5\n7. Bb3 d6 8. c3 O-O 9. h3 Bb7\n10. d4 Re8 11. Nbd2 Bf8 12. a4 h6\n13. Bc2 exd4 14. cxd4 Nb4 15. Bb1 c5\n16. d5 Nd7 17. Ra3 c4 18. Nd4 Qf6\n19. N2f3 Nc5 20. axb5 Nbd3 21. Bxd3 Nxd3\n22. Re2 axb5 23. Nxb5 Rxa3 24. Nxa3 Ba6\n25. Qa4 Ra8 26. Be3 Nxb2\n27. Qc6 Rc8 28. Qxa6 c3 29. Qxc8 Nd3",
          "neutral_tokens": {
            "prompt_tokens": 113,
            "completion_tokens": 230,
            "total_tokens": 343,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Bb7",
            "d4",
            "Re8",
            "Nbd2",
            "Bf8",
            "a4",
            "h6",
            "Bc2",
            "exd4",
            "cxd4",
            "Nb4",
            "Bb1",
            "c5",
            "d5",
            "Nd7",
            "Ra3",
            "c4",
            "Nd4",
            "Qf6",
            "N2f3",
            "Nc5",
            "axb5",
            "Nbd3",
            "Bxd3",
            "Nxd3",
            "Re2",
            "axb5",
            "Nxb5",
            "Rxa3",
            "Nxa3",
            "Ba6",
            "Qa4",
            "Ra8",
            "Be3",
            "Nxb2",
            "Qc6",
            "Rc8",
            "Qxa6",
            "c3",
            "Qxc8",
            "Nd3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e7e5",
          "positional_uci": "e7e5",
          "neutral_uci": "e7e5",
          "consensus_move": "e7e5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Qc7",
              "Nbd2",
              "cxd4",
              "cxd4",
              "Nc6",
              "Nb3",
              "a5",
              "Be3",
              "a4",
              "Nbd2",
              "Bd7",
              "Rc1",
              "Qb7",
              "Bb1",
              "Rfc8",
              "Nf1",
              "Bd8",
              "Ng3",
              "Bb6",
              "Qd2",
              "Ba5",
              "Qe2",
              "Bxe1",
              "Rxe1",
              "h6",
              "Qd2",
              "Qa6",
              "Bxh6",
              "gxh6",
              "Qxh6",
              "Nh7",
              "Nh5",
              "Nh7",
              "Nxd4",
              "Nf6+",
              "Kh8"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "d5",
              "Nf6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Bb7",
              "d4",
              "Re8",
              "Nbd2",
              "Bf8",
              "a4",
              "h6",
              "Bc2",
              "exd4",
              "cxd4",
              "Nb4",
              "Bb1",
              "c5",
              "d5",
              "Nd7",
              "Ra3",
              "c4",
              "Nd4",
              "Qf6",
              "N2f3",
              "Nc5",
              "axb5",
              "Nbd3",
              "Bxd3",
              "Nxd3",
              "Re2",
              "axb5",
              "Nxb5",
              "Rxa3",
              "Nxa3",
              "Ba6",
              "Qa4",
              "Ra8",
              "Be3",
              "Nxb2",
              "Qc6",
              "Rc8",
              "Qxa6",
              "c3",
              "Qxc8",
              "Nd3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 114,
            "completion_tokens": 236,
            "total_tokens": 350,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 114,
            "completion_tokens": 622,
            "total_tokens": 736,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 113,
            "completion_tokens": 230,
            "total_tokens": 343,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 341,
          "total_completion_tokens": 1088,
          "total_tokens": 1429
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 114,
            "completion_tokens": 236,
            "total_tokens": 350,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 114,
            "completion_tokens": 622,
            "total_tokens": 736,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 113,
            "completion_tokens": 230,
            "total_tokens": 343,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": "d2d4",
          "aggressive_response": "d4 d5 2. Nf3 Nf6 3. Bg5 Bf5 4. e3 e6 5. Bd3 Bxd3 6. Qxd3 Be7 7. O-O O-O 8. Nbd2 Nbd7 9. c4 c6 10. Rac1 Rc8 11. cxd5 exd5 12. a3 Re8 13. b4 a6 14. Nb3 Ne4 15. Bxe7 Qxe7 16. Nc5 Ndxc5 17. bxc5 f5 18. Rb1 Rc7 19. Rb6 Qf6 20. Rfb1 Ree7 21. Ne5 Qh4 22. g3 Qh3 23. Nxc6 bxc6 24. Rb8+ Kf7 25. Qxa6 Nd2 26. Qe2 Nxb1 27. Rxb1 Rb7 28. Ra1 Qg4 29. Qc2 Qe4 30. Qd1 Rb2 31. a4 Reb7 32. Qh5+ Kf8 33. Rf1 Rb1 34. Qe2 Rxf1+ 35. Qxf1 Rb1 36. a5 Rxf1+ 37. Kxf1 Qb1+ 38. Kg2 Qa2 39. Kf3 Qxa5 40. Kf4 g6 41. Ke5 Ke7 42. f4 Qc7# d4 d5 2. Nf3 Nf6 3. Bg5 Bf5 4. e3 e6 5. Bd3 Bxd3 6. Qxd3 Be7 7. O-O O-O 8. Nbd2 Nbd7 9. c4 c6 10. Rac1 Rc8 11. cxd5 exd5 12. a3 Re8 13. b4 a6 14. Nb3 Ne4 15. Bxe7 Qxe7 16. Nc5 Ndxc5 17. bxc5 f5 18. Rb1 Rc7 19. Rb6 Qf6 20. Rfb1 Ree7 21. Ne5 Qh4 22. g3 Qh3 23. Nxc6 bxc6 24. Rb8+ Kf7 25. Qxa6 Nd2 26. Qe2 Nxb1 27. Rxb1 Rb7 28. Ra1 Qg4 29. Qc2 Qe4 30. Qd1 Rb2 31. a4 Reb7 32. Qh5+ Kf8 33. Rf1 Rb1 34. Qe2 Rxf1+ 35. Qxf1 Rb1 36. a",
          "aggressive_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Nf3",
            "Nf6",
            "Bg5",
            "Bf5",
            "e3",
            "e6",
            "Bd3",
            "Bxd3",
            "Qxd3",
            "Be7",
            "O-O",
            "O-O",
            "Nbd2",
            "Nbd7",
            "c4",
            "c6",
            "Rac1",
            "Rc8",
            "cxd5",
            "exd5",
            "a3",
            "Re8",
            "b4",
            "a6",
            "Nb3",
            "Ne4",
            "Bxe7",
            "Qxe7",
            "Nc5",
            "Ndxc5",
            "bxc5",
            "f5",
            "Rb1",
            "Rc7",
            "Rb6",
            "Qf6",
            "Rfb1",
            "Ree7",
            "Ne5",
            "Qh4",
            "g3",
            "Qh3",
            "Nxc6",
            "bxc6",
            "Rb8+",
            "Kf7",
            "Qxa6",
            "Nd2",
            "Qe2",
            "Nxb1",
            "Rxb1",
            "Rb7",
            "Ra1",
            "Qg4",
            "Qc2",
            "Qe4",
            "Qd1",
            "Rb2",
            "a4",
            "Reb7",
            "Qh5+",
            "Kf8",
            "Rf1",
            "Rb1",
            "Qe2",
            "Rxf1+",
            "Qxf1",
            "Rb1"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 Bc5 4. O-O Nge7 5. Nxe5 Nxe5 6. d4 Bxd4 7. Qxd4 N7c6 8. Bxc6 Nxc6 9. Qxg7 Rf8 10. Bh6 Qe7 11. Nc3 d6 12. Nd5 Qe5 13. Qxf8+ Kd7 14. Qxf7+ Kd8 15. Qxc7+ Ke8 16. Qxh7 Kd8 17. Qg8+ Kd7 18. Qf7+ Kd8 19. Bg7 Qe8 20. Bf6+ Ne7 21. Bxe7+ Kd7 22. Nf6+ Kc6 23. Qxe8+ Kb6 24. Nd5+ Ka6 25. Qa4# 1-0 e4 Nc6 Nf3 Nf6 d4 d5 Bg5 Be7 Bxf6 Bxf6 Nxd5 Nxd5 Qxd5 O-O Nxe5 Bxe5 Qxe5 Nc6 Qe7 Qf3 d6 Re1 Be6 Bxc6+ bxc6 Qxc6+ Qd7 Qxa8+ Ke7 Qxh8 Qc6 Qxg7 Qxc2 Bg5+ Kd7 Qf6 Qxb2 Qxb2 Kc6 Qc3+ Kd7 Qd4 Kc6 Rac1+ Kd7 Qa4# 1-0 e4 e5 Nf3 Nc6 Bb5 Bc5 O-O Nge7 Nxe5 Nxe5 d4 Bxd4 Qxd4 N7c6 Bxc6 Nxc6 Qxg7 Rf8 Bh6 Qe7 Nc3 d6 Nd5 Qe5 Qxf8+ Kd7 Qxf7+ Kd8 Qxc7+ Ke8 Qxh7 Kd8 Qg8+ Kd7 Qf7+ Kd8 Bg7 Qe5 Bxe5 Nxe5 Qe7#",
          "positional_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 492,
            "total_tokens": 602,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bb5",
            "Bc5",
            "O-O",
            "Nge7",
            "Nxe5",
            "Nxe5",
            "d4",
            "Bxd4",
            "Qxd4",
            "N7c6",
            "Bxc6",
            "Nxc6",
            "Qxg7",
            "Rf8",
            "Bh6",
            "Qe7",
            "Nc3",
            "d6",
            "Nd5",
            "Qe5",
            "Qxf8+",
            "Kd7",
            "Qxf7+",
            "Kd8",
            "Qxc7+",
            "Ke8",
            "Qxh7",
            "Kd8",
            "Qg8+",
            "Kd7",
            "Qf7+",
            "Kd8",
            "Bg7",
            "Qe5",
            "Bxe5",
            "Nxe5",
            "Qe7#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": "d2d4",
          "neutral_response": "d4 d5 2. Nf3 Nf6 3. Bg5 Bf5 4. e3 e6\n5. Bd3 Bxd3 6. Qxd3 Be7 7. O-O O-O 8. Nbd2 Nbd7\n9. c4 c6 10. Rac1 Rc8 11. cxd5 exd5 12. a3 Re8\n13. b4 Ne4 14. Bxe7 Qxe7 15. Nb3 f5 16. Nc5 Ndxc5\n17. bxc5 g5 18. Ne5 g4 19. f3 gxf3 20. Rxf3 Rf8\n21. Rcf1 Qe6 22. Rf4 Rc7 23. Qe2 Rg7 24. Qh5 Rg5\n25. Qh3 Rf6 26. Nf3 Rh6 27. Nh4 Rgh5 28. g3 Ng5\n29. Qg2 Qxe3+ 30. Kh1 Qe6 31. Rxf5 Qe4 32. Rf8+ Kg7\n33. Qxe4 Nxe4 34. R1f7# 1-0\nd4 d5 2. Nf3 Nf6 3. Bg5 Bf5 4. e3 e6 5. Bd3 Bxd3 6. Qxd3 Be7 7. O-O O-O 8. Nbd2 Nbd7 9. c4 c6 10. Rac1 Rc8 11. cxd5 exd5 12. a3 Re8 13. b4 Ne4 14. Bxe7 Qxe7 15. Nb3 f5 16. Nc5 Ndxc5 17. bxc5 g5 18. Ne5 g4 19. f3 gxf3 20. Rxf3 Rf8 21. Rcf1 Qe6 22. Rf4 Rc7 23. Qe2 Rg7 24. Qh5 Rg5 25. Qh3 Rf6 26. Nf3 Rh6 27. Nh4 Rgh5 28. g3 Ng5 29. Qg2 Qxe3+ 30. Kh1 Qe6 31. Rxf5 Qe4 32. Rf8+ Kg7 33. Qxe4 Nxe4 34. R1f7# 1-0 Nf6 Nxe4 Nxe4",
          "neutral_tokens": {
            "prompt_tokens": 109,
            "completion_tokens": 568,
            "total_tokens": 677,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d5",
            "Nf3",
            "Nf6",
            "Bg5",
            "Bf5",
            "e3",
            "e6",
            "Bd3",
            "Bxd3",
            "Qxd3",
            "Be7",
            "O-O",
            "O-O",
            "Nbd2",
            "Nbd7",
            "c4",
            "c6",
            "Rac1",
            "Rc8",
            "cxd5",
            "exd5",
            "a3",
            "Re8",
            "b4",
            "Ne4",
            "Bxe7",
            "Qxe7",
            "Nb3",
            "f5",
            "Nc5",
            "Ndxc5",
            "bxc5",
            "g5",
            "Ne5",
            "g4",
            "f3",
            "gxf3",
            "Rxf3",
            "Rf8",
            "Rcf1",
            "Qe6",
            "Rf4",
            "Rc7",
            "Qe2",
            "Rg7",
            "Qh5",
            "Rg5",
            "Qh3",
            "Rf6",
            "Nf3",
            "Rh6",
            "Nh4",
            "Rgh5",
            "g3",
            "Ng5",
            "Qg2",
            "Qxe3+",
            "Kh1",
            "Qe6",
            "Rxf5",
            "Qe4",
            "Rf8+",
            "Kg7",
            "Qxe4",
            "Nxe4",
            "R1f7#",
            "Nf6",
            "Nxe4",
            "Nxe4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d2d4",
          "positional_uci": "e2e4",
          "neutral_uci": "d2d4",
          "consensus_move": "d2d4",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d5",
              "Nf3",
              "Nf6",
              "Bg5",
              "Bf5",
              "e3",
              "e6",
              "Bd3",
              "Bxd3",
              "Qxd3",
              "Be7",
              "O-O",
              "O-O",
              "Nbd2",
              "Nbd7",
              "c4",
              "c6",
              "Rac1",
              "Rc8",
              "cxd5",
              "exd5",
              "a3",
              "Re8",
              "b4",
              "a6",
              "Nb3",
              "Ne4",
              "Bxe7",
              "Qxe7",
              "Nc5",
              "Ndxc5",
              "bxc5",
              "f5",
              "Rb1",
              "Rc7",
              "Rb6",
              "Qf6",
              "Rfb1",
              "Ree7",
              "Ne5",
              "Qh4",
              "g3",
              "Qh3",
              "Nxc6",
              "bxc6",
              "Rb8+",
              "Kf7",
              "Qxa6",
              "Nd2",
              "Qe2",
              "Nxb1",
              "Rxb1",
              "Rb7",
              "Ra1",
              "Qg4",
              "Qc2",
              "Qe4",
              "Qd1",
              "Rb2",
              "a4",
              "Reb7",
              "Qh5+",
              "Kf8",
              "Rf1",
              "Rb1",
              "Qe2",
              "Rxf1+",
              "Qxf1",
              "Rb1"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bb5",
              "Bc5",
              "O-O",
              "Nge7",
              "Nxe5",
              "Nxe5",
              "d4",
              "Bxd4",
              "Qxd4",
              "N7c6",
              "Bxc6",
              "Nxc6",
              "Qxg7",
              "Rf8",
              "Bh6",
              "Qe7",
              "Nc3",
              "d6",
              "Nd5",
              "Qe5",
              "Qxf8+",
              "Kd7",
              "Qxf7+",
              "Kd8",
              "Qxc7+",
              "Ke8",
              "Qxh7",
              "Kd8",
              "Qg8+",
              "Kd7",
              "Qf7+",
              "Kd8",
              "Bg7",
              "Qe5",
              "Bxe5",
              "Nxe5",
              "Qe7#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "d5",
              "Nf3",
              "Nf6",
              "Bg5",
              "Bf5",
              "e3",
              "e6",
              "Bd3",
              "Bxd3",
              "Qxd3",
              "Be7",
              "O-O",
              "O-O",
              "Nbd2",
              "Nbd7",
              "c4",
              "c6",
              "Rac1",
              "Rc8",
              "cxd5",
              "exd5",
              "a3",
              "Re8",
              "b4",
              "Ne4",
              "Bxe7",
              "Qxe7",
              "Nb3",
              "f5",
              "Nc5",
              "Ndxc5",
              "bxc5",
              "g5",
              "Ne5",
              "g4",
              "f3",
              "gxf3",
              "Rxf3",
              "Rf8",
              "Rcf1",
              "Qe6",
              "Rf4",
              "Rc7",
              "Qe2",
              "Rg7",
              "Qh5",
              "Rg5",
              "Qh3",
              "Rf6",
              "Nf3",
              "Rh6",
              "Nh4",
              "Rgh5",
              "g3",
              "Ng5",
              "Qg2",
              "Qxe3+",
              "Kh1",
              "Qe6",
              "Rxf5",
              "Qe4",
              "Rf8+",
              "Kg7",
              "Qxe4",
              "Nxe4",
              "R1f7#",
              "Nf6",
              "Nxe4",
              "Nxe4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 110,
            "completion_tokens": 492,
            "total_tokens": 602,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 109,
            "completion_tokens": 568,
            "total_tokens": 677,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 329,
          "total_completion_tokens": 1700,
          "total_tokens": 2029
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 110,
            "completion_tokens": 492,
            "total_tokens": 602,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 109,
            "completion_tokens": 568,
            "total_tokens": 677,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 3,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "c4",
          "aggressive_uci": "c2c4",
          "aggressive_response": "2. c4 c6 3. Nf3 Nf6 4. Nc3 dxc4 5. a4 Bf5 6. e3 e6 7. Bxc4 Bb4 8. O-O O-O 9. Qe2 Bg6 10. Ne5 Nbd7 11. Nxg6 hxg6 12. Rd1 Qa5 13. Bd2 e5 14. dxe5 Nxe5 15. Bb3 Rad8 16. Be1 Rxd1 17. Rxd1 Rd8 18. Rxd8+ Qxd8 19. Qd1 Qxd1 20. Bxd1 Nd3 21. Kf1 Nxb2 22. Bb3 Nd3 23. Bd2 Nc5 24. Bc2 Bxc3 25. Bxc3 Nd5 26. Bd4 b6 27. a5 Kf8 28. axb6 axb6 29. Ke2 Ne6 30. Bb2 Ke7 31. g3 Kd6 32. e4 Nb4 33. Bb3 c5 34. f4 b5 35. f5 gxf5 36. exf5 Nd4+ 37. Bxd4 cxd4 38. Bxf7 Ke5 39. Be8 Kxf5 40. Bxb5 Kg4 41. Bd7+ Kg5 42. h4+ Kh5 43. Kf3 d3 44. Bg4+ Kh6 45. Ke3 Kg6 46. Bf3 Kf5 47. g4+ Ke5 48. Be4 d2 49. Bf3 Nc2+ 50. Kxd2 Nd4 51. Ke3 Nxf3 52. Kxf3 Kf6 53. Kf4 Kg6 54. Ke5 Kh6 55. Kf5 Kh7 56. Kg5 Kh8 57. Kg6 Kg8 58. h5 Kh8 59. Kf7 Kh7 60. g5 Kh8 61. h6 gxh6 62. g6 h5 63. g7+ Kh7 64. g8=Q+ Kh6 65. Qg6# Nf3 Nf4 Nf5",
          "aggressive_tokens": {
            "prompt_tokens": 116,
            "completion_tokens": 525,
            "total_tokens": 641,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "c6",
            "Nf3",
            "Nf6",
            "Nc3",
            "dxc4",
            "a4",
            "Bf5",
            "e3",
            "e6",
            "Bxc4",
            "Bb4",
            "O-O",
            "O-O",
            "Qe2",
            "Bg6",
            "Ne5",
            "Nbd7",
            "Nxg6",
            "hxg6",
            "Rd1",
            "Qa5",
            "Bd2",
            "e5",
            "dxe5",
            "Nxe5",
            "Bb3",
            "Rad8",
            "Be1",
            "Rxd1",
            "Rxd1",
            "Rd8",
            "Rxd8+",
            "Qxd8",
            "Qd1",
            "Qxd1",
            "Bxd1",
            "Nd3",
            "Kf1",
            "Nxb2",
            "Bb3",
            "Nd3",
            "Bd2",
            "Nc5",
            "Bc2",
            "Bxc3",
            "Bxc3",
            "Nd5",
            "Bd4",
            "b6",
            "a5",
            "Kf8",
            "axb6",
            "axb6",
            "Ke2",
            "Ne6",
            "Bb2",
            "Ke7",
            "g3",
            "Kd6",
            "e4",
            "Nb4",
            "Bb3",
            "c5",
            "f4",
            "b5",
            "f5",
            "gxf5",
            "exf5",
            "Nd4+",
            "Bxd4",
            "cxd4",
            "Bxf7",
            "Ke5",
            "Be8",
            "Kxf5",
            "Bxb5",
            "Kg4",
            "Bd7+",
            "Kg5",
            "h4+",
            "Kh5",
            "Kf3",
            "d3",
            "Bg4+",
            "Kh6",
            "Ke3",
            "Kg6",
            "Bf3",
            "Kf5",
            "g4+",
            "Ke5",
            "Be4",
            "d2",
            "Bf3",
            "Nc2+",
            "Kxd2",
            "Nd4",
            "Ke3",
            "Nxf3",
            "Kxf3",
            "Kf6",
            "Kf4",
            "Kg6",
            "Ke5",
            "Kh6",
            "Kf5",
            "Kh7",
            "Kg5",
            "Kh8",
            "Kg6",
            "Kg8",
            "h5",
            "Kh8",
            "Kf7",
            "Kh7",
            "g5",
            "Kh8",
            "h6",
            "gxh6",
            "g6",
            "h5",
            "g7+",
            "Kh7",
            "g8=Q+",
            "Kh6",
            "Qg6#",
            "Nf3",
            "Nf4",
            "Nf5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf3",
          "positional_uci": "g1f3",
          "positional_response": "2. Nf3 Nf6 3. c4 c6 4. Nc3 dxc4 5. a4 Bf5 6. e3 e6 7. Bxc4 Bb4 8. O-O O-O 9. Qe2 Nbd7 10. e4 Bg6 11. Bd3 Bh5 12. e5 Nd5 13. Nxd5 cxd5 14. Qe3 Bg6 15. Bxg6 hxg6 16. Ng5 Re8 17. Qh3 Nf8 18. Qf3 Qd7 19. Qb3 Be7 20. Bd2 Rec8 21. Rfc1 Rxc1+ 22. Rxc1 Rc8 23. Rxc8 Qxc8 24. Nf3 Nd7 25. Qc3 Qxc3 26. Bxc3 Nb6 27. b3 Kf8 28. Kf1 Ke8 29. Ke2 Kd7 30. Kd3 Kc6 31. h4 Nd7 32. g3 b5 33. axb5+ Kxb5 34. Ne1 Nb8 35. Nc2 Nc6 36. Bd2 a5 37. Bc3 a4 38. bxa4+ Kxa4 39. Bd2 Kb3 40. Bc3 Bd8 41. Bd2 Bb6 42. Bc3 Ba5 43. Bxa5 Nxa5 44. Ne3 Nc4 45. Ng2 Nb2+ 46. Ke3 Kc3 47. Nf4 Nc4+ 48. Kf3 Kxd4 49. h5 Nxe5+ 50. Ke2 gxh5 51. Nxh5 g6 52. Nf4 Ke4 53. Ng2 d4 54. Ne1 d3+ 55. Kd2 Nf3+ 56. Nxf3 Kxf3 57. Kxd3 Kxf2 58. g4 Kf3 59. g5 Kf4 60. Ke2 Kxg5 61. Kf3 Kf5 62. Kg3 e5 63. Kf3 e4+ 64. Ke3 Ke5 65. Kf2 Kf4 66. Ke2 e3 67. Kf1 Kf3 68. Ke1 e2 69. Kd2 Kf2 70. Kd3 e1=Q 71. Kd4 Qe6 72. Kc5 Ke3 73. Kb4 Kd4 74. Kb5 Qd6 75. Ka4 Qb6 76. Ka3 Kc3 77. Ka2 Qb2# d4 d5 e4",
          "positional_tokens": {
            "prompt_tokens": 116,
            "completion_tokens": 636,
            "total_tokens": 752,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxf3",
            "Kxf3",
            "Kxd3",
            "Kxf2",
            "g4",
            "Kf3",
            "g5",
            "Kf4",
            "Ke2",
            "Kxg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "e5",
            "Kf3",
            "e4+",
            "Ke3",
            "Ke5",
            "Kf2",
            "Kf4",
            "Ke2",
            "e3",
            "Kf1",
            "Kf3",
            "Ke1",
            "e2",
            "Kd2",
            "Kf2",
            "Kd3",
            "e1=Q",
            "Kd4",
            "Qe6",
            "Kc5",
            "Ke3",
            "Kb4",
            "Kd4",
            "Kb5",
            "Qd6",
            "Ka4",
            "Qb6",
            "Ka3",
            "Kc3",
            "Ka2",
            "Qb2#",
            "d4",
            "d5",
            "e4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "c4",
          "neutral_uci": "c2c4",
          "neutral_response": "2. c4 c6 3. Nf3 Nf6 4. Nc3 dxc4 5. a4 Bf5 6. e3 e6 7. Bxc4 Bb4 8. O-O O-O 9. Qe2 Nbd7 10. e4 Bg6 11. Bd3 Bh5 12. e5 Nd5 13. Nxd5 cxd5 14. Qe3 Bg6 15. Bxg6 hxg6 16. Ng5 Re8 17. Qh3 Nf8 18. Rd1 Rc8 19. Qb3 Qb6 20. Rd3 Be7 21. Qxb6 axb6 22. Nf3 Rc4 23. Bd2 Ra8 24. b3 Rc6 25. Rc1 Rac8 26. Rdc3 Rxc3 27. Rxc3 Rxc3 28. Bxc3 Nd7 29. Kf1 Nb8 30. Ke2 Nc6 31. Kd3 Kf8 32. Ne1 Ke8 33. Nc2 Kd7 34. b4 f6 35. f4 g5 36. g3 gxf4 37. gxf4 fxe5 38. fxe5 Bg5 39. b5 Ne7 40. Bb4 Nf5 41. Bf8 Bf4 42. h3 g5 43. Bb4 Ke8 44. Bd2 Bxd2 45. Kxd2 Kf7 46. Kd3 Kg6 47. Ne1 Kh5 48. Nf3 Nh4 49. Ng1 Ng2 50. Ke2 Kh4 51. Kf3 Nf4 52. Ke3 Nxh3 53. Nf3+ Kg4 54. Nh2+ Kf5 55. Kf3 Nf4 56. Ng4 Ne2 57. Nh6+ Kg6 58. Ng8 Nxd4+ 59. Kg4 Nf5 60. Nf6 d4 61. Ne4 Ne3+ 62. Kf3 Nc4 63. Nd6 Nxe5+ 64. Ke4 d3 65. Ke3 g4 66. Nxb7 g3 67. Nd6 g2 68. Kf2 d2 69. Kxg2 d1=Q 70. Ne4 Qf3+ 71. Kh2 Ng4+ 72. Kg1 Qxe4 73. a5 Qe2 74. axb6 Qf2+ 75. Kh1 Qh2# 0-1 1. d4 d5 2. c4 c6 3. Nf3 Nf6 4. Nc3 dxc4",
          "neutral_tokens": {
            "prompt_tokens": 115,
            "completion_tokens": 640,
            "total_tokens": 755,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "c6",
            "Nf3",
            "Nf6",
            "Nc3",
            "dxc4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "c2c4",
          "positional_uci": "g1f3",
          "neutral_uci": "c2c4",
          "consensus_move": "c2c4",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "c6",
              "Nf3",
              "Nf6",
              "Nc3",
              "dxc4",
              "a4",
              "Bf5",
              "e3",
              "e6",
              "Bxc4",
              "Bb4",
              "O-O",
              "O-O",
              "Qe2",
              "Bg6",
              "Ne5",
              "Nbd7",
              "Nxg6",
              "hxg6",
              "Rd1",
              "Qa5",
              "Bd2",
              "e5",
              "dxe5",
              "Nxe5",
              "Bb3",
              "Rad8",
              "Be1",
              "Rxd1",
              "Rxd1",
              "Rd8",
              "Rxd8+",
              "Qxd8",
              "Qd1",
              "Qxd1",
              "Bxd1",
              "Nd3",
              "Kf1",
              "Nxb2",
              "Bb3",
              "Nd3",
              "Bd2",
              "Nc5",
              "Bc2",
              "Bxc3",
              "Bxc3",
              "Nd5",
              "Bd4",
              "b6",
              "a5",
              "Kf8",
              "axb6",
              "axb6",
              "Ke2",
              "Ne6",
              "Bb2",
              "Ke7",
              "g3",
              "Kd6",
              "e4",
              "Nb4",
              "Bb3",
              "c5",
              "f4",
              "b5",
              "f5",
              "gxf5",
              "exf5",
              "Nd4+",
              "Bxd4",
              "cxd4",
              "Bxf7",
              "Ke5",
              "Be8",
              "Kxf5",
              "Bxb5",
              "Kg4",
              "Bd7+",
              "Kg5",
              "h4+",
              "Kh5",
              "Kf3",
              "d3",
              "Bg4+",
              "Kh6",
              "Ke3",
              "Kg6",
              "Bf3",
              "Kf5",
              "g4+",
              "Ke5",
              "Be4",
              "d2",
              "Bf3",
              "Nc2+",
              "Kxd2",
              "Nd4",
              "Ke3",
              "Nxf3",
              "Kxf3",
              "Kf6",
              "Kf4",
              "Kg6",
              "Ke5",
              "Kh6",
              "Kf5",
              "Kh7",
              "Kg5",
              "Kh8",
              "Kg6",
              "Kg8",
              "h5",
              "Kh8",
              "Kf7",
              "Kh7",
              "g5",
              "Kh8",
              "h6",
              "gxh6",
              "g6",
              "h5",
              "g7+",
              "Kh7",
              "g8=Q+",
              "Kh6",
              "Qg6#",
              "Nf3",
              "Nf4",
              "Nf5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nxf3",
              "Kxf3",
              "Kxd3",
              "Kxf2",
              "g4",
              "Kf3",
              "g5",
              "Kf4",
              "Ke2",
              "Kxg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "e5",
              "Kf3",
              "e4+",
              "Ke3",
              "Ke5",
              "Kf2",
              "Kf4",
              "Ke2",
              "e3",
              "Kf1",
              "Kf3",
              "Ke1",
              "e2",
              "Kd2",
              "Kf2",
              "Kd3",
              "e1=Q",
              "Kd4",
              "Qe6",
              "Kc5",
              "Ke3",
              "Kb4",
              "Kd4",
              "Kb5",
              "Qd6",
              "Ka4",
              "Qb6",
              "Ka3",
              "Kc3",
              "Ka2",
              "Qb2#",
              "d4",
              "d5",
              "e4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "c6",
              "Nf3",
              "Nf6",
              "Nc3",
              "dxc4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 116,
            "completion_tokens": 525,
            "total_tokens": 641,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 116,
            "completion_tokens": 636,
            "total_tokens": 752,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 115,
            "completion_tokens": 640,
            "total_tokens": 755,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 347,
          "total_completion_tokens": 1801,
          "total_tokens": 2148
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 116,
            "completion_tokens": 525,
            "total_tokens": 641,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 116,
            "completion_tokens": 636,
            "total_tokens": 752,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 115,
            "completion_tokens": 640,
            "total_tokens": 755,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/3p4/3P4/8/PPP1PPPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 5,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf3",
          "aggressive_uci": "g1f3",
          "aggressive_response": "3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. dxc5 Qxd1 8. Rxd1 Bxc5 9. b3 O-O 10. Bb2 b5 11. Be2 Bb7 12. Nbd2 Nbd7 13. Rac1 Rac8 14. a4 bxa4 15. bxa4 Nb6 16. a5 Na4 17. Bxf6 gxf6 18. Nb3 Bb4 19. Rxc8 Rxc8 20. Rd7 Bd5 21. Nfd4 Nc5 22. Nxc5 Rxc5 23. h3 Rxa5 24. Bh5 e5 25. Nf5 Be6 26. Rd8+ Bf8 27. Ne7+ Kg7 28. Nc6 Ra1+ 29. Kh2 a5 30. Ra8 a4 31. Nd8 Bd5 32. Ra7 a3 33. Bxf7 Bxf7 34. Nxf7 Kg6 35. Nh8+ Kf5 36. Rxh7 a2 37. Ra7 Bb4 38. Ra4 Bc3 39. Nf7 Ke6 40. Nd8+ Kd5 41. Nb7 Kc6 42. Nd8+ Kb5 43. Ra8 Ba5 44. Nb7 Rh1+ 45. Kxh1 a1=Q+ 46. Kh2 Bc7 47. Rxa1 e4+ 48. g3 Kc6 49. Na5+ Kd5 50. Nb3 Kc4 51. Nd4 Be5 52. Rc1+ Kd3 53. Rd1+ Kc3 54. Kg2 Kc4 55. h4 Kd5 56. h5 Kd6 57. h6 Ke7 58. h7 f5 59. Nxf5+ Kf7 60. Rd5 Bf6 61. Rd6 Be5 62. Rd7+ Kg6 63. Nh4+ Kh6 64. Re7 Bf6 65. Re6 Kxh7 66. Rxf6 Kg7 67. Rf4 Kh6 68. Rxe4 Kg5 69. Rf4 Kh5 70. e4 Kg5 71. e5 Kh5 72. e6 Kg5 73. e7 Kh5 74. e8=Q+ Kg5 75. Qg6# e5 Qg6# e4 Qg6#",
          "aggressive_tokens": {
            "prompt_tokens": 124,
            "completion_tokens": 601,
            "total_tokens": 725,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "e3",
            "e6",
            "Bxc4",
            "c5",
            "O-O",
            "a6",
            "dxc5",
            "Qxd1",
            "Rxd1",
            "Bxc5",
            "b3",
            "O-O",
            "Bb2",
            "b5",
            "Be2",
            "Bb7",
            "Nbd2",
            "Nbd7",
            "Rac1",
            "Rac8",
            "a4",
            "bxa4",
            "bxa4",
            "Nb6",
            "a5",
            "Na4",
            "Bxf6",
            "gxf6",
            "Nb3",
            "Bb4",
            "Rxc8",
            "Rxc8",
            "Rd7",
            "Bd5",
            "Nfd4",
            "Nc5",
            "Nxc5",
            "Rxc5",
            "h3",
            "Rxa5",
            "Bh5",
            "e5",
            "Nf5",
            "Be6",
            "Rd8+",
            "Bf8",
            "Ne7+",
            "Kg7",
            "Nc6",
            "Ra1+",
            "Kh2",
            "a5",
            "Ra8",
            "a4",
            "Nd8",
            "Bd5",
            "Ra7",
            "a3",
            "Bxf7",
            "Bxf7",
            "Nxf7",
            "Kg6",
            "Nh8+",
            "Kf5",
            "Rxh7",
            "a2",
            "Ra7",
            "Bb4",
            "Ra4",
            "Bc3",
            "Nf7",
            "Ke6",
            "Nd8+",
            "Kd5",
            "Nb7",
            "Kc6",
            "Nd8+",
            "Kb5",
            "Ra8",
            "Ba5",
            "Nb7",
            "Rh1+",
            "Kxh1",
            "a1=Q+",
            "Kh2",
            "Bc7",
            "Rxa1",
            "e4+",
            "g3",
            "Kc6",
            "Na5+",
            "Kd5",
            "Nb3",
            "Kc4",
            "Nd4",
            "Be5",
            "Rc1+",
            "Kd3",
            "Rd1+",
            "Kc3",
            "Kg2",
            "Kc4",
            "h4",
            "Kd5",
            "h5",
            "Kd6",
            "h6",
            "Ke7",
            "h7",
            "f5",
            "Nxf5+",
            "Kf7",
            "Rd5",
            "Bf6",
            "Rd6",
            "Be5",
            "Rd7+",
            "Kg6",
            "Nh4+",
            "Kh6",
            "Re7",
            "Bf6",
            "Re6",
            "Kxh7",
            "Rxf6",
            "Kg7",
            "Rf4",
            "Kh6",
            "Rxe4",
            "Kg5",
            "Rf4",
            "Kh5",
            "e4",
            "Kg5",
            "e5",
            "Kh5",
            "e6",
            "Kg5",
            "e7",
            "Kh5",
            "e8=Q+",
            "Kg5",
            "Qg6#",
            "e5",
            "Qg6#",
            "e4",
            "Qg6#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf3",
          "positional_uci": "g1f3",
          "positional_response": "3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. dxc5 Qxd1 8. Rxd1 Bxc5 9. b3 b5 10. Be2 Bb7 11. Bb2 Nbd7 12. Nbd2 Ke7 13. Rac1 Rac8 14. a4 bxa4 15. bxa4 Rhd8 16. Nb3 Bb4 17. Nfd4 Nb6 18. Rxc8 Rxc8 19. a5 Nc4 20. Bxc4 Rxc4 21. Rc1 Rxc1+ 22. Bxc1 Bd5 23. Bd2 Bxd2 24. Nxd2 Kd6 25. f3 Kc5 26. Kf2 Kb4 27. e4 Bb7 28. N2b3 Nd7 29. Ke3 e5 30. Nf5 Kxb3 31. Nd6 Bc6 32. Nxf7 Kb4 33. Ng5 h6 34. Ne6 g6 35. f4 exf4+ 36. Nxf4 g5 37. Nd3+ Kxa5 38. Kd4 Kb6 39. e5 Bxg2 40. e6 Nf6 41. Ke5 Ne8 42. Nb4 a5 43. Nd5+ Bxd5 44. Kxd5 a4 45. Kc4 Kc6 46. Kb4 Kd6 47. Kxa4 Kxe6 48. Kb4 Kf5 49. Kc3 Kg4 50. Kd2 Kh3 51. Ke2 Kxh2 52. Kf3 h5 53. Kf2 g4 54. Kf1 g3 55. Ke2 g2 56. Kf3 g1=Q 57. Kf4 Qg4+ 58. Ke5 Kg3 59. Kd5 h4 60. Ke5 h3 61. Kd5 h2 62. Ke5 h1=Q 63. 1-0 Nf6 gxf6+ Kxf6",
          "positional_tokens": {
            "prompt_tokens": 124,
            "completion_tokens": 508,
            "total_tokens": 632,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "e3",
            "e6",
            "Bxc4",
            "c5",
            "O-O",
            "a6",
            "dxc5",
            "Qxd1",
            "Rxd1",
            "Bxc5",
            "b3",
            "b5",
            "Be2",
            "Bb7",
            "Bb2",
            "Nbd7",
            "Nbd2",
            "Ke7",
            "Rac1",
            "Rac8",
            "a4",
            "bxa4",
            "bxa4",
            "Rhd8",
            "Nb3",
            "Bb4",
            "Nfd4",
            "Nb6",
            "Rxc8",
            "Rxc8",
            "a5",
            "Nc4",
            "Bxc4",
            "Rxc4",
            "Rc1",
            "Rxc1+",
            "Bxc1",
            "Bd5",
            "Bd2",
            "Bxd2",
            "Nxd2",
            "Kd6",
            "f3",
            "Kc5",
            "Kf2",
            "Kb4",
            "e4",
            "Bb7",
            "N2b3",
            "Nd7",
            "Ke3",
            "e5",
            "Nf5",
            "Kxb3",
            "Nd6",
            "Bc6",
            "Nxf7",
            "Kb4",
            "Ng5",
            "h6",
            "Ne6",
            "g6",
            "f4",
            "exf4+",
            "Nxf4",
            "g5",
            "Nd3+",
            "Kxa5",
            "Kd4",
            "Kb6",
            "e5",
            "Bxg2",
            "e6",
            "Nf6",
            "Ke5",
            "Ne8",
            "Nb4",
            "a5",
            "Nd5+",
            "Bxd5",
            "Kxd5",
            "a4",
            "Kc4",
            "Kc6",
            "Kb4",
            "Kd6",
            "Kxa4",
            "Kxe6",
            "Kb4",
            "Kf5",
            "Kc3",
            "Kg4",
            "Kd2",
            "Kh3",
            "Ke2",
            "Kxh2",
            "Kf3",
            "h5",
            "Kf2",
            "g4",
            "Kf1",
            "g3",
            "Ke2",
            "g2",
            "Kf3",
            "g1=Q",
            "Kf4",
            "Qg4+",
            "Ke5",
            "Kg3",
            "Kd5",
            "h4",
            "Ke5",
            "h3",
            "Kd5",
            "h2",
            "Ke5",
            "h1=Q",
            "Nf6",
            "gxf6+",
            "Kxf6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf3",
          "neutral_uci": "g1f3",
          "neutral_response": "3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. dxc5 Qxd1 8. Rxd1 Bxc5 9. b3 b5 10. Be2 Bb7 11. Bb2 Nbd7 12. Nbd2 Ke7 13. Rac1 Rac8 14. a4 bxa4 15. bxa4 Rhd8 16. Nb3 Bb4 17. Nfd4 Nb6 18. Rxc8 Rxc8 19. a5 Nc4 20. Bxc4 Rxc4 21. Rc1 Rxc1+ 22. Bxc1 Bd5 23. Bd2 Bxd2 24. Nxd2 Kd6 25. f3 Kc5 26. Kf2 Kb4 27. e4 Bb7 28. N2b3 Nd7 29. Ke3 e5 30. Nf5 Kxb3 31. Nd6 Bc6 32. Nxf7 Kb4 33. Ng5 h6 34. Ne6 g6 35. f4 exf4+ 36. Nxf4 g5 37. Nd3+ Kxa5 38. Kd4 Kb5 39. e5 Bxg2 40. e6 Nf6 41. Ke5 Ne8 42. e7 Kc6 43. Ke6 Bd5+ 44. Ke5 Bc4 45. Nb4+ Kd7 46. Kd4 Bb5 47. Nd5 Ke6 48. Kc5 h5 49. h3 g4 50. hxg4 hxg4 51. Ne3 g3 52. Kb6 Kxe7 53. Ka5 Kd6 54. Kb4 Ke5 55. Kc3 Ke4 56. Kd2 Kf3 57. Nf5 g2 58. Nh4+ Kf2 59. Nxg2 Kxg2 60. Kc3 Kf3 61. Kb4 Ke4 62. Ka5 Kd5 63. Kb4 Kc6 64. Ka5 Nc7 65. Kb4 Kb6 66. Ka3 a5 67. Kb3 Kc5 68. Ka3 Nd5 69. Kb3 a4+ 70. Ka3 Kc4 71. Kb2 Kb4 72. Ka2 Bc4+ 73. Kb2 a3+ 74. Ka1 Kb3 75. Kb1 Nb4 76. Ka1 Nc2+ 77. Kb1 a2+ 78. Kc1 a1=Q+ 79. Kd2 Qe1# 0-1 1. d4 d5 2.",
          "neutral_tokens": {
            "prompt_tokens": 123,
            "completion_tokens": 640,
            "total_tokens": 763,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf6",
            "e3",
            "e6",
            "Bxc4",
            "c5",
            "O-O",
            "a6",
            "dxc5",
            "Qxd1",
            "Rxd1",
            "Bxc5",
            "b3",
            "b5",
            "Be2",
            "Bb7",
            "Bb2",
            "Nbd7",
            "Nbd2",
            "Ke7",
            "Rac1",
            "Rac8",
            "a4",
            "bxa4",
            "bxa4",
            "Rhd8",
            "Nb3",
            "Bb4",
            "Nfd4",
            "Nb6",
            "Rxc8",
            "Rxc8",
            "a5",
            "Nc4",
            "Bxc4",
            "Rxc4",
            "Rc1",
            "Rxc1+",
            "Bxc1",
            "Bd5",
            "Bd2",
            "Bxd2",
            "Nxd2",
            "Kd6",
            "f3",
            "Kc5",
            "Kf2",
            "Kb4",
            "e4",
            "Bb7",
            "N2b3",
            "Nd7",
            "Ke3",
            "e5",
            "Nf5",
            "Kxb3",
            "Nd6",
            "Bc6",
            "Nxf7",
            "Kb4",
            "Ng5",
            "h6",
            "Ne6",
            "g6",
            "f4",
            "exf4+",
            "Nxf4",
            "g5",
            "Nd3+",
            "Kxa5",
            "Kd4",
            "Kb5",
            "e5",
            "Bxg2",
            "e6",
            "Nf6",
            "Ke5",
            "Ne8",
            "e7",
            "Kc6",
            "Ke6",
            "Bd5+",
            "Ke5",
            "Bc4",
            "Nb4+",
            "Kd7",
            "Kd4",
            "Bb5",
            "Nd5",
            "Ke6",
            "Kc5",
            "h5",
            "h3",
            "g4",
            "hxg4",
            "hxg4",
            "Ne3",
            "g3",
            "Kb6",
            "Kxe7",
            "Ka5",
            "Kd6",
            "Kb4",
            "Ke5",
            "Kc3",
            "Ke4",
            "Kd2",
            "Kf3",
            "Nf5",
            "g2",
            "Nh4+",
            "Kf2",
            "Nxg2",
            "Kxg2",
            "Kc3",
            "Kf3",
            "Kb4",
            "Ke4",
            "Ka5",
            "Kd5",
            "Kb4",
            "Kc6",
            "Ka5",
            "Nc7",
            "Kb4",
            "Kb6",
            "Ka3",
            "a5",
            "Kb3",
            "Kc5",
            "Ka3",
            "Nd5",
            "Kb3",
            "a4+",
            "Ka3",
            "Kc4",
            "Kb2",
            "Kb4",
            "Ka2",
            "Bc4+",
            "Kb2",
            "a3+",
            "Ka1",
            "Kb3",
            "Kb1",
            "Nb4",
            "Ka1",
            "Nc2+",
            "Kb1",
            "a2+",
            "Kc1",
            "a1=Q+",
            "Kd2",
            "Qe1#",
            "d4",
            "d5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g1f3",
          "positional_uci": "g1f3",
          "neutral_uci": "g1f3",
          "consensus_move": "g1f3",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf6",
              "e3",
              "e6",
              "Bxc4",
              "c5",
              "O-O",
              "a6",
              "dxc5",
              "Qxd1",
              "Rxd1",
              "Bxc5",
              "b3",
              "O-O",
              "Bb2",
              "b5",
              "Be2",
              "Bb7",
              "Nbd2",
              "Nbd7",
              "Rac1",
              "Rac8",
              "a4",
              "bxa4",
              "bxa4",
              "Nb6",
              "a5",
              "Na4",
              "Bxf6",
              "gxf6",
              "Nb3",
              "Bb4",
              "Rxc8",
              "Rxc8",
              "Rd7",
              "Bd5",
              "Nfd4",
              "Nc5",
              "Nxc5",
              "Rxc5",
              "h3",
              "Rxa5",
              "Bh5",
              "e5",
              "Nf5",
              "Be6",
              "Rd8+",
              "Bf8",
              "Ne7+",
              "Kg7",
              "Nc6",
              "Ra1+",
              "Kh2",
              "a5",
              "Ra8",
              "a4",
              "Nd8",
              "Bd5",
              "Ra7",
              "a3",
              "Bxf7",
              "Bxf7",
              "Nxf7",
              "Kg6",
              "Nh8+",
              "Kf5",
              "Rxh7",
              "a2",
              "Ra7",
              "Bb4",
              "Ra4",
              "Bc3",
              "Nf7",
              "Ke6",
              "Nd8+",
              "Kd5",
              "Nb7",
              "Kc6",
              "Nd8+",
              "Kb5",
              "Ra8",
              "Ba5",
              "Nb7",
              "Rh1+",
              "Kxh1",
              "a1=Q+",
              "Kh2",
              "Bc7",
              "Rxa1",
              "e4+",
              "g3",
              "Kc6",
              "Na5+",
              "Kd5",
              "Nb3",
              "Kc4",
              "Nd4",
              "Be5",
              "Rc1+",
              "Kd3",
              "Rd1+",
              "Kc3",
              "Kg2",
              "Kc4",
              "h4",
              "Kd5",
              "h5",
              "Kd6",
              "h6",
              "Ke7",
              "h7",
              "f5",
              "Nxf5+",
              "Kf7",
              "Rd5",
              "Bf6",
              "Rd6",
              "Be5",
              "Rd7+",
              "Kg6",
              "Nh4+",
              "Kh6",
              "Re7",
              "Bf6",
              "Re6",
              "Kxh7",
              "Rxf6",
              "Kg7",
              "Rf4",
              "Kh6",
              "Rxe4",
              "Kg5",
              "Rf4",
              "Kh5",
              "e4",
              "Kg5",
              "e5",
              "Kh5",
              "e6",
              "Kg5",
              "e7",
              "Kh5",
              "e8=Q+",
              "Kg5",
              "Qg6#",
              "e5",
              "Qg6#",
              "e4",
              "Qg6#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf6",
              "e3",
              "e6",
              "Bxc4",
              "c5",
              "O-O",
              "a6",
              "dxc5",
              "Qxd1",
              "Rxd1",
              "Bxc5",
              "b3",
              "b5",
              "Be2",
              "Bb7",
              "Bb2",
              "Nbd7",
              "Nbd2",
              "Ke7",
              "Rac1",
              "Rac8",
              "a4",
              "bxa4",
              "bxa4",
              "Rhd8",
              "Nb3",
              "Bb4",
              "Nfd4",
              "Nb6",
              "Rxc8",
              "Rxc8",
              "a5",
              "Nc4",
              "Bxc4",
              "Rxc4",
              "Rc1",
              "Rxc1+",
              "Bxc1",
              "Bd5",
              "Bd2",
              "Bxd2",
              "Nxd2",
              "Kd6",
              "f3",
              "Kc5",
              "Kf2",
              "Kb4",
              "e4",
              "Bb7",
              "N2b3",
              "Nd7",
              "Ke3",
              "e5",
              "Nf5",
              "Kxb3",
              "Nd6",
              "Bc6",
              "Nxf7",
              "Kb4",
              "Ng5",
              "h6",
              "Ne6",
              "g6",
              "f4",
              "exf4+",
              "Nxf4",
              "g5",
              "Nd3+",
              "Kxa5",
              "Kd4",
              "Kb6",
              "e5",
              "Bxg2",
              "e6",
              "Nf6",
              "Ke5",
              "Ne8",
              "Nb4",
              "a5",
              "Nd5+",
              "Bxd5",
              "Kxd5",
              "a4",
              "Kc4",
              "Kc6",
              "Kb4",
              "Kd6",
              "Kxa4",
              "Kxe6",
              "Kb4",
              "Kf5",
              "Kc3",
              "Kg4",
              "Kd2",
              "Kh3",
              "Ke2",
              "Kxh2",
              "Kf3",
              "h5",
              "Kf2",
              "g4",
              "Kf1",
              "g3",
              "Ke2",
              "g2",
              "Kf3",
              "g1=Q",
              "Kf4",
              "Qg4+",
              "Ke5",
              "Kg3",
              "Kd5",
              "h4",
              "Ke5",
              "h3",
              "Kd5",
              "h2",
              "Ke5",
              "h1=Q",
              "Nf6",
              "gxf6+",
              "Kxf6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf6",
              "e3",
              "e6",
              "Bxc4",
              "c5",
              "O-O",
              "a6",
              "dxc5",
              "Qxd1",
              "Rxd1",
              "Bxc5",
              "b3",
              "b5",
              "Be2",
              "Bb7",
              "Bb2",
              "Nbd7",
              "Nbd2",
              "Ke7",
              "Rac1",
              "Rac8",
              "a4",
              "bxa4",
              "bxa4",
              "Rhd8",
              "Nb3",
              "Bb4",
              "Nfd4",
              "Nb6",
              "Rxc8",
              "Rxc8",
              "a5",
              "Nc4",
              "Bxc4",
              "Rxc4",
              "Rc1",
              "Rxc1+",
              "Bxc1",
              "Bd5",
              "Bd2",
              "Bxd2",
              "Nxd2",
              "Kd6",
              "f3",
              "Kc5",
              "Kf2",
              "Kb4",
              "e4",
              "Bb7",
              "N2b3",
              "Nd7",
              "Ke3",
              "e5",
              "Nf5",
              "Kxb3",
              "Nd6",
              "Bc6",
              "Nxf7",
              "Kb4",
              "Ng5",
              "h6",
              "Ne6",
              "g6",
              "f4",
              "exf4+",
              "Nxf4",
              "g5",
              "Nd3+",
              "Kxa5",
              "Kd4",
              "Kb5",
              "e5",
              "Bxg2",
              "e6",
              "Nf6",
              "Ke5",
              "Ne8",
              "e7",
              "Kc6",
              "Ke6",
              "Bd5+",
              "Ke5",
              "Bc4",
              "Nb4+",
              "Kd7",
              "Kd4",
              "Bb5",
              "Nd5",
              "Ke6",
              "Kc5",
              "h5",
              "h3",
              "g4",
              "hxg4",
              "hxg4",
              "Ne3",
              "g3",
              "Kb6",
              "Kxe7",
              "Ka5",
              "Kd6",
              "Kb4",
              "Ke5",
              "Kc3",
              "Ke4",
              "Kd2",
              "Kf3",
              "Nf5",
              "g2",
              "Nh4+",
              "Kf2",
              "Nxg2",
              "Kxg2",
              "Kc3",
              "Kf3",
              "Kb4",
              "Ke4",
              "Ka5",
              "Kd5",
              "Kb4",
              "Kc6",
              "Ka5",
              "Nc7",
              "Kb4",
              "Kb6",
              "Ka3",
              "a5",
              "Kb3",
              "Kc5",
              "Ka3",
              "Nd5",
              "Kb3",
              "a4+",
              "Ka3",
              "Kc4",
              "Kb2",
              "Kb4",
              "Ka2",
              "Bc4+",
              "Kb2",
              "a3+",
              "Ka1",
              "Kb3",
              "Kb1",
              "Nb4",
              "Ka1",
              "Nc2+",
              "Kb1",
              "a2+",
              "Kc1",
              "a1=Q+",
              "Kd2",
              "Qe1#",
              "d4",
              "d5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 124,
            "completion_tokens": 601,
            "total_tokens": 725,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 124,
            "completion_tokens": 508,
            "total_tokens": 632,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 123,
            "completion_tokens": 640,
            "total_tokens": 763,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 371,
          "total_completion_tokens": 1749,
          "total_tokens": 2120
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 124,
            "completion_tokens": 601,
            "total_tokens": 725,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 124,
            "completion_tokens": 508,
            "total_tokens": 632,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 123,
            "completion_tokens": 640,
            "total_tokens": 763,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/8/2pP4/8/PP2PPPP/RNBQKBNR w KQkq - 0 3"
    },
    {
      "turn": 7,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc3",
          "aggressive_uci": "b1c3",
          "aggressive_response": "4. Nc3 Nf6 5. e4 Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Na5 14. Bc2 c5 15. dxc5 Qc7 16. f4 Rfd8 17. Qe2 Bxc3 18. bxc3 h6 19. f5 Bh7 20. g5 hxg5 21. Bxg5 Qxc5 22. Bxf6 gxf6 23. Rhg1+ Kh8 24. Qh5 Qxc3 25. Qxf7 Rg8 26. Nb1 Qe3+ 27. Kb2 Nc4+ 28. Ka1 Qh6 29. fxe6 Ne3 30. Rxg8+ Rxg8 31. Rd2 Nxc2+ 32. Rxc2 Bxe4 33. Rc7 Bg6 34. Qxf6+ Rg7 35. e7 Kh7 36. Rc8 Rf7 37. Rh8# e4 Nc6 Qe7",
          "aggressive_tokens": {
            "prompt_tokens": 133,
            "completion_tokens": 291,
            "total_tokens": 424,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "e4",
            "Bg4",
            "Be3",
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O",
            "O-O-O",
            "a6",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Nd2",
            "b5",
            "Bb3",
            "Na5",
            "Bc2",
            "c5",
            "dxc5",
            "Qc7",
            "f4",
            "Rfd8",
            "Qe2",
            "Bxc3",
            "bxc3",
            "h6",
            "f5",
            "Bh7",
            "g5",
            "hxg5",
            "Bxg5",
            "Qxc5",
            "Bxf6",
            "gxf6",
            "Rhg1+",
            "Kh8",
            "Qh5",
            "Qxc3",
            "Qxf7",
            "Rg8",
            "Nb1",
            "Qe3+",
            "Kb2",
            "Nc4+",
            "Ka1",
            "Qh6",
            "fxe6",
            "Ne3",
            "Rxg8+",
            "Rxg8",
            "Rd2",
            "Nxc2+",
            "Rxc2",
            "Bxe4",
            "Rc7",
            "Bg6",
            "Qxf6+",
            "Rg7",
            "e7",
            "Kh7",
            "Rc8",
            "Rf7",
            "Rh8#",
            "e4",
            "Nc6",
            "Qe7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e3",
          "positional_uci": "e2e3",
          "positional_response": "4. e3 e5 5. Bxc4 exd4 6. exd4 Bg4 7. O-O Nf6 8. Re1+ Be7 9. d5 Na5 10. Qa4+ c6 11. dxc6 Nxc6 12. Ne5 Bd7 13. Nxf7 Qa5 14. Qxa5 Nxa5 15. Nxh8 Nxc4 16. b3 Nb6 17. Ba3 Nbd5 18. Nc3 Kf8 19. Nxd5 Nxd5 20. Rad1 Bxa3 21. Rxd5 Bc6 22. Rf5+ Kg8 23. Nf7 Rf8 24. Nh6+ gxh6 25. Rxf8+ Kxf8 26. Re6 Kg7 27. f3 Bc5+ 28. Kh1 h5 29. Re5 Bd6 30. Rxh5 Kg6 31. Rh4 h5 32. Rd4 Bc5 33. Rd8 h4 34. h3 Kf5 35. Rh8 Bf2 36. Rf8+ Ke5 37. f4+ Ke4 38. f5 Ke3 39. f6 Ke2 40. f7 Kf1 41. Rg8 Bc5 42. f8=Q+ Bxf8 43. Rxf8+ Ke2 44. Rh8 Kf2 45. Rf8+ Kg3 46. Rg8+ Kf2 47. Kh2 a6 48. g4 hxg3+ 49. Rxg3 Ke2 50. h4 Kd2 51. h5 Kc2 52. h6 Kb2 53. h7 Kxa2 54. h8=Q Ka3 55. Qf8+ Kb2 56. Qf2+ Ka3 57. b4+ Kxb4 58. Qb2+ Kc5 59. Rc3+ Kd6 60. Qd2+ Kc7 61. Qf4+ Kb6 62. Rb3+ Ka7 63. Qd4+ Kb8 64. Qd8+ Ka7 65. Qb6+ Kb8 66. Rd3 Kc8 67. Rd8# Bb5 68. Qc5+ Kb8 69. Qd6+ Ka7 70. Qc5+ b6 71. Qc7# Bxd7 72. Rxd7+ Ka8 73. Rd8# Bxd8",
          "positional_tokens": {
            "prompt_tokens": 133,
            "completion_tokens": 595,
            "total_tokens": 728,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Bxc4",
            "exd4",
            "exd4",
            "Bg4",
            "O-O",
            "Nf6",
            "Re1+",
            "Be7",
            "d5",
            "Na5",
            "Qa4+",
            "c6",
            "dxc6",
            "Nxc6",
            "Ne5",
            "Bd7",
            "Nxf7",
            "Qa5",
            "Qxa5",
            "Nxa5",
            "Nxh8",
            "Nxc4",
            "b3",
            "Nb6",
            "Ba3",
            "Nbd5",
            "Nc3",
            "Kf8",
            "Nxd5",
            "Nxd5",
            "Rad1",
            "Bxa3",
            "Rxd5",
            "Bc6",
            "Rf5+",
            "Kg8",
            "Nf7",
            "Rf8",
            "Nh6+",
            "gxh6",
            "Rxf8+",
            "Kxf8",
            "Re6",
            "Kg7",
            "f3",
            "Bc5+",
            "Kh1",
            "h5",
            "Re5",
            "Bd6",
            "Rxh5",
            "Kg6",
            "Rh4",
            "h5",
            "Rd4",
            "Bc5",
            "Rd8",
            "h4",
            "h3",
            "Kf5",
            "Rh8",
            "Bf2",
            "Rf8+",
            "Ke5",
            "f4+",
            "Ke4",
            "f5",
            "Ke3",
            "f6",
            "Ke2",
            "f7",
            "Kf1",
            "Rg8",
            "Bc5",
            "f8=Q+",
            "Bxf8",
            "Rxf8+",
            "Ke2",
            "Rh8",
            "Kf2",
            "Rf8+",
            "Kg3",
            "Rg8+",
            "Kf2",
            "Kh2",
            "a6",
            "g4",
            "hxg3+",
            "Rxg3",
            "Ke2",
            "h4",
            "Kd2",
            "h5",
            "Kc2",
            "h6",
            "Kb2",
            "h7",
            "Kxa2",
            "h8=Q",
            "Ka3",
            "Qf8+",
            "Kb2",
            "Qf2+",
            "Ka3",
            "b4+",
            "Kxb4",
            "Qb2+",
            "Kc5",
            "Rc3+",
            "Kd6",
            "Qd2+",
            "Kc7",
            "Qf4+",
            "Kb6",
            "Rb3+",
            "Ka7",
            "Qd4+",
            "Kb8",
            "Qd8+",
            "Ka7",
            "Qb6+",
            "Kb8",
            "Rd3",
            "Kc8",
            "Rd8#",
            "Bb5",
            "Qc5+",
            "Kb8",
            "Qd6+",
            "Ka7",
            "Qc5+",
            "b6",
            "Qc7#",
            "Bxd7",
            "Rxd7+",
            "Ka8",
            "Rd8#",
            "Bxd8"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc3",
          "neutral_uci": "b1c3",
          "neutral_response": "4. Nc3 Nf6 5. e4 Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Na5 14. Bc2 c5 15. dxc5 Qc7 16. f4 Rfd8 17. Qe2 Bxc3 18. bxc3 h6 19. f5 exf5 20. gxf5 Bh7 21. Rhg1 Kh8 22. Qg2 Rg8 23. Bd4 Qe7 24. e5 Nd7 25. f6 gxf6 26. exf6 Qe6 27. Qg7+ Rxg7 28. fxg7+ Kg8 29. Bxh7+ Kxh7 30. Rde1 Qxa2 31. Ne4 Nb3+ 32. Kd1 Nxd4 33. cxd4 Qa1+ 34. Ke2 Qxd4 35. Kf3 Ne5+ 36. Kf4 Ng6+ 37. Kf5 Qe5+ 38. Kg4 Qf4+ 39. Kh5 Qh4# 0-1\nNf6+ Nxf6+ gxf6",
          "neutral_tokens": {
            "prompt_tokens": 132,
            "completion_tokens": 323,
            "total_tokens": 455,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "e4",
            "Bg4",
            "Be3",
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O",
            "O-O-O",
            "a6",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Nd2",
            "b5",
            "Bb3",
            "Na5",
            "Bc2",
            "c5",
            "dxc5",
            "Qc7",
            "f4",
            "Rfd8",
            "Qe2",
            "Bxc3",
            "bxc3",
            "h6",
            "f5",
            "exf5",
            "gxf5",
            "Bh7",
            "Rhg1",
            "Kh8",
            "Qg2",
            "Rg8",
            "Bd4",
            "Qe7",
            "e5",
            "Nd7",
            "f6",
            "gxf6",
            "exf6",
            "Qe6",
            "Qg7+",
            "Rxg7",
            "fxg7+",
            "Kg8",
            "Bxh7+",
            "Kxh7",
            "Rde1",
            "Qxa2",
            "Ne4",
            "Nb3+",
            "Kd1",
            "Nxd4",
            "cxd4",
            "Qa1+",
            "Ke2",
            "Qxd4",
            "Kf3",
            "Ne5+",
            "Kf4",
            "Ng6+",
            "Kf5",
            "Qe5+",
            "Kg4",
            "Qf4+",
            "Kh5",
            "Qh4#",
            "Nf6+",
            "Nxf6+",
            "gxf6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b1c3",
          "positional_uci": "e2e3",
          "neutral_uci": "b1c3",
          "consensus_move": "b1c3",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf6",
              "e4",
              "Bg4",
              "Be3",
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O",
              "O-O-O",
              "a6",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Nd2",
              "b5",
              "Bb3",
              "Na5",
              "Bc2",
              "c5",
              "dxc5",
              "Qc7",
              "f4",
              "Rfd8",
              "Qe2",
              "Bxc3",
              "bxc3",
              "h6",
              "f5",
              "Bh7",
              "g5",
              "hxg5",
              "Bxg5",
              "Qxc5",
              "Bxf6",
              "gxf6",
              "Rhg1+",
              "Kh8",
              "Qh5",
              "Qxc3",
              "Qxf7",
              "Rg8",
              "Nb1",
              "Qe3+",
              "Kb2",
              "Nc4+",
              "Ka1",
              "Qh6",
              "fxe6",
              "Ne3",
              "Rxg8+",
              "Rxg8",
              "Rd2",
              "Nxc2+",
              "Rxc2",
              "Bxe4",
              "Rc7",
              "Bg6",
              "Qxf6+",
              "Rg7",
              "e7",
              "Kh7",
              "Rc8",
              "Rf7",
              "Rh8#",
              "e4",
              "Nc6",
              "Qe7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Bxc4",
              "exd4",
              "exd4",
              "Bg4",
              "O-O",
              "Nf6",
              "Re1+",
              "Be7",
              "d5",
              "Na5",
              "Qa4+",
              "c6",
              "dxc6",
              "Nxc6",
              "Ne5",
              "Bd7",
              "Nxf7",
              "Qa5",
              "Qxa5",
              "Nxa5",
              "Nxh8",
              "Nxc4",
              "b3",
              "Nb6",
              "Ba3",
              "Nbd5",
              "Nc3",
              "Kf8",
              "Nxd5",
              "Nxd5",
              "Rad1",
              "Bxa3",
              "Rxd5",
              "Bc6",
              "Rf5+",
              "Kg8",
              "Nf7",
              "Rf8",
              "Nh6+",
              "gxh6",
              "Rxf8+",
              "Kxf8",
              "Re6",
              "Kg7",
              "f3",
              "Bc5+",
              "Kh1",
              "h5",
              "Re5",
              "Bd6",
              "Rxh5",
              "Kg6",
              "Rh4",
              "h5",
              "Rd4",
              "Bc5",
              "Rd8",
              "h4",
              "h3",
              "Kf5",
              "Rh8",
              "Bf2",
              "Rf8+",
              "Ke5",
              "f4+",
              "Ke4",
              "f5",
              "Ke3",
              "f6",
              "Ke2",
              "f7",
              "Kf1",
              "Rg8",
              "Bc5",
              "f8=Q+",
              "Bxf8",
              "Rxf8+",
              "Ke2",
              "Rh8",
              "Kf2",
              "Rf8+",
              "Kg3",
              "Rg8+",
              "Kf2",
              "Kh2",
              "a6",
              "g4",
              "hxg3+",
              "Rxg3",
              "Ke2",
              "h4",
              "Kd2",
              "h5",
              "Kc2",
              "h6",
              "Kb2",
              "h7",
              "Kxa2",
              "h8=Q",
              "Ka3",
              "Qf8+",
              "Kb2",
              "Qf2+",
              "Ka3",
              "b4+",
              "Kxb4",
              "Qb2+",
              "Kc5",
              "Rc3+",
              "Kd6",
              "Qd2+",
              "Kc7",
              "Qf4+",
              "Kb6",
              "Rb3+",
              "Ka7",
              "Qd4+",
              "Kb8",
              "Qd8+",
              "Ka7",
              "Qb6+",
              "Kb8",
              "Rd3",
              "Kc8",
              "Rd8#",
              "Bb5",
              "Qc5+",
              "Kb8",
              "Qd6+",
              "Ka7",
              "Qc5+",
              "b6",
              "Qc7#",
              "Bxd7",
              "Rxd7+",
              "Ka8",
              "Rd8#",
              "Bxd8"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf6",
              "e4",
              "Bg4",
              "Be3",
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O",
              "O-O-O",
              "a6",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Nd2",
              "b5",
              "Bb3",
              "Na5",
              "Bc2",
              "c5",
              "dxc5",
              "Qc7",
              "f4",
              "Rfd8",
              "Qe2",
              "Bxc3",
              "bxc3",
              "h6",
              "f5",
              "exf5",
              "gxf5",
              "Bh7",
              "Rhg1",
              "Kh8",
              "Qg2",
              "Rg8",
              "Bd4",
              "Qe7",
              "e5",
              "Nd7",
              "f6",
              "gxf6",
              "exf6",
              "Qe6",
              "Qg7+",
              "Rxg7",
              "fxg7+",
              "Kg8",
              "Bxh7+",
              "Kxh7",
              "Rde1",
              "Qxa2",
              "Ne4",
              "Nb3+",
              "Kd1",
              "Nxd4",
              "cxd4",
              "Qa1+",
              "Ke2",
              "Qxd4",
              "Kf3",
              "Ne5+",
              "Kf4",
              "Ng6+",
              "Kf5",
              "Qe5+",
              "Kg4",
              "Qf4+",
              "Kh5",
              "Qh4#",
              "Nf6+",
              "Nxf6+",
              "gxf6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 133,
            "completion_tokens": 291,
            "total_tokens": 424,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 133,
            "completion_tokens": 595,
            "total_tokens": 728,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 132,
            "completion_tokens": 323,
            "total_tokens": 455,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 398,
          "total_completion_tokens": 1209,
          "total_tokens": 1607
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 133,
            "completion_tokens": 291,
            "total_tokens": 424,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 133,
            "completion_tokens": 595,
            "total_tokens": 728,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 132,
            "completion_tokens": 323,
            "total_tokens": 455,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkbnr/ppp1pppp/2n5/8/2pP4/5N2/PP2PPPP/RNBQKB1R w KQkq - 2 4"
    },
    {
      "turn": 9,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": "e2e4",
          "aggressive_response": "5. e4 Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Ne5 14. Qc2 Nc6 15. f4 Nxd4 16. Bxd4 Qxd4 17. f5 exf5 18. exf5 Bh5 19. gxh5 Nxh5 20. Nde4 Qe3+ 21. Kb1 c5 22. Nd5 Qf3 23. Rhf1 Qxh3 24. Rh1 Qf3 25. Rdf1 Qg4 26. Rfg1 Qf3 27. Rxh5 Qxh5 28. Nef6+ Kh8 29. Nxh5 c4 30. Rxg7 cxb3 31. Rxh7+ Kxh7 32. Nhf6+ Kg7 33. Qg2+ Kh6 34. Qh3+ Kg5 35. Qg4+ Kh6 36. Qh5+ Kg7 37. Qh7# Bb4 38. Qh7# Bb4 39. Qh7# Bb4 40. Qh7# Bb4 41. Qh7# Bb4 42. Qh7# Bb4 43. Qh7# Bb4 44. Qh7# Bb4 45. Qh7# Bb4 46. Qh7# Bb4 47. Qh7# Bb4 48. Qh7# Bb4 49. Qh7# Bb4 50. Qh7# Bb4 51. Qh7# Bb4 52. Qh7# Bb4 53. Qh7# Bb4 54. Qh7# Bb4 55. Qh7# Bb4 56. Qh7# Bb4 57. Qh7# Bb4 58. Qh7# Bb4 59. Qh7# Bb4 60. Qh7# Bb4 61. Qh7# Bb4 62. Qh7# Bb4 63. Qh7# Bb4 64. Qh7# Bb4 65. Qh7# Bb4 66. Qh7# Bb4 67. Qh7# Bb4 68. Qh7# Bb4 69. Qh7# Bb4 70. Qh7# Bb4 71. Qh7# Bb4 72. Qh7# Bb4 73. Qh7",
          "aggressive_tokens": {
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bg4",
            "Be3",
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O",
            "O-O-O",
            "a6",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Nd2",
            "b5",
            "Bb3",
            "Ne5",
            "Qc2",
            "Nc6",
            "f4",
            "Nxd4",
            "Bxd4",
            "Qxd4",
            "f5",
            "exf5",
            "exf5",
            "Bh5",
            "gxh5",
            "Nxh5",
            "Nde4",
            "Qe3+",
            "Kb1",
            "c5",
            "Nd5",
            "Qf3",
            "Rhf1",
            "Qxh3",
            "Rh1",
            "Qf3",
            "Rdf1",
            "Qg4",
            "Rfg1",
            "Qf3",
            "Rxh5",
            "Qxh5",
            "Nef6+",
            "Kh8",
            "Nxh5",
            "c4",
            "Rxg7",
            "cxb3",
            "Rxh7+",
            "Kxh7",
            "Nhf6+",
            "Kg7",
            "Qg2+",
            "Kh6",
            "Qh3+",
            "Kg5",
            "Qg4+",
            "Kh6",
            "Qh5+",
            "Kg7",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7#",
            "Bb4",
            "Qh7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "5. e4 Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Na5 14. Bc2 c5 15. f4 c4 16. Qe2 Bxc3 17. bxc3 Qe7 18. f5 Qa3+ 19. Kb1 Qxc3 20. fxg6 Qb4+ 21. Ka1 Qc3+ 22. Kb1 Qb4+ 23. Ka1 Qc3+ 24. Kb1 Qb4+ 25. Ka1 Qc3+ 26. Kb1 Qb4+ 27. Ka1 Qc3+ 28. Kb1 Qb4+ 29. Ka1 Qc3+ 30. Kb1 Qb4+ 31. Ka1 Qc3+ 32. Kb1 Qb4+ 33. Ka1 Qc3+ 34. Kb1 Qb4+ 35. Ka1 Qc3+ 36. Kb1 Qb4+ 37. Ka1 Qc3+ 38. Kb1 Qb4+ 39. Ka1 Qc3+ 40. Kb1 Qb4+ 41. Ka1 Qc3+ 42. Kb1 Qb4+ 43. Ka1 Qc3+ 44. Kb1 Qb4+ 45. Ka1 Qc3+ 46. Kb1 Qb4+ 47. Ka1 Qc3+ 48. Kb1 Qb4+ 49. Ka1 Qc3+ 50. Kb1 Qb4+ 51. Ka1 Qc3+ 52. Kb1 Qb4+ 53. Ka1 Qc3+ 54. Kb1 Qb4+ 55. Ka1 Qc3+ 56. Kb1 Qb4+ 57. Ka1 Qc3+ 58. Kb1 Qb4+ 59. Ka1 Qc3+ 60. Kb1 Qb4+ 61. Ka1 Qc3+ 62. Kb1 Qb4+ 63. Ka1 Qc3+ 64. Kb1 Qb4+ 65. Ka1 Qc3+ 66. Kb1 Qb4+ 67. Ka1 Qc3+ 68. Kb1 Qb4+ 69. Ka1 Qc3+ 70. Kb1 Qb4+ 71. Ka1 Qc3+ 72. Kb1 Qb4+ 73. Ka1 Qc3+ 74. Kb1 Qb4+ 75. Ka1 Qc3+ 76. Kb1 Qb4+ 77. Ka1 Q",
          "positional_tokens": {
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bg4",
            "Be3",
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O",
            "O-O-O",
            "a6",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Nd2",
            "b5",
            "Bb3",
            "Na5",
            "Bc2",
            "c5",
            "f4",
            "c4",
            "Qe2",
            "Bxc3",
            "bxc3",
            "Qe7",
            "f5",
            "Qa3+",
            "Kb1",
            "Qxc3",
            "fxg6",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bg4",
          "neutral_uci": null,
          "neutral_response": "5. e4 Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Na5 14. Bc2 c5 15. f4 cxd4 16. Bxd4 h6 17. f5 exf5 18. gxf5 Bh7 19. Rhg1 Kh8 20. Rxg7 Kxg7 21. Rg1+ Kh8 22. Nd5 Be7 23. Qe3 Nc6 24. Qxh6 Nxd4 25. Qg7# 1-0\n5. ... Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Na5 14. Bc2 c5 15. f4 cxd4 16. Bxd4 h6 17. f5 exf5 18. gxf5 Bh7 19. Rhg1 Kh8 20. Rxg7 Kxg7 21. Rg1+ Kh8 22. Nd5 Be7 23. Qe3 Nc6 24. Qxh6 Nxd4 25. Qg7# 1-0 25. ... Nxd4 26. Qg7# 1-0",
          "neutral_tokens": {
            "prompt_tokens": 141,
            "completion_tokens": 365,
            "total_tokens": 506,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Be3",
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O",
            "O-O-O",
            "a6",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Nd2",
            "b5",
            "Bb3",
            "Na5",
            "Bc2",
            "c5",
            "f4",
            "cxd4",
            "Bxd4",
            "h6",
            "f5",
            "exf5",
            "gxf5",
            "Bh7",
            "Rhg1",
            "Kh8",
            "Rxg7",
            "Kxg7",
            "Rg1+",
            "Kh8",
            "Nd5",
            "Be7",
            "Qe3",
            "Nc6",
            "Qxh6",
            "Nxd4",
            "Qg7#",
            "Nxd4",
            "Qg7#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e2e4",
          "positional_uci": "e2e4",
          "neutral_uci": null,
          "consensus_move": "e2e4",
          "source_agents": [
            "aggressive",
            "positional"
          ],
          "selected_agent": "aggressive"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bg4",
              "Be3",
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O",
              "O-O-O",
              "a6",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Nd2",
              "b5",
              "Bb3",
              "Ne5",
              "Qc2",
              "Nc6",
              "f4",
              "Nxd4",
              "Bxd4",
              "Qxd4",
              "f5",
              "exf5",
              "exf5",
              "Bh5",
              "gxh5",
              "Nxh5",
              "Nde4",
              "Qe3+",
              "Kb1",
              "c5",
              "Nd5",
              "Qf3",
              "Rhf1",
              "Qxh3",
              "Rh1",
              "Qf3",
              "Rdf1",
              "Qg4",
              "Rfg1",
              "Qf3",
              "Rxh5",
              "Qxh5",
              "Nef6+",
              "Kh8",
              "Nxh5",
              "c4",
              "Rxg7",
              "cxb3",
              "Rxh7+",
              "Kxh7",
              "Nhf6+",
              "Kg7",
              "Qg2+",
              "Kh6",
              "Qh3+",
              "Kg5",
              "Qg4+",
              "Kh6",
              "Qh5+",
              "Kg7",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7#",
              "Bb4",
              "Qh7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bg4",
              "Be3",
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O",
              "O-O-O",
              "a6",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Nd2",
              "b5",
              "Bb3",
              "Na5",
              "Bc2",
              "c5",
              "f4",
              "c4",
              "Qe2",
              "Bxc3",
              "bxc3",
              "Qe7",
              "f5",
              "Qa3+",
              "Kb1",
              "Qxc3",
              "fxg6",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Be3",
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O",
              "O-O-O",
              "a6",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Nd2",
              "b5",
              "Bb3",
              "Na5",
              "Bc2",
              "c5",
              "f4",
              "cxd4",
              "Bxd4",
              "h6",
              "f5",
              "exf5",
              "gxf5",
              "Bh7",
              "Rhg1",
              "Kh8",
              "Rxg7",
              "Kxg7",
              "Rg1+",
              "Kh8",
              "Nd5",
              "Be7",
              "Qe3",
              "Nc6",
              "Qxh6",
              "Nxd4",
              "Qg7#",
              "Nxd4",
              "Qg7#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 141,
            "completion_tokens": 365,
            "total_tokens": 506,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 425,
          "total_completion_tokens": 1645,
          "total_tokens": 2070
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 141,
            "completion_tokens": 365,
            "total_tokens": 506,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp1pppp/2n2n2/8/2pP4/2N2N2/PP2PPPP/R1BQKB1R w KQkq - 4 5"
    },
    {
      "turn": 11,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d5",
          "aggressive_uci": "d4d5",
          "aggressive_response": "6. d5 Ne5 7. Bf4 Nfd7 8. Bxe5 Nxe5 9. Bxc4 Bxf3 10. gxf3 Nxc4 11. Qa4+ Qd7 12. Qxc4 O-O-O 13. O-O-O Kb8 14. Kb1 g6 15. Rc1 Bg7 16. Nb5 Rc8 17. Qc5 b6 18. Qc4 e6 19. Rhd1 exd5 20. Rxd5 Qe7 21. Qa4 c6 22. Rxc6 Rxc6 23. Nd4 Bxd4 24. Qxc6 Rd8 25. Rxd8+ Qxd8 26. Qc4 Qd7 27. Qb4 Bc5 28. Qc3 Qd1+ 29. Qc1 Qxc1+ 30. Kxc1 Bxf2 31. Kd2 Kc7 32. Kd3 Kd6 33. Kc4 Ke5 34. Kb5 Kf4 35. Ka6 Kxf3 36. Kxa7 Kxe4 37. Ka6 f5 38. Kb5 f4 39. Kc4 f3 40. Kc3 Be3 41. Kc2 f2 42. Kd1 f1=Q+ 43. Kc2 Qd3# f2 Qd2+ Kb1 Qc1#",
          "aggressive_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 338,
            "total_tokens": 488,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Ne5",
            "Bf4",
            "Nfd7",
            "Bxe5",
            "Nxe5",
            "Bxc4",
            "Bxf3",
            "gxf3",
            "Nxc4",
            "Qa4+",
            "Qd7",
            "Qxc4",
            "O-O-O",
            "O-O-O",
            "Kb8",
            "Kb1",
            "g6",
            "Rc1",
            "Bg7",
            "Nb5",
            "Rc8",
            "Qc5",
            "b6",
            "Qc4",
            "e6",
            "Rhd1",
            "exd5",
            "Rxd5",
            "Qe7",
            "Qa4",
            "c6",
            "Rxc6",
            "Rxc6",
            "Nd4",
            "Bxd4",
            "Qxc6",
            "Rd8",
            "Rxd8+",
            "Qxd8",
            "Qc4",
            "Qd7",
            "Qb4",
            "Bc5",
            "Qc3",
            "Qd1+",
            "Qc1",
            "Qxc1+",
            "Kxc1",
            "Bxf2",
            "Kd2",
            "Kc7",
            "Kd3",
            "Kd6",
            "Kc4",
            "Ke5",
            "Kb5",
            "Kf4",
            "Ka6",
            "Kxf3",
            "Kxa7",
            "Kxe4",
            "Ka6",
            "f5",
            "Kb5",
            "f4",
            "Kc4",
            "f3",
            "Kc3",
            "Be3",
            "Kc2",
            "f2",
            "Kd1",
            "f1=Q+",
            "Kc2",
            "Qd3#",
            "f2",
            "Qd2+",
            "Kb1",
            "Qc1#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d5",
          "positional_uci": "d4d5",
          "positional_response": "6. d5 Ne5 7. Bf4 Nfd7 8. Bxe5 Nxe5 9. Bxc4 Bxf3 10. gxf3 Nxc4 11. Qa4+ Qd7 12. Qxc4 O-O-O 13. O-O-O e6 14. dxe6 Qxe6 15. Rxd8+ Kxd8 16. Rd1+ Bd6 17. Qd4 Kc8 18. Qxa7 Rd8 19. Qa8+ Kd7 20. Qxb7 Ke8 21. Nd5 Qh6+ 22. Kb1 Qxh2 23. Nxc7+ Bxc7 24. Qc6+ Kf8 25. Qc5+ Ke8 26. Qc6+ Kf8 27. Qc5+ Ke8 28. Qc6+ Kf8 29. Qc5+ Ke8 30. Qc6+ Kf8 31. Qc5+ Ke8 32. Qc6+ Kf8 33. Qc5+ Ke8 34. Qc6+ Kf8 35. Qc5+ Ke8 36. Qc6+ Kf8 37. Qc5+ Ke8 38. Qc6+ Kf8 39. Qc5+ Ke8 40. Qc6+ Kf8 41. Qc5+ Ke8 42. Qc6+ Kf8 43. Qc5+ Ke8 44. Qc6+ Kf8 45. Qc5+ Ke8 46. Qc6+ Kf8 47. Qc5+ Ke8 48. Qc6+ Kf8 49. Qc5+ Ke8 50. Qc6+ Kf8 51. Qc5+ Ke8 52. Qc6+ Kf8 53. Qc5+ Ke8 54. Qc6+ Kf8 55. Qc5+ Ke8 56. Qc6+ Kf8 57. Qc5+ Ke8 58. Qc6+ Kf8 59. Qc5+ Ke8 60. Qc6+ Kf8 61. Qc5+ Ke8 62. Qc6+ Kf8 63. Qc5+ Ke8 64. Qc6+ Kf8 65. Qc5+ Ke8 66. Qc6+ Kf8 67. Qc5+ Ke8 68. Qc6+ Kf8 69. Qc5+ Ke8 70. Qc6+ Kf8 71. Qc5+ Ke8 72. Qc6+ Kf8 73. Qc5+ Ke8 74. Qc",
          "positional_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 640,
            "total_tokens": 790,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Ne5",
            "Bf4",
            "Nfd7",
            "Bxe5",
            "Nxe5",
            "Bxc4",
            "Bxf3",
            "gxf3",
            "Nxc4",
            "Qa4+",
            "Qd7",
            "Qxc4",
            "O-O-O",
            "O-O-O",
            "e6",
            "dxe6",
            "Qxe6",
            "Rxd8+",
            "Kxd8",
            "Rd1+",
            "Bd6",
            "Qd4",
            "Kc8",
            "Qxa7",
            "Rd8",
            "Qa8+",
            "Kd7",
            "Qxb7",
            "Ke8",
            "Nd5",
            "Qh6+",
            "Kb1",
            "Qxh2",
            "Nxc7+",
            "Bxc7",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Qc5+",
            "Ke8"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d5",
          "neutral_uci": "d4d5",
          "neutral_response": "6. d5 Ne5 7. Bf4 Nfd7 8. Bxe5 Nxe5 9. Bxc4 Bxf3 10. gxf3 Nxc4 11. Qa4+ Qd7 12. Qxc4 O-O-O 13. O-O-O e6 14. dxe6 Qxe6 15. Rxd8+ Kxd8 16. Rd1+ Kc8 17. Qd4 Bd6 18. Qxa7 Rd8 19. Qa8+ Kd7 20. Qxb7 Ke8 21. Qc6+ Kf8 22. Kb1 Rb8 23. Nb5 Qe5 24. Nxd6 Qxb2# 1-0\n\n1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Ne5 7. Bf4 Nfd7 8. Bxe5 Nxe5 9. Bxc4 Bxf3 10. gxf3 Nxc4 11. Qa4+ Qd7 12. Qxc4 O-O-O 13. O-O-O e6 14. dxe6 Qxe6 15. Rxd8+ Kxd8 16. Rd1+ Kc8 17. Qd4 Bd6 18. Qxa7 Rd8 19. Qa8+ Kd7 20. Qxb7 Ke8 21. Qc6+ Kf8 22. Kb1 Rb8 23. Nb5 Qe5 24. Nxd6 Qxb2# 1-0 d5 dxc4 Nf3 Nc6 Nc3 Nf6 e4 Bg4 d5 Ne5 Bf4 Nfd7 Bxe5 Nxe5 Bxc4 Bxf3 gxf3 Nxc4 Qa4+ Qd7 Qxc4 O-O-O O-O-O e6 dxe6 Qxe6 Rxd8+ Kxd8 Rd1+ Kc8 Qd4 Bd6 Qxa7 Rd8 Qa8+ Kd7 Qxb7 Ke8 Qc6+ Kf8 Kb1 Rb8 Nb5 Qe5 Nxd6 Qxb2# 1-0",
          "neutral_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 522,
            "total_tokens": 671,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Ne5",
            "Bf4",
            "Nfd7",
            "Bxe5",
            "Nxe5",
            "Bxc4",
            "Bxf3",
            "gxf3",
            "Nxc4",
            "Qa4+",
            "Qd7",
            "Qxc4",
            "O-O-O",
            "O-O-O",
            "e6",
            "dxe6",
            "Qxe6",
            "Rxd8+",
            "Kxd8",
            "Rd1+",
            "Kc8",
            "Qd4",
            "Bd6",
            "Qxa7",
            "Rd8",
            "Qa8+",
            "Kd7",
            "Qxb7",
            "Ke8",
            "Qc6+",
            "Kf8",
            "Kb1",
            "Rb8",
            "Nb5",
            "Qe5",
            "Nxd6",
            "Qxb2#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d4d5",
          "positional_uci": "d4d5",
          "neutral_uci": "d4d5",
          "consensus_move": "d4d5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Ne5",
              "Bf4",
              "Nfd7",
              "Bxe5",
              "Nxe5",
              "Bxc4",
              "Bxf3",
              "gxf3",
              "Nxc4",
              "Qa4+",
              "Qd7",
              "Qxc4",
              "O-O-O",
              "O-O-O",
              "Kb8",
              "Kb1",
              "g6",
              "Rc1",
              "Bg7",
              "Nb5",
              "Rc8",
              "Qc5",
              "b6",
              "Qc4",
              "e6",
              "Rhd1",
              "exd5",
              "Rxd5",
              "Qe7",
              "Qa4",
              "c6",
              "Rxc6",
              "Rxc6",
              "Nd4",
              "Bxd4",
              "Qxc6",
              "Rd8",
              "Rxd8+",
              "Qxd8",
              "Qc4",
              "Qd7",
              "Qb4",
              "Bc5",
              "Qc3",
              "Qd1+",
              "Qc1",
              "Qxc1+",
              "Kxc1",
              "Bxf2",
              "Kd2",
              "Kc7",
              "Kd3",
              "Kd6",
              "Kc4",
              "Ke5",
              "Kb5",
              "Kf4",
              "Ka6",
              "Kxf3",
              "Kxa7",
              "Kxe4",
              "Ka6",
              "f5",
              "Kb5",
              "f4",
              "Kc4",
              "f3",
              "Kc3",
              "Be3",
              "Kc2",
              "f2",
              "Kd1",
              "f1=Q+",
              "Kc2",
              "Qd3#",
              "f2",
              "Qd2+",
              "Kb1",
              "Qc1#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Ne5",
              "Bf4",
              "Nfd7",
              "Bxe5",
              "Nxe5",
              "Bxc4",
              "Bxf3",
              "gxf3",
              "Nxc4",
              "Qa4+",
              "Qd7",
              "Qxc4",
              "O-O-O",
              "O-O-O",
              "e6",
              "dxe6",
              "Qxe6",
              "Rxd8+",
              "Kxd8",
              "Rd1+",
              "Bd6",
              "Qd4",
              "Kc8",
              "Qxa7",
              "Rd8",
              "Qa8+",
              "Kd7",
              "Qxb7",
              "Ke8",
              "Nd5",
              "Qh6+",
              "Kb1",
              "Qxh2",
              "Nxc7+",
              "Bxc7",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Qc5+",
              "Ke8"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Ne5",
              "Bf4",
              "Nfd7",
              "Bxe5",
              "Nxe5",
              "Bxc4",
              "Bxf3",
              "gxf3",
              "Nxc4",
              "Qa4+",
              "Qd7",
              "Qxc4",
              "O-O-O",
              "O-O-O",
              "e6",
              "dxe6",
              "Qxe6",
              "Rxd8+",
              "Kxd8",
              "Rd1+",
              "Kc8",
              "Qd4",
              "Bd6",
              "Qxa7",
              "Rd8",
              "Qa8+",
              "Kd7",
              "Qxb7",
              "Ke8",
              "Qc6+",
              "Kf8",
              "Kb1",
              "Rb8",
              "Nb5",
              "Qe5",
              "Nxd6",
              "Qxb2#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 150,
            "completion_tokens": 338,
            "total_tokens": 488,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 150,
            "completion_tokens": 640,
            "total_tokens": 790,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 149,
            "completion_tokens": 522,
            "total_tokens": 671,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 449,
          "total_completion_tokens": 1500,
          "total_tokens": 1949
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 150,
            "completion_tokens": 338,
            "total_tokens": 488,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 150,
            "completion_tokens": 640,
            "total_tokens": 790,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 149,
            "completion_tokens": 522,
            "total_tokens": 671,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/2n2n2/8/2pPP1b1/2N2N2/PP3PPP/R1BQKB1R w KQkq - 1 6"
    },
    {
      "turn": 13,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qa4+",
          "aggressive_uci": "d1a4",
          "aggressive_response": "7. Qa4+ c6 8. b4 cxb3 9. axb3 Bxf3 10. gxf3 b6 11. dxc6 Qc7 12. Nd5 Nxd5 13. exd5 e6 14. Bf4 Bd6 15. Bxd6 Qxd6 16. c7+ Ke7 17. Rc1 Rhc8 18. Ba6 Rxc7 19. Rxc7+ Qxc7 20. O-O exd5 21. Re1+ Kf8 22. Qb4+ Kg8 23. Re7 Qc1+ 24. Kg2 Qg5+ 25. Kf1 Nc6 26. Qd6 Nxe7 27. Bb7 Re8 28. Qd7 Kf8 29. Bc6 Nxc6 30. Qxc6 Qe5 31. Kg2 Qg5+ 32. Kf1 Qe5 33. Kg2 Qg5+ 34. Kf1 Qe5 35. Kg2 Qg5+ 36. Kf1 Qe5 37. Kg2 Qg5+ 38. Kf1 Qe5 39. Kg2 Qg5+ 40. Kf1 Qe5 41. Kg2 Qg5+ 42. Kf1 Qe5 43. Kg2 Qg5+ 44. Kf1 Qe5 45. Kg2 Qg5+ 46. Kf1 Qe5 47. Kg2 Qg5+ 48. Kf1 Qe5 49. Kg2 Qg5+ 50. Kf1 Qe5 51. Kg2 Qg5+ 52. Kf1 Qe5 53. Kg2 Qg5+ 54. Kf1 Qe5 55. Kg2 Qg5+ 56. Kf1 Qe5 57. Kg2 Qg5+ 58. Kf1 Qe5 59. Kg2 Qg5+ 60. Kf1 Qe5 61. Kg2 Qg5+ 62. Kf1 Qe5 63. Kg2 Qg5+ 64. Kf1 Qe5 65. Kg2 Qg5+ 66. Kf1 Qe5 67. Kg2 Qg5+ 68. Kf1 Qe5 69. Kg2 Qg5+ 70. Kf1 Qe5 71. Kg2 Qg5+ 72. Kf1 Qe5 73. Kg2 Qg5+ 74. Kf1 Qe5 75. Kg2 Qg5+ 76. Kf1 Qe5 77. Kg2 Qg5+ 78. Kf1 Qe5",
          "aggressive_tokens": {
            "prompt_tokens": 157,
            "completion_tokens": 640,
            "total_tokens": 797,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "c6",
            "b4",
            "cxb3",
            "axb3",
            "Bxf3",
            "gxf3",
            "b6",
            "dxc6",
            "Qc7",
            "Nd5",
            "Nxd5",
            "exd5",
            "e6",
            "Bf4",
            "Bd6",
            "Bxd6",
            "Qxd6",
            "c7+",
            "Ke7",
            "Rc1",
            "Rhc8",
            "Ba6",
            "Rxc7",
            "Rxc7+",
            "Qxc7",
            "O-O",
            "exd5",
            "Re1+",
            "Kf8",
            "Qb4+",
            "Kg8",
            "Re7",
            "Qc1+",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Nc6",
            "Qd6",
            "Nxe7",
            "Bb7",
            "Re8",
            "Qd7",
            "Kf8",
            "Bc6",
            "Nxc6",
            "Qxc6",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5",
            "Kg2",
            "Qg5+",
            "Kf1",
            "Qe5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qa4+",
          "positional_uci": "d1a4",
          "positional_response": "7. Qa4+ c6 8. b4 cxb3 9. axb3 Bxf3 10. gxf3 b6 11. dxc6 Qc7 12. Nd5 Nxd5 13. exd5 e6 14. Bf4 Bd6 15. Bxd6 Qxd6 16. c7+ Ke7 17. Rc1 Rhc8 18. Ba6 Rxc7 19. Rxc7+ Qxc7 20. O-O Rd8 21. dxe6 fxe6 22. Qh4+ Kf8 23. Qxh7 Nxb3 24. Qh8+ Kf7 25. Qh5+ Kg8 26. Re1 Nc5 27. Bc4 Qf7 28. Qg4 Rd6 29. Kh1 a5 30. Rg1 a4 31. h4 a3 32. h5 Nd3 33. Bxd3 Rxd3 34. h6 Qxf3+ 35. Qxf3 Rxf3 36. Rxg7+ Kh8 37. Kg2 Rf6 38. Ra7 Rxh6 39. Rxa3 Kg7 40. Rb3 e5 41. Kf3 Kf7 42. Ke4 Re6 43. Rb5 Kf6 44. f3 Kf7 45. Rxe5 Rxe5+ 46. Kxe5 Ke7 47. Kd5 Kf6 48. Kc6 Kf5 49. Kxb6 Kf4 50. Kc5 Kxf3 51.  Kd4 Kf4 52. Kd3 Kf3 53. Kd2 Kf2 54. Kd1 Kf1 55. Kd2 Kf2 56. Kd3 Kf3 57. Kd4 Kf4 58. Kd5 Kf5 59. Kd6 Kf6 60. Kd7 Kf7 61. Kd8 Kf8 62. Kd7 Kf7 63. Kd6 Kf6 64. Kd5 Kf5 65. Kd4 Kf4 66. Kd3 Kf3 67. Kd2 Kf2 68. Kd1 Kf1 69. Kd2 Kf2 70. Kd3 Kf3 71. Kd4 Kf4 72. Kd5 Kf5 73. Kd6 Kf6 74. Kd7 Kf7 75. Kd8 Kf8 76. Kd7 Kf7 77. Kd6 Kf6 78. Kd5 Kf5 79. Kd4 Kf4 80. K",
          "positional_tokens": {
            "prompt_tokens": 157,
            "completion_tokens": 640,
            "total_tokens": 797,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "c6",
            "b4",
            "cxb3",
            "axb3",
            "Bxf3",
            "gxf3",
            "b6",
            "dxc6",
            "Qc7",
            "Nd5",
            "Nxd5",
            "exd5",
            "e6",
            "Bf4",
            "Bd6",
            "Bxd6",
            "Qxd6",
            "c7+",
            "Ke7",
            "Rc1",
            "Rhc8",
            "Ba6",
            "Rxc7",
            "Rxc7+",
            "Qxc7",
            "O-O",
            "Rd8",
            "dxe6",
            "fxe6",
            "Qh4+",
            "Kf8",
            "Qxh7",
            "Nxb3",
            "Qh8+",
            "Kf7",
            "Qh5+",
            "Kg8",
            "Re1",
            "Nc5",
            "Bc4",
            "Qf7",
            "Qg4",
            "Rd6",
            "Kh1",
            "a5",
            "Rg1",
            "a4",
            "h4",
            "a3",
            "h5",
            "Nd3",
            "Bxd3",
            "Rxd3",
            "h6",
            "Qxf3+",
            "Qxf3",
            "Rxf3",
            "Rxg7+",
            "Kh8",
            "Kg2",
            "Rf6",
            "Ra7",
            "Rxh6",
            "Rxa3",
            "Kg7",
            "Rb3",
            "e5",
            "Kf3",
            "Kf7",
            "Ke4",
            "Re6",
            "Rb5",
            "Kf6",
            "f3",
            "Kf7",
            "Rxe5",
            "Rxe5+",
            "Kxe5",
            "Ke7",
            "Kd5",
            "Kf6",
            "Kc6",
            "Kf5",
            "Kxb6",
            "Kf4",
            "Kc5",
            "Kxf3",
            "Kd4",
            "Kf4",
            "Kd3",
            "Kf3",
            "Kd2",
            "Kf2",
            "Kd1",
            "Kf1",
            "Kd2",
            "Kf2",
            "Kd3",
            "Kf3",
            "Kd4",
            "Kf4",
            "Kd5",
            "Kf5",
            "Kd6",
            "Kf6",
            "Kd7",
            "Kf7",
            "Kd8",
            "Kf8",
            "Kd7",
            "Kf7",
            "Kd6",
            "Kf6",
            "Kd5",
            "Kf5",
            "Kd4",
            "Kf4",
            "Kd3",
            "Kf3",
            "Kd2",
            "Kf2",
            "Kd1",
            "Kf1",
            "Kd2",
            "Kf2",
            "Kd3",
            "Kf3",
            "Kd4",
            "Kf4",
            "Kd5",
            "Kf5",
            "Kd6",
            "Kf6",
            "Kd7",
            "Kf7",
            "Kd8",
            "Kf8",
            "Kd7",
            "Kf7",
            "Kd6",
            "Kf6",
            "Kd5",
            "Kf5",
            "Kd4",
            "Kf4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qa4+",
          "neutral_uci": "d1a4",
          "neutral_response": "7. Qa4+ c6 8. b4 cxb3 9. axb3 Bxf3 10. gxf3 b6 11. b4 Nb7 12. dxc6 Nd6 13. c7+ Qd7 14. Nb5 Nc8 15. Bf4 e6 16. Rd1 Qe7 17. Nd6# cxd6 18. Bb5+ Nd7 19. e5 d5 20. Bc6 Qxb4+ 21. Qxb4 Bxb4+ 22. Ke2 Ke7 23. Bxa8 Nc5 24. Bc6 a5 25. Rhg1 g6 26. Bg5+ Kf8 27. Bf6 Rg8 28. Rg4 Na7 29. Bxd5 exd5 30. Rxd5 Ne6 31. Rd8+ Nxd8 32. cxd8=Q# Rxd8 33. Bxd8 Nc6 34. Bxb6 Nxe5 35. Re4 Nd7 36. Bd4 f5 37. Re6 Kf7 38. Ra6 Ke7 39. Ra7 Ke6 40. Kd3 h5 41. Kc4 Ne5+ 42. Bxe5 Kxe5 43. Ra6 g5 44. Rh6 h4 45. Rg6 Kf4 46. Rf6 Be1 47. Kd3 Bxf2 48. Ke2 Bg1 49. Ra6 Bxh2 50. Rxa5 Bg1 51. Ra4+ Kg3 52. Ra5 Kf4 53. Ra4+ Ke5 54. Ra5+ Kf6 55. Ra6+ Kg7 56. Kf1 Bd4 57. Kg2 Be5 58. Kh3 Bf6 59. Ra5 Kg6 60. Ra6 Kf7 61. Ra7+ Ke6 62. Ra6+ Ke5 63. Ra5+ Kf4 64. Ra6 Be5 65. Ra3 Bd4 66. Ra4 Ke3 67. Ra3+ Kf2 68. Ra2+ Kxf3 69. Ra3+ Be3 70. Ra5 Ke4 71. Ra4+ Bd4 72. Ra5 Bf2 73. Ra4+ Kf3 74. Ra3+ Be3 75. Ra5 g4+ 76. Kxh4 f4 77. Ra3 g3 78. Kh3 g2 79. Ra1 g1=Q 80. Rxg1 Bxg1 81. Kh4 Kg2 82. Kg4 f3 83. Kf4 f2 84. Ke3 f1",
          "neutral_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "c6",
            "b4",
            "cxb3",
            "axb3",
            "Bxf3",
            "gxf3",
            "b6",
            "b4",
            "Nb7",
            "dxc6",
            "Nd6",
            "c7+",
            "Qd7",
            "Nb5",
            "Nc8",
            "Bf4",
            "e6",
            "Rd1",
            "Qe7",
            "Nd6#",
            "cxd6",
            "Bb5+",
            "Nd7",
            "e5",
            "d5",
            "Bc6",
            "Qxb4+",
            "Qxb4",
            "Bxb4+",
            "Ke2",
            "Ke7",
            "Bxa8",
            "Nc5",
            "Bc6",
            "a5",
            "Rhg1",
            "g6",
            "Bg5+",
            "Kf8",
            "Bf6",
            "Rg8",
            "Rg4",
            "Na7",
            "Bxd5",
            "exd5",
            "Rxd5",
            "Ne6",
            "Rd8+",
            "Nxd8",
            "cxd8=Q#",
            "Rxd8",
            "Bxd8",
            "Nc6",
            "Bxb6",
            "Nxe5",
            "Re4",
            "Nd7",
            "Bd4",
            "f5",
            "Re6",
            "Kf7",
            "Ra6",
            "Ke7",
            "Ra7",
            "Ke6",
            "Kd3",
            "h5",
            "Kc4",
            "Ne5+",
            "Bxe5",
            "Kxe5",
            "Ra6",
            "g5",
            "Rh6",
            "h4",
            "Rg6",
            "Kf4",
            "Rf6",
            "Be1",
            "Kd3",
            "Bxf2",
            "Ke2",
            "Bg1",
            "Ra6",
            "Bxh2",
            "Rxa5",
            "Bg1",
            "Ra4+",
            "Kg3",
            "Ra5",
            "Kf4",
            "Ra4+",
            "Ke5",
            "Ra5+",
            "Kf6",
            "Ra6+",
            "Kg7",
            "Kf1",
            "Bd4",
            "Kg2",
            "Be5",
            "Kh3",
            "Bf6",
            "Ra5",
            "Kg6",
            "Ra6",
            "Kf7",
            "Ra7+",
            "Ke6",
            "Ra6+",
            "Ke5",
            "Ra5+",
            "Kf4",
            "Ra6",
            "Be5",
            "Ra3",
            "Bd4",
            "Ra4",
            "Ke3",
            "Ra3+",
            "Kf2",
            "Ra2+",
            "Kxf3",
            "Ra3+",
            "Be3",
            "Ra5",
            "Ke4",
            "Ra4+",
            "Bd4",
            "Ra5",
            "Bf2",
            "Ra4+",
            "Kf3",
            "Ra3+",
            "Be3",
            "Ra5",
            "g4+",
            "Kxh4",
            "f4",
            "Ra3",
            "g3",
            "Kh3",
            "g2",
            "Ra1",
            "g1=Q",
            "Rxg1",
            "Bxg1",
            "Kh4",
            "Kg2",
            "Kg4",
            "f3",
            "Kf4",
            "f2",
            "Ke3",
            "f1"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d1a4",
          "positional_uci": "d1a4",
          "neutral_uci": "d1a4",
          "consensus_move": "d1a4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "c6",
              "b4",
              "cxb3",
              "axb3",
              "Bxf3",
              "gxf3",
              "b6",
              "dxc6",
              "Qc7",
              "Nd5",
              "Nxd5",
              "exd5",
              "e6",
              "Bf4",
              "Bd6",
              "Bxd6",
              "Qxd6",
              "c7+",
              "Ke7",
              "Rc1",
              "Rhc8",
              "Ba6",
              "Rxc7",
              "Rxc7+",
              "Qxc7",
              "O-O",
              "exd5",
              "Re1+",
              "Kf8",
              "Qb4+",
              "Kg8",
              "Re7",
              "Qc1+",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Nc6",
              "Qd6",
              "Nxe7",
              "Bb7",
              "Re8",
              "Qd7",
              "Kf8",
              "Bc6",
              "Nxc6",
              "Qxc6",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5",
              "Kg2",
              "Qg5+",
              "Kf1",
              "Qe5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "c6",
              "b4",
              "cxb3",
              "axb3",
              "Bxf3",
              "gxf3",
              "b6",
              "dxc6",
              "Qc7",
              "Nd5",
              "Nxd5",
              "exd5",
              "e6",
              "Bf4",
              "Bd6",
              "Bxd6",
              "Qxd6",
              "c7+",
              "Ke7",
              "Rc1",
              "Rhc8",
              "Ba6",
              "Rxc7",
              "Rxc7+",
              "Qxc7",
              "O-O",
              "Rd8",
              "dxe6",
              "fxe6",
              "Qh4+",
              "Kf8",
              "Qxh7",
              "Nxb3",
              "Qh8+",
              "Kf7",
              "Qh5+",
              "Kg8",
              "Re1",
              "Nc5",
              "Bc4",
              "Qf7",
              "Qg4",
              "Rd6",
              "Kh1",
              "a5",
              "Rg1",
              "a4",
              "h4",
              "a3",
              "h5",
              "Nd3",
              "Bxd3",
              "Rxd3",
              "h6",
              "Qxf3+",
              "Qxf3",
              "Rxf3",
              "Rxg7+",
              "Kh8",
              "Kg2",
              "Rf6",
              "Ra7",
              "Rxh6",
              "Rxa3",
              "Kg7",
              "Rb3",
              "e5",
              "Kf3",
              "Kf7",
              "Ke4",
              "Re6",
              "Rb5",
              "Kf6",
              "f3",
              "Kf7",
              "Rxe5",
              "Rxe5+",
              "Kxe5",
              "Ke7",
              "Kd5",
              "Kf6",
              "Kc6",
              "Kf5",
              "Kxb6",
              "Kf4",
              "Kc5",
              "Kxf3",
              "Kd4",
              "Kf4",
              "Kd3",
              "Kf3",
              "Kd2",
              "Kf2",
              "Kd1",
              "Kf1",
              "Kd2",
              "Kf2",
              "Kd3",
              "Kf3",
              "Kd4",
              "Kf4",
              "Kd5",
              "Kf5",
              "Kd6",
              "Kf6",
              "Kd7",
              "Kf7",
              "Kd8",
              "Kf8",
              "Kd7",
              "Kf7",
              "Kd6",
              "Kf6",
              "Kd5",
              "Kf5",
              "Kd4",
              "Kf4",
              "Kd3",
              "Kf3",
              "Kd2",
              "Kf2",
              "Kd1",
              "Kf1",
              "Kd2",
              "Kf2",
              "Kd3",
              "Kf3",
              "Kd4",
              "Kf4",
              "Kd5",
              "Kf5",
              "Kd6",
              "Kf6",
              "Kd7",
              "Kf7",
              "Kd8",
              "Kf8",
              "Kd7",
              "Kf7",
              "Kd6",
              "Kf6",
              "Kd5",
              "Kf5",
              "Kd4",
              "Kf4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "c6",
              "b4",
              "cxb3",
              "axb3",
              "Bxf3",
              "gxf3",
              "b6",
              "b4",
              "Nb7",
              "dxc6",
              "Nd6",
              "c7+",
              "Qd7",
              "Nb5",
              "Nc8",
              "Bf4",
              "e6",
              "Rd1",
              "Qe7",
              "Nd6#",
              "cxd6",
              "Bb5+",
              "Nd7",
              "e5",
              "d5",
              "Bc6",
              "Qxb4+",
              "Qxb4",
              "Bxb4+",
              "Ke2",
              "Ke7",
              "Bxa8",
              "Nc5",
              "Bc6",
              "a5",
              "Rhg1",
              "g6",
              "Bg5+",
              "Kf8",
              "Bf6",
              "Rg8",
              "Rg4",
              "Na7",
              "Bxd5",
              "exd5",
              "Rxd5",
              "Ne6",
              "Rd8+",
              "Nxd8",
              "cxd8=Q#",
              "Rxd8",
              "Bxd8",
              "Nc6",
              "Bxb6",
              "Nxe5",
              "Re4",
              "Nd7",
              "Bd4",
              "f5",
              "Re6",
              "Kf7",
              "Ra6",
              "Ke7",
              "Ra7",
              "Ke6",
              "Kd3",
              "h5",
              "Kc4",
              "Ne5+",
              "Bxe5",
              "Kxe5",
              "Ra6",
              "g5",
              "Rh6",
              "h4",
              "Rg6",
              "Kf4",
              "Rf6",
              "Be1",
              "Kd3",
              "Bxf2",
              "Ke2",
              "Bg1",
              "Ra6",
              "Bxh2",
              "Rxa5",
              "Bg1",
              "Ra4+",
              "Kg3",
              "Ra5",
              "Kf4",
              "Ra4+",
              "Ke5",
              "Ra5+",
              "Kf6",
              "Ra6+",
              "Kg7",
              "Kf1",
              "Bd4",
              "Kg2",
              "Be5",
              "Kh3",
              "Bf6",
              "Ra5",
              "Kg6",
              "Ra6",
              "Kf7",
              "Ra7+",
              "Ke6",
              "Ra6+",
              "Ke5",
              "Ra5+",
              "Kf4",
              "Ra6",
              "Be5",
              "Ra3",
              "Bd4",
              "Ra4",
              "Ke3",
              "Ra3+",
              "Kf2",
              "Ra2+",
              "Kxf3",
              "Ra3+",
              "Be3",
              "Ra5",
              "Ke4",
              "Ra4+",
              "Bd4",
              "Ra5",
              "Bf2",
              "Ra4+",
              "Kf3",
              "Ra3+",
              "Be3",
              "Ra5",
              "g4+",
              "Kxh4",
              "f4",
              "Ra3",
              "g3",
              "Kh3",
              "g2",
              "Ra1",
              "g1=Q",
              "Rxg1",
              "Bxg1",
              "Kh4",
              "Kg2",
              "Kg4",
              "f3",
              "Kf4",
              "f2",
              "Ke3",
              "f1"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 157,
            "completion_tokens": 640,
            "total_tokens": 797,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 157,
            "completion_tokens": 640,
            "total_tokens": 797,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 470,
          "total_completion_tokens": 1920,
          "total_tokens": 2390
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 157,
            "completion_tokens": 640,
            "total_tokens": 797,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 157,
            "completion_tokens": 640,
            "total_tokens": 797,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/5n2/n2P4/2p1P1b1/2N2N2/PP3PPP/R1BQKB1R w KQkq - 1 7"
    },
    {
      "turn": 15,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "dxc6",
          "aggressive_uci": "d5c6",
          "aggressive_response": "8. dxc6 bxc6 9. Qxc6+ Nd7 10. Ne5 Be6 11. Bxc4 Bxc4 12. Nxc4 e6 13. Nb5 Bb4+ 14. Bd2 Bxd2+ 15. Nxd2 O-O 16. Qxc7 Qxc7 17. Nxc7 Rac8 18. Nb5 Rc2 19. b3 a6 20. Nd4 Rb2 21. Nc4 *",
          "aggressive_tokens": {
            "prompt_tokens": 167,
            "completion_tokens": 114,
            "total_tokens": 281,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "bxc6",
            "Qxc6+",
            "Nd7",
            "Ne5",
            "Be6",
            "Bxc4",
            "Bxc4",
            "Nxc4",
            "e6",
            "Nb5",
            "Bb4+",
            "Bd2",
            "Bxd2+",
            "Nxd2",
            "O-O",
            "Qxc7",
            "Qxc7",
            "Nxc7",
            "Rac8",
            "Nb5",
            "Rc2",
            "b3",
            "a6",
            "Nd4",
            "Rb2",
            "Nc4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "dxc6",
          "positional_uci": "d5c6",
          "positional_response": "8. dxc6 bxc6 9. Qxc6+ Nd7 10. Ne5 Be6 11. Bxc4 Bxc4 12. Nxc4 e6 13. Nb5 Bb4+ 14. Bd2 Bxd2+ 15. Nxd2 O-O 16. Qxc7 Qxc7 17. Nxc7 Rac8 18. Nb5 Rc2 19. b3 a6 20. Nd4 Rc5 21. O-O Rfc8 22. Nc4 Ne5 23. Nxe5 Rxe5 24. f3 Rec5 25. Rfd1 e5 26. Nf5 Kf8 27. Rd5 Rc1+ 28. Rxc1 Rxc1+ 29. Kf2 Rc2+ 30. Kg3 Rxa2 31. Rd8# g6 32. Nd6+ Ke7 33. Nb7 Rb2 34. Rd3 f5 35. exf5 gxf5 36. Nc5 f4+ 37. Kh3 a5 38. Rd7+ Kf6 39. Rxh7 Rc2 40. Ne4+ Kg6 41. Ra7 Ra2 42. b4 a4 43. b5 a3 44. b6 Rb2 45. b7 a2 46. Nc5 Kf5 47. Rxa2 Rb6 48. Ra8 Rh6# Rg6 50. b8=Q Rh6# Rg6 52. Qf8+ Kg5 53. Ne4+ Kh5 54. Qh8+ Rh6 55. Qxe5+ Kg6+ 56. Kg4 Rxh2 57. Qf6+ Kh7 58. Rh8# Kg6 60. Qf6# Kh5 61. Qg5#",
          "positional_tokens": {
            "prompt_tokens": 167,
            "completion_tokens": 426,
            "total_tokens": 593,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "bxc6",
            "Qxc6+",
            "Nd7",
            "Ne5",
            "Be6",
            "Bxc4",
            "Bxc4",
            "Nxc4",
            "e6",
            "Nb5",
            "Bb4+",
            "Bd2",
            "Bxd2+",
            "Nxd2",
            "O-O",
            "Qxc7",
            "Qxc7",
            "Nxc7",
            "Rac8",
            "Nb5",
            "Rc2",
            "b3",
            "a6",
            "Nd4",
            "Rc5",
            "O-O",
            "Rfc8",
            "Nc4",
            "Ne5",
            "Nxe5",
            "Rxe5",
            "f3",
            "Rec5",
            "Rfd1",
            "e5",
            "Nf5",
            "Kf8",
            "Rd5",
            "Rc1+",
            "Rxc1",
            "Rxc1+",
            "Kf2",
            "Rc2+",
            "Kg3",
            "Rxa2",
            "Rd8#",
            "g6",
            "Nd6+",
            "Ke7",
            "Nb7",
            "Rb2",
            "Rd3",
            "f5",
            "exf5",
            "gxf5",
            "Nc5",
            "f4+",
            "Kh3",
            "a5",
            "Rd7+",
            "Kf6",
            "Rxh7",
            "Rc2",
            "Ne4+",
            "Kg6",
            "Ra7",
            "Ra2",
            "b4",
            "a4",
            "b5",
            "a3",
            "b6",
            "Rb2",
            "b7",
            "a2",
            "Nc5",
            "Kf5",
            "Rxa2",
            "Rb6",
            "Ra8",
            "Rh6#",
            "Rg6",
            "b8=Q",
            "Rh6#",
            "Rg6",
            "Qf8+",
            "Kg5",
            "Ne4+",
            "Kh5",
            "Qh8+",
            "Rh6",
            "Qxe5+",
            "Kg6+",
            "Kg4",
            "Rxh2",
            "Qf6+",
            "Kh7",
            "Rh8#",
            "Kg6",
            "Qf6#",
            "Kh5",
            "Qg5#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "dxc6",
          "neutral_uci": "d5c6",
          "neutral_response": "8. dxc6 bxc6 9. Qxc6+ Nd7 10. Ne5 Be6 11. Bxc4 Bxc4 12. Nxc4 e6 13. Nb5 Bb4+ 14. Bd2 Bxd2+ 15. Nxd2 O-O 16. Qxc7 Qxc7 17. Nxc7 Rac8 18. Nb5 Rc2 19. b3 a6 20. Nd4 Rb2 21. Nc4 Rxf2 22. Kxf2 Nc5 23. Ke3 Rd8 24. Rhd1 f6 25. Nc6 Rxd1 26. Rxd1 Kf7 27. b4 Na4 28. Rd7+ Kg6 29. Ne7+ Kh6 30. Ng8+ Kg6 31. Nd6 Nc3 32. Ne8 Nxa2 33. Rxg7+ Kh5 34. Nexf6+ Kh4 35. Rg4# Nc3 36. Kf3 h5 37. g3+ Kh3 38. Rh4# Kg1 39. Nxh5 Kf1 40. Nf4 Ke1 41. Nxe6 Kd2 42. Nc5 Kc2 43. Nxa6 Kb3 44. e5 Nd5 45. e6 Kc3 46. b5 Kd3 47. Nb4+ Nxb4 48. Rxb4 Kc3 49. Re4 Kd3 50. e7 Kc3 51. e8=Q Kd3 52. Qd7+ Kc3 53. Qd4+ Kc2 54. Re2+ Kb3 55. Rb2+ Ka3 56. Qb4# Kg1 57. Rb1+ Kxh2 58. Qh4# Kg1 59. Qh1# Kh2 60. Qg2# Kg1 61. Qg1# Kh2 62. Qh1# Kg1 63. Qh1# Kh2 64. Qg2# Kg1 65. Qg1# Kh2 66. Qh1# Kg1 67. Qh1# Kh2 68. Qg2# Kg1 69. Qg1# Kh2 70. Qh1# Kg1 71. Qh1# Kh2 72. Qg2# Kg1 73. Qg1# Kh2 74. Qh1# Kg1 75. Qh1# Kh2 76. Qg2# Kg1 77. Qg1# Kh2 78. Qh1# Kg1 79. Qh1# Kh2 80. Qg2# Kg1 81. Qg1# Kh2",
          "neutral_tokens": {
            "prompt_tokens": 166,
            "completion_tokens": 640,
            "total_tokens": 806,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "bxc6",
            "Qxc6+",
            "Nd7",
            "Ne5",
            "Be6",
            "Bxc4",
            "Bxc4",
            "Nxc4",
            "e6",
            "Nb5",
            "Bb4+",
            "Bd2",
            "Bxd2+",
            "Nxd2",
            "O-O",
            "Qxc7",
            "Qxc7",
            "Nxc7",
            "Rac8",
            "Nb5",
            "Rc2",
            "b3",
            "a6",
            "Nd4",
            "Rb2",
            "Nc4",
            "Rxf2",
            "Kxf2",
            "Nc5",
            "Ke3",
            "Rd8",
            "Rhd1",
            "f6",
            "Nc6",
            "Rxd1",
            "Rxd1",
            "Kf7",
            "b4",
            "Na4",
            "Rd7+",
            "Kg6",
            "Ne7+",
            "Kh6",
            "Ng8+",
            "Kg6",
            "Nd6",
            "Nc3",
            "Ne8",
            "Nxa2",
            "Rxg7+",
            "Kh5",
            "Nexf6+",
            "Kh4",
            "Rg4#",
            "Nc3",
            "Kf3",
            "h5",
            "g3+",
            "Kh3",
            "Rh4#",
            "Kg1",
            "Nxh5",
            "Kf1",
            "Nf4",
            "Ke1",
            "Nxe6",
            "Kd2",
            "Nc5",
            "Kc2",
            "Nxa6",
            "Kb3",
            "e5",
            "Nd5",
            "e6",
            "Kc3",
            "b5",
            "Kd3",
            "Nb4+",
            "Nxb4",
            "Rxb4",
            "Kc3",
            "Re4",
            "Kd3",
            "e7",
            "Kc3",
            "e8=Q",
            "Kd3",
            "Qd7+",
            "Kc3",
            "Qd4+",
            "Kc2",
            "Re2+",
            "Kb3",
            "Rb2+",
            "Ka3",
            "Qb4#",
            "Kg1",
            "Rb1+",
            "Kxh2",
            "Qh4#",
            "Kg1",
            "Qh1#",
            "Kh2",
            "Qg2#",
            "Kg1",
            "Qg1#",
            "Kh2",
            "Qh1#",
            "Kg1",
            "Qh1#",
            "Kh2",
            "Qg2#",
            "Kg1",
            "Qg1#",
            "Kh2",
            "Qh1#",
            "Kg1",
            "Qh1#",
            "Kh2",
            "Qg2#",
            "Kg1",
            "Qg1#",
            "Kh2",
            "Qh1#",
            "Kg1",
            "Qh1#",
            "Kh2",
            "Qg2#",
            "Kg1",
            "Qg1#",
            "Kh2",
            "Qh1#",
            "Kg1",
            "Qh1#",
            "Kh2",
            "Qg2#",
            "Kg1",
            "Qg1#",
            "Kh2",
            "Qh1#",
            "Kg1",
            "Qh1#",
            "Kh2",
            "Qg2#",
            "Kg1",
            "Qg1#",
            "Kh2"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d5c6",
          "positional_uci": "d5c6",
          "neutral_uci": "d5c6",
          "consensus_move": "d5c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "bxc6",
              "Qxc6+",
              "Nd7",
              "Ne5",
              "Be6",
              "Bxc4",
              "Bxc4",
              "Nxc4",
              "e6",
              "Nb5",
              "Bb4+",
              "Bd2",
              "Bxd2+",
              "Nxd2",
              "O-O",
              "Qxc7",
              "Qxc7",
              "Nxc7",
              "Rac8",
              "Nb5",
              "Rc2",
              "b3",
              "a6",
              "Nd4",
              "Rb2",
              "Nc4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "bxc6",
              "Qxc6+",
              "Nd7",
              "Ne5",
              "Be6",
              "Bxc4",
              "Bxc4",
              "Nxc4",
              "e6",
              "Nb5",
              "Bb4+",
              "Bd2",
              "Bxd2+",
              "Nxd2",
              "O-O",
              "Qxc7",
              "Qxc7",
              "Nxc7",
              "Rac8",
              "Nb5",
              "Rc2",
              "b3",
              "a6",
              "Nd4",
              "Rc5",
              "O-O",
              "Rfc8",
              "Nc4",
              "Ne5",
              "Nxe5",
              "Rxe5",
              "f3",
              "Rec5",
              "Rfd1",
              "e5",
              "Nf5",
              "Kf8",
              "Rd5",
              "Rc1+",
              "Rxc1",
              "Rxc1+",
              "Kf2",
              "Rc2+",
              "Kg3",
              "Rxa2",
              "Rd8#",
              "g6",
              "Nd6+",
              "Ke7",
              "Nb7",
              "Rb2",
              "Rd3",
              "f5",
              "exf5",
              "gxf5",
              "Nc5",
              "f4+",
              "Kh3",
              "a5",
              "Rd7+",
              "Kf6",
              "Rxh7",
              "Rc2",
              "Ne4+",
              "Kg6",
              "Ra7",
              "Ra2",
              "b4",
              "a4",
              "b5",
              "a3",
              "b6",
              "Rb2",
              "b7",
              "a2",
              "Nc5",
              "Kf5",
              "Rxa2",
              "Rb6",
              "Ra8",
              "Rh6#",
              "Rg6",
              "b8=Q",
              "Rh6#",
              "Rg6",
              "Qf8+",
              "Kg5",
              "Ne4+",
              "Kh5",
              "Qh8+",
              "Rh6",
              "Qxe5+",
              "Kg6+",
              "Kg4",
              "Rxh2",
              "Qf6+",
              "Kh7",
              "Rh8#",
              "Kg6",
              "Qf6#",
              "Kh5",
              "Qg5#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "bxc6",
              "Qxc6+",
              "Nd7",
              "Ne5",
              "Be6",
              "Bxc4",
              "Bxc4",
              "Nxc4",
              "e6",
              "Nb5",
              "Bb4+",
              "Bd2",
              "Bxd2+",
              "Nxd2",
              "O-O",
              "Qxc7",
              "Qxc7",
              "Nxc7",
              "Rac8",
              "Nb5",
              "Rc2",
              "b3",
              "a6",
              "Nd4",
              "Rb2",
              "Nc4",
              "Rxf2",
              "Kxf2",
              "Nc5",
              "Ke3",
              "Rd8",
              "Rhd1",
              "f6",
              "Nc6",
              "Rxd1",
              "Rxd1",
              "Kf7",
              "b4",
              "Na4",
              "Rd7+",
              "Kg6",
              "Ne7+",
              "Kh6",
              "Ng8+",
              "Kg6",
              "Nd6",
              "Nc3",
              "Ne8",
              "Nxa2",
              "Rxg7+",
              "Kh5",
              "Nexf6+",
              "Kh4",
              "Rg4#",
              "Nc3",
              "Kf3",
              "h5",
              "g3+",
              "Kh3",
              "Rh4#",
              "Kg1",
              "Nxh5",
              "Kf1",
              "Nf4",
              "Ke1",
              "Nxe6",
              "Kd2",
              "Nc5",
              "Kc2",
              "Nxa6",
              "Kb3",
              "e5",
              "Nd5",
              "e6",
              "Kc3",
              "b5",
              "Kd3",
              "Nb4+",
              "Nxb4",
              "Rxb4",
              "Kc3",
              "Re4",
              "Kd3",
              "e7",
              "Kc3",
              "e8=Q",
              "Kd3",
              "Qd7+",
              "Kc3",
              "Qd4+",
              "Kc2",
              "Re2+",
              "Kb3",
              "Rb2+",
              "Ka3",
              "Qb4#",
              "Kg1",
              "Rb1+",
              "Kxh2",
              "Qh4#",
              "Kg1",
              "Qh1#",
              "Kh2",
              "Qg2#",
              "Kg1",
              "Qg1#",
              "Kh2",
              "Qh1#",
              "Kg1",
              "Qh1#",
              "Kh2",
              "Qg2#",
              "Kg1",
              "Qg1#",
              "Kh2",
              "Qh1#",
              "Kg1",
              "Qh1#",
              "Kh2",
              "Qg2#",
              "Kg1",
              "Qg1#",
              "Kh2",
              "Qh1#",
              "Kg1",
              "Qh1#",
              "Kh2",
              "Qg2#",
              "Kg1",
              "Qg1#",
              "Kh2",
              "Qh1#",
              "Kg1",
              "Qh1#",
              "Kh2",
              "Qg2#",
              "Kg1",
              "Qg1#",
              "Kh2",
              "Qh1#",
              "Kg1",
              "Qh1#",
              "Kh2",
              "Qg2#",
              "Kg1",
              "Qg1#",
              "Kh2"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 167,
            "completion_tokens": 114,
            "total_tokens": 281,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 167,
            "completion_tokens": 426,
            "total_tokens": 593,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 166,
            "completion_tokens": 640,
            "total_tokens": 806,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 500,
          "total_completion_tokens": 1180,
          "total_tokens": 1680
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 167,
            "completion_tokens": 114,
            "total_tokens": 281,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 167,
            "completion_tokens": 426,
            "total_tokens": 593,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 166,
            "completion_tokens": 640,
            "total_tokens": 806,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/2n2n2/3P4/Q1p1P1b1/2N2N2/PP3PPP/R1B1KB1R w KQkq - 3 8"
    },
    {
      "turn": 17,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bxc4",
          "aggressive_uci": "f1c4",
          "aggressive_response": "9. Bxc4 Bxf3 10. gxf3 e6 11. Bg5 Be7 12. Rd1 Qc8 13. Ba6 Qb8 14. Bb7 O-O 15.\nBxa8 Qxa8 16. e5 Nd5 17. Nxd5 Bxg5 18. Nxc7 Qc8 19. Rd7 Rd8 20. Qxa7 Rxd7 21.\ncxd7 Qxd7 22. Qb8+ Bd8 23. O-O Qxc7 24. Qxc7 Bxc7 25. Rc1 Bd8 26. Rc8 Kf8 27.\nRxd8+ Ke7 28. Rb8 f6 29. Rb7+ Kf8 30. exf6 gxf6 31. Rxb6 Kf7 32. a4 e5 33. a5\nKg6 34. a6 Kf5 35. a7 Kf4 36. a8=Q f5 37. Rb4+ e4 38. Qb8+ Kxf3 39. Qg3+ Ke2\n40. Qe3+ Kd1 41. Rc4 f4 42. Rc1# Qxc1 43. Qxc1+ Kxc1 44. b4 Kd2 45. b5 e3\n46. fxe3 fxe3 47. b6 e2 48. b7 e1=Q+ 49. Kg2 Qe4+ 50. Kg3 Qxb7 51. h4 Qe4\n52. h5 Ke3 53. h6 Qf4+ 54. Kh3 Kf3 55. b4 Qg3# Qg2+ 57. Kh4 Qg4# Qg4#",
          "aggressive_tokens": {
            "prompt_tokens": 175,
            "completion_tokens": 416,
            "total_tokens": 591,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bxf3",
            "gxf3",
            "e6",
            "Bg5",
            "Be7",
            "Rd1",
            "Qc8",
            "Ba6",
            "Qb8",
            "Bb7",
            "O-O",
            "Bxa8",
            "Qxa8",
            "e5",
            "Nd5",
            "Nxd5",
            "Bxg5",
            "Nxc7",
            "Qc8",
            "Rd7",
            "Rd8",
            "Qxa7",
            "Rxd7",
            "cxd7",
            "Qxd7",
            "Qb8+",
            "Bd8",
            "O-O",
            "Qxc7",
            "Qxc7",
            "Bxc7",
            "Rc1",
            "Bd8",
            "Rc8",
            "Kf8",
            "Rxd8+",
            "Ke7",
            "Rb8",
            "f6",
            "Rb7+",
            "Kf8",
            "exf6",
            "gxf6",
            "Rxb6",
            "Kf7",
            "a4",
            "e5",
            "a5",
            "Kg6",
            "a6",
            "Kf5",
            "a7",
            "Kf4",
            "a8=Q",
            "f5",
            "Rb4+",
            "e4",
            "Qb8+",
            "Kxf3",
            "Qg3+",
            "Ke2",
            "Qe3+",
            "Kd1",
            "Rc4",
            "f4",
            "Rc1#",
            "Qxc1",
            "Qxc1+",
            "Kxc1",
            "b4",
            "Kd2",
            "b5",
            "e3",
            "fxe3",
            "fxe3",
            "b6",
            "e2",
            "b7",
            "e1=Q+",
            "Kg2",
            "Qe4+",
            "Kg3",
            "Qxb7",
            "h4",
            "Qe4",
            "h5",
            "Ke3",
            "h6",
            "Qf4+",
            "Kh3",
            "Kf3",
            "b4",
            "Qg3#",
            "Qg2+",
            "Kh4",
            "Qg4#",
            "Qg4#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bxc4",
          "positional_uci": "f1c4",
          "positional_response": "9. Bxc4 e6 10. Ne5 Bd6 11. Nxg4 Nxg4 12. h3 Ne5 13. O-O O-O 14. Be3 Qh4 15.\nBe2 f5 16. exf5 Qxa4 17. Nxa4 exf5 18. Rac1 f4 19. Bd4 f3 20. Bxf3 Nxf3+ 21.\ngxf3 Rxf3 22. Kg2 Raf8 23. Rc3 R3f4 24. Rd3 R8f5 25. Nc3 Rg5+ 26. Kh1 Rh4 27.\nNe2 Rgh5 28. Kg2 Rg5+ 29. Ng3 Rd5 30. Rfd1 c5 31. Bc3 Rxd3 32. Rxd3 Bxg3 33.\nfxg3 Ra4 34. a3 b5 35. Rd7 b4 36. axb4 cxb4 37. Rxg7+ Kf8 38. Rg4 a5 39. Kf3\nRa1 40. Bd4 Rf1+ 41. Ke4 Re1+ 42. Kd5 Rd1 43. Kc5 Rc1+ 44. Kb5 Ra1 45. Bb6\nRb1 46. Bd4 Ra1 47. Rf4+ Ke7 48. Rf5 Ke6 49. Rh5 Rd1 50. Bb6 Rd2 51. b3 Rd3\n52. Kc4 Rxg3 53. Bxa5 Kf6 54. Rxh7 Kg6 55. Rh4 Kg5 56. Rg4+ Rxg4+ 57. hxg4\nKxg4 58. Bxb4 Kf5 59. Kd5 Kf6 60. Kd6 Kf7 61. Bc5 Ke8 62. Kc7 Kf7 63. b4 Ke6\n64. b5 Kd5 65. b6 Kxc5 66. b7 Kd5 67. b8=Q Ke6 68. Qb5 Kf6 69. Qd5 Ke7 70.\nQd6+ Kf7 71. Kd7 Kg7 72. Qe6 Kf8 73. Qe7+ Kg8 74. Ke6 Kh8 75. Kf6 Kg8 76.\nQg7# e4 e5 e6",
          "positional_tokens": {
            "prompt_tokens": 175,
            "completion_tokens": 572,
            "total_tokens": 747,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e6",
            "Ne5",
            "Bd6",
            "Nxg4",
            "Nxg4",
            "h3",
            "Ne5",
            "O-O",
            "O-O",
            "Be3",
            "Qh4",
            "Be2",
            "f5",
            "exf5",
            "Qxa4",
            "Nxa4",
            "exf5",
            "Rac1",
            "f4",
            "Bd4",
            "f3",
            "Bxf3",
            "Nxf3+",
            "gxf3",
            "Rxf3",
            "Kg2",
            "Raf8",
            "Rc3",
            "R3f4",
            "Rd3",
            "R8f5",
            "Nc3",
            "Rg5+",
            "Kh1",
            "Rh4",
            "Ne2",
            "Rgh5",
            "Kg2",
            "Rg5+",
            "Ng3",
            "Rd5",
            "Rfd1",
            "c5",
            "Bc3",
            "Rxd3",
            "Rxd3",
            "Bxg3",
            "fxg3",
            "Ra4",
            "a3",
            "b5",
            "Rd7",
            "b4",
            "axb4",
            "cxb4",
            "Rxg7+",
            "Kf8",
            "Rg4",
            "a5",
            "Kf3",
            "Ra1",
            "Bd4",
            "Rf1+",
            "Ke4",
            "Re1+",
            "Kd5",
            "Rd1",
            "Kc5",
            "Rc1+",
            "Kb5",
            "Ra1",
            "Bb6",
            "Rb1",
            "Bd4",
            "Ra1",
            "Rf4+",
            "Ke7",
            "Rf5",
            "Ke6",
            "Rh5",
            "Rd1",
            "Bb6",
            "Rd2",
            "b3",
            "Rd3",
            "Kc4",
            "Rxg3",
            "Bxa5",
            "Kf6",
            "Rxh7",
            "Kg6",
            "Rh4",
            "Kg5",
            "Rg4+",
            "Rxg4+",
            "hxg4",
            "Kxg4",
            "Bxb4",
            "Kf5",
            "Kd5",
            "Kf6",
            "Kd6",
            "Kf7",
            "Bc5",
            "Ke8",
            "Kc7",
            "Kf7",
            "b4",
            "Ke6",
            "b5",
            "Kd5",
            "b6",
            "Kxc5",
            "b7",
            "Kd5",
            "b8=Q",
            "Ke6",
            "Qb5",
            "Kf6",
            "Qd5",
            "Ke7",
            "Qd6+",
            "Kf7",
            "Kd7",
            "Kg7",
            "Qe6",
            "Kf8",
            "Qe7+",
            "Kg8",
            "Ke6",
            "Kh8",
            "Kf6",
            "Kg8",
            "Qg7#",
            "e4",
            "e5",
            "e6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bxc4",
          "neutral_uci": "f1c4",
          "neutral_response": "9. Bxc4 e6 10. Ne5 Bd6 11. Nxg4 Nxg4 12. h3 Ne5 13. O-O O-O 14. Be3 Qh4 15.\nBe2 f5 16. exf5 Qxa4 17. Nxa4 exf5 18. Nc3 f4 19. Bd4 Nxc6 20. Bc4+ Kh8 21.\nRad1 Rad8 22. Nb5 Nxd4 23. Nxd4 Be5 24. Ne6 Rxd1 25. Rxd1 Re8 26. Rd8 Rxd8\n27. Nxd8 Bxb2 28. Nc6 a5 29. Kf1 g5 30. Ke2 Kg7 31. Kd3 Kf6 32. Ke4 h5 33.\nNd4 Bxd4 34. Kxd4 Kf5 35. Bd3+ Ke6 36. Ke4 c6 37. Bc4+ Kf6 38. a4 h4 39.\nBb3 b5 40. axb5 cxb5 41. Kd5 a4 42. Ba2 b4 43. Kc4 b3 44. Bb1 Ke5 45. Kb4\nKd4 46. Kxa4 Kc3 47. Ka3 b2 48. Ka2 Kd2 49. Kxb2 Ke2 50. f3 Kf2 51. Bf5\nKxg2 52. Bg4 Kf2 53. Kc3 Ke3 54. Kc4 Kf2 55. Kd5 Kg3 56. Ke5 Kf2 57. Kf5\nKe3 58. Kxg5 Kd4 59. Kxf4 Kd5 60. Kg5 Ke5 61. Kxh4 Kf6 62. Kh5 Kg7 63. Kg5\nKh7 64. Kf6 Kh6 65. f4 Kh7 66. f5 Kh6 67. Ke7 Kg7 68. f6+ Kg8 69. f7+ Kh7\n70. f8=Q Kg6 71. Qf6+ Kh7 72. Bf5+ Kg8 73. Qf7+ Kh8 74. Qh7# e4 f3 gxf3",
          "neutral_tokens": {
            "prompt_tokens": 174,
            "completion_tokens": 545,
            "total_tokens": 719,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e6",
            "Ne5",
            "Bd6",
            "Nxg4",
            "Nxg4",
            "h3",
            "Ne5",
            "O-O",
            "O-O",
            "Be3",
            "Qh4",
            "Be2",
            "f5",
            "exf5",
            "Qxa4",
            "Nxa4",
            "exf5",
            "Nc3",
            "f4",
            "Bd4",
            "Nxc6",
            "Bc4+",
            "Kh8",
            "Rad1",
            "Rad8",
            "Nb5",
            "Nxd4",
            "Nxd4",
            "Be5",
            "Ne6",
            "Rxd1",
            "Rxd1",
            "Re8",
            "Rd8",
            "Rxd8",
            "Nxd8",
            "Bxb2",
            "Nc6",
            "a5",
            "Kf1",
            "g5",
            "Ke2",
            "Kg7",
            "Kd3",
            "Kf6",
            "Ke4",
            "h5",
            "Nd4",
            "Bxd4",
            "Kxd4",
            "Kf5",
            "Bd3+",
            "Ke6",
            "Ke4",
            "c6",
            "Bc4+",
            "Kf6",
            "a4",
            "h4",
            "Bb3",
            "b5",
            "axb5",
            "cxb5",
            "Kd5",
            "a4",
            "Ba2",
            "b4",
            "Kc4",
            "b3",
            "Bb1",
            "Ke5",
            "Kb4",
            "Kd4",
            "Kxa4",
            "Kc3",
            "Ka3",
            "b2",
            "Ka2",
            "Kd2",
            "Kxb2",
            "Ke2",
            "f3",
            "Kf2",
            "Bf5",
            "Kxg2",
            "Bg4",
            "Kf2",
            "Kc3",
            "Ke3",
            "Kc4",
            "Kf2",
            "Kd5",
            "Kg3",
            "Ke5",
            "Kf2",
            "Kf5",
            "Ke3",
            "Kxg5",
            "Kd4",
            "Kxf4",
            "Kd5",
            "Kg5",
            "Ke5",
            "Kxh4",
            "Kf6",
            "Kh5",
            "Kg7",
            "Kg5",
            "Kh7",
            "Kf6",
            "Kh6",
            "f4",
            "Kh7",
            "f5",
            "Kh6",
            "Ke7",
            "Kg7",
            "f6+",
            "Kg8",
            "f7+",
            "Kh7",
            "f8=Q",
            "Kg6",
            "Qf6+",
            "Kh7",
            "Bf5+",
            "Kg8",
            "Qf7+",
            "Kh8",
            "Qh7#",
            "e4",
            "f3",
            "gxf3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f1c4",
          "positional_uci": "f1c4",
          "neutral_uci": "f1c4",
          "consensus_move": "f1c4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bxf3",
              "gxf3",
              "e6",
              "Bg5",
              "Be7",
              "Rd1",
              "Qc8",
              "Ba6",
              "Qb8",
              "Bb7",
              "O-O",
              "Bxa8",
              "Qxa8",
              "e5",
              "Nd5",
              "Nxd5",
              "Bxg5",
              "Nxc7",
              "Qc8",
              "Rd7",
              "Rd8",
              "Qxa7",
              "Rxd7",
              "cxd7",
              "Qxd7",
              "Qb8+",
              "Bd8",
              "O-O",
              "Qxc7",
              "Qxc7",
              "Bxc7",
              "Rc1",
              "Bd8",
              "Rc8",
              "Kf8",
              "Rxd8+",
              "Ke7",
              "Rb8",
              "f6",
              "Rb7+",
              "Kf8",
              "exf6",
              "gxf6",
              "Rxb6",
              "Kf7",
              "a4",
              "e5",
              "a5",
              "Kg6",
              "a6",
              "Kf5",
              "a7",
              "Kf4",
              "a8=Q",
              "f5",
              "Rb4+",
              "e4",
              "Qb8+",
              "Kxf3",
              "Qg3+",
              "Ke2",
              "Qe3+",
              "Kd1",
              "Rc4",
              "f4",
              "Rc1#",
              "Qxc1",
              "Qxc1+",
              "Kxc1",
              "b4",
              "Kd2",
              "b5",
              "e3",
              "fxe3",
              "fxe3",
              "b6",
              "e2",
              "b7",
              "e1=Q+",
              "Kg2",
              "Qe4+",
              "Kg3",
              "Qxb7",
              "h4",
              "Qe4",
              "h5",
              "Ke3",
              "h6",
              "Qf4+",
              "Kh3",
              "Kf3",
              "b4",
              "Qg3#",
              "Qg2+",
              "Kh4",
              "Qg4#",
              "Qg4#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e6",
              "Ne5",
              "Bd6",
              "Nxg4",
              "Nxg4",
              "h3",
              "Ne5",
              "O-O",
              "O-O",
              "Be3",
              "Qh4",
              "Be2",
              "f5",
              "exf5",
              "Qxa4",
              "Nxa4",
              "exf5",
              "Rac1",
              "f4",
              "Bd4",
              "f3",
              "Bxf3",
              "Nxf3+",
              "gxf3",
              "Rxf3",
              "Kg2",
              "Raf8",
              "Rc3",
              "R3f4",
              "Rd3",
              "R8f5",
              "Nc3",
              "Rg5+",
              "Kh1",
              "Rh4",
              "Ne2",
              "Rgh5",
              "Kg2",
              "Rg5+",
              "Ng3",
              "Rd5",
              "Rfd1",
              "c5",
              "Bc3",
              "Rxd3",
              "Rxd3",
              "Bxg3",
              "fxg3",
              "Ra4",
              "a3",
              "b5",
              "Rd7",
              "b4",
              "axb4",
              "cxb4",
              "Rxg7+",
              "Kf8",
              "Rg4",
              "a5",
              "Kf3",
              "Ra1",
              "Bd4",
              "Rf1+",
              "Ke4",
              "Re1+",
              "Kd5",
              "Rd1",
              "Kc5",
              "Rc1+",
              "Kb5",
              "Ra1",
              "Bb6",
              "Rb1",
              "Bd4",
              "Ra1",
              "Rf4+",
              "Ke7",
              "Rf5",
              "Ke6",
              "Rh5",
              "Rd1",
              "Bb6",
              "Rd2",
              "b3",
              "Rd3",
              "Kc4",
              "Rxg3",
              "Bxa5",
              "Kf6",
              "Rxh7",
              "Kg6",
              "Rh4",
              "Kg5",
              "Rg4+",
              "Rxg4+",
              "hxg4",
              "Kxg4",
              "Bxb4",
              "Kf5",
              "Kd5",
              "Kf6",
              "Kd6",
              "Kf7",
              "Bc5",
              "Ke8",
              "Kc7",
              "Kf7",
              "b4",
              "Ke6",
              "b5",
              "Kd5",
              "b6",
              "Kxc5",
              "b7",
              "Kd5",
              "b8=Q",
              "Ke6",
              "Qb5",
              "Kf6",
              "Qd5",
              "Ke7",
              "Qd6+",
              "Kf7",
              "Kd7",
              "Kg7",
              "Qe6",
              "Kf8",
              "Qe7+",
              "Kg8",
              "Ke6",
              "Kh8",
              "Kf6",
              "Kg8",
              "Qg7#",
              "e4",
              "e5",
              "e6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e6",
              "Ne5",
              "Bd6",
              "Nxg4",
              "Nxg4",
              "h3",
              "Ne5",
              "O-O",
              "O-O",
              "Be3",
              "Qh4",
              "Be2",
              "f5",
              "exf5",
              "Qxa4",
              "Nxa4",
              "exf5",
              "Nc3",
              "f4",
              "Bd4",
              "Nxc6",
              "Bc4+",
              "Kh8",
              "Rad1",
              "Rad8",
              "Nb5",
              "Nxd4",
              "Nxd4",
              "Be5",
              "Ne6",
              "Rxd1",
              "Rxd1",
              "Re8",
              "Rd8",
              "Rxd8",
              "Nxd8",
              "Bxb2",
              "Nc6",
              "a5",
              "Kf1",
              "g5",
              "Ke2",
              "Kg7",
              "Kd3",
              "Kf6",
              "Ke4",
              "h5",
              "Nd4",
              "Bxd4",
              "Kxd4",
              "Kf5",
              "Bd3+",
              "Ke6",
              "Ke4",
              "c6",
              "Bc4+",
              "Kf6",
              "a4",
              "h4",
              "Bb3",
              "b5",
              "axb5",
              "cxb5",
              "Kd5",
              "a4",
              "Ba2",
              "b4",
              "Kc4",
              "b3",
              "Bb1",
              "Ke5",
              "Kb4",
              "Kd4",
              "Kxa4",
              "Kc3",
              "Ka3",
              "b2",
              "Ka2",
              "Kd2",
              "Kxb2",
              "Ke2",
              "f3",
              "Kf2",
              "Bf5",
              "Kxg2",
              "Bg4",
              "Kf2",
              "Kc3",
              "Ke3",
              "Kc4",
              "Kf2",
              "Kd5",
              "Kg3",
              "Ke5",
              "Kf2",
              "Kf5",
              "Ke3",
              "Kxg5",
              "Kd4",
              "Kxf4",
              "Kd5",
              "Kg5",
              "Ke5",
              "Kxh4",
              "Kf6",
              "Kh5",
              "Kg7",
              "Kg5",
              "Kh7",
              "Kf6",
              "Kh6",
              "f4",
              "Kh7",
              "f5",
              "Kh6",
              "Ke7",
              "Kg7",
              "f6+",
              "Kg8",
              "f7+",
              "Kh7",
              "f8=Q",
              "Kg6",
              "Qf6+",
              "Kh7",
              "Bf5+",
              "Kg8",
              "Qf7+",
              "Kh8",
              "Qh7#",
              "e4",
              "f3",
              "gxf3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 175,
            "completion_tokens": 416,
            "total_tokens": 591,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 175,
            "completion_tokens": 572,
            "total_tokens": 747,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 174,
            "completion_tokens": 545,
            "total_tokens": 719,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 524,
          "total_completion_tokens": 1533,
          "total_tokens": 2057
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 175,
            "completion_tokens": 416,
            "total_tokens": 591,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 175,
            "completion_tokens": 572,
            "total_tokens": 747,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 174,
            "completion_tokens": 545,
            "total_tokens": 719,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/p1p1pppp/1pP2n2/8/Q1p1P1b1/2N2N2/PP3PPP/R1B1KB1R w KQkq - 0 9"
    },
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e5",
          "aggressive_uci": "e7e5",
          "aggressive_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6\n4. Ba4 Nf6 5. O-O Be7 6. Re1 b5\n7. Bb3 d6 8. c3 O-O 9. h3 Bb7\n10. d4 Re8 11. Nbd2 Bf8 12. a4 h6\n13. Bc2 exd4 14. cxd4 Nb4 15. Bb1 c5\n16. d5 Nd7 17. Ra3 c4 18. Nd4 Qf6\n19. N2f3 Nc5 20. axb5 Nbd3 21. Bxd3 Nxd3\n22. Re2 axb5 23. Nxb5 Rxa3 24. Nxa3 Ba6\n25. Qa4 Ra8 26. Nxc4 Nxc1 27. Rc2 Nd3\n28. Nb6 Ra7 29. Nd7 Qd8 30. Nxf8 Kxf8\n31. Nd4 Nc5 32. Qa3 Qb6 33. Nc6 Ra8\n34. e5 Bb7 35. Qf3 dxe5 36. Nxe5 Qc7\n37. Rxc5 Ra1+ 38. Kh2 Qxe5+ 39. g3 Qe1\n40. Qg2 Rd1 41. Rb5 Bxd5 42. Rxd5 Rxd5\n43. Qxd5 Qxf2+ 44. Qg2 Qxg2+ 45. Kxg2 Ke7\n46. Kf3 Kd6 47. Ke4 Kc5 48. Ke5 Kc4\n49. Kd6 Kb3 50. Ke7 Kxb2 51. Kxf7 Kc3\n52. Kxg7 Kd4 53. Kxh6 Ke5 54. g4 Kf6\n55. g5+ Kf7 56. Kh7 Kf8 57. g6 Ke7\n58. g7 Kf6 59. g8=Q Kf5 60. Qg4+ Kf6\n61. Qf4+ Ke6 62. Kg6 Kd5 63. Kf6 Kc5\n64. Qe4 Kd6 65. Qd4+ Kc6 66. Ke6 Kb5\n67. Kd6 Ka5 68. Qb2 Ka4 69. Kc5 Ka5\n70. Qa3# d4 e5 Qf3",
          "aggressive_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 590,
            "total_tokens": 704,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qf3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e5",
          "positional_uci": "e7e5",
          "positional_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7\n6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5\n11. d4 Qc7 12. Nbd2 cxd4 13. cxd4 Nc6 14. Nb3 a5\n15. Be3 a4 16. Nbd2 Bd7 17. Rc1 Qb7 18. Bb1 Rfc8\n19. Nf1 Bd8 20. Ng3 Bb6 21. Qd2 Ba5 22. Qe2 Bxe1\n23. Rxe1 Re8 24. Qd2 Rac8 25. Bg5 Kh8 26. Bxf6 gxf6\n27. Qh6 Rg8 28. Qxf6+ Rg7 29. Nh5 Rg8 30. Ng5 Be8\n31. Nxg7 Rxg7 32. dxe5 dxe5 33. Rd1 Qe7 34. Qxe7 Nxe7\n35. Rd8 Rg8 36. Nf3 f6 37. Bd3 Kg7 38. Rb8 Kf7\n39. Bxb5 Bxb5 40. Rxb5 Rc8 41. Rb4 Rc1+ 42. Kh2 Rc2\n43. Kg3 Ng6 44. Ne1 Re2 45. Nd3 Rd2 46. Nc5 Nf4\n47. Nxa4 Nd3 48. Rb7+ Kg6 49. f3 Nf4 50. Kh2 Rxg2+\n51. Kh1 Rf2 52. Nc3 Rxf3 53. a4 Rxh3+ 54. Kg1 Rd3\n55. a5 Rd6 56. Rb6 Rd7 57. a6 Ra7 58. Nb5 Ra8\n59. a7 Ne2+ 60. Kf2 Nd4 61. Rb8 Rxa7 62. Nxa7 h5\n63. Nb5 Ne6 64. Nd6 Kg5 65. Rg8+ Kf4 66. b4 h4\n67. b5 Nc5 68. b6 h3 69. b7 Nxb7 70. Nxb7 Kxe4\n71. Rh8 f5 72. Rxh3 f4 73. Nd6+ Kd5 74. Nf5 Ke4\n75. Nh4 Kd5 76. Ng2 Ke4 77. Rh4 Kf5 78. Nxf4 exf4\n79. Kf3 Kg5 80. Rxf4 Kg6",
          "positional_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Qc7",
            "Nbd2",
            "cxd4",
            "cxd4",
            "Nc6",
            "Nb3",
            "a5",
            "Be3",
            "a4",
            "Nbd2",
            "Bd7",
            "Rc1",
            "Qb7",
            "Bb1",
            "Rfc8",
            "Nf1",
            "Bd8",
            "Ng3",
            "Bb6",
            "Qd2",
            "Ba5",
            "Qe2",
            "Bxe1",
            "Rxe1",
            "Re8",
            "Qd2",
            "Rac8",
            "Bg5",
            "Kh8",
            "Bxf6",
            "gxf6",
            "Qh6",
            "Rg8",
            "Qxf6+",
            "Rg7",
            "Nh5",
            "Rg8",
            "Ng5",
            "Be8",
            "Nxg7",
            "Rxg7",
            "dxe5",
            "dxe5",
            "Rd1",
            "Qe7",
            "Qxe7",
            "Nxe7",
            "Rd8",
            "Rg8",
            "Nf3",
            "f6",
            "Bd3",
            "Kg7",
            "Rb8",
            "Kf7",
            "Bxb5",
            "Bxb5",
            "Rxb5",
            "Rc8",
            "Rb4",
            "Rc1+",
            "Kh2",
            "Rc2",
            "Kg3",
            "Ng6",
            "Ne1",
            "Re2",
            "Nd3",
            "Rd2",
            "Nc5",
            "Nf4",
            "Nxa4",
            "Nd3",
            "Rb7+",
            "Kg6",
            "f3",
            "Nf4",
            "Kh2",
            "Rxg2+",
            "Kh1",
            "Rf2",
            "Nc3",
            "Rxf3",
            "a4",
            "Rxh3+",
            "Kg1",
            "Rd3",
            "a5",
            "Rd6",
            "Rb6",
            "Rd7",
            "a6",
            "Ra7",
            "Nb5",
            "Ra8",
            "a7",
            "Ne2+",
            "Kf2",
            "Nd4",
            "Rb8",
            "Rxa7",
            "Nxa7",
            "h5",
            "Nb5",
            "Ne6",
            "Nd6",
            "Kg5",
            "Rg8+",
            "Kf4",
            "b4",
            "h4",
            "b5",
            "Nc5",
            "b6",
            "h3",
            "b7",
            "Nxb7",
            "Nxb7",
            "Kxe4",
            "Rh8",
            "f5",
            "Rxh3",
            "f4",
            "Nd6+",
            "Kd5",
            "Nf5",
            "Ke4",
            "Nh4",
            "Kd5",
            "Ng2",
            "Ke4",
            "Rh4",
            "Kf5",
            "Nxf4",
            "exf4",
            "Kf3",
            "Kg5",
            "Rxf4",
            "Kg6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e5",
          "neutral_uci": "e7e5",
          "neutral_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6\n4. Ba4 Nf6 5. O-O Be7 6. Re1 b5\n7. Bb3 d6 8. c3 O-O 9. h3 Bb7\n10. d4 Re8 11. Nbd2 Bf8 12. a4 h6\n13. Bc2 exd4 14. cxd4 Nb4 15. Bb1 c5\n16. d5 Nd7 17. Ra3 c4 18. Nd4 Qf6\n19. N2f3 Nc5 20. axb5 Nbd3 21. Bxd3 Nxd3\n22. Re2 axb5 23. Nxb5 Rxa3 24. Nxa3 Ba6\n25. Qa4 Ra8 26. Be3 Nxb2\n27. Qc6 Rc8 28. Qxa6 c3 29. Qxc8 Nd3",
          "neutral_tokens": {
            "prompt_tokens": 113,
            "completion_tokens": 230,
            "total_tokens": 343,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Bb7",
            "d4",
            "Re8",
            "Nbd2",
            "Bf8",
            "a4",
            "h6",
            "Bc2",
            "exd4",
            "cxd4",
            "Nb4",
            "Bb1",
            "c5",
            "d5",
            "Nd7",
            "Ra3",
            "c4",
            "Nd4",
            "Qf6",
            "N2f3",
            "Nc5",
            "axb5",
            "Nbd3",
            "Bxd3",
            "Nxd3",
            "Re2",
            "axb5",
            "Nxb5",
            "Rxa3",
            "Nxa3",
            "Ba6",
            "Qa4",
            "Ra8",
            "Be3",
            "Nxb2",
            "Qc6",
            "Rc8",
            "Qxa6",
            "c3",
            "Qxc8",
            "Nd3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e7e5",
          "positional_uci": "e7e5",
          "neutral_uci": "e7e5",
          "consensus_move": "e7e5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qf3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Qc7",
              "Nbd2",
              "cxd4",
              "cxd4",
              "Nc6",
              "Nb3",
              "a5",
              "Be3",
              "a4",
              "Nbd2",
              "Bd7",
              "Rc1",
              "Qb7",
              "Bb1",
              "Rfc8",
              "Nf1",
              "Bd8",
              "Ng3",
              "Bb6",
              "Qd2",
              "Ba5",
              "Qe2",
              "Bxe1",
              "Rxe1",
              "Re8",
              "Qd2",
              "Rac8",
              "Bg5",
              "Kh8",
              "Bxf6",
              "gxf6",
              "Qh6",
              "Rg8",
              "Qxf6+",
              "Rg7",
              "Nh5",
              "Rg8",
              "Ng5",
              "Be8",
              "Nxg7",
              "Rxg7",
              "dxe5",
              "dxe5",
              "Rd1",
              "Qe7",
              "Qxe7",
              "Nxe7",
              "Rd8",
              "Rg8",
              "Nf3",
              "f6",
              "Bd3",
              "Kg7",
              "Rb8",
              "Kf7",
              "Bxb5",
              "Bxb5",
              "Rxb5",
              "Rc8",
              "Rb4",
              "Rc1+",
              "Kh2",
              "Rc2",
              "Kg3",
              "Ng6",
              "Ne1",
              "Re2",
              "Nd3",
              "Rd2",
              "Nc5",
              "Nf4",
              "Nxa4",
              "Nd3",
              "Rb7+",
              "Kg6",
              "f3",
              "Nf4",
              "Kh2",
              "Rxg2+",
              "Kh1",
              "Rf2",
              "Nc3",
              "Rxf3",
              "a4",
              "Rxh3+",
              "Kg1",
              "Rd3",
              "a5",
              "Rd6",
              "Rb6",
              "Rd7",
              "a6",
              "Ra7",
              "Nb5",
              "Ra8",
              "a7",
              "Ne2+",
              "Kf2",
              "Nd4",
              "Rb8",
              "Rxa7",
              "Nxa7",
              "h5",
              "Nb5",
              "Ne6",
              "Nd6",
              "Kg5",
              "Rg8+",
              "Kf4",
              "b4",
              "h4",
              "b5",
              "Nc5",
              "b6",
              "h3",
              "b7",
              "Nxb7",
              "Nxb7",
              "Kxe4",
              "Rh8",
              "f5",
              "Rxh3",
              "f4",
              "Nd6+",
              "Kd5",
              "Nf5",
              "Ke4",
              "Nh4",
              "Kd5",
              "Ng2",
              "Ke4",
              "Rh4",
              "Kf5",
              "Nxf4",
              "exf4",
              "Kf3",
              "Kg5",
              "Rxf4",
              "Kg6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Bb7",
              "d4",
              "Re8",
              "Nbd2",
              "Bf8",
              "a4",
              "h6",
              "Bc2",
              "exd4",
              "cxd4",
              "Nb4",
              "Bb1",
              "c5",
              "d5",
              "Nd7",
              "Ra3",
              "c4",
              "Nd4",
              "Qf6",
              "N2f3",
              "Nc5",
              "axb5",
              "Nbd3",
              "Bxd3",
              "Nxd3",
              "Re2",
              "axb5",
              "Nxb5",
              "Rxa3",
              "Nxa3",
              "Ba6",
              "Qa4",
              "Ra8",
              "Be3",
              "Nxb2",
              "Qc6",
              "Rc8",
              "Qxa6",
              "c3",
              "Qxc8",
              "Nd3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 114,
            "completion_tokens": 590,
            "total_tokens": 704,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 113,
            "completion_tokens": 230,
            "total_tokens": 343,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 341,
          "total_completion_tokens": 1460,
          "total_tokens": 1801
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 114,
            "completion_tokens": 590,
            "total_tokens": 704,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 113,
            "completion_tokens": 230,
            "total_tokens": 343,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1"
    }
  ],
  "white_errors": [],
  "black_errors": [],
  "white_plan_log": [],
  "black_plan_log": [],
  "timestamp": "2025-11-30T00:46:24.695189",
  "game_id": "meta-llama_llama-3.3-70b-instruct_vs_gpt-3.5-turbo-instruct_SC_4",
  "configuration": "SC"
}