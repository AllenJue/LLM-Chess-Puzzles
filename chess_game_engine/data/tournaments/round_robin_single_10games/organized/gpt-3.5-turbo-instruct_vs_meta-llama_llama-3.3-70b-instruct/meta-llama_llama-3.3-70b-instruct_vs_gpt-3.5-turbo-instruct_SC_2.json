{
  "result": "gpt-3.5-turbo-instruct wins by disqualification (meta-llama/llama-3.3-70b-instruct failed to produce a legal move)",
  "white_player": "meta-llama/llama-3.3-70b-instruct",
  "black_player": "gpt-3.5-turbo-instruct",
  "white_score": 0.0,
  "black_score": 1.0,
  "total_moves": 2,
  "moves": [
    "e2e4",
    "e7e5"
  ],
  "game_history": [
    {
      "move_number": 1,
      "player": "meta-llama/llama-3.3-70b-instruct",
      "color": "white",
      "move_uci": "e2e4",
      "move_san": "e4",
      "fen_after": "rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "move_number": 2,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "e7e5",
      "move_san": "e5",
      "fen_after": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    }
  ],
  "final_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2",
  "white_tokens": {
    "prompt_tokens": 4815,
    "completion_tokens": 1037,
    "total_tokens": 5852
  },
  "black_tokens": {
    "prompt_tokens": 4154,
    "completion_tokens": 16002,
    "total_tokens": 20156
  },
  "white_token_log": [
    {
      "turn": 2,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 14,
          "total_tokens": 162,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 446,
        "total_completion_tokens": 28,
        "total_tokens": 474
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 7,
          "total_tokens": 156,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 14,
          "total_tokens": 162,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 4,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 21,
          "total_tokens": 177,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 18,
          "total_tokens": 174,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 18,
          "total_tokens": 173,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 467,
        "total_completion_tokens": 57,
        "total_tokens": 524
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 21,
          "total_tokens": 177,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 18,
          "total_tokens": 174,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 18,
          "total_tokens": 173,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 6,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 165,
          "completion_tokens": 28,
          "total_tokens": 193,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 165,
          "completion_tokens": 30,
          "total_tokens": 195,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 164,
          "completion_tokens": 27,
          "total_tokens": 191,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 494,
        "total_completion_tokens": 85,
        "total_tokens": 579
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 165,
          "completion_tokens": 28,
          "total_tokens": 193,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 165,
          "completion_tokens": 30,
          "total_tokens": 195,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 164,
          "completion_tokens": 27,
          "total_tokens": 191,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 8,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 174,
          "completion_tokens": 36,
          "total_tokens": 210,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 174,
          "completion_tokens": 36,
          "total_tokens": 210,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 173,
          "completion_tokens": 36,
          "total_tokens": 209,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 521,
        "total_completion_tokens": 108,
        "total_tokens": 629
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 174,
          "completion_tokens": 36,
          "total_tokens": 210,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 174,
          "completion_tokens": 36,
          "total_tokens": 210,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 173,
          "completion_tokens": 36,
          "total_tokens": 209,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 10,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 182,
          "completion_tokens": 47,
          "total_tokens": 229,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 182,
          "completion_tokens": 48,
          "total_tokens": 230,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 181,
          "completion_tokens": 48,
          "total_tokens": 229,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 545,
        "total_completion_tokens": 143,
        "total_tokens": 688
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 182,
          "completion_tokens": 47,
          "total_tokens": 229,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 182,
          "completion_tokens": 48,
          "total_tokens": 230,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 181,
          "completion_tokens": 48,
          "total_tokens": 229,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 12,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 190,
          "completion_tokens": 53,
          "total_tokens": 243,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 190,
          "completion_tokens": 53,
          "total_tokens": 243,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 189,
          "completion_tokens": 53,
          "total_tokens": 242,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 569,
        "total_completion_tokens": 159,
        "total_tokens": 728
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 190,
          "completion_tokens": 53,
          "total_tokens": 243,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 190,
          "completion_tokens": 53,
          "total_tokens": 243,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 189,
          "completion_tokens": 53,
          "total_tokens": 242,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 14,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 199,
          "completion_tokens": 65,
          "total_tokens": 264,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 199,
          "completion_tokens": 63,
          "total_tokens": 262,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 198,
          "completion_tokens": 65,
          "total_tokens": 263,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 596,
        "total_completion_tokens": 193,
        "total_tokens": 789
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 199,
          "completion_tokens": 65,
          "total_tokens": 264,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 199,
          "completion_tokens": 63,
          "total_tokens": 262,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 198,
          "completion_tokens": 65,
          "total_tokens": 263,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 16,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 208,
          "completion_tokens": 72,
          "total_tokens": 280,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 208,
          "completion_tokens": 72,
          "total_tokens": 280,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 207,
          "completion_tokens": 72,
          "total_tokens": 279,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 623,
        "total_completion_tokens": 216,
        "total_tokens": 839
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 208,
          "completion_tokens": 72,
          "total_tokens": 280,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 208,
          "completion_tokens": 72,
          "total_tokens": 280,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 207,
          "completion_tokens": 72,
          "total_tokens": 279,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 184,
          "completion_tokens": 16,
          "total_tokens": 200,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 554,
        "total_completion_tokens": 48,
        "total_tokens": 602
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 185,
          "completion_tokens": 16,
          "total_tokens": 201,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 184,
          "completion_tokens": 16,
          "total_tokens": 200,
          "model": "meta-llama/llama-3.3-70b-instruct",
          "finish_reason": "stop"
        }
      }
    }
  ],
  "black_token_log": [
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 547,
          "total_tokens": 657,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 640,
          "total_tokens": 749,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 329,
        "total_completion_tokens": 1827,
        "total_tokens": 2156
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 547,
          "total_tokens": 657,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 640,
          "total_tokens": 749,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 3,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 116,
          "completion_tokens": 640,
          "total_tokens": 756,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 116,
          "completion_tokens": 640,
          "total_tokens": 756,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 115,
          "completion_tokens": 640,
          "total_tokens": 755,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 347,
        "total_completion_tokens": 1920,
        "total_tokens": 2267
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 116,
          "completion_tokens": 640,
          "total_tokens": 756,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 116,
          "completion_tokens": 640,
          "total_tokens": 756,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 115,
          "completion_tokens": 640,
          "total_tokens": 755,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 5,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 124,
          "completion_tokens": 640,
          "total_tokens": 764,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 124,
          "completion_tokens": 640,
          "total_tokens": 764,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 123,
          "completion_tokens": 640,
          "total_tokens": 763,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 371,
        "total_completion_tokens": 1920,
        "total_tokens": 2291
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 124,
          "completion_tokens": 640,
          "total_tokens": 764,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 124,
          "completion_tokens": 640,
          "total_tokens": 764,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 123,
          "completion_tokens": 640,
          "total_tokens": 763,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 7,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 133,
          "completion_tokens": 519,
          "total_tokens": 652,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 133,
          "completion_tokens": 640,
          "total_tokens": 773,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 132,
          "completion_tokens": 254,
          "total_tokens": 386,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 398,
        "total_completion_tokens": 1413,
        "total_tokens": 1811
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 133,
          "completion_tokens": 519,
          "total_tokens": 652,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 133,
          "completion_tokens": 640,
          "total_tokens": 773,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 132,
          "completion_tokens": 254,
          "total_tokens": 386,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 9,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 142,
          "completion_tokens": 640,
          "total_tokens": 782,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 142,
          "completion_tokens": 640,
          "total_tokens": 782,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 141,
          "completion_tokens": 180,
          "total_tokens": 321,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 425,
        "total_completion_tokens": 1460,
        "total_tokens": 1885
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 142,
          "completion_tokens": 640,
          "total_tokens": 782,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 142,
          "completion_tokens": 640,
          "total_tokens": 782,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 141,
          "completion_tokens": 180,
          "total_tokens": 321,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 11,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 150,
          "completion_tokens": 291,
          "total_tokens": 441,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 150,
          "completion_tokens": 640,
          "total_tokens": 790,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 149,
          "completion_tokens": 640,
          "total_tokens": 789,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 449,
        "total_completion_tokens": 1571,
        "total_tokens": 2020
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 150,
          "completion_tokens": 291,
          "total_tokens": 441,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 150,
          "completion_tokens": 640,
          "total_tokens": 790,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 149,
          "completion_tokens": 640,
          "total_tokens": 789,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 13,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 157,
          "completion_tokens": 446,
          "total_tokens": 603,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 157,
          "completion_tokens": 640,
          "total_tokens": 797,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 156,
          "completion_tokens": 460,
          "total_tokens": 616,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 470,
        "total_completion_tokens": 1546,
        "total_tokens": 2016
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 157,
          "completion_tokens": 446,
          "total_tokens": 603,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 157,
          "completion_tokens": 640,
          "total_tokens": 797,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 156,
          "completion_tokens": 460,
          "total_tokens": 616,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 15,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 167,
          "completion_tokens": 640,
          "total_tokens": 807,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 167,
          "completion_tokens": 640,
          "total_tokens": 807,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 166,
          "completion_tokens": 409,
          "total_tokens": 575,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 500,
        "total_completion_tokens": 1689,
        "total_tokens": 2189
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 167,
          "completion_tokens": 640,
          "total_tokens": 807,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 167,
          "completion_tokens": 640,
          "total_tokens": 807,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 166,
          "completion_tokens": 409,
          "total_tokens": 575,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 17,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 175,
          "completion_tokens": 288,
          "total_tokens": 463,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 175,
          "completion_tokens": 640,
          "total_tokens": 815,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 174,
          "completion_tokens": 640,
          "total_tokens": 814,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 524,
        "total_completion_tokens": 1568,
        "total_tokens": 2092
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 175,
          "completion_tokens": 288,
          "total_tokens": 463,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 175,
          "completion_tokens": 640,
          "total_tokens": 815,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 174,
          "completion_tokens": 640,
          "total_tokens": 814,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 2,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 236,
          "total_tokens": 350,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 622,
          "total_tokens": 736,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 230,
          "total_tokens": 343,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 341,
        "total_completion_tokens": 1088,
        "total_tokens": 1429
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 236,
          "total_tokens": 350,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 622,
          "total_tokens": 736,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 230,
          "total_tokens": 343,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    }
  ],
  "white_responses": [
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 \nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4 \nWhite",
          "positional_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d5",
          "neutral_uci": "d7d5",
          "neutral_response": "1. d4 \nd5 Nf3 Nc6",
          "neutral_tokens": {
            "prompt_tokens": 148,
            "completion_tokens": 14,
            "total_tokens": 162,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": "d7d5",
          "consensus_move": "d7d5",
          "source_agents": [
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf3",
              "Nc6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 148,
            "completion_tokens": 14,
            "total_tokens": 162,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 446,
          "total_completion_tokens": 28,
          "total_tokens": 474
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 149,
            "completion_tokens": 7,
            "total_tokens": 156,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 148,
            "completion_tokens": 14,
            "total_tokens": 162,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 4,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d6",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 \nd6 Nf3 Nc6",
          "aggressive_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 21,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "dxc4",
          "positional_uci": "d5c4",
          "positional_response": "1. d4 d5 2. c4 \ndxc4 b4",
          "positional_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 18,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "dxc4",
          "neutral_uci": "d5c4",
          "neutral_response": "1. d4 d5 2. c4 \ndxc4 b3",
          "neutral_tokens": {
            "prompt_tokens": 155,
            "completion_tokens": 18,
            "total_tokens": 173,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": "d5c4",
          "neutral_uci": "d5c4",
          "consensus_move": "d5c4",
          "source_agents": [
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf3",
              "Nc6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "b4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "b3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 156,
            "completion_tokens": 21,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 156,
            "completion_tokens": 18,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 155,
            "completion_tokens": 18,
            "total_tokens": 173,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 467,
          "total_completion_tokens": 57,
          "total_tokens": 524
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 156,
            "completion_tokens": 21,
            "total_tokens": 177,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 156,
            "completion_tokens": 18,
            "total_tokens": 174,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 155,
            "completion_tokens": 18,
            "total_tokens": 173,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/3p4/2PP4/8/PP2PPPP/RNBQKBNR b KQkq - 0 2"
    },
    {
      "turn": 6,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc6",
          "aggressive_uci": "b8c6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 \nNc6 Bg5",
          "aggressive_tokens": {
            "prompt_tokens": 165,
            "completion_tokens": 28,
            "total_tokens": 193,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bg5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nc6",
          "positional_uci": "b8c6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 \nNc6 e6 Nbd2",
          "positional_tokens": {
            "prompt_tokens": 165,
            "completion_tokens": 30,
            "total_tokens": 195,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e6",
            "Nbd2"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc6",
          "neutral_uci": "b8c6",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 \nNc6 e6",
          "neutral_tokens": {
            "prompt_tokens": 164,
            "completion_tokens": 27,
            "total_tokens": 191,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b8c6",
          "positional_uci": "b8c6",
          "neutral_uci": "b8c6",
          "consensus_move": "b8c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bg5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e6",
              "Nbd2"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 165,
            "completion_tokens": 28,
            "total_tokens": 193,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 165,
            "completion_tokens": 30,
            "total_tokens": 195,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 164,
            "completion_tokens": 27,
            "total_tokens": 191,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 494,
          "total_completion_tokens": 85,
          "total_tokens": 579
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 165,
            "completion_tokens": 28,
            "total_tokens": 193,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 165,
            "completion_tokens": 30,
            "total_tokens": 195,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 164,
            "completion_tokens": 27,
            "total_tokens": 191,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/8/2pP4/5N2/PP2PPPP/RNBQKB1R b KQkq - 1 3"
    },
    {
      "turn": 8,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf6",
          "aggressive_uci": "g8f6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 \nNf6 e3",
          "aggressive_tokens": {
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf6",
          "positional_uci": "g8f6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 \nNf6 e3",
          "positional_tokens": {
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf6",
          "neutral_uci": "g8f6",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 \nNf6 e3",
          "neutral_tokens": {
            "prompt_tokens": 173,
            "completion_tokens": 36,
            "total_tokens": 209,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g8f6",
          "positional_uci": "g8f6",
          "neutral_uci": "g8f6",
          "consensus_move": "g8f6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "e3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 173,
            "completion_tokens": 36,
            "total_tokens": 209,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 521,
          "total_completion_tokens": 108,
          "total_tokens": 629
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 174,
            "completion_tokens": 36,
            "total_tokens": 210,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 173,
            "completion_tokens": 36,
            "total_tokens": 209,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkbnr/ppp1pppp/2n5/8/2pP4/2N2N2/PP2PPPP/R1BQKB1R b KQkq - 3 4"
    },
    {
      "turn": 10,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e5",
          "aggressive_uci": "e7e5",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 \ne5 Nxe4 Nxd4",
          "aggressive_tokens": {
            "prompt_tokens": 182,
            "completion_tokens": 47,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxe4",
            "Nxd4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bg4",
          "positional_uci": "c8g4",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 Nc2 a6 b3",
          "positional_tokens": {
            "prompt_tokens": 182,
            "completion_tokens": 48,
            "total_tokens": 230,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc2",
            "a6",
            "b3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bg4",
          "neutral_uci": "c8g4",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 Nc2 a6 b3",
          "neutral_tokens": {
            "prompt_tokens": 181,
            "completion_tokens": 48,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc2",
            "a6",
            "b3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e7e5",
          "positional_uci": "c8g4",
          "neutral_uci": "c8g4",
          "consensus_move": "c8g4",
          "source_agents": [
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nxe4",
              "Nxd4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nc2",
              "a6",
              "b3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc2",
              "a6",
              "b3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 182,
            "completion_tokens": 47,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 182,
            "completion_tokens": 48,
            "total_tokens": 230,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 181,
            "completion_tokens": 48,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 545,
          "total_completion_tokens": 143,
          "total_tokens": 688
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 182,
            "completion_tokens": 47,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 182,
            "completion_tokens": 48,
            "total_tokens": 230,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 181,
            "completion_tokens": 48,
            "total_tokens": 229,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp1pppp/2n2n2/8/2pPP3/2N2N2/PP3PPP/R1BQKB1R b KQkq - 0 5"
    },
    {
      "turn": 12,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Na5",
          "aggressive_uci": "c6a5",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 Nc2 e6",
          "aggressive_tokens": {
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc2",
            "e6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Na5",
          "positional_uci": "c6a5",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 Nc2 h6",
          "positional_tokens": {
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc2",
            "h6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Na5",
          "neutral_uci": "c6a5",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 Nc2 e6",
          "neutral_tokens": {
            "prompt_tokens": 189,
            "completion_tokens": 53,
            "total_tokens": 242,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc2",
            "e6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "c6a5",
          "positional_uci": "c6a5",
          "neutral_uci": "c6a5",
          "consensus_move": "c6a5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nc2",
              "e6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nc2",
              "h6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc2",
              "e6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 189,
            "completion_tokens": 53,
            "total_tokens": 242,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 569,
          "total_completion_tokens": 159,
          "total_tokens": 728
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 190,
            "completion_tokens": 53,
            "total_tokens": 243,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 189,
            "completion_tokens": 53,
            "total_tokens": 242,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/2n2n2/3P4/2p1P1b1/2N2N2/PP3PPP/R1BQKB1R b KQkq - 0 6"
    },
    {
      "turn": 14,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc6",
          "aggressive_uci": "a5c6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 b5 Qxb5 a6",
          "aggressive_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 65,
            "total_tokens": 264,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b5",
            "Qxb5",
            "a6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nc6",
          "positional_uci": "a5c6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 b5 Qxb5",
          "positional_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 63,
            "total_tokens": 262,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b5",
            "Qxb5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc6",
          "neutral_uci": "a5c6",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 b5 Qxb5 a6",
          "neutral_tokens": {
            "prompt_tokens": 198,
            "completion_tokens": 65,
            "total_tokens": 263,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "b5",
            "Qxb5",
            "a6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "a5c6",
          "positional_uci": "a5c6",
          "neutral_uci": "a5c6",
          "consensus_move": "a5c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "b5",
              "Qxb5",
              "a6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "b5",
              "Qxb5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "b5",
              "Qxb5",
              "a6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 199,
            "completion_tokens": 65,
            "total_tokens": 264,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 199,
            "completion_tokens": 63,
            "total_tokens": 262,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 198,
            "completion_tokens": 65,
            "total_tokens": 263,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 596,
          "total_completion_tokens": 193,
          "total_tokens": 789
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 199,
            "completion_tokens": 65,
            "total_tokens": 264,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 199,
            "completion_tokens": 63,
            "total_tokens": 262,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 198,
            "completion_tokens": 65,
            "total_tokens": 263,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/5n2/n2P4/Q1p1P1b1/2N2N2/PP3PPP/R1B1KB1R b KQkq - 2 7"
    },
    {
      "turn": 16,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "b6",
          "aggressive_uci": "b7b6",
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 Bb4 Qe7",
          "aggressive_tokens": {
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bb4",
            "Qe7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "b6",
          "positional_uci": "b7b6",
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 Bc5 Qe7",
          "positional_tokens": {
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "Qe7"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "b6",
          "neutral_uci": "b7b6",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 Bb4+ Bd7",
          "neutral_tokens": {
            "prompt_tokens": 207,
            "completion_tokens": 72,
            "total_tokens": 279,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bb4+",
            "Bd7"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b7b6",
          "positional_uci": "b7b6",
          "neutral_uci": "b7b6",
          "consensus_move": "b7b6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bb4",
              "Qe7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bc5",
              "Qe7"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bb4+",
              "Bd7"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 207,
            "completion_tokens": 72,
            "total_tokens": 279,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 623,
          "total_completion_tokens": 216,
          "total_tokens": 839
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 208,
            "completion_tokens": 72,
            "total_tokens": 280,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 207,
            "completion_tokens": 72,
            "total_tokens": 279,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/2P2n2/8/Q1p1P1b1/2N2N2/PP3PPP/R1B1KB1R b KQkq - 0 8"
    },
    {
      "turn": 18,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qe7",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6 b4",
          "aggressive_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6",
            "b4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qe7",
          "positional_uci": null,
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 Nd7 Bb5",
          "positional_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "Nd7",
            "Bb5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qe7",
          "neutral_uci": null,
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6",
          "neutral_tokens": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qe2",
              "a6",
              "b4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qe2",
              "Nd7",
              "Bb5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qe2",
              "a6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 647,
          "total_completion_tokens": 245,
          "total_tokens": 892
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/p1p1pppp/1pP2n2/8/Q1B1P1b1/2N2N2/PP3PPP/R1B1K2R b KQkq - 0 9"
    },
    {
      "turn": 18,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qe7",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 Nd7 b4",
          "aggressive_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "Nd7",
            "b4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qe7",
          "positional_uci": null,
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 Nd7 Bb5",
          "positional_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "Nd7",
            "Bb5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qe7",
          "neutral_uci": null,
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6",
          "neutral_tokens": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qe2",
              "Nd7",
              "b4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qe2",
              "Nd7",
              "Bb5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qe2",
              "a6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 647,
          "total_completion_tokens": 245,
          "total_tokens": 892
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/p1p1pppp/1pP2n2/8/Q1B1P1b1/2N2N2/PP3PPP/R1B1K2R b KQkq - 0 9"
    },
    {
      "turn": 18,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qe7",
          "aggressive_uci": null,
          "aggressive_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6 b4",
          "aggressive_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6",
            "b4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qe7",
          "positional_uci": null,
          "positional_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 Nd7 Bb5",
          "positional_tokens": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "Nd7",
            "Bb5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qe7",
          "neutral_uci": null,
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nc6 4. Nc3 Nf6 5. e4 Bg4 6. d5 Na5 7. Qa4+ Nc6 8. dxc6 b6 9. Bxc4 Qe7 Qe2 a6",
          "neutral_tokens": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qe2",
            "a6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qe2",
              "a6",
              "b4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qe2",
              "Nd7",
              "Bb5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qe2",
              "a6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 647,
          "total_completion_tokens": 245,
          "total_tokens": 892
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 216,
            "completion_tokens": 82,
            "total_tokens": 298,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 216,
            "completion_tokens": 83,
            "total_tokens": 299,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 215,
            "completion_tokens": 80,
            "total_tokens": 295,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/p1p1pppp/1pP2n2/8/Q1B1P1b1/2N2N2/PP3PPP/R1B1K2R b KQkq - 0 9"
    },
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": "e2e4",
          "aggressive_response": "Move: e4\nPlan: e5 Nf3 Nc6",
          "aggressive_tokens": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "Move: e4 \nPlan: e5 Nf3 Nc6",
          "positional_tokens": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": "e2e4",
          "neutral_response": "Move: e4\nPlan: e5 Nf3 Nc6",
          "neutral_tokens": {
            "prompt_tokens": 184,
            "completion_tokens": 16,
            "total_tokens": 200,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e2e4",
          "positional_uci": "e2e4",
          "neutral_uci": "e2e4",
          "consensus_move": "e2e4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5",
              "Nf3",
              "Nc6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Nf3",
              "Nc6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5",
              "Nf3",
              "Nc6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 184,
            "completion_tokens": 16,
            "total_tokens": 200,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 554,
          "total_completion_tokens": 48,
          "total_tokens": 602
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 185,
            "completion_tokens": 16,
            "total_tokens": 201,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 184,
            "completion_tokens": 16,
            "total_tokens": 200,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 3,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": null,
          "positional_response": "1. e4 e5\nWhite",
          "positional_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\nWhite",
          "neutral_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 452,
          "total_completion_tokens": 27,
          "total_tokens": 479
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 3,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": null,
          "positional_response": "1. e4 e5\nWhite",
          "positional_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\nWhite",
          "neutral_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 452,
          "total_completion_tokens": 27,
          "total_tokens": 479
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 3,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\nWhite",
          "aggressive_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": null,
          "positional_response": "1. e4 e5\nWhite",
          "positional_tokens": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\nWhite",
          "neutral_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 452,
          "total_completion_tokens": 27,
          "total_tokens": 479
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 151,
            "completion_tokens": 9,
            "total_tokens": 160,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 150,
            "completion_tokens": 9,
            "total_tokens": 159,
            "model": "meta-llama/llama-3.3-70b-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    }
  ],
  "black_responses": [
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": "d2d4",
          "aggressive_response": "d4 d5 2. Nf3 Nf6 3. Bg5 Bf5 4. e3 e6 5. Bd3 Bxd3 6. Qxd3 Be7 7. O-O O-O 8. Nbd2 Nbd7 9. c4 c6 10. Rac1 Rc8 11. cxd5 exd5 12. a3 Re8 13. b4 a6 14. Nb3 Ne4 15. Bxe7 Qxe7 16. Nc5 Ndxc5 17. bxc5 f5 18. Rb1 Rc7 19. Rb2 Qf6 20. Rfb1 Ree7 21. Ne5 Nxc5 22. dxc5 Qxe5 23. Rb4 Qf6 24. Rf4 g6 25. g4 Re4 26. Rxe4 fxe4 27. Qd4 Qxd4 28. exd4 Kf7 29. Kg2 Kf6 30. Kg3 Kg5 31. h4+ Kf6 32. Kf4 h6 33. g5+ hxg5+ 34. hxg5+ Ke6 35. Rh1 Rf7+ 36. Ke3 Kf5 37. Rg1 Rh7 38. f3 Rh3 39. Rf1 Rxf3+ 40. Rxf3+ exf3 41. Kxf3 Kxg5 42. Kg3 Kf5 43. Kf3 g5 44. a4 g4+ 45. Ke3 g3 46. Kf3 g2 47. Kxg2 Ke4 48. Kf2 Kxd4 49. Ke2 Kxc5 50. Kd3 Kb4 51. Kd4 Kxa4 52. Kc5 Kb3 53. Kb6 d4 54. Kxb7 d3 55. Kxc6 d2 56. Kb6 d1=Q 57. Kxa6 Qd5 58. Kb6 Kb4 59. Kc7 Qe6 60. Kb7 Kb5 61. Kc7 Qe7+ 62. Kc8 Kb6 63. Kb8 Qb7# d4 d5 2. Nf3 Nf6 3. Bg5 Bf5 4. e3 e6 5. Bd3 Bxd3 6. Qxd3 Be7 7. O-O O-O 8. Nbd2 Nbd7 9. c4 c6 10. Rac1 Rc8 11. cxd5 exd5 12. a3 Re8 13. b4 a6 14. Nb3 Ne4 15. Bxe7 Qxe7 16. Nc5 Ndxc",
          "aggressive_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Nf3",
            "Nf6",
            "Bg5",
            "Bf5",
            "e3",
            "e6",
            "Bd3",
            "Bxd3",
            "Qxd3",
            "Be7",
            "O-O",
            "O-O",
            "Nbd2",
            "Nbd7",
            "c4",
            "c6",
            "Rac1",
            "Rc8",
            "cxd5",
            "exd5",
            "a3",
            "Re8",
            "b4",
            "a6",
            "Nb3",
            "Ne4",
            "Bxe7",
            "Qxe7",
            "Nc5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 Bc5 4. O-O Nge7 5. Nxe5 Nxe5 6. d4 Bxd4 7. Qxd4 N7c6 8. Bxc6 Nxc6 9. Qxg7 Rf8 10. Bh6 Qe7 11. Nc3 d6 12. Nd5 Qe5 13. Qxf8+ Kd7 14. Qxf7+ Kd8 15. Qxc7+ Ke8 16. Qxh7 Be6 17. Nc7+ Kd8 18. Nxa8 Nd4 19. Qc7+ Ke8 20. Qxb7 Ne2+ 21. Kh1 Qh5 22. Nc7+ Kf7 23. Nxe6+ Kxe6 24. Qd5+ Qxd5 25. exd5+ Kxd5 26. Rad1+ Kc4 27. Rfe1 Nd4 28. Re4 d5 29. Rexd4+ Kc5 30. Rxd5+ Kc6 31. Rd6+ Kc5 32. R1d5+ Kc4 33. b3+ Kc3 34. Rc6+ Kb2 35. Bg7+ Kxa2 36. Ra5+ Kb1 37. Ra1# 1-0 e4 Nf6 d4 Nxd4 Nxe5 Nxe5 d4 Bxd4 Qxd4 N7c6 Bxc6 Nxc6 Qxg7 Rf8 Bh6 Qe7 Nc3 d6 Nd5 Qe5 Qxf8+ Kd7 Qxf7+ Kd8 Qxc7+ Ke8 Qxh7 Be6 Nc7+ Kd8 Nxa8 Nd4 Qc7+ Ke8 Qxb7 Ne2+ Kh1 Qh5 Nc7+ Kf7 Nxe6+ Kxe6 Qd5+ Qxd5 exd5+ Kxd5 Rad1+ Kc4 Rfe1 Nd4 Re4 d5 Rexd4+ Kc5 Rxd5+ Kc6 Rd6+ Kc5 R1d5+ Kc4 b3+ Kc3 Rc5+ Kb2 Rc2+ Kxc2 a5 Rc6+ Kb4 Rd4+ Ka3 Ra4#",
          "positional_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 547,
            "total_tokens": 657,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "d4",
            "Nxd4",
            "Nxe5",
            "Nxe5",
            "d4",
            "Bxd4",
            "Qxd4",
            "N7c6",
            "Bxc6",
            "Nxc6",
            "Qxg7",
            "Rf8",
            "Bh6",
            "Qe7",
            "Nc3",
            "d6",
            "Nd5",
            "Qe5",
            "Qxf8+",
            "Kd7",
            "Qxf7+",
            "Kd8",
            "Qxc7+",
            "Ke8",
            "Qxh7",
            "Be6",
            "Nc7+",
            "Kd8",
            "Nxa8",
            "Nd4",
            "Qc7+",
            "Ke8",
            "Qxb7",
            "Ne2+",
            "Kh1",
            "Qh5",
            "Nc7+",
            "Kf7",
            "Nxe6+",
            "Kxe6",
            "Qd5+",
            "Qxd5",
            "exd5+",
            "Kxd5",
            "Rad1+",
            "Kc4",
            "Rfe1",
            "Nd4",
            "Re4",
            "d5",
            "Rexd4+",
            "Kc5",
            "Rxd5+",
            "Kc6",
            "Rd6+",
            "Kc5",
            "R1d5+",
            "Kc4",
            "b3+",
            "Kc3",
            "Rc5+",
            "Kb2",
            "Rc2+",
            "Kxc2",
            "a5",
            "Rc6+",
            "Kb4",
            "Rd4+",
            "Ka3",
            "Ra4#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": "d2d4",
          "neutral_response": "d4 Nf6\nd4 Nf6 Nc3 d5 Bg5 e6 e4 dxe4 Nxe4 Be7 Nxf6+ Bxf6 Bxf6 Qxf6 Nf3 O-O Bd3 Nc6 O-O Rd8 c3 e5 dxe5 Nxe5 Nxe5 Qxe5 Re1 Qd6 Re3 Bf5 Qe2 Bxd3 Qe1 Bb5 a4 Bc6 b4 Re8 b5 Rxe3 Qxe3 Bd7 Re1 h6 c4 Re8 Qc1 Rxe1+ Qxe1 Qe6 Qc1 Qe4 h3 Be6 c5 Qxa4 b6 axb6 cxb6 cxb6 Qxb6 Qd1+ Kh2 Qd6+ g3 Qd5 Qb8+ Kh7 Qf4 b5 h4 Qc4 Qe3 b4 f4 b3 f5 b2 fxe6 b1=Q exf7 Qbc2+ Kh3 Qc8+ g4 Q8c3+ Qg3 Qxg3+ Kxg3 Qd3+ Kf4 Qd4+ Kg3 Qe3+ Kg2 Qe2+ Kg3 Qe1+ Kg2 Qxh4 f8=Q Qxg4+ Kf2 Qf5+ Qxf5+ Qxf5+ Kg3 Kg6 Kh4 h5 Kg3 Kg5 Kh3 h4 Kg2 Kg4 Kh2 h3 Kg1 Kg3 Kh1 h2+ Kg2 Kh4 Kxh2 g5 Kg2 g4 Kh1 g3 Kg2 Kg4 Kg1 Kf3 Kh2 Kf2 Kh3 g2 Kh4 g1=Q Kh5 Kf4 Kh6 Kf3 Kh4 Qg4#\nd4 Nf6 Nc3 d5 Bg5 e6 e4 dxe4 Nxe4 Be7 Nxf6+ Bxf6 Bxf6 Qxf6 Nf3 O-O Bd3 Nc6 O-O Rd8 c3 e5 dxe5 Nxe5 Nxe5 Qxe5 Re1 Qd6 Re3 Bf5 Qe2 Bxd3 Qe1 Bb5 a4 Bc6 b4 Re8 b5 Rxe3 Qxe3 Bd7 Re1 h6 c4 Re8 Qc1 Rxe1+ Qxe1 Qe6 Qc1 Qe4 h3 Be6 c5 Qxa4 b6 axb6 cxb6 cxb6 Qxb6 Qd1+ Kh2 Qd6+ g3 Qd5 Qb8+ Kh7 Qf4 b5 h4 Qc4 Qe3 b4 f4 b3 f5 Bxf5 Qf3 Qc2+ Kg1 b2 g4 b1=Q+ Qf1 Qxf1+ Kxf1 Bxg4 Kf2 Kg6",
          "neutral_tokens": {
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf6",
            "Nc3",
            "d5",
            "Bg5",
            "e6",
            "e4",
            "dxe4",
            "Nxe4",
            "Be7",
            "Nxf6+",
            "Bxf6",
            "Bxf6",
            "Qxf6",
            "Nf3",
            "O-O",
            "Bd3",
            "Nc6",
            "O-O",
            "Rd8",
            "c3",
            "e5",
            "dxe5",
            "Nxe5",
            "Nxe5",
            "Qxe5",
            "Re1",
            "Qd6",
            "Re3",
            "Bf5",
            "Qe2",
            "Bxd3",
            "Qe1",
            "Bb5",
            "a4",
            "Bc6",
            "b4",
            "Re8",
            "b5",
            "Rxe3",
            "Qxe3",
            "Bd7",
            "Re1",
            "h6",
            "c4",
            "Re8",
            "Qc1",
            "Rxe1+",
            "Qxe1",
            "Qe6",
            "Qc1",
            "Qe4",
            "h3",
            "Be6",
            "c5",
            "Qxa4",
            "b6",
            "axb6",
            "cxb6",
            "cxb6",
            "Qxb6",
            "Qd1+",
            "Kh2",
            "Qd6+",
            "g3",
            "Qd5",
            "Qb8+",
            "Kh7",
            "Qf4",
            "b5",
            "h4",
            "Qc4",
            "Qe3",
            "b4",
            "f4",
            "b3",
            "f5",
            "Bxf5",
            "Qf3",
            "Qc2+",
            "Kg1",
            "b2",
            "g4",
            "b1=Q+",
            "Qf1",
            "Qxf1+",
            "Kxf1",
            "Bxg4",
            "Kf2",
            "Kg6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d2d4",
          "positional_uci": "e2e4",
          "neutral_uci": "d2d4",
          "consensus_move": "d2d4",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d5",
              "Nf3",
              "Nf6",
              "Bg5",
              "Bf5",
              "e3",
              "e6",
              "Bd3",
              "Bxd3",
              "Qxd3",
              "Be7",
              "O-O",
              "O-O",
              "Nbd2",
              "Nbd7",
              "c4",
              "c6",
              "Rac1",
              "Rc8",
              "cxd5",
              "exd5",
              "a3",
              "Re8",
              "b4",
              "a6",
              "Nb3",
              "Ne4",
              "Bxe7",
              "Qxe7",
              "Nc5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf6",
              "d4",
              "Nxd4",
              "Nxe5",
              "Nxe5",
              "d4",
              "Bxd4",
              "Qxd4",
              "N7c6",
              "Bxc6",
              "Nxc6",
              "Qxg7",
              "Rf8",
              "Bh6",
              "Qe7",
              "Nc3",
              "d6",
              "Nd5",
              "Qe5",
              "Qxf8+",
              "Kd7",
              "Qxf7+",
              "Kd8",
              "Qxc7+",
              "Ke8",
              "Qxh7",
              "Be6",
              "Nc7+",
              "Kd8",
              "Nxa8",
              "Nd4",
              "Qc7+",
              "Ke8",
              "Qxb7",
              "Ne2+",
              "Kh1",
              "Qh5",
              "Nc7+",
              "Kf7",
              "Nxe6+",
              "Kxe6",
              "Qd5+",
              "Qxd5",
              "exd5+",
              "Kxd5",
              "Rad1+",
              "Kc4",
              "Rfe1",
              "Nd4",
              "Re4",
              "d5",
              "Rexd4+",
              "Kc5",
              "Rxd5+",
              "Kc6",
              "Rd6+",
              "Kc5",
              "R1d5+",
              "Kc4",
              "b3+",
              "Kc3",
              "Rc5+",
              "Kb2",
              "Rc2+",
              "Kxc2",
              "a5",
              "Rc6+",
              "Kb4",
              "Rd4+",
              "Ka3",
              "Ra4#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf6",
              "Nc3",
              "d5",
              "Bg5",
              "e6",
              "e4",
              "dxe4",
              "Nxe4",
              "Be7",
              "Nxf6+",
              "Bxf6",
              "Bxf6",
              "Qxf6",
              "Nf3",
              "O-O",
              "Bd3",
              "Nc6",
              "O-O",
              "Rd8",
              "c3",
              "e5",
              "dxe5",
              "Nxe5",
              "Nxe5",
              "Qxe5",
              "Re1",
              "Qd6",
              "Re3",
              "Bf5",
              "Qe2",
              "Bxd3",
              "Qe1",
              "Bb5",
              "a4",
              "Bc6",
              "b4",
              "Re8",
              "b5",
              "Rxe3",
              "Qxe3",
              "Bd7",
              "Re1",
              "h6",
              "c4",
              "Re8",
              "Qc1",
              "Rxe1+",
              "Qxe1",
              "Qe6",
              "Qc1",
              "Qe4",
              "h3",
              "Be6",
              "c5",
              "Qxa4",
              "b6",
              "axb6",
              "cxb6",
              "cxb6",
              "Qxb6",
              "Qd1+",
              "Kh2",
              "Qd6+",
              "g3",
              "Qd5",
              "Qb8+",
              "Kh7",
              "Qf4",
              "b5",
              "h4",
              "Qc4",
              "Qe3",
              "b4",
              "f4",
              "b3",
              "f5",
              "Bxf5",
              "Qf3",
              "Qc2+",
              "Kg1",
              "b2",
              "g4",
              "b1=Q+",
              "Qf1",
              "Qxf1+",
              "Kxf1",
              "Bxg4",
              "Kf2",
              "Kg6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 110,
            "completion_tokens": 547,
            "total_tokens": 657,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 329,
          "total_completion_tokens": 1827,
          "total_tokens": 2156
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 110,
            "completion_tokens": 547,
            "total_tokens": 657,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 3,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "c4",
          "aggressive_uci": "c2c4",
          "aggressive_response": "2. c4 c6 3. Nc3 Nf6 4. Nf3 dxc4 5. a4 Bf5 6. e3 e6 7. Bxc4 Bb4 8. O-O O-O 9. Qe2 Nbd7 10. e4 Bg6 11. Bd3 Bh5 12. e5 Nd5 13. Nxd5 cxd5 14. Qe3 Bg6 15. Bxg6 hxg6 16. Ng5 Re8 17. Qh3 Nf8 18. Rd1 Rc8 19. Qb3 Qb6 20. Qf3 Re7 21. Qd3 Rec7 22. Be3 Be7 23. Nf3 Qxb2 24. Rab1 Qc2 25. Qxc2 Rxc2 26. Rxb7 R2c7 27. Rdb1 Nd7 28. g3 Nb6 29. Rxc7 Rxc7 30. a5 Nc4 31. Rb8+ Kh7 32. Bg5 f6 33. Bf4 g5 34. exf6 gxf4 35. fxe7 Rxe7 36. a6 fxg3 37. hxg3 Kg6 38. Rb7 Kf6 39. Ne5 Nxe5 40. dxe5+ Kf7 41. f4 Ke8 42. Kf2 Kd8 43. Rb8+ Kc7 44. Rb7+ Kd8 45. Rb8+ Kc7 46. Rb7+ Kd8 47. Rb8+ Kc7 48. Rb7+ Kd8 49. Rb8+ Kc7 50. Rb7+ Kd8 51. Rb8+ Kc7 52. Rb7+ Kd8 53. Rb8+ Kc7 54. Rb7+ Kd8 55. Rb8+ Kc7 56. Rb7+ Kd8 57. Rb8+ Kc7 58. Rb7+ Kd8 59. Rb8+ Kc7 60. Rb7+ Kd8 61. Rb8+ Kc7 62. Rb7+ Kd8 63. Rb8+ Kc7 64. Rb7+ Kd8 65. Rb8+ Kc7 66. Rb7+ Kd8 67. Rb8+ Kc7 68. Rb7+ Kd8 69. Rb8+ Kc7 70. Rb7+ Kd8 71. Rb8+ Kc7 72. Rb7+ Kd8",
          "aggressive_tokens": {
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "c6",
            "Nc3",
            "Nf6",
            "Nf3",
            "dxc4",
            "a4",
            "Bf5",
            "e3",
            "e6",
            "Bxc4",
            "Bb4",
            "O-O",
            "O-O",
            "Qe2",
            "Nbd7",
            "e4",
            "Bg6",
            "Bd3",
            "Bh5",
            "e5",
            "Nd5",
            "Nxd5",
            "cxd5",
            "Qe3",
            "Bg6",
            "Bxg6",
            "hxg6",
            "Ng5",
            "Re8",
            "Qh3",
            "Nf8",
            "Rd1",
            "Rc8",
            "Qb3",
            "Qb6",
            "Qf3",
            "Re7",
            "Qd3",
            "Rec7",
            "Be3",
            "Be7",
            "Nf3",
            "Qxb2",
            "Rab1",
            "Qc2",
            "Qxc2",
            "Rxc2",
            "Rxb7",
            "R2c7",
            "Rdb1",
            "Nd7",
            "g3",
            "Nb6",
            "Rxc7",
            "Rxc7",
            "a5",
            "Nc4",
            "Rb8+",
            "Kh7",
            "Bg5",
            "f6",
            "Bf4",
            "g5",
            "exf6",
            "gxf4",
            "fxe7",
            "Rxe7",
            "a6",
            "fxg3",
            "hxg3",
            "Kg6",
            "Rb7",
            "Kf6",
            "Ne5",
            "Nxe5",
            "dxe5+",
            "Kf7",
            "f4",
            "Ke8",
            "Kf2",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8",
            "Rb8+",
            "Kc7",
            "Rb7+",
            "Kd8"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf3",
          "positional_uci": "g1f3",
          "positional_response": "2. Nf3 Nf6 3. c4 c6 4. Nc3 dxc4 5. a4 Bf5 6. e3 e6 7. Bxc4 Bb4 8. O-O O-O 9. Qe2 Bg6 10. Ne5 Nbd7 11. Nxg6 hxg6 12. Rd1 Qa5 13. Bd2 e5 14. dxe5 Nxe5 15. Bb3 Rad8 16. Be1 Rxd1 17. Rxd1 Rd8 18. Rxd8+ Qxd8 19. Qd1 Qxd1 20. Bxd1 Nd3 21. Kf1 Nxb2 22. Bb3 Nd3 23. Bd2 Nc5 24. Bc2 Bxc3 25. Bxc3 Nd5 26. Bd4 b6 27. a5 Kf8 28. axb6 axb6 29. Ke2 Ne6 30. Bb2 Ke7 31. g3 Kd6 32. e4 Nb4 33. Bb3 c5 34. f4 b5 35. f5 gxf5 36. exf5 Nd4+ 37. Bxd4 cxd4 38. Bxf7 Ke5 39. Be8 Kxf5 40. Bxb5 Kg4 41. Bd7+ Kg5 42. h4+ Kf6 43. g4 Nd5 44. Kd3 Ke5 45. Bf5 Nf4+ 46. Kd2 Ng2 47. h5 Ne3 48. Bc8 Kf4 49. Kd3 Nxg4 50. Kxd4 Kg5 51. Ke4 Kxh5 52. Kf4 Nf6 53. Bb7 g5+ 54. Kg3 Kg6 55. Bc6 Kf5 56. Bb7 Ne4+ 57. Kg2 Kf4 58. Bc6 Nf6 59. Bb7 Ng4 60. Bc6 Ne3+ 61. Kg1 Kg3 62. Bb7 Nf5 63. Bc6 Nd4 64. Bb7 Nf3+ 65. Kf1 g4 66. Bc6 Kh2 67. Kf2 Ne5 68. Bd5 g3+ 69. Kf1 Ng4 70. Ke2 Nf6 71. Bc6 Nh5 72. Kf1 Nf4 73. Bf3 Nd3 74. Be4 Nf2 75. Bc6 Ng4 76. Ke2 Ne5 77. Be4 Kg1 78. Ke3 Kf1 79. Kf4 Kf",
          "positional_tokens": {
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Kf1",
            "g4",
            "Bc6",
            "Kh2",
            "Kf2",
            "Ne5",
            "Bd5",
            "g3+",
            "Kf1",
            "Ng4",
            "Ke2",
            "Nf6",
            "Bc6",
            "Nh5",
            "Kf1",
            "Nf4",
            "Bf3",
            "Nd3",
            "Be4",
            "Nf2",
            "Bc6",
            "Ng4",
            "Ke2",
            "Ne5",
            "Be4",
            "Kg1",
            "Ke3",
            "Kf1",
            "Kf4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "c4",
          "neutral_uci": "c2c4",
          "neutral_response": "2. c4 c6 *\n3. Nc3 Nf6 *\n4. Nf3 e6 *\n5. Bg5 Nbd7 *\n6. e3 Qa5 *\n7. Nd2 Bb4 *\n8. Qc2 O-O *\n9. Be2 dxc4 *\n10. Bxf6 Nxf6 *\n11. Nxc4 Qc7 *\n12. O-O b6 *\n13. Rac1 Bb7 *\n14. a3 Be7 *\n15. b4 Rac8 *\n16. Qb3 Qb8 *\n17. Rfd1 Rfd8 *\n18. Ne5 c5 *\n19. bxc5 bxc5 *\n20. Rb1 cxd4 *\n21. exd4 Ba8 *\n22. Qxb8 Rxb8 *\n23. Rxb8 Rxb8 *\n24. Nb5 a6 *\n25. Nc7 Bb7 *\n26. Rb1 Bd6 *\n27. Nxa6 Bxa6 *\n28. Rxb8+ Bxb8 *\n29. Bxa6 Bxe5 *\n30. dxe5 Nd7 *\n31. f4 Nc5 *\n32. Bb5 Kf8 *\n33. Kf2 Ke7 *\n34. Ke3 Kd8 *\n35. Kd4 Nb7 *\n36. a4 Kc7 *\n37. Be8 Nd8 *\n38. Kc5 Nb7+ *\n39. Kb5 Nd8 *\n40. a5 Kb7 *\n41. a6+ Ka7 *\n42. g4 h6 *\n43. h4 g6 *\n44. g5 hxg5 *\n45. hxg5 Ka8 *\n46. Kb6 Kb8 *\n47. a7+ Ka8 *\n48. Ka6 Nb7 *\n49. Bc6 f5 *\n50. Bxb7# f5 *\n51. exf6 e5 *\n52. f7 exf4 *\n53. f8=Q# f3 *\n54. Qc8# f2 *\n55. Qa8# f1=Q *\n56. Bxf1# f4 *\n57. Qb7# f3 *\n58. Qc8# f2 *\n59. Qb7# f1=Q *\n60. Bxf1# f4 *\n61. Qc8# f3 *\n62. Qb7# f2 *\n63. Qc8# f1=Q *\n64. Bxf1# f4 *\n65. Qc8# f3 *\n66. Qb7# f2 *\n67. Qc8# f1=Q *\n68. Bxf1# f4 *\n69. Qc8#",
          "neutral_tokens": {
            "prompt_tokens": 115,
            "completion_tokens": 640,
            "total_tokens": 755,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "c6",
            "Nc3",
            "Nf6",
            "Nf3",
            "e6",
            "Bg5",
            "Nbd7",
            "e3",
            "Qa5",
            "Nd2",
            "Bb4",
            "Qc2",
            "O-O",
            "Be2",
            "dxc4",
            "Bxf6",
            "Nxf6",
            "Nxc4",
            "Qc7",
            "O-O",
            "b6",
            "Rac1",
            "Bb7",
            "a3",
            "Be7",
            "b4",
            "Rac8",
            "Qb3",
            "Qb8",
            "Rfd1",
            "Rfd8",
            "Ne5",
            "c5",
            "bxc5",
            "bxc5",
            "Rb1",
            "cxd4",
            "exd4",
            "Ba8",
            "Qxb8",
            "Rxb8",
            "Rxb8",
            "Rxb8",
            "Nb5",
            "a6",
            "Nc7",
            "Bb7",
            "Rb1",
            "Bd6",
            "Nxa6",
            "Bxa6",
            "Rxb8+",
            "Bxb8",
            "Bxa6",
            "Bxe5",
            "dxe5",
            "Nd7",
            "f4",
            "Nc5",
            "Bb5",
            "Kf8",
            "Kf2",
            "Ke7",
            "Ke3",
            "Kd8",
            "Kd4",
            "Nb7",
            "a4",
            "Kc7",
            "Be8",
            "Nd8",
            "Kc5",
            "Nb7+",
            "Kb5",
            "Nd8",
            "a5",
            "Kb7",
            "a6+",
            "Ka7",
            "g4",
            "h6",
            "h4",
            "g6",
            "g5",
            "hxg5",
            "hxg5",
            "Ka8",
            "Kb6",
            "Kb8",
            "a7+",
            "Ka8",
            "Ka6",
            "Nb7",
            "Bc6",
            "f5",
            "Bxb7#",
            "f5",
            "exf6",
            "e5",
            "f7",
            "exf4",
            "f8=Q#",
            "f3",
            "Qc8#",
            "f2",
            "Qa8#",
            "f1=Q",
            "Bxf1#",
            "f4",
            "Qb7#",
            "f3",
            "Qc8#",
            "f2",
            "Qb7#",
            "f1=Q",
            "Bxf1#",
            "f4",
            "Qc8#",
            "f3",
            "Qb7#",
            "f2",
            "Qc8#",
            "f1=Q",
            "Bxf1#",
            "f4",
            "Qc8#",
            "f3",
            "Qb7#",
            "f2",
            "Qc8#",
            "f1=Q",
            "Bxf1#",
            "f4",
            "Qc8#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "c2c4",
          "positional_uci": "g1f3",
          "neutral_uci": "c2c4",
          "consensus_move": "c2c4",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "c6",
              "Nc3",
              "Nf6",
              "Nf3",
              "dxc4",
              "a4",
              "Bf5",
              "e3",
              "e6",
              "Bxc4",
              "Bb4",
              "O-O",
              "O-O",
              "Qe2",
              "Nbd7",
              "e4",
              "Bg6",
              "Bd3",
              "Bh5",
              "e5",
              "Nd5",
              "Nxd5",
              "cxd5",
              "Qe3",
              "Bg6",
              "Bxg6",
              "hxg6",
              "Ng5",
              "Re8",
              "Qh3",
              "Nf8",
              "Rd1",
              "Rc8",
              "Qb3",
              "Qb6",
              "Qf3",
              "Re7",
              "Qd3",
              "Rec7",
              "Be3",
              "Be7",
              "Nf3",
              "Qxb2",
              "Rab1",
              "Qc2",
              "Qxc2",
              "Rxc2",
              "Rxb7",
              "R2c7",
              "Rdb1",
              "Nd7",
              "g3",
              "Nb6",
              "Rxc7",
              "Rxc7",
              "a5",
              "Nc4",
              "Rb8+",
              "Kh7",
              "Bg5",
              "f6",
              "Bf4",
              "g5",
              "exf6",
              "gxf4",
              "fxe7",
              "Rxe7",
              "a6",
              "fxg3",
              "hxg3",
              "Kg6",
              "Rb7",
              "Kf6",
              "Ne5",
              "Nxe5",
              "dxe5+",
              "Kf7",
              "f4",
              "Ke8",
              "Kf2",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8",
              "Rb8+",
              "Kc7",
              "Rb7+",
              "Kd8"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Kf1",
              "g4",
              "Bc6",
              "Kh2",
              "Kf2",
              "Ne5",
              "Bd5",
              "g3+",
              "Kf1",
              "Ng4",
              "Ke2",
              "Nf6",
              "Bc6",
              "Nh5",
              "Kf1",
              "Nf4",
              "Bf3",
              "Nd3",
              "Be4",
              "Nf2",
              "Bc6",
              "Ng4",
              "Ke2",
              "Ne5",
              "Be4",
              "Kg1",
              "Ke3",
              "Kf1",
              "Kf4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "c6",
              "Nc3",
              "Nf6",
              "Nf3",
              "e6",
              "Bg5",
              "Nbd7",
              "e3",
              "Qa5",
              "Nd2",
              "Bb4",
              "Qc2",
              "O-O",
              "Be2",
              "dxc4",
              "Bxf6",
              "Nxf6",
              "Nxc4",
              "Qc7",
              "O-O",
              "b6",
              "Rac1",
              "Bb7",
              "a3",
              "Be7",
              "b4",
              "Rac8",
              "Qb3",
              "Qb8",
              "Rfd1",
              "Rfd8",
              "Ne5",
              "c5",
              "bxc5",
              "bxc5",
              "Rb1",
              "cxd4",
              "exd4",
              "Ba8",
              "Qxb8",
              "Rxb8",
              "Rxb8",
              "Rxb8",
              "Nb5",
              "a6",
              "Nc7",
              "Bb7",
              "Rb1",
              "Bd6",
              "Nxa6",
              "Bxa6",
              "Rxb8+",
              "Bxb8",
              "Bxa6",
              "Bxe5",
              "dxe5",
              "Nd7",
              "f4",
              "Nc5",
              "Bb5",
              "Kf8",
              "Kf2",
              "Ke7",
              "Ke3",
              "Kd8",
              "Kd4",
              "Nb7",
              "a4",
              "Kc7",
              "Be8",
              "Nd8",
              "Kc5",
              "Nb7+",
              "Kb5",
              "Nd8",
              "a5",
              "Kb7",
              "a6+",
              "Ka7",
              "g4",
              "h6",
              "h4",
              "g6",
              "g5",
              "hxg5",
              "hxg5",
              "Ka8",
              "Kb6",
              "Kb8",
              "a7+",
              "Ka8",
              "Ka6",
              "Nb7",
              "Bc6",
              "f5",
              "Bxb7#",
              "f5",
              "exf6",
              "e5",
              "f7",
              "exf4",
              "f8=Q#",
              "f3",
              "Qc8#",
              "f2",
              "Qa8#",
              "f1=Q",
              "Bxf1#",
              "f4",
              "Qb7#",
              "f3",
              "Qc8#",
              "f2",
              "Qb7#",
              "f1=Q",
              "Bxf1#",
              "f4",
              "Qc8#",
              "f3",
              "Qb7#",
              "f2",
              "Qc8#",
              "f1=Q",
              "Bxf1#",
              "f4",
              "Qc8#",
              "f3",
              "Qb7#",
              "f2",
              "Qc8#",
              "f1=Q",
              "Bxf1#",
              "f4",
              "Qc8#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 115,
            "completion_tokens": 640,
            "total_tokens": 755,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 347,
          "total_completion_tokens": 1920,
          "total_tokens": 2267
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 116,
            "completion_tokens": 640,
            "total_tokens": 756,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 115,
            "completion_tokens": 640,
            "total_tokens": 755,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/3p4/3P4/8/PPP1PPPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 5,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf3",
          "aggressive_uci": "g1f3",
          "aggressive_response": "3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. dxc5 Qxd1 8. Rxd1 Bxc5 9. Nbd2 O-O 10. Nb3 Be7 11. Be2 Nc6 12. Bd2 e5 13. Bc3 e4 14. Nfd4 Nxd4 15. Bxd4 Be6 16. Nc5 Bxc5 17. Bxc5 Rfd8 18. Bd4 Rac8 19. Bc3 Nd5 20. Ba5 b6 21. Be1 a5 22. Rac1 Rxc1 23. Rxc1 Rc8 24. Rxc8+ Bxc8 25. Bc4 Be6 26. Bxd5 Bxd5 27. a3 f5 28. Bc3 Kf7 29. Bd4 b5 30. Bb6 a4 31. Bd4 g6 32. Kf1 Ke6 33. Ke1 Bc4 34. Kd2 Kd5 35. Kc3 Bf1 36. g3 Bc4 37. Kb4 Bf1 38. Ka5 Kc4 39. Kb6 Kb3 40. Kc6 Kc2 41. Kd6 Kd2 42. Ke6 Ke2 43. Kf6 Kxf2 44. Kg7 Kg2 45. Kxh7 Kxh2 46. Kxg6 Bh3 47. Be5 Kg2 48. Kf6 Kf3 49. Bf4 Ke2 50. Ke5 Kd3 51. Kd5 Kc2 52. Be5 Kd3 53. Bd4 Kc2 54. Kc5 Bf1 55. Kd5 Kd3 56. Ke5 Bh3 57. Kf4 Kc2 58. Kg5 Kd3 59. Kh4 Bg4 60. Kg5 Kc2 61. Kf4 Kd3 62. Ke5 Kc2 63. Kd5 Kd3 64. Kc5 Bh3 65. Kxb5 Bf1 66. Kxa4 Kc4 67. b3+ Kd5 68. Kb4 Kc6 69. a4 Bd3 70. Kc3 Be2 71. b4 Bd1 72. b5+ Kb7 73. Kb4 Be2 74. a5 Bd3 75. a6+ Ka8 76. Ka5 Bc4 77. b6 Bd5 78. b7+ Bxb7 79. axb7+ Kxb7 80. Kb5 Kc7 81. K",
          "aggressive_tokens": {
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf6",
            "e3",
            "e6",
            "Bxc4",
            "c5",
            "O-O",
            "a6",
            "dxc5",
            "Qxd1",
            "Rxd1",
            "Bxc5",
            "Nbd2",
            "O-O",
            "Nb3",
            "Be7",
            "Be2",
            "Nc6",
            "Bd2",
            "e5",
            "Bc3",
            "e4",
            "Nfd4",
            "Nxd4",
            "Bxd4",
            "Be6",
            "Nc5",
            "Bxc5",
            "Bxc5",
            "Rfd8",
            "Bd4",
            "Rac8",
            "Bc3",
            "Nd5",
            "Ba5",
            "b6",
            "Be1",
            "a5",
            "Rac1",
            "Rxc1",
            "Rxc1",
            "Rc8",
            "Rxc8+",
            "Bxc8",
            "Bc4",
            "Be6",
            "Bxd5",
            "Bxd5",
            "a3",
            "f5",
            "Bc3",
            "Kf7",
            "Bd4",
            "b5",
            "Bb6",
            "a4",
            "Bd4",
            "g6",
            "Kf1",
            "Ke6",
            "Ke1",
            "Bc4",
            "Kd2",
            "Kd5",
            "Kc3",
            "Bf1",
            "g3",
            "Bc4",
            "Kb4",
            "Bf1",
            "Ka5",
            "Kc4",
            "Kb6",
            "Kb3",
            "Kc6",
            "Kc2",
            "Kd6",
            "Kd2",
            "Ke6",
            "Ke2",
            "Kf6",
            "Kxf2",
            "Kg7",
            "Kg2",
            "Kxh7",
            "Kxh2",
            "Kxg6",
            "Bh3",
            "Be5",
            "Kg2",
            "Kf6",
            "Kf3",
            "Bf4",
            "Ke2",
            "Ke5",
            "Kd3",
            "Kd5",
            "Kc2",
            "Be5",
            "Kd3",
            "Bd4",
            "Kc2",
            "Kc5",
            "Bf1",
            "Kd5",
            "Kd3",
            "Ke5",
            "Bh3",
            "Kf4",
            "Kc2",
            "Kg5",
            "Kd3",
            "Kh4",
            "Bg4",
            "Kg5",
            "Kc2",
            "Kf4",
            "Kd3",
            "Ke5",
            "Kc2",
            "Kd5",
            "Kd3",
            "Kc5",
            "Bh3",
            "Kxb5",
            "Bf1",
            "Kxa4",
            "Kc4",
            "b3+",
            "Kd5",
            "Kb4",
            "Kc6",
            "a4",
            "Bd3",
            "Kc3",
            "Be2",
            "b4",
            "Bd1",
            "b5+",
            "Kb7",
            "Kb4",
            "Be2",
            "a5",
            "Bd3",
            "a6+",
            "Ka8",
            "Ka5",
            "Bc4",
            "b6",
            "Bd5",
            "b7+",
            "Bxb7",
            "axb7+",
            "Kxb7",
            "Kb5",
            "Kc7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf3",
          "positional_uci": "g1f3",
          "positional_response": "3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. dxc5 Qxd1 8. Rxd1 Bxc5 9. b3 O-O 10. Bb2 b5 11. Be2 Bb7 12. Nbd2 Nbd7 13. Rac1 Rac8 14. a4 bxa4 15. bxa4 Nb6 16. a5 Na4 17. Bxf6 gxf6 18. Nb3 Bb4 19. Rxc8 Rxc8 20. Rd7 Bd5 21. Nfd4 Nc5 22. Nxc5 Rxc5 23. h3 Rxa5 24. Bh5 e5 25. Nf5 Be6 26. Rd8+ Bf8 27. Ne7+ Kg7 28. Nc6 Ra1+ 29. Kh2 a5 30. Ra8 a4 31. Nd8 Bd5 32. Ra7 a3 33. Nxf7 Bxf7 34. Bxf7 Kh6 35. Bg8 Bg7 36. g4 Kg6 37. h4 h6 38. f4 exf4 39. exf4 f5 40. h5+ Kf6 41. Rf7+ Ke6 42. Rxg7+ Kd6 43. gxf5 a2 44. Ra7 Rf1 45. Rxa2 Rxf4 46. Be6 Ke5 47. Kg3 Rb4 48. Ra5+ Kf6 49. Ra8 Kg5 50. Rg8+ Kxh5 51. Bf7# 1-0 1. d4 d5 2. c4 dxc4 3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. dxc5 Qxd1 8. Rxd1 Bxc5 9. b3 O-O 10. Bb2 b5 11. Be2 Bb7 12. Nbd2 Nbd7 13. Rac1 Rac8 14. a4 bxa4 15. bxa4 Nb6 16. a5 Na4 17. Bxf6 gxf6 18. Nb3 Bb4 19. Rxc8 Rxc8 20. Rd7 Bd5 21. Nfd4 Nc5 22. Nxc5 Rxc5 23. h3 Rxa5 24. Bh5 e5 25. Nf5 Be6 26. Rd8+ Bf8 27. Ne7+ Kg7 28. Nc6 Ra1+ 29. Kh2 a5 30. Ra8 a",
          "positional_tokens": {
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf6",
            "e3",
            "e6",
            "Bxc4",
            "c5",
            "O-O",
            "a6",
            "dxc5",
            "Qxd1",
            "Rxd1",
            "Bxc5",
            "b3",
            "O-O",
            "Bb2",
            "b5",
            "Be2",
            "Bb7",
            "Nbd2",
            "Nbd7",
            "Rac1",
            "Rac8",
            "a4",
            "bxa4",
            "bxa4",
            "Nb6",
            "a5",
            "Na4",
            "Bxf6",
            "gxf6",
            "Nb3",
            "Bb4",
            "Rxc8",
            "Rxc8",
            "Rd7",
            "Bd5",
            "Nfd4",
            "Nc5",
            "Nxc5",
            "Rxc5",
            "h3",
            "Rxa5",
            "Bh5",
            "e5",
            "Nf5",
            "Be6",
            "Rd8+",
            "Bf8",
            "Ne7+",
            "Kg7",
            "Nc6",
            "Ra1+",
            "Kh2",
            "a5",
            "Ra8"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf3",
          "neutral_uci": "g1f3",
          "neutral_response": "1. d4 d5 2. c4 dxc4 3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. Qe2 b5 8. Bb3 Bb7 9. Rd1 Nbd7 10. Nc3 Qb8 11. d5 c4 12. dxe6 fxe6 13. Bc2 Bd6 14. h3 O-O 15. Ng5 Nc5 16. b4 cxb3 17. axb3 h6 18. b4 hxg5 19. bxc5 Bxc5 20. Bb3 Qe5 21. Bb2 Bd6 22. g3 Qf5 23. Rxd6 Qxh3 24. Bxe6+ Qxe6 25. Rxe6 Kf7 26. Rxf6+ gxf6 27. Qh5+ Kg7 28. Rd1 Rf7 29. Nd5 Rh8 30. Qxg5+ Kf8 31. Ba3+ Ke8 32. Nxf6+ Rxf6 33. Qe5+ Kf7 34. Rd7+ Kg6 35. Rxb7 Rh5 36. Qe4+ Rhf5 37. g4 1-0 3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6. O-O a6 7. Qe2 b5 8. Bb3 Bb7 9. Rd1 Nbd7 10. Nc3 Qb8 11. d5 c4 12. dxe6 fxe6 13. Bc2 Bd6 14. h3 O-O 15. Ng5 Nc5 16. b4 cxb3 17. axb3 h6 18. b4 hxg5 19. bxc5 Bxc5 20. Bb3 Qe5 21. Bb2 Bd6 22. g3 Qf5 23. Rxd6 Qxh3 24. Bxe6+ Qxe6 25. Rxe6 Kf7 26. Rxf6+ gxf6 27. Qh5+ Kg7 28. Rd1 Rf7 29. Nd5 Rh8 30. Qxg5+ Kf8 31. Ba3+ Ke8 32. Nxf6+ Rxf6 33. Qe5+ Kf7 34. Rd7+ Kg6 35. Rxb7 Rh5 36. Qe4+ Rhf5 37. g4 1-0 3. Nf3 Nf6 4. e3 e6 5. Bxc4 c5 6.",
          "neutral_tokens": {
            "prompt_tokens": 123,
            "completion_tokens": 640,
            "total_tokens": 763,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf6",
            "e3",
            "e6",
            "Bxc4",
            "c5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g1f3",
          "positional_uci": "g1f3",
          "neutral_uci": "g1f3",
          "consensus_move": "g1f3",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf6",
              "e3",
              "e6",
              "Bxc4",
              "c5",
              "O-O",
              "a6",
              "dxc5",
              "Qxd1",
              "Rxd1",
              "Bxc5",
              "Nbd2",
              "O-O",
              "Nb3",
              "Be7",
              "Be2",
              "Nc6",
              "Bd2",
              "e5",
              "Bc3",
              "e4",
              "Nfd4",
              "Nxd4",
              "Bxd4",
              "Be6",
              "Nc5",
              "Bxc5",
              "Bxc5",
              "Rfd8",
              "Bd4",
              "Rac8",
              "Bc3",
              "Nd5",
              "Ba5",
              "b6",
              "Be1",
              "a5",
              "Rac1",
              "Rxc1",
              "Rxc1",
              "Rc8",
              "Rxc8+",
              "Bxc8",
              "Bc4",
              "Be6",
              "Bxd5",
              "Bxd5",
              "a3",
              "f5",
              "Bc3",
              "Kf7",
              "Bd4",
              "b5",
              "Bb6",
              "a4",
              "Bd4",
              "g6",
              "Kf1",
              "Ke6",
              "Ke1",
              "Bc4",
              "Kd2",
              "Kd5",
              "Kc3",
              "Bf1",
              "g3",
              "Bc4",
              "Kb4",
              "Bf1",
              "Ka5",
              "Kc4",
              "Kb6",
              "Kb3",
              "Kc6",
              "Kc2",
              "Kd6",
              "Kd2",
              "Ke6",
              "Ke2",
              "Kf6",
              "Kxf2",
              "Kg7",
              "Kg2",
              "Kxh7",
              "Kxh2",
              "Kxg6",
              "Bh3",
              "Be5",
              "Kg2",
              "Kf6",
              "Kf3",
              "Bf4",
              "Ke2",
              "Ke5",
              "Kd3",
              "Kd5",
              "Kc2",
              "Be5",
              "Kd3",
              "Bd4",
              "Kc2",
              "Kc5",
              "Bf1",
              "Kd5",
              "Kd3",
              "Ke5",
              "Bh3",
              "Kf4",
              "Kc2",
              "Kg5",
              "Kd3",
              "Kh4",
              "Bg4",
              "Kg5",
              "Kc2",
              "Kf4",
              "Kd3",
              "Ke5",
              "Kc2",
              "Kd5",
              "Kd3",
              "Kc5",
              "Bh3",
              "Kxb5",
              "Bf1",
              "Kxa4",
              "Kc4",
              "b3+",
              "Kd5",
              "Kb4",
              "Kc6",
              "a4",
              "Bd3",
              "Kc3",
              "Be2",
              "b4",
              "Bd1",
              "b5+",
              "Kb7",
              "Kb4",
              "Be2",
              "a5",
              "Bd3",
              "a6+",
              "Ka8",
              "Ka5",
              "Bc4",
              "b6",
              "Bd5",
              "b7+",
              "Bxb7",
              "axb7+",
              "Kxb7",
              "Kb5",
              "Kc7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf6",
              "e3",
              "e6",
              "Bxc4",
              "c5",
              "O-O",
              "a6",
              "dxc5",
              "Qxd1",
              "Rxd1",
              "Bxc5",
              "b3",
              "O-O",
              "Bb2",
              "b5",
              "Be2",
              "Bb7",
              "Nbd2",
              "Nbd7",
              "Rac1",
              "Rac8",
              "a4",
              "bxa4",
              "bxa4",
              "Nb6",
              "a5",
              "Na4",
              "Bxf6",
              "gxf6",
              "Nb3",
              "Bb4",
              "Rxc8",
              "Rxc8",
              "Rd7",
              "Bd5",
              "Nfd4",
              "Nc5",
              "Nxc5",
              "Rxc5",
              "h3",
              "Rxa5",
              "Bh5",
              "e5",
              "Nf5",
              "Be6",
              "Rd8+",
              "Bf8",
              "Ne7+",
              "Kg7",
              "Nc6",
              "Ra1+",
              "Kh2",
              "a5",
              "Ra8"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf6",
              "e3",
              "e6",
              "Bxc4",
              "c5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 123,
            "completion_tokens": 640,
            "total_tokens": 763,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 371,
          "total_completion_tokens": 1920,
          "total_tokens": 2291
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 124,
            "completion_tokens": 640,
            "total_tokens": 764,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 123,
            "completion_tokens": 640,
            "total_tokens": 763,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/ppp1pppp/8/8/2pP4/8/PP2PPPP/RNBQKBNR w KQkq - 0 3"
    },
    {
      "turn": 7,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc3",
          "aggressive_uci": "b1c3",
          "aggressive_response": "4. Nc3 Nf6 5. e4 Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Ne5 14. Qe2 Nc6 15. e5 Nd5 16. Nxd5 exd5 17. f4 f5 18. Nf3 Na5 19. Bc2 Nc4 20. Ng5 Qe7 21. h4 h6 22. h5 Be8 23. gxf5 hxg5 24. fxg5 Nxe3 25. Qxe3 c5 26. f6 Qc7 27. Qd3 gxf6 28. gxf6 Kh8 29. Rdg1 c4 30. Qg3 Qf7 31. Qg7+ Qxg7 32. fxg7+ Kg8 33. Bh7+ Kxh7 34. gxf8=Q Bxf8 35. Rf1 Bh6+ 36. Kc2 Ra7 37. Rf6 Bg7 38. Rd6 Bf7 39. h6 Bf8 40. Rf6 Bg6+ 41. Kc3 b4+ 42. Kd2 Be7 43. Re6 Bg5+ 44. Ke2 Be4 45. Rg1 Bxh6 46. Rg3 Rg7 47. Rh3 Rg6 48. Re7+ Kg8 49. Re8+ Kf7 50. Rh8 Bg7 51. R8h7 Rg2+ 52. Kf1 Rxb2 53. Rg3 Bxh7 54. Rf3+ Ke6 55. Rh3 Bd3+ 56. Ke1 Rxa2 57. Rg3 Bh6 58. Rh3 Bd2+ 59. Kd1 Bc3 60. Rh6+ Kf5 61. Rf6+ Ke4 62. Rf4+ Kxf4 63. e6 Ra1# e4 d5 Nc6",
          "aggressive_tokens": {
            "prompt_tokens": 133,
            "completion_tokens": 519,
            "total_tokens": 652,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "e4",
            "Bg4",
            "Be3",
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O",
            "O-O-O",
            "a6",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Nd2",
            "b5",
            "Bb3",
            "Ne5",
            "Qe2",
            "Nc6",
            "e5",
            "Nd5",
            "Nxd5",
            "exd5",
            "f4",
            "f5",
            "Nf3",
            "Na5",
            "Bc2",
            "Nc4",
            "Ng5",
            "Qe7",
            "h4",
            "h6",
            "h5",
            "Be8",
            "gxf5",
            "hxg5",
            "fxg5",
            "Nxe3",
            "Qxe3",
            "c5",
            "f6",
            "Qc7",
            "Qd3",
            "gxf6",
            "gxf6",
            "Kh8",
            "Rdg1",
            "c4",
            "Qg3",
            "Qf7",
            "Qg7+",
            "Qxg7",
            "fxg7+",
            "Kg8",
            "Bh7+",
            "Kxh7",
            "gxf8=Q",
            "Bxf8",
            "Rf1",
            "Bh6+",
            "Kc2",
            "Ra7",
            "Rf6",
            "Bg7",
            "Rd6",
            "Bf7",
            "h6",
            "Bf8",
            "Rf6",
            "Bg6+",
            "Kc3",
            "b4+",
            "Kd2",
            "Be7",
            "Re6",
            "Bg5+",
            "Ke2",
            "Be4",
            "Rg1",
            "Bxh6",
            "Rg3",
            "Rg7",
            "Rh3",
            "Rg6",
            "Re7+",
            "Kg8",
            "Re8+",
            "Kf7",
            "Rh8",
            "Bg7",
            "R8h7",
            "Rg2+",
            "Kf1",
            "Rxb2",
            "Rg3",
            "Bxh7",
            "Rf3+",
            "Ke6",
            "Rh3",
            "Bd3+",
            "Ke1",
            "Rxa2",
            "Rg3",
            "Bh6",
            "Rh3",
            "Bd2+",
            "Kd1",
            "Bc3",
            "Rh6+",
            "Kf5",
            "Rf6+",
            "Ke4",
            "Rf4+",
            "Kxf4",
            "e6",
            "Ra1#",
            "e4",
            "d5",
            "Nc6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e3",
          "positional_uci": "e2e3",
          "positional_response": "4. e3 e5 5. Bxc4 exd4 6. exd4 Bb4+ 7. Nc3 Nf6 8. O-O O-O 9. Bg5 Bg4 10. Nd5 Be7 11. Nxe7+ Qxe7 12. Re1 Qd6 13. d5 Ne5 14. Be2 Bxf3 15. Bxf3 Nxf3+ 16. Qxf3 Qxd5 17. Qg3 Ne4 18. Rxe4 Qxe4 19. Bh6 Qg6 20. Qxg6 hxg6 21. Bf4 c6 22. Rd1 Rad8 23. Re1 Rfe8 24. Be3 f5 25. g3 g5 26. Rc1 g4 27. Bxa7 Ra8 28. Bd4 Rxa2 29. Rc5 g6 30. Rc3 Rxb2 31. Ra3 Rd2 32. Be3 Rd1+ 33. Kg2 b5 34. Ra6 Rc8 35. Ra7 c5 36. Bh6 c4 37. Rg7+ Kh8 38. Rxg6 c3 39. Rb6 c2 40. Rxb5 c1=Q 41. Bxc1 Rcxc1 42. Rxf5 Rg1# 0-1 d4 d5 c4 dxc4 Nf3 Nc6 e3 e5 Bxc4 exd4 exd4 Bb4+ Nc3 Nf6 O-O O-O Bg5 Bg4 Nd5 Be7 Nxe7+ Qxe7 Re1 Qd6 d5 Ne5 Be2 Bxf3 Bxf3 Nxf3+ Qxf3 Qxd5 Qg3 Ne4 Bf4 Nxg3 hxg3 Qd4 Be3 Qxb2 Rab1 Qxa2 Rxb7 Rab8 Rxc7 Rb1 Rxb1 Qxb1+ Kh2 Qf5 Bxa7 Qh5+ Kg1 Rd8 f3 Rd1+ Kf2 Qa5 Be3 Qe1# 0-1 d4 d5 c4 dxc4 Nf3 Nc6 e3 e5 Bxc4 exd4 exd4 Bb4+ Nc3 Nf6 O-O O-O Bg5 Bg4 Nd5 Be7 Nxe7+ Qxe7 Re1 Qd6 d5 Ne5 Be2 Bxf3 Bxf3 Nxf3+ Qxf3 Qxd5 Qg3 Ne4 Bf4 Nxg3 hxg3 Qd4 Be3 Qxb2 Rab1 Qxa2 Rxb7 Rab8 Rxc7 Rb1 Rxb1 Qxb1+ Kh2 Qf5 Bxa7 Qh5+ Kg1 Rd8 f3 Rd1",
          "positional_tokens": {
            "prompt_tokens": 133,
            "completion_tokens": 640,
            "total_tokens": 773,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "e5",
            "Bxc4",
            "exd4",
            "exd4",
            "Bb4+",
            "Nc3",
            "Nf6",
            "O-O",
            "O-O",
            "Bg5",
            "Bg4",
            "Nd5",
            "Be7",
            "Nxe7+",
            "Qxe7",
            "Re1",
            "Qd6",
            "d5",
            "Ne5",
            "Be2",
            "Bxf3",
            "Bxf3",
            "Nxf3+",
            "Qxf3",
            "Qxd5",
            "Qg3",
            "Ne4",
            "Bf4",
            "Nxg3",
            "hxg3",
            "Qd4",
            "Be3",
            "Qxb2",
            "Rab1",
            "Qxa2",
            "Rxb7",
            "Rab8",
            "Rxc7",
            "Rb1",
            "Rxb1",
            "Qxb1+",
            "Kh2",
            "Qf5",
            "Bxa7",
            "Qh5+",
            "Kg1",
            "Rd8",
            "f3",
            "Rd1"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc3",
          "neutral_uci": "b1c3",
          "neutral_response": "4. Nc3 Nf6 5. e4 Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Ne5 14. Qc2 Nc6 15. f4 Nxd4 16. Bxd4 Qxd4 17. f5 exf5 18. exf5 Bxf5 19. gxf5 c5 20. Nde4 Qe5 21. Nxf6+ Qxf6 22. Nd5 Qh6+ 23. Kb1 c4 24. Nxb4 cxb3 25. Qxb3 a5 26. Nd5 a4 27. Qxb5 Rfb8 28. Qc5 a3 29. b3 Qh4 30. Qd4 Qg3 31. Rhg1 Qxh3 32. Qxg7# 1-0 Nf6 Nxe4 Qxe4",
          "neutral_tokens": {
            "prompt_tokens": 132,
            "completion_tokens": 254,
            "total_tokens": 386,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "e4",
            "Bg4",
            "Be3",
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O",
            "O-O-O",
            "a6",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Nd2",
            "b5",
            "Bb3",
            "Ne5",
            "Qc2",
            "Nc6",
            "f4",
            "Nxd4",
            "Bxd4",
            "Qxd4",
            "f5",
            "exf5",
            "exf5",
            "Bxf5",
            "gxf5",
            "c5",
            "Nde4",
            "Qe5",
            "Nxf6+",
            "Qxf6",
            "Nd5",
            "Qh6+",
            "Kb1",
            "c4",
            "Nxb4",
            "cxb3",
            "Qxb3",
            "a5",
            "Nd5",
            "a4",
            "Qxb5",
            "Rfb8",
            "Qc5",
            "a3",
            "b3",
            "Qh4",
            "Qd4",
            "Qg3",
            "Rhg1",
            "Qxh3",
            "Qxg7#",
            "Nf6",
            "Nxe4",
            "Qxe4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b1c3",
          "positional_uci": "e2e3",
          "neutral_uci": "b1c3",
          "consensus_move": "b1c3",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf6",
              "e4",
              "Bg4",
              "Be3",
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O",
              "O-O-O",
              "a6",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Nd2",
              "b5",
              "Bb3",
              "Ne5",
              "Qe2",
              "Nc6",
              "e5",
              "Nd5",
              "Nxd5",
              "exd5",
              "f4",
              "f5",
              "Nf3",
              "Na5",
              "Bc2",
              "Nc4",
              "Ng5",
              "Qe7",
              "h4",
              "h6",
              "h5",
              "Be8",
              "gxf5",
              "hxg5",
              "fxg5",
              "Nxe3",
              "Qxe3",
              "c5",
              "f6",
              "Qc7",
              "Qd3",
              "gxf6",
              "gxf6",
              "Kh8",
              "Rdg1",
              "c4",
              "Qg3",
              "Qf7",
              "Qg7+",
              "Qxg7",
              "fxg7+",
              "Kg8",
              "Bh7+",
              "Kxh7",
              "gxf8=Q",
              "Bxf8",
              "Rf1",
              "Bh6+",
              "Kc2",
              "Ra7",
              "Rf6",
              "Bg7",
              "Rd6",
              "Bf7",
              "h6",
              "Bf8",
              "Rf6",
              "Bg6+",
              "Kc3",
              "b4+",
              "Kd2",
              "Be7",
              "Re6",
              "Bg5+",
              "Ke2",
              "Be4",
              "Rg1",
              "Bxh6",
              "Rg3",
              "Rg7",
              "Rh3",
              "Rg6",
              "Re7+",
              "Kg8",
              "Re8+",
              "Kf7",
              "Rh8",
              "Bg7",
              "R8h7",
              "Rg2+",
              "Kf1",
              "Rxb2",
              "Rg3",
              "Bxh7",
              "Rf3+",
              "Ke6",
              "Rh3",
              "Bd3+",
              "Ke1",
              "Rxa2",
              "Rg3",
              "Bh6",
              "Rh3",
              "Bd2+",
              "Kd1",
              "Bc3",
              "Rh6+",
              "Kf5",
              "Rf6+",
              "Ke4",
              "Rf4+",
              "Kxf4",
              "e6",
              "Ra1#",
              "e4",
              "d5",
              "Nc6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Bxc4",
              "exd4",
              "exd4",
              "Bb4+",
              "Nc3",
              "Nf6",
              "O-O",
              "O-O",
              "Bg5",
              "Bg4",
              "Nd5",
              "Be7",
              "Nxe7+",
              "Qxe7",
              "Re1",
              "Qd6",
              "d5",
              "Ne5",
              "Be2",
              "Bxf3",
              "Bxf3",
              "Nxf3+",
              "Qxf3",
              "Qxd5",
              "Qg3",
              "Ne4",
              "Bf4",
              "Nxg3",
              "hxg3",
              "Qd4",
              "Be3",
              "Qxb2",
              "Rab1",
              "Qxa2",
              "Rxb7",
              "Rab8",
              "Rxc7",
              "Rb1",
              "Rxb1",
              "Qxb1+",
              "Kh2",
              "Qf5",
              "Bxa7",
              "Qh5+",
              "Kg1",
              "Rd8",
              "f3",
              "Rd1"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf6",
              "e4",
              "Bg4",
              "Be3",
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O",
              "O-O-O",
              "a6",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Nd2",
              "b5",
              "Bb3",
              "Ne5",
              "Qc2",
              "Nc6",
              "f4",
              "Nxd4",
              "Bxd4",
              "Qxd4",
              "f5",
              "exf5",
              "exf5",
              "Bxf5",
              "gxf5",
              "c5",
              "Nde4",
              "Qe5",
              "Nxf6+",
              "Qxf6",
              "Nd5",
              "Qh6+",
              "Kb1",
              "c4",
              "Nxb4",
              "cxb3",
              "Qxb3",
              "a5",
              "Nd5",
              "a4",
              "Qxb5",
              "Rfb8",
              "Qc5",
              "a3",
              "b3",
              "Qh4",
              "Qd4",
              "Qg3",
              "Rhg1",
              "Qxh3",
              "Qxg7#",
              "Nf6",
              "Nxe4",
              "Qxe4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 133,
            "completion_tokens": 519,
            "total_tokens": 652,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 133,
            "completion_tokens": 640,
            "total_tokens": 773,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 132,
            "completion_tokens": 254,
            "total_tokens": 386,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 398,
          "total_completion_tokens": 1413,
          "total_tokens": 1811
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 133,
            "completion_tokens": 519,
            "total_tokens": 652,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 133,
            "completion_tokens": 640,
            "total_tokens": 773,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 132,
            "completion_tokens": 254,
            "total_tokens": 386,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkbnr/ppp1pppp/2n5/8/2pP4/5N2/PP2PPPP/RNBQKB1R w KQkq - 2 4"
    },
    {
      "turn": 9,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": "e2e4",
          "aggressive_response": "5. e4 Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Na5 14. Bc2 c5 15. dxc5 Qxd3 16. Bxd3 Rfd8 17. Bc2 Bxc3 18. bxc3 Rac8 19. f3 Nd7 20. Nb3 Nc4 21. Bf2 Nde5 22. Rxd8+ Rxd8 23. Rd1 Rxd1+ 24. Bxd1 Nd3+ 25. Kc2 Nxf2 26. c6 Nd6 27. c7 Nc8 28. Nc5 Nxd1 29. Kxd1 Kf8 30. Nxa6 Ke7 31. Kd2 Kd6 32. Kd3 f5 33. gxf5 exf5 34. Kd4 fxe4 35. fxe4 Bf7 36. e5+ Kc6 37. Nb4+ Kxc7 38. Kc5 Bc4 39. a4 Bf1 40. axb5 Bxh3 41. Nd5+ Kb7 42. c4 Be6 43. Nf4 Bf5 44. Kd5 g5 45. Ne2 h5 46. c5 h4 47. Nd4 Ne7+ 48. Kd6 h3 49. Nf3 g4 50. Nh2 g3 51. Nf3 h2 52. Kxe7 h1=Q 53. Nd4 Qh7+ 54. Kd6 g2 55. c6+ Kb6 56. e6 g1=Q 57. Nxf5 Qxf5 58. e7 Qd4# e4 d5 c4 dxc4 Nf3 Nc6 Nc3 Nf6 e4 Bg4 Be3 e6 Bxc4 Bb4 Qd3 O-O-O h3 Bg6 g4 Bg6 Nd2 b5 Bb3 Na5 Bc2 c5 dxc5 Qxd3 Bxd3 Rfd8 Bc2 Bxc3 bxc3 Rac8 f3 Nd7 Nb3 Nc4 Bf2 Nde5 Rd1 Rxd1+ Bxd1 Nd3+ Kc2 Nxf2 c6 Nd6 Nxa6 Kd6 c7 Kxc7 Kd3 Kb6 Nb4 Kc5 a3 Nc4 Nc2 Ne5+ Ke3 Nc4+ Kd3 Ne5+ Ke3 Nc4+ Kd3 Ne5+ Ke3 Nc4+ K",
          "aggressive_tokens": {
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bg4",
            "Be3",
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O-O",
            "h3",
            "Bg6",
            "g4",
            "Bg6",
            "Nd2",
            "b5",
            "Bb3",
            "Na5",
            "Bc2",
            "c5",
            "dxc5",
            "Qxd3",
            "Bxd3",
            "Rfd8",
            "Bc2",
            "Bxc3",
            "bxc3",
            "Rac8",
            "f3",
            "Nd7",
            "Nb3",
            "Nc4",
            "Bf2",
            "Nde5",
            "Rd1",
            "Rxd1+",
            "Bxd1",
            "Nd3+",
            "Kc2",
            "Nxf2",
            "c6",
            "Nd6",
            "Nxa6",
            "Kd6",
            "c7",
            "Kxc7",
            "Kd3",
            "Kb6",
            "Nb4",
            "Kc5",
            "a3",
            "Nc4",
            "Nc2",
            "Ne5+",
            "Ke3",
            "Nc4+",
            "Kd3",
            "Ne5+",
            "Ke3",
            "Nc4+",
            "Kd3",
            "Ne5+",
            "Ke3",
            "Nc4+"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "5. e4 Bg4 6. Be3 e6 7. Bxc4 Bb4 8. Qd3 O-O 9. O-O-O a6 10. h3 Bh5 11. g4 Bg6 12. Nd2 b5 13. Bb3 Na5 14. Bc2 c5 15. f4 c4 16. Qe2 Bxc3 17. bxc3 Qe7 18. f5 Qa3+ 19. Kb1 Qxc3 20. fxg6 Qb4+ 21. Ka1 Qc3+ 22. Kb1 Qb4+ 23. Ka1 Qc3+ 24. Kb1 Qb4+ 25. Ka1 Qc3+ 26. Kb1 Qb4+ 27. Ka1 Qc3+ 28. Kb1 Qb4+ 29. Ka1 Qc3+ 30. Kb1 Qb4+ 31. Ka1 Qc3+ 32. Kb1 Qb4+ 33. Ka1 Qc3+ 34. Kb1 Qb4+ 35. Ka1 Qc3+ 36. Kb1 Qb4+ 37. Ka1 Qc3+ 38. Kb1 Qb4+ 39. Ka1 Qc3+ 40. Kb1 Qb4+ 41. Ka1 Qc3+ 42. Kb1 Qb4+ 43. Ka1 Qc3+ 44. Kb1 Qb4+ 45. Ka1 Qc3+ 46. Kb1 Qb4+ 47. Ka1 Qc3+ 48. Kb1 Qb4+ 49. Ka1 Qc3+ 50. Kb1 Qb4+ 51. Ka1 Qc3+ 52. Kb1 Qb4+ 53. Ka1 Qc3+ 54. Kb1 Qb4+ 55. Ka1 Qc3+ 56. Kb1 Qb4+ 57. Ka1 Qc3+ 58. Kb1 Qb4+ 59. Ka1 Qc3+ 60. Kb1 Qb4+ 61. Ka1 Qc3+ 62. Kb1 Qb4+ 63. Ka1 Qc3+ 64. Kb1 Qb4+ 65. Ka1 Qc3+ 66. Kb1 Qb4+ 67. Ka1 Qc3+ 68. Kb1 Qb4+ 69. Ka1 Qc3+ 70. Kb1 Qb4+ 71. Ka1 Qc3+ 72. Kb1 Qb4+ 73. Ka1 Qc3+ 74. Kb1 Qb4+ 75. Ka1 Qc3+ 76. Kb1 Qb4+ 77. Ka1 Q",
          "positional_tokens": {
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bg4",
            "Be3",
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O",
            "O-O-O",
            "a6",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Nd2",
            "b5",
            "Bb3",
            "Na5",
            "Bc2",
            "c5",
            "f4",
            "c4",
            "Qe2",
            "Bxc3",
            "bxc3",
            "Qe7",
            "f5",
            "Qa3+",
            "Kb1",
            "Qxc3",
            "fxg6",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1",
            "Qc3+",
            "Kb1",
            "Qb4+",
            "Ka1"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": "e2e4",
          "neutral_response": "5. e4 e6 6. Bxc4 Bb4 7. Qd3 O-O 8. O-O Re8 9. Bg5 h6 10. Bh4 g5 11. Bg3 g4 12. Ne5 Nxd4 13. Rad1 c5 14. Bh4 Kg7 15. Nxg4 Nxg4 16. Bxd8 Rxd8 17. Qg3 h5 18. h3 f5 19. hxg4 hxg4 20. Qe5+ Kg6 21. exf5+ exf5 22. Nd5 Be6 23. Qf6+ Kh5 24. Nf4# 1-0\n25. Qg6+ Kh4 26. Qh6#",
          "neutral_tokens": {
            "prompt_tokens": 141,
            "completion_tokens": 180,
            "total_tokens": 321,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e6",
            "Bxc4",
            "Bb4",
            "Qd3",
            "O-O",
            "O-O",
            "Re8",
            "Bg5",
            "h6",
            "Bh4",
            "g5",
            "Bg3",
            "g4",
            "Ne5",
            "Nxd4",
            "Rad1",
            "c5",
            "Bh4",
            "Kg7",
            "Nxg4",
            "Nxg4",
            "Bxd8",
            "Rxd8",
            "Qg3",
            "h5",
            "h3",
            "f5",
            "hxg4",
            "hxg4",
            "Qe5+",
            "Kg6",
            "exf5+",
            "exf5",
            "Nd5",
            "Be6",
            "Qf6+",
            "Kh5",
            "Nf4#",
            "Qg6+",
            "Kh4",
            "Qh6#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e2e4",
          "positional_uci": "e2e4",
          "neutral_uci": "e2e4",
          "consensus_move": "e2e4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bg4",
              "Be3",
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O-O",
              "h3",
              "Bg6",
              "g4",
              "Bg6",
              "Nd2",
              "b5",
              "Bb3",
              "Na5",
              "Bc2",
              "c5",
              "dxc5",
              "Qxd3",
              "Bxd3",
              "Rfd8",
              "Bc2",
              "Bxc3",
              "bxc3",
              "Rac8",
              "f3",
              "Nd7",
              "Nb3",
              "Nc4",
              "Bf2",
              "Nde5",
              "Rd1",
              "Rxd1+",
              "Bxd1",
              "Nd3+",
              "Kc2",
              "Nxf2",
              "c6",
              "Nd6",
              "Nxa6",
              "Kd6",
              "c7",
              "Kxc7",
              "Kd3",
              "Kb6",
              "Nb4",
              "Kc5",
              "a3",
              "Nc4",
              "Nc2",
              "Ne5+",
              "Ke3",
              "Nc4+",
              "Kd3",
              "Ne5+",
              "Ke3",
              "Nc4+",
              "Kd3",
              "Ne5+",
              "Ke3",
              "Nc4+"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bg4",
              "Be3",
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O",
              "O-O-O",
              "a6",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Nd2",
              "b5",
              "Bb3",
              "Na5",
              "Bc2",
              "c5",
              "f4",
              "c4",
              "Qe2",
              "Bxc3",
              "bxc3",
              "Qe7",
              "f5",
              "Qa3+",
              "Kb1",
              "Qxc3",
              "fxg6",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1",
              "Qc3+",
              "Kb1",
              "Qb4+",
              "Ka1"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e6",
              "Bxc4",
              "Bb4",
              "Qd3",
              "O-O",
              "O-O",
              "Re8",
              "Bg5",
              "h6",
              "Bh4",
              "g5",
              "Bg3",
              "g4",
              "Ne5",
              "Nxd4",
              "Rad1",
              "c5",
              "Bh4",
              "Kg7",
              "Nxg4",
              "Nxg4",
              "Bxd8",
              "Rxd8",
              "Qg3",
              "h5",
              "h3",
              "f5",
              "hxg4",
              "hxg4",
              "Qe5+",
              "Kg6",
              "exf5+",
              "exf5",
              "Nd5",
              "Be6",
              "Qf6+",
              "Kh5",
              "Nf4#",
              "Qg6+",
              "Kh4",
              "Qh6#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 141,
            "completion_tokens": 180,
            "total_tokens": 321,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 425,
          "total_completion_tokens": 1460,
          "total_tokens": 1885
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 142,
            "completion_tokens": 640,
            "total_tokens": 782,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 141,
            "completion_tokens": 180,
            "total_tokens": 321,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp1pppp/2n2n2/8/2pP4/2N2N2/PP2PPPP/R1BQKB1R w KQkq - 4 5"
    },
    {
      "turn": 11,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d5",
          "aggressive_uci": "d4d5",
          "aggressive_response": "6. d5 Ne5 7. Bf4 Nfd7 8. Bxe5 Nxe5 9. Bxc4 Nxc4 10. Qa4+ Qd7 11. Qxc4 Bxf3 12. gxf3 g6 13. Qd4 f6 14. O-O-O Bg7 15. f4 O-O 16. h4 f5 17. e5 c6 18. h5 cxd5 19. hxg6 hxg6 20. Nxd5 Rac8+ 21. Kb1 Qe6 22. Qxa7 Ra8 23. Qxb7 Rfb8 24. Qxe7 Qa6 25. Nb4 Qc4 26. Rd7 Qe4+ 27. Nc2 Qxh1+ 28. Ne1 Qxe1+ 29. Kc2 Qxf2+ 30. Kd3 Qf3+ 31. Kc2 Qe2+ 32. Kc3 Qxb2+ 33. Kd3 Ra3+ 34. Kc4 Qb5+ 35. Kd4 Rd3# Rb1 Qd3+ Ke1 Qe3+ Kd1 Qe2+ Kc1 Qc2#",
          "aggressive_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 291,
            "total_tokens": 441,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Ne5",
            "Bf4",
            "Nfd7",
            "Bxe5",
            "Nxe5",
            "Bxc4",
            "Nxc4",
            "Qa4+",
            "Qd7",
            "Qxc4",
            "Bxf3",
            "gxf3",
            "g6",
            "Qd4",
            "f6",
            "O-O-O",
            "Bg7",
            "f4",
            "O-O",
            "h4",
            "f5",
            "e5",
            "c6",
            "h5",
            "cxd5",
            "hxg6",
            "hxg6",
            "Nxd5",
            "Rac8+",
            "Kb1",
            "Qe6",
            "Qxa7",
            "Ra8",
            "Qxb7",
            "Rfb8",
            "Qxe7",
            "Qa6",
            "Nb4",
            "Qc4",
            "Rd7",
            "Qe4+",
            "Nc2",
            "Qxh1+",
            "Ne1",
            "Qxe1+",
            "Kc2",
            "Qxf2+",
            "Kd3",
            "Qf3+",
            "Kc2",
            "Qe2+",
            "Kc3",
            "Qxb2+",
            "Kd3",
            "Ra3+",
            "Kc4",
            "Qb5+",
            "Kd4",
            "Rd3#",
            "Rb1",
            "Qd3+",
            "Ke1",
            "Qe3+",
            "Kd1",
            "Qe2+",
            "Kc1",
            "Qc2#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d5",
          "positional_uci": "d4d5",
          "positional_response": "6. d5 Ne5 7. Bf4 Nfd7 8. Bxe5 Nxe5 9. Qa4+ Bd7 10. Qd1 Nxf3+ 11. Qxf3 b5 12. a4 a6 13. Be2 e6 14. O-O Bd6 15. dxe6 fxe6 16. axb5 axb5 17. Rxa8 Qxa8 18. Qh5+ g6 19. Qh6 Bf8 20. Qf4 Bd6 21. Qh6 Bf8 22. Qf4 Bd6 23. Qh6 Bf8 24. Qf4 Bd6 25. Qh6 Bf8 26. Qf4 Bd6 27. Qh6 Bf8 28. Qf4 Bd6 29. Qh6 Bf8 30. Qf4 Bd6 31. Qh6 Bf8 32. Qf4 Bd6 33. Qh6 Bf8 34. Qf4 Bd6 35. Qh6 Bf8 36. Qf4 Bd6 37. Qh6 Bf8 38. Qf4 Bd6 39. Qh6 Bf8 40. Qf4 Bd6 41. Qh6 Bf8 42. Qf4 Bd6 43. Qh6 Bf8 44. Qf4 Bd6 45. Qh6 Bf8 46. Qf4 Bd6 47. Qh6 Bf8 48. Qf4 Bd6 49. Qh6 Bf8 50. Qf4 Bd6 51. Qh6 Bf8 52. Qf4 Bd6 53. Qh6 Bf8 54. Qf4 Bd6 55. Qh6 Bf8 56. Qf4 Bd6 57. Qh6 Bf8 58. Qf4 Bd6 59. Qh6 Bf8 60. Qf4 Bd6 61. Qh6 Bf8 62. Qf4 Bd6 63. Qh6 Bf8 64. Qf4 Bd6 65. Qh6 Bf8 66. Qf4 Bd6 67. Qh6 Bf8 68. Qf4 Bd6 69. Qh6 Bf8 70. Qf4 Bd6 71. Qh6 Bf8 72. Qf4 Bd6 73. Qh6 Bf8 74. Qf4 Bd6 75. Qh6 Bf8 76. Qf4 Bd6 77. Qh6 Bf8 78. Qf4 Bd6 79. Qh6 Bf8 80. Qf4 Bd6 81. Q",
          "positional_tokens": {
            "prompt_tokens": 150,
            "completion_tokens": 640,
            "total_tokens": 790,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Ne5",
            "Bf4",
            "Nfd7",
            "Bxe5",
            "Nxe5",
            "Qa4+",
            "Bd7",
            "Qd1",
            "Nxf3+",
            "Qxf3",
            "b5",
            "a4",
            "a6",
            "Be2",
            "e6",
            "O-O",
            "Bd6",
            "dxe6",
            "fxe6",
            "axb5",
            "axb5",
            "Rxa8",
            "Qxa8",
            "Qh5+",
            "g6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6",
            "Qh6",
            "Bf8",
            "Qf4",
            "Bd6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d5",
          "neutral_uci": "d4d5",
          "neutral_response": "6. d5 Ne5 7. Bf4 Nfd7 8. Bxe5 Nxe5 9. Qa4+ Bd7 10. Qd1 Nxf3+ 11. Qxf3 b5 12. a4 a6 13. Be2 e6 14. O-O Bc5 15. dxe6 fxe6 16. Qh5+ g6 17. Qxc5 Qe7 18. Qxe7+ Kxe7 19. axb5 axb5 20. Rxa8 Rxa8 21. f4 c6 22. e5 c5 23. Ne4 Ra2 24. Rb1 Bc6 25. Nxc5 Bd5 26. Bf3 Bxf3 27. gxf3 b4 28. Ne4 Kd7 29. Nd6 c3 30. bxc3 bxc3 31. Rc1 c2 32. Kf2 Kc6 33. Ke2 Kd5 34. Kd2 Ra3 35. Rxc2 Rxf3 36. Rc4 Rf2+ 37. Ke3 Rxh2 38. Rd4+ Kc5 39. Ne4+ Kc6 40. Rd6+ Kc7 41. Rxe6 Rh3+ 42. Kd4 Rf3 43. Rf6 Kd7 44. Rf7+ Ke6 45. Ng5# Kd7 46. Nxf3 Ke6 47. Ng5# Kd7 48. Nxh7 Ke6 49. Ng5# Kd7 50. Rf6 Ke7 51. Rxg6 Kd7 52. Rd6+ Ke7 53. f5 Ke8 54. f6 Kf8 55. Rd8# Kxd8 56. e6 Ke8 57. Kd5 Kf8 58. Kd6 Ke8 59. f7+ Kf8 60. e7+ Kg7 61. f8=Q+ Kg6 62. e8=Q+ Kxg5 63. Qe5+ Kg4 64. Qff4+ Kh3 65. Qg3# Kxg3 66. Qf5 Kg2 67. Qf4 Kh3 68. Qg5 Kh2 69. Qg4 Kh1 70. Ke5 Kh2 71. Kf4 Kh1 72. Kf3 Kh2 73. Qg2# Kh1 74. Qg3# Kh1 75. Qg2# Kh1 76. Qg4# Kh2 77. Qg3+ Kh1 78. Qg2# Kh1 79. Qg4# Kh2 80. Qg3+ Kh1",
          "neutral_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Ne5",
            "Bf4",
            "Nfd7",
            "Bxe5",
            "Nxe5",
            "Qa4+",
            "Bd7",
            "Qd1",
            "Nxf3+",
            "Qxf3",
            "b5",
            "a4",
            "a6",
            "Be2",
            "e6",
            "O-O",
            "Bc5",
            "dxe6",
            "fxe6",
            "Qh5+",
            "g6",
            "Qxc5",
            "Qe7",
            "Qxe7+",
            "Kxe7",
            "axb5",
            "axb5",
            "Rxa8",
            "Rxa8",
            "f4",
            "c6",
            "e5",
            "c5",
            "Ne4",
            "Ra2",
            "Rb1",
            "Bc6",
            "Nxc5",
            "Bd5",
            "Bf3",
            "Bxf3",
            "gxf3",
            "b4",
            "Ne4",
            "Kd7",
            "Nd6",
            "c3",
            "bxc3",
            "bxc3",
            "Rc1",
            "c2",
            "Kf2",
            "Kc6",
            "Ke2",
            "Kd5",
            "Kd2",
            "Ra3",
            "Rxc2",
            "Rxf3",
            "Rc4",
            "Rf2+",
            "Ke3",
            "Rxh2",
            "Rd4+",
            "Kc5",
            "Ne4+",
            "Kc6",
            "Rd6+",
            "Kc7",
            "Rxe6",
            "Rh3+",
            "Kd4",
            "Rf3",
            "Rf6",
            "Kd7",
            "Rf7+",
            "Ke6",
            "Ng5#",
            "Kd7",
            "Nxf3",
            "Ke6",
            "Ng5#",
            "Kd7",
            "Nxh7",
            "Ke6",
            "Ng5#",
            "Kd7",
            "Rf6",
            "Ke7",
            "Rxg6",
            "Kd7",
            "Rd6+",
            "Ke7",
            "f5",
            "Ke8",
            "f6",
            "Kf8",
            "Rd8#",
            "Kxd8",
            "e6",
            "Ke8",
            "Kd5",
            "Kf8",
            "Kd6",
            "Ke8",
            "f7+",
            "Kf8",
            "e7+",
            "Kg7",
            "f8=Q+",
            "Kg6",
            "e8=Q+",
            "Kxg5",
            "Qe5+",
            "Kg4",
            "Qff4+",
            "Kh3",
            "Qg3#",
            "Kxg3",
            "Qf5",
            "Kg2",
            "Qf4",
            "Kh3",
            "Qg5",
            "Kh2",
            "Qg4",
            "Kh1",
            "Ke5",
            "Kh2",
            "Kf4",
            "Kh1",
            "Kf3",
            "Kh2",
            "Qg2#",
            "Kh1",
            "Qg3#",
            "Kh1",
            "Qg2#",
            "Kh1",
            "Qg4#",
            "Kh2",
            "Qg3+",
            "Kh1",
            "Qg2#",
            "Kh1",
            "Qg4#",
            "Kh2",
            "Qg3+",
            "Kh1"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d4d5",
          "positional_uci": "d4d5",
          "neutral_uci": "d4d5",
          "consensus_move": "d4d5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Ne5",
              "Bf4",
              "Nfd7",
              "Bxe5",
              "Nxe5",
              "Bxc4",
              "Nxc4",
              "Qa4+",
              "Qd7",
              "Qxc4",
              "Bxf3",
              "gxf3",
              "g6",
              "Qd4",
              "f6",
              "O-O-O",
              "Bg7",
              "f4",
              "O-O",
              "h4",
              "f5",
              "e5",
              "c6",
              "h5",
              "cxd5",
              "hxg6",
              "hxg6",
              "Nxd5",
              "Rac8+",
              "Kb1",
              "Qe6",
              "Qxa7",
              "Ra8",
              "Qxb7",
              "Rfb8",
              "Qxe7",
              "Qa6",
              "Nb4",
              "Qc4",
              "Rd7",
              "Qe4+",
              "Nc2",
              "Qxh1+",
              "Ne1",
              "Qxe1+",
              "Kc2",
              "Qxf2+",
              "Kd3",
              "Qf3+",
              "Kc2",
              "Qe2+",
              "Kc3",
              "Qxb2+",
              "Kd3",
              "Ra3+",
              "Kc4",
              "Qb5+",
              "Kd4",
              "Rd3#",
              "Rb1",
              "Qd3+",
              "Ke1",
              "Qe3+",
              "Kd1",
              "Qe2+",
              "Kc1",
              "Qc2#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Ne5",
              "Bf4",
              "Nfd7",
              "Bxe5",
              "Nxe5",
              "Qa4+",
              "Bd7",
              "Qd1",
              "Nxf3+",
              "Qxf3",
              "b5",
              "a4",
              "a6",
              "Be2",
              "e6",
              "O-O",
              "Bd6",
              "dxe6",
              "fxe6",
              "axb5",
              "axb5",
              "Rxa8",
              "Qxa8",
              "Qh5+",
              "g6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6",
              "Qh6",
              "Bf8",
              "Qf4",
              "Bd6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Ne5",
              "Bf4",
              "Nfd7",
              "Bxe5",
              "Nxe5",
              "Qa4+",
              "Bd7",
              "Qd1",
              "Nxf3+",
              "Qxf3",
              "b5",
              "a4",
              "a6",
              "Be2",
              "e6",
              "O-O",
              "Bc5",
              "dxe6",
              "fxe6",
              "Qh5+",
              "g6",
              "Qxc5",
              "Qe7",
              "Qxe7+",
              "Kxe7",
              "axb5",
              "axb5",
              "Rxa8",
              "Rxa8",
              "f4",
              "c6",
              "e5",
              "c5",
              "Ne4",
              "Ra2",
              "Rb1",
              "Bc6",
              "Nxc5",
              "Bd5",
              "Bf3",
              "Bxf3",
              "gxf3",
              "b4",
              "Ne4",
              "Kd7",
              "Nd6",
              "c3",
              "bxc3",
              "bxc3",
              "Rc1",
              "c2",
              "Kf2",
              "Kc6",
              "Ke2",
              "Kd5",
              "Kd2",
              "Ra3",
              "Rxc2",
              "Rxf3",
              "Rc4",
              "Rf2+",
              "Ke3",
              "Rxh2",
              "Rd4+",
              "Kc5",
              "Ne4+",
              "Kc6",
              "Rd6+",
              "Kc7",
              "Rxe6",
              "Rh3+",
              "Kd4",
              "Rf3",
              "Rf6",
              "Kd7",
              "Rf7+",
              "Ke6",
              "Ng5#",
              "Kd7",
              "Nxf3",
              "Ke6",
              "Ng5#",
              "Kd7",
              "Nxh7",
              "Ke6",
              "Ng5#",
              "Kd7",
              "Rf6",
              "Ke7",
              "Rxg6",
              "Kd7",
              "Rd6+",
              "Ke7",
              "f5",
              "Ke8",
              "f6",
              "Kf8",
              "Rd8#",
              "Kxd8",
              "e6",
              "Ke8",
              "Kd5",
              "Kf8",
              "Kd6",
              "Ke8",
              "f7+",
              "Kf8",
              "e7+",
              "Kg7",
              "f8=Q+",
              "Kg6",
              "e8=Q+",
              "Kxg5",
              "Qe5+",
              "Kg4",
              "Qff4+",
              "Kh3",
              "Qg3#",
              "Kxg3",
              "Qf5",
              "Kg2",
              "Qf4",
              "Kh3",
              "Qg5",
              "Kh2",
              "Qg4",
              "Kh1",
              "Ke5",
              "Kh2",
              "Kf4",
              "Kh1",
              "Kf3",
              "Kh2",
              "Qg2#",
              "Kh1",
              "Qg3#",
              "Kh1",
              "Qg2#",
              "Kh1",
              "Qg4#",
              "Kh2",
              "Qg3+",
              "Kh1",
              "Qg2#",
              "Kh1",
              "Qg4#",
              "Kh2",
              "Qg3+",
              "Kh1"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 150,
            "completion_tokens": 291,
            "total_tokens": 441,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 150,
            "completion_tokens": 640,
            "total_tokens": 790,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 449,
          "total_completion_tokens": 1571,
          "total_tokens": 2020
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 150,
            "completion_tokens": 291,
            "total_tokens": 441,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 150,
            "completion_tokens": 640,
            "total_tokens": 790,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/2n2n2/8/2pPP1b1/2N2N2/PP3PPP/R1BQKB1R w KQkq - 1 6"
    },
    {
      "turn": 13,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qa4+",
          "aggressive_uci": "d1a4",
          "aggressive_response": "7. Qa4+ c6 8. b4 cxb3 9. axb3 b6 10. dxc6 Qc7 11. Nb5 Qxc6 12. Nc7+ Kd8 13. Nxa8 Qxa4 14. Rxa4 Bd7 15. Rd4 Nxb3 16. Rd1 Nxc1 17. Rxc1 Nxe4 18. Ne5 Be6 19. Nc7 Nc5 20. Nxe6+ Nxe6 21. Nxf7+ Ke8 22. Nxh8 g6 23. Bb5+ Kd8 24. Nf7# c5 bxc5 bxc5 *\n25. Rxc5 Nxc5 26. O-O a6 27. Rd1+ Kc7 28. Rc1 Kb6 29. Bc4 e6 30. Ng5 Bh6 31. Rb1+ Kc6 32. Nxe6 Nxe6 33. Bxe6 a5 34. Bg8 a4 35. Bxh7 a3 36. Bxg6 Bg7 37. Bf7 Kc5 38. h4 Kd4 39. g4 Ke4 40. Kg2 Kf4 41. Rb4+ Ke5 42. f4+ Kf6 43. Ba2 Bf8 44. Rb6+ Kg7 45. h5 Bc5 46. Rb7+ Kh6 47. Kh3 Be3 48. Kh4 Bxf4 49. Bb1 Bg5+ 50. Kh3 Bf6 51. Rh7+ Kg5 52. Rf7 Bb2 53. Rf5+ Kh6 54. Kh4 Bc1 55. g5+ Kg7 56. h6+ Kh8 57. Rf8# g5+ Kh7 Kh5 *",
          "aggressive_tokens": {
            "prompt_tokens": 157,
            "completion_tokens": 446,
            "total_tokens": 603,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "c6",
            "b4",
            "cxb3",
            "axb3",
            "b6",
            "dxc6",
            "Qc7",
            "Nb5",
            "Qxc6",
            "Nc7+",
            "Kd8",
            "Nxa8",
            "Qxa4",
            "Rxa4",
            "Bd7",
            "Rd4",
            "Nxb3",
            "Rd1",
            "Nxc1",
            "Rxc1",
            "Nxe4",
            "Ne5",
            "Be6",
            "Nc7",
            "Nc5",
            "Nxe6+",
            "Nxe6",
            "Nxf7+",
            "Ke8",
            "Nxh8",
            "g6",
            "Bb5+",
            "Kd8",
            "Nf7#",
            "c5",
            "bxc5",
            "bxc5",
            "Rxc5",
            "Nxc5",
            "O-O",
            "a6",
            "Rd1+",
            "Kc7",
            "Rc1",
            "Kb6",
            "Bc4",
            "e6",
            "Ng5",
            "Bh6",
            "Rb1+",
            "Kc6",
            "Nxe6",
            "Nxe6",
            "Bxe6",
            "a5",
            "Bg8",
            "a4",
            "Bxh7",
            "a3",
            "Bxg6",
            "Bg7",
            "Bf7",
            "Kc5",
            "h4",
            "Kd4",
            "g4",
            "Ke4",
            "Kg2",
            "Kf4",
            "Rb4+",
            "Ke5",
            "f4+",
            "Kf6",
            "Ba2",
            "Bf8",
            "Rb6+",
            "Kg7",
            "h5",
            "Bc5",
            "Rb7+",
            "Kh6",
            "Kh3",
            "Be3",
            "Kh4",
            "Bxf4",
            "Bb1",
            "Bg5+",
            "Kh3",
            "Bf6",
            "Rh7+",
            "Kg5",
            "Rf7",
            "Bb2",
            "Rf5+",
            "Kh6",
            "Kh4",
            "Bc1",
            "g5+",
            "Kg7",
            "h6+",
            "Kh8",
            "Rf8#",
            "g5+",
            "Kh7",
            "Kh5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qa4+",
          "positional_uci": "d1a4",
          "positional_response": "7. Qa4+ c6 8. b4 cxb3 9. axb3 Bxf3 10. gxf3 e6 11. Qxa5 Qxa5 12. Rxa5 Bb4 13. Ra4 Bxc3+ 14. Bd2 Bxd2+ 15. Kxd2 exd5 16. exd5 Nxd5 17. Bc4 O-O-O 18. Rxa7 Nb6+ 19. Kc3 Nxc4 20. bxc4 Kc7 21. Rb1 Rb8 22. c5 Rhd8 23. Re1 Rd7 24. Re4 Rbd8 25. Rb4 Rd3+ 26. Kc2 Rd2+ 27. Kc1 Rd1+ 28. Kc2 R8d2+ 29. Kc3 Rd3+ 30. Kc2 R1d2+ 31. Kc1 Rd1+ 32. Kc2 R3d2+ 33. Kc3 Rd3+ 34. Kc2 R1d2+ 35. Kc1 Rd1+ 36. Kc2 R3d2+ 37. Kc3 Rd3+ 38. Kc2 R1d2+ 39. Kc1 Rd1+ 40. Kc2 R3d2+ 41. Kc3 Rd3+ 42. Kc2 R1d2+ 43. Kc1 Rd1+ 44. Kc2 R3d2+ 45. Kc3 Rd3+ 46. Kc2 R1d2+ 47. Kc1 Rd1+ 48. Kc2 R3d2+ 49. Kc3 Rd3+ 50. Kc2 R1d2+ 51. Kc1 Rd1+ 52. Kc2 R3d2+ 53. Kc3 Rd3+ 54. Kc2 R1d2+ 55. Kc1 Rd1+ 56. Kc2 R3d2+ 57. Kc3 Rd3+ 58. Kc2 R1d2+ 59. Kc1 Rd1+ 60. Kc2 R3d2+ 61. Kc3 Rd3+ 62. Kc2 R1d2+ 63. Kc1 Rd1+ 64. Kc2 R3d2+ 65. Kc3 Rd3+ 66. Kc2 R1d2+ 67. Kc1 Rd1+ 68. Kc2 R3d2+ 69. Kc3 Rd3+ 70. Kc2 R1d2+ 71. Kc1 Rd1+ 72. Kc2 R3d2+ 73. Kc3",
          "positional_tokens": {
            "prompt_tokens": 157,
            "completion_tokens": 640,
            "total_tokens": 797,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "c6",
            "b4",
            "cxb3",
            "axb3",
            "Bxf3",
            "gxf3",
            "e6",
            "Qxa5",
            "Qxa5",
            "Rxa5",
            "Bb4",
            "Ra4",
            "Bxc3+",
            "Bd2",
            "Bxd2+",
            "Kxd2",
            "exd5",
            "exd5",
            "Nxd5",
            "Bc4",
            "O-O-O",
            "Rxa7",
            "Nb6+",
            "Kc3",
            "Nxc4",
            "bxc4",
            "Kc7",
            "Rb1",
            "Rb8",
            "c5",
            "Rhd8",
            "Re1",
            "Rd7",
            "Re4",
            "Rbd8",
            "Rb4",
            "Rd3+",
            "Kc2",
            "Rd2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R8d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3",
            "Rd3+",
            "Kc2",
            "R1d2+",
            "Kc1",
            "Rd1+",
            "Kc2",
            "R3d2+",
            "Kc3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qa4+",
          "neutral_uci": "d1a4",
          "neutral_response": "7. Qa4+ c6 8. b4 cxb3 9. axb3 Bxf3 10. gxf3 b6 11. dxc6 Qc7 12. Nd5 Nxd5 13. exd5 e6 14. Bf4 Bd6 15. Bxd6 Qxd6 16. c7+ Ke7 17. Rc1 Rhc8 18. Ba6 Rxc7 19. Rxc7+ Qxc7 20. O-O Rd8 21. dxe6 fxe6 22. Qh4+ Kf7 23. Qxh7 Nxb3 24. Qh5+ Kg8 25. Re1 Nc5 26. Bc4 Qf7 27. Qg4 Rd6 28. Kh1 a5 29. Rg1 a4 30. f4 Ne4 31. Qf3 Nd2 32. Qa8+ Kh7 33. Qxa4 Qxf4 34. Qc2+ Kh8 35. Be2 Qe4+ 36. Qxe4 Nxe4 37. Kg2 Rd2 38. Kf3 Nc3 39. Bc4 Nd5 40. Rb1 Rd4 41. Bxd5 Rxd5 42. Rxb6 Re5 43. Kf4 Re2 44. Kf3 Re5 45. Kf4 Re2 46. Kf3 Re1 47. Rb4 Kh7 48. Re4 Rxe4 49. Kxe4 Kg6 50. Ke5 Kf7 51. f4 Ke7 52. h4 Kf7 53. h5 Ke7 54. f5 exf5 55. Kxf5 Kf7 56. Kg5 Kg8 57. Kg6 Kh8 58. h6 gxh6 59. Kxh6 * e5 60. Kg5 e4 61. Kf4 e3 62. Kxe3",
          "neutral_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 460,
            "total_tokens": 616,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "c6",
            "b4",
            "cxb3",
            "axb3",
            "Bxf3",
            "gxf3",
            "b6",
            "dxc6",
            "Qc7",
            "Nd5",
            "Nxd5",
            "exd5",
            "e6",
            "Bf4",
            "Bd6",
            "Bxd6",
            "Qxd6",
            "c7+",
            "Ke7",
            "Rc1",
            "Rhc8",
            "Ba6",
            "Rxc7",
            "Rxc7+",
            "Qxc7",
            "O-O",
            "Rd8",
            "dxe6",
            "fxe6",
            "Qh4+",
            "Kf7",
            "Qxh7",
            "Nxb3",
            "Qh5+",
            "Kg8",
            "Re1",
            "Nc5",
            "Bc4",
            "Qf7",
            "Qg4",
            "Rd6",
            "Kh1",
            "a5",
            "Rg1",
            "a4",
            "f4",
            "Ne4",
            "Qf3",
            "Nd2",
            "Qa8+",
            "Kh7",
            "Qxa4",
            "Qxf4",
            "Qc2+",
            "Kh8",
            "Be2",
            "Qe4+",
            "Qxe4",
            "Nxe4",
            "Kg2",
            "Rd2",
            "Kf3",
            "Nc3",
            "Bc4",
            "Nd5",
            "Rb1",
            "Rd4",
            "Bxd5",
            "Rxd5",
            "Rxb6",
            "Re5",
            "Kf4",
            "Re2",
            "Kf3",
            "Re5",
            "Kf4",
            "Re2",
            "Kf3",
            "Re1",
            "Rb4",
            "Kh7",
            "Re4",
            "Rxe4",
            "Kxe4",
            "Kg6",
            "Ke5",
            "Kf7",
            "f4",
            "Ke7",
            "h4",
            "Kf7",
            "h5",
            "Ke7",
            "f5",
            "exf5",
            "Kxf5",
            "Kf7",
            "Kg5",
            "Kg8",
            "Kg6",
            "Kh8",
            "h6",
            "gxh6",
            "Kxh6",
            "e5",
            "Kg5",
            "e4",
            "Kf4",
            "e3",
            "Kxe3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d1a4",
          "positional_uci": "d1a4",
          "neutral_uci": "d1a4",
          "consensus_move": "d1a4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "c6",
              "b4",
              "cxb3",
              "axb3",
              "b6",
              "dxc6",
              "Qc7",
              "Nb5",
              "Qxc6",
              "Nc7+",
              "Kd8",
              "Nxa8",
              "Qxa4",
              "Rxa4",
              "Bd7",
              "Rd4",
              "Nxb3",
              "Rd1",
              "Nxc1",
              "Rxc1",
              "Nxe4",
              "Ne5",
              "Be6",
              "Nc7",
              "Nc5",
              "Nxe6+",
              "Nxe6",
              "Nxf7+",
              "Ke8",
              "Nxh8",
              "g6",
              "Bb5+",
              "Kd8",
              "Nf7#",
              "c5",
              "bxc5",
              "bxc5",
              "Rxc5",
              "Nxc5",
              "O-O",
              "a6",
              "Rd1+",
              "Kc7",
              "Rc1",
              "Kb6",
              "Bc4",
              "e6",
              "Ng5",
              "Bh6",
              "Rb1+",
              "Kc6",
              "Nxe6",
              "Nxe6",
              "Bxe6",
              "a5",
              "Bg8",
              "a4",
              "Bxh7",
              "a3",
              "Bxg6",
              "Bg7",
              "Bf7",
              "Kc5",
              "h4",
              "Kd4",
              "g4",
              "Ke4",
              "Kg2",
              "Kf4",
              "Rb4+",
              "Ke5",
              "f4+",
              "Kf6",
              "Ba2",
              "Bf8",
              "Rb6+",
              "Kg7",
              "h5",
              "Bc5",
              "Rb7+",
              "Kh6",
              "Kh3",
              "Be3",
              "Kh4",
              "Bxf4",
              "Bb1",
              "Bg5+",
              "Kh3",
              "Bf6",
              "Rh7+",
              "Kg5",
              "Rf7",
              "Bb2",
              "Rf5+",
              "Kh6",
              "Kh4",
              "Bc1",
              "g5+",
              "Kg7",
              "h6+",
              "Kh8",
              "Rf8#",
              "g5+",
              "Kh7",
              "Kh5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "c6",
              "b4",
              "cxb3",
              "axb3",
              "Bxf3",
              "gxf3",
              "e6",
              "Qxa5",
              "Qxa5",
              "Rxa5",
              "Bb4",
              "Ra4",
              "Bxc3+",
              "Bd2",
              "Bxd2+",
              "Kxd2",
              "exd5",
              "exd5",
              "Nxd5",
              "Bc4",
              "O-O-O",
              "Rxa7",
              "Nb6+",
              "Kc3",
              "Nxc4",
              "bxc4",
              "Kc7",
              "Rb1",
              "Rb8",
              "c5",
              "Rhd8",
              "Re1",
              "Rd7",
              "Re4",
              "Rbd8",
              "Rb4",
              "Rd3+",
              "Kc2",
              "Rd2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R8d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3",
              "Rd3+",
              "Kc2",
              "R1d2+",
              "Kc1",
              "Rd1+",
              "Kc2",
              "R3d2+",
              "Kc3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "c6",
              "b4",
              "cxb3",
              "axb3",
              "Bxf3",
              "gxf3",
              "b6",
              "dxc6",
              "Qc7",
              "Nd5",
              "Nxd5",
              "exd5",
              "e6",
              "Bf4",
              "Bd6",
              "Bxd6",
              "Qxd6",
              "c7+",
              "Ke7",
              "Rc1",
              "Rhc8",
              "Ba6",
              "Rxc7",
              "Rxc7+",
              "Qxc7",
              "O-O",
              "Rd8",
              "dxe6",
              "fxe6",
              "Qh4+",
              "Kf7",
              "Qxh7",
              "Nxb3",
              "Qh5+",
              "Kg8",
              "Re1",
              "Nc5",
              "Bc4",
              "Qf7",
              "Qg4",
              "Rd6",
              "Kh1",
              "a5",
              "Rg1",
              "a4",
              "f4",
              "Ne4",
              "Qf3",
              "Nd2",
              "Qa8+",
              "Kh7",
              "Qxa4",
              "Qxf4",
              "Qc2+",
              "Kh8",
              "Be2",
              "Qe4+",
              "Qxe4",
              "Nxe4",
              "Kg2",
              "Rd2",
              "Kf3",
              "Nc3",
              "Bc4",
              "Nd5",
              "Rb1",
              "Rd4",
              "Bxd5",
              "Rxd5",
              "Rxb6",
              "Re5",
              "Kf4",
              "Re2",
              "Kf3",
              "Re5",
              "Kf4",
              "Re2",
              "Kf3",
              "Re1",
              "Rb4",
              "Kh7",
              "Re4",
              "Rxe4",
              "Kxe4",
              "Kg6",
              "Ke5",
              "Kf7",
              "f4",
              "Ke7",
              "h4",
              "Kf7",
              "h5",
              "Ke7",
              "f5",
              "exf5",
              "Kxf5",
              "Kf7",
              "Kg5",
              "Kg8",
              "Kg6",
              "Kh8",
              "h6",
              "gxh6",
              "Kxh6",
              "e5",
              "Kg5",
              "e4",
              "Kf4",
              "e3",
              "Kxe3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 157,
            "completion_tokens": 446,
            "total_tokens": 603,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 157,
            "completion_tokens": 640,
            "total_tokens": 797,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 156,
            "completion_tokens": 460,
            "total_tokens": 616,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 470,
          "total_completion_tokens": 1546,
          "total_tokens": 2016
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 157,
            "completion_tokens": 446,
            "total_tokens": 603,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 157,
            "completion_tokens": 640,
            "total_tokens": 797,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 156,
            "completion_tokens": 460,
            "total_tokens": 616,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/5n2/n2P4/2p1P1b1/2N2N2/PP3PPP/R1BQKB1R w KQkq - 1 7"
    },
    {
      "turn": 15,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "dxc6",
          "aggressive_uci": "d5c6",
          "aggressive_response": "8. dxc6 bxc6 9. Qxc6+ Nd7 10. Ne5 Be6 11. Bxc4 Bxc4 12. Nxc4 e6 13. Nb5 Bb4+ 14. Bd2 Bxd2+ 15. Nxd2 O-O 16. Qxc7 Qe7 17. O-O Rfc8 18. Qd6 Qe8 19. Nc7 Rxc7 20. Qxc7 Rc8 21. Qxa7 Rc2 22. Qd4 e5 23. Qd3 Rxb2 24. Rfb1 Rxb1+ 25. Rxb1 Nc5 26. Qb5 Qxb5 27. Rxb5 Nd7 28. Rd5 Nf8 29. Rxe5 f6 30. Re7 Ng6 31. Rb7 Ne5 32. a4 Nc6 33. Nc4 Nd4 34. a5 Nc6 35. a6 Kf8 36. a7 Nxa7 37. Rxa7 h6 38. Nd6 Kg8 39. Nf5 g6 40. Nxh6+ Kf8 41. Ng4 f5 42. exf5 gxf5 43. Nh6 f4 44. g3 fxg3 45. hxg3 Ke8 46. g4 Kf8 47. g5 Ke8 48. g6 Kf8 49. g7+ Ke8 50. g8=Q# e4 f5 g5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fxg5 fx",
          "aggressive_tokens": {
            "prompt_tokens": 167,
            "completion_tokens": 640,
            "total_tokens": 807,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "bxc6",
            "Qxc6+",
            "Nd7",
            "Ne5",
            "Be6",
            "Bxc4",
            "Bxc4",
            "Nxc4",
            "e6",
            "Nb5",
            "Bb4+",
            "Bd2",
            "Bxd2+",
            "Nxd2",
            "O-O",
            "Qxc7",
            "Qe7",
            "O-O",
            "Rfc8",
            "Qd6",
            "Qe8",
            "Nc7",
            "Rxc7",
            "Qxc7",
            "Rc8",
            "Qxa7",
            "Rc2",
            "Qd4",
            "e5",
            "Qd3",
            "Rxb2",
            "Rfb1",
            "Rxb1+",
            "Rxb1",
            "Nc5",
            "Qb5",
            "Qxb5",
            "Rxb5",
            "Nd7",
            "Rd5",
            "Nf8",
            "Rxe5",
            "f6",
            "Re7",
            "Ng6",
            "Rb7",
            "Ne5",
            "a4",
            "Nc6",
            "Nc4",
            "Nd4",
            "a5",
            "Nc6",
            "a6",
            "Kf8",
            "a7",
            "Nxa7",
            "Rxa7",
            "h6",
            "Nd6",
            "Kg8",
            "Nf5",
            "g6",
            "Nxh6+",
            "Kf8",
            "Ng4",
            "f5",
            "exf5",
            "gxf5",
            "Nh6",
            "f4",
            "g3",
            "fxg3",
            "hxg3",
            "Ke8",
            "g4",
            "Kf8",
            "g5",
            "Ke8",
            "g6",
            "Kf8",
            "g7+",
            "Ke8",
            "g8=Q#",
            "e4",
            "f5",
            "g5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5",
            "fxg5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "dxc6",
          "positional_uci": "d5c6",
          "positional_response": "8. dxc6 bxc6 9. Qxc6+ Nd7 10. Ne5 Be6 11. Bxc4 Bxc4 12. Nxc4 e6 13. Nb5 Bb4+ 14. Bd2 Bxd2+ 15. Nxd2 O-O 16. Qxc7 Qxc7 17. Nxc7 Rac8 18. Nb5 Rc2 19. b3 a6 20. Nd4 Rc3 21. O-O Rd3 22. N2f3 e5 23. Nc6 f6 24. Rfd1 Rxd1+ 25. Rxd1 Nc5 26. Nd2 Kf7 27. f3 Rc8 28. Nb4 a5 29. Nd5 Nd3 30. Nc4 Nb2 31. Nxb2 Rc2 32. Nc4 Rxa2 33. Rd2 Ra1+ 34. Kf2 a4 35. bxa4 Rxa4 36. Nd6+ Kg6 37. Ne7+ Kh6 38. Ndf5+ Kg5 39. h4+ Kh5 40. g4# Nxe7 41. Nxe7 Kxh4 42. Nf5+ Kg5 43. Nxg7 Kg6 44. Nf5 h5 45. gxh5+ Kxh5 46. Rd6 Kg5 47. Kg3 Ra1 48. Rd8 Rg1+ 49. Kf2 Ra1 50. Rg8+ Kh5 51. Ng3+ Kh6 52. Rf8 Kg7 53. Rc8 Ra2+ 54. Ke3 Ra3+ 55. Kf2 Ra2+ 56. Ne2 Kg6 57. Rc6 Kg5 58. Ke3 Ra3+ 59. Rc3 Ra1 60. Ng3 Re1+ 61. Kf2 Ra1 62. Rc8 Ra2+ 63. Ne2 Kg6 64. Rg8+ Kf7 65. Rg4 Ke6 66. f4 exf4 67. Rxf4 Ke5 68. Kf3 Ra3+ 69. Kg4 Ra2 70. Ng3 Ra8 71. Rf5+ Ke6 72. Kf4 Ra1 73. Nh5 Rf1+ 74. Kg4 Rg1+ 75. Ng3 Re1 76. Rf4 Ke5 77. Rf5+ Ke6 78. Kf4 Rg1 79. Nh5 Rf1+ 80. Kg4 Rg1+ 81. Ng3 Re1 82. Rf4 Ke5 83. Rf5+ Ke6 84. Kf4 Rg1 85. Nh5 Rf",
          "positional_tokens": {
            "prompt_tokens": 167,
            "completion_tokens": 640,
            "total_tokens": 807,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "bxc6",
            "Qxc6+",
            "Nd7",
            "Ne5",
            "Be6",
            "Bxc4",
            "Bxc4",
            "Nxc4",
            "e6",
            "Nb5",
            "Bb4+",
            "Bd2",
            "Bxd2+",
            "Nxd2",
            "O-O",
            "Qxc7",
            "Qxc7",
            "Nxc7",
            "Rac8",
            "Nb5",
            "Rc2",
            "b3",
            "a6",
            "Nd4",
            "Rc3",
            "O-O",
            "Rd3",
            "N2f3",
            "e5",
            "Nc6",
            "f6",
            "Rfd1",
            "Rxd1+",
            "Rxd1",
            "Nc5",
            "Nd2",
            "Kf7",
            "f3",
            "Rc8",
            "Nb4",
            "a5",
            "Nd5",
            "Nd3",
            "Nc4",
            "Nb2",
            "Nxb2",
            "Rc2",
            "Nc4",
            "Rxa2",
            "Rd2",
            "Ra1+",
            "Kf2",
            "a4",
            "bxa4",
            "Rxa4",
            "Nd6+",
            "Kg6",
            "Ne7+",
            "Kh6",
            "Ndf5+",
            "Kg5",
            "h4+",
            "Kh5",
            "g4#",
            "Nxe7",
            "Nxe7",
            "Kxh4",
            "Nf5+",
            "Kg5",
            "Nxg7",
            "Kg6",
            "Nf5",
            "h5",
            "gxh5+",
            "Kxh5",
            "Rd6",
            "Kg5",
            "Kg3",
            "Ra1",
            "Rd8",
            "Rg1+",
            "Kf2",
            "Ra1",
            "Rg8+",
            "Kh5",
            "Ng3+",
            "Kh6",
            "Rf8",
            "Kg7",
            "Rc8",
            "Ra2+",
            "Ke3",
            "Ra3+",
            "Kf2",
            "Ra2+",
            "Ne2",
            "Kg6",
            "Rc6",
            "Kg5",
            "Ke3",
            "Ra3+",
            "Rc3",
            "Ra1",
            "Ng3",
            "Re1+",
            "Kf2",
            "Ra1",
            "Rc8",
            "Ra2+",
            "Ne2",
            "Kg6",
            "Rg8+",
            "Kf7",
            "Rg4",
            "Ke6",
            "f4",
            "exf4",
            "Rxf4",
            "Ke5",
            "Kf3",
            "Ra3+",
            "Kg4",
            "Ra2",
            "Ng3",
            "Ra8",
            "Rf5+",
            "Ke6",
            "Kf4",
            "Ra1",
            "Nh5",
            "Rf1+",
            "Kg4",
            "Rg1+",
            "Ng3",
            "Re1",
            "Rf4",
            "Ke5",
            "Rf5+",
            "Ke6",
            "Kf4",
            "Rg1",
            "Nh5",
            "Rf1+",
            "Kg4",
            "Rg1+",
            "Ng3",
            "Re1",
            "Rf4",
            "Ke5",
            "Rf5+",
            "Ke6",
            "Kf4",
            "Rg1",
            "Nh5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "dxc6",
          "neutral_uci": "d5c6",
          "neutral_response": "8. dxc6 bxc6 9. Qxc6+ Bd7 10. Qxc4 e6 11. Bg5 Be7 12. Rd1 O-O 13. Ne5 Bd6 14. Nxd7 Qxd7 15. e5 Nd5 16. exd6 cxd6 17. Nxd5 exd5 18. Qxd5 Rfe8+ 19. Be2 Qg4 20. Be3 Qb4+ 21. Qd2 Qb7 22. O-O Rad8 23. Bf3 Qa6 24. a3 Re5 25. Bd4 Rf5 26. Be4 Rh5 27. Qf4 d5 28. Qg4 Qh6 29. Be3 g5 30. Rxd5 Rxd5 31. Bxd5 Rxh2 32. Qxg5+ Qxg5 33. Bxg5 Rh5 34. f4 h6 35. Bf3 hxg5 36. Bxh5 gxf4 37. Rxf4 Kg7 38. Rxf7+ Kh6 39. g4 Kg5 40. Rxa7 Kh4 41. b4 Kg3 42. b5 Kf4 43. b6 Ke5 44. b7 Kd6 45. b8=Q+ Kc6 46. Rc7+ Kd6 47. Qd8+ Ke6 48. Re7+ Kf6 49. Qf8+ Kg5 50. Rg7+ Kh4 51. Qf2+ Kh3 52. Qh2# 1-0 8. dxc6 bxc6 9. Qxc6+ Bd7 10. Qxc4 e6",
          "neutral_tokens": {
            "prompt_tokens": 166,
            "completion_tokens": 409,
            "total_tokens": 575,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "bxc6",
            "Qxc6+",
            "Bd7",
            "Qxc4",
            "e6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d5c6",
          "positional_uci": "d5c6",
          "neutral_uci": "d5c6",
          "consensus_move": "d5c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "bxc6",
              "Qxc6+",
              "Nd7",
              "Ne5",
              "Be6",
              "Bxc4",
              "Bxc4",
              "Nxc4",
              "e6",
              "Nb5",
              "Bb4+",
              "Bd2",
              "Bxd2+",
              "Nxd2",
              "O-O",
              "Qxc7",
              "Qe7",
              "O-O",
              "Rfc8",
              "Qd6",
              "Qe8",
              "Nc7",
              "Rxc7",
              "Qxc7",
              "Rc8",
              "Qxa7",
              "Rc2",
              "Qd4",
              "e5",
              "Qd3",
              "Rxb2",
              "Rfb1",
              "Rxb1+",
              "Rxb1",
              "Nc5",
              "Qb5",
              "Qxb5",
              "Rxb5",
              "Nd7",
              "Rd5",
              "Nf8",
              "Rxe5",
              "f6",
              "Re7",
              "Ng6",
              "Rb7",
              "Ne5",
              "a4",
              "Nc6",
              "Nc4",
              "Nd4",
              "a5",
              "Nc6",
              "a6",
              "Kf8",
              "a7",
              "Nxa7",
              "Rxa7",
              "h6",
              "Nd6",
              "Kg8",
              "Nf5",
              "g6",
              "Nxh6+",
              "Kf8",
              "Ng4",
              "f5",
              "exf5",
              "gxf5",
              "Nh6",
              "f4",
              "g3",
              "fxg3",
              "hxg3",
              "Ke8",
              "g4",
              "Kf8",
              "g5",
              "Ke8",
              "g6",
              "Kf8",
              "g7+",
              "Ke8",
              "g8=Q#",
              "e4",
              "f5",
              "g5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5",
              "fxg5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "bxc6",
              "Qxc6+",
              "Nd7",
              "Ne5",
              "Be6",
              "Bxc4",
              "Bxc4",
              "Nxc4",
              "e6",
              "Nb5",
              "Bb4+",
              "Bd2",
              "Bxd2+",
              "Nxd2",
              "O-O",
              "Qxc7",
              "Qxc7",
              "Nxc7",
              "Rac8",
              "Nb5",
              "Rc2",
              "b3",
              "a6",
              "Nd4",
              "Rc3",
              "O-O",
              "Rd3",
              "N2f3",
              "e5",
              "Nc6",
              "f6",
              "Rfd1",
              "Rxd1+",
              "Rxd1",
              "Nc5",
              "Nd2",
              "Kf7",
              "f3",
              "Rc8",
              "Nb4",
              "a5",
              "Nd5",
              "Nd3",
              "Nc4",
              "Nb2",
              "Nxb2",
              "Rc2",
              "Nc4",
              "Rxa2",
              "Rd2",
              "Ra1+",
              "Kf2",
              "a4",
              "bxa4",
              "Rxa4",
              "Nd6+",
              "Kg6",
              "Ne7+",
              "Kh6",
              "Ndf5+",
              "Kg5",
              "h4+",
              "Kh5",
              "g4#",
              "Nxe7",
              "Nxe7",
              "Kxh4",
              "Nf5+",
              "Kg5",
              "Nxg7",
              "Kg6",
              "Nf5",
              "h5",
              "gxh5+",
              "Kxh5",
              "Rd6",
              "Kg5",
              "Kg3",
              "Ra1",
              "Rd8",
              "Rg1+",
              "Kf2",
              "Ra1",
              "Rg8+",
              "Kh5",
              "Ng3+",
              "Kh6",
              "Rf8",
              "Kg7",
              "Rc8",
              "Ra2+",
              "Ke3",
              "Ra3+",
              "Kf2",
              "Ra2+",
              "Ne2",
              "Kg6",
              "Rc6",
              "Kg5",
              "Ke3",
              "Ra3+",
              "Rc3",
              "Ra1",
              "Ng3",
              "Re1+",
              "Kf2",
              "Ra1",
              "Rc8",
              "Ra2+",
              "Ne2",
              "Kg6",
              "Rg8+",
              "Kf7",
              "Rg4",
              "Ke6",
              "f4",
              "exf4",
              "Rxf4",
              "Ke5",
              "Kf3",
              "Ra3+",
              "Kg4",
              "Ra2",
              "Ng3",
              "Ra8",
              "Rf5+",
              "Ke6",
              "Kf4",
              "Ra1",
              "Nh5",
              "Rf1+",
              "Kg4",
              "Rg1+",
              "Ng3",
              "Re1",
              "Rf4",
              "Ke5",
              "Rf5+",
              "Ke6",
              "Kf4",
              "Rg1",
              "Nh5",
              "Rf1+",
              "Kg4",
              "Rg1+",
              "Ng3",
              "Re1",
              "Rf4",
              "Ke5",
              "Rf5+",
              "Ke6",
              "Kf4",
              "Rg1",
              "Nh5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "bxc6",
              "Qxc6+",
              "Bd7",
              "Qxc4",
              "e6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 167,
            "completion_tokens": 640,
            "total_tokens": 807,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 167,
            "completion_tokens": 640,
            "total_tokens": 807,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 166,
            "completion_tokens": 409,
            "total_tokens": 575,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 500,
          "total_completion_tokens": 1689,
          "total_tokens": 2189
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 167,
            "completion_tokens": 640,
            "total_tokens": 807,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 167,
            "completion_tokens": 640,
            "total_tokens": 807,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 166,
            "completion_tokens": 409,
            "total_tokens": 575,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r2qkb1r/ppp1pppp/2n2n2/3P4/Q1p1P1b1/2N2N2/PP3PPP/R1B1KB1R w KQkq - 3 8"
    },
    {
      "turn": 17,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bxc4",
          "aggressive_uci": "f1c4",
          "aggressive_response": "9. Bxc4 Bxf3 10. gxf3 e6 11. Bg5 Be7 12. Rd1 Qc8 13. Ba6 Qb8 14. Bb7 O-O 15.\nBxa8 Qxa8 16. e5 Nd5 17. Nxd5 Bxg5 18. Nxc7 Qb8 19. Rd7 Rd8 20. Na6 Qxe5+ 21.\nKf1 Rxd7 22. cxd7 Qd6 23. Kg2 h6 24. Rd1 Qe7 25. d8=Q+ Qxd8 26. Rxd8+ Bxd8 27.\nQe8+ Kh7 28. Qxd8 Kg6 29. Qd3+ f5 30. Nc7 Kf6 31. Qd6 Kg5 32. Nxe6+ Kh5 33.\nNxg7+ Kg5 34. f4+ Kg4 35. Qg6+ Kxf4 36. Qxf5# e4 Qg4+ Kf3 Qg6+ Ke2 Qg4+ Kd2 Qg6+ Kc3 Qg4+ Kb4 Qg6+ Ka5 Qg4+ b5#",
          "aggressive_tokens": {
            "prompt_tokens": 175,
            "completion_tokens": 288,
            "total_tokens": 463,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bxf3",
            "gxf3",
            "e6",
            "Bg5",
            "Be7",
            "Rd1",
            "Qc8",
            "Ba6",
            "Qb8",
            "Bb7",
            "O-O",
            "Bxa8",
            "Qxa8",
            "e5",
            "Nd5",
            "Nxd5",
            "Bxg5",
            "Nxc7",
            "Qb8",
            "Rd7",
            "Rd8",
            "Na6",
            "Qxe5+",
            "Kf1",
            "Rxd7",
            "cxd7",
            "Qd6",
            "Kg2",
            "h6",
            "Rd1",
            "Qe7",
            "d8=Q+",
            "Qxd8",
            "Rxd8+",
            "Bxd8",
            "Qe8+",
            "Kh7",
            "Qxd8",
            "Kg6",
            "Qd3+",
            "f5",
            "Nc7",
            "Kf6",
            "Qd6",
            "Kg5",
            "Nxe6+",
            "Kh5",
            "Nxg7+",
            "Kg5",
            "f4+",
            "Kg4",
            "Qg6+",
            "Kxf4",
            "Qxf5#",
            "e4",
            "Qg4+",
            "Kf3",
            "Qg6+",
            "Ke2",
            "Qg4+",
            "Kd2",
            "Qg6+",
            "Kc3",
            "Qg4+",
            "Kb4",
            "Qg6+",
            "Ka5",
            "Qg4+",
            "b5#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bxc4",
          "positional_uci": "f1c4",
          "positional_response": "9. Bxc4 Bxf3 10. gxf3 e6 11. Bg5 Be7 12. Rd1 Qc8 13. Ba6 Qb8 14. Bb7 O-O 15.\nBxa8 Qxa8 16. e5 Nd5 17. Nxd5 Bxg5 18. Nxc7 Qb8 19. Rd7 Rd8 20. Na6 Qxe5+ 21.\nKf1 Rc8 22. c7 h6 23. Qc6 Qxb2 24. Qb7 Rf8 25. c8=Q Rxc8 26. Qxc8+ Kh7 27. Rxf7\nQxa2 28. Qb7 Qa1+ 29. Kg2 Qe5 30. Qxa7 Bf6 31. Qxb6 Qg5+ 32. Kf1 Qc1+ 33. Ke2\nQxh1 34. Qxe6 Qa1 35. Qf5+ Kg8 36. Qd5 Kh7 37. Nc5 Qb2+ 38. Kf1 Qc1+ 39. Kg2\nQg5+ 40. Qxg5 hxg5 41. Ne4 Kg6 42. Rb7 Kf5 43. Rb5+ Kg6 44. Kg3 Kh6 45. Kg4 Kg6\n46. Nxg5 Kh6 47. Nf7+ Kg6 48. Nh8+ Kh7 49. Rh5+ Kg8 50. Kf5 Kf8 51. Ng6+ Kf7\n52. Rh8 Be7 53. Nxe7 Kxe7 54. Kg6 Ke6 55. Kxg7 Kf5 56. h4 Kf4 57. h5 Kxf3 58.\nh6 Kxf2 59. h7 Kf3 60. Rf8+ Kg4 61. h8=Q Kg5 62. Qh6+ Kg4 63. Rf4+ Kg3 64.\nQh4+ Kg2 65. Rf2+ Kg1 66. Qh2# Qd4+ 68. Kg6 Qg4+ 69. Kf6 Qd4+ 70. Ke6 Qe4+ 71.\nKd6 Qd4+ 72. Kc6 Qc4+ 73. Kb6 Qb4+ 74. Kc7 Qc5+ 75. Kd7 Qd5+ 76. Qd6 Qb7+\n77. Ke6 Qe4+ 78. Qe5 Qc6+ 79. Kf5 Qd7+ 80. Kg5 Qd8+ 81. Kg4 Qg8+ 82",
          "positional_tokens": {
            "prompt_tokens": 175,
            "completion_tokens": 640,
            "total_tokens": 815,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bxf3",
            "gxf3",
            "e6",
            "Bg5",
            "Be7",
            "Rd1",
            "Qc8",
            "Ba6",
            "Qb8",
            "Bb7",
            "O-O",
            "Bxa8",
            "Qxa8",
            "e5",
            "Nd5",
            "Nxd5",
            "Bxg5",
            "Nxc7",
            "Qb8",
            "Rd7",
            "Rd8",
            "Na6",
            "Qxe5+",
            "Kf1",
            "Rc8",
            "c7",
            "h6",
            "Qc6",
            "Qxb2",
            "Qb7",
            "Rf8",
            "c8=Q",
            "Rxc8",
            "Qxc8+",
            "Kh7",
            "Rxf7",
            "Qxa2",
            "Qb7",
            "Qa1+",
            "Kg2",
            "Qe5",
            "Qxa7",
            "Bf6",
            "Qxb6",
            "Qg5+",
            "Kf1",
            "Qc1+",
            "Ke2",
            "Qxh1",
            "Qxe6",
            "Qa1",
            "Qf5+",
            "Kg8",
            "Qd5",
            "Kh7",
            "Nc5",
            "Qb2+",
            "Kf1",
            "Qc1+",
            "Kg2",
            "Qg5+",
            "Qxg5",
            "hxg5",
            "Ne4",
            "Kg6",
            "Rb7",
            "Kf5",
            "Rb5+",
            "Kg6",
            "Kg3",
            "Kh6",
            "Kg4",
            "Kg6",
            "Nxg5",
            "Kh6",
            "Nf7+",
            "Kg6",
            "Nh8+",
            "Kh7",
            "Rh5+",
            "Kg8",
            "Kf5",
            "Kf8",
            "Ng6+",
            "Kf7",
            "Rh8",
            "Be7",
            "Nxe7",
            "Kxe7",
            "Kg6",
            "Ke6",
            "Kxg7",
            "Kf5",
            "h4",
            "Kf4",
            "h5",
            "Kxf3",
            "h6",
            "Kxf2",
            "h7",
            "Kf3",
            "Rf8+",
            "Kg4",
            "h8=Q",
            "Kg5",
            "Qh6+",
            "Kg4",
            "Rf4+",
            "Kg3",
            "Qh4+",
            "Kg2",
            "Rf2+",
            "Kg1",
            "Qh2#",
            "Qd4+",
            "Kg6",
            "Qg4+",
            "Kf6",
            "Qd4+",
            "Ke6",
            "Qe4+",
            "Kd6",
            "Qd4+",
            "Kc6",
            "Qc4+",
            "Kb6",
            "Qb4+",
            "Kc7",
            "Qc5+",
            "Kd7",
            "Qd5+",
            "Qd6",
            "Qb7+",
            "Ke6",
            "Qe4+",
            "Qe5",
            "Qc6+",
            "Kf5",
            "Qd7+",
            "Kg5",
            "Qd8+",
            "Kg4",
            "Qg8+"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bxc4",
          "neutral_uci": "f1c4",
          "neutral_response": "9. Bxc4 e6 10. Ne5 Bd6 11. Nxg4 Nxg4 12. h3 Ne5 13. O-O O-O 14. Be3 Qh4 15.\nBe2 f5 16. exf5 Qxa4 17. Nxa4 exf5 18. Rac1 f4 19. Bd4 f3 20. Bxe5 fxe2 21.\nRfe1 Bxe5 22. Rxe2 Rae8 23. Rce1 Rf5 24. g4 Rg5 25. Kg2 Kf7 26. f4 Bxf4 27.\nRxe8 Ra5 28. R8e7+ Kf6 29. R1e6+ Kg5 30. Rxg7+ Kh4 31. Rxh7+ Kg5 32. Rh5# Bg5\n33. Nc3 Rc5 34. Ne4+ Kf4 35. Nxc5 bxc5 36. Rg6 Be7 37. Rf5+ Ke4 38. Re6+ Kd4\n39. Rxe7 c4 40. Rxc7 c3 41. bxc3+ Kxc3 42. Rxa7 Kb2 43. c7 Ka1 44. c8=Q Kb2\n45. Rb7+ Ka1 46. Qc1+ Kxa2 47. Ra5# Bb6 48. Rxb6# Bb4 49. Rxb4# Bb6 50.\nQb2# Bb4 51. Qxb4# Bb6 52. Qb2# Bb4 53. Qxb4# Bb6 54. Qb2# Bb4 55. Qxb4# Bb6\n56. Qb2# Bb4 57. Qxb4# Bb6 58. Qb2# Bb4 59. Qxb4# Bb6 60. Qb2# Bb4 61. Qxb4# Bb6\n62. Qb2# Bb4 63. Qxb4# Bb6 64. Qb2# Bb4 65. Qxb4# Bb6 66. Qb2# Bb4 67. Qxb4# Bb6\n68. Qb2# Bb4 69. Qxb4# Bb6 70. Qb2# Bb4 71. Qxb4# Bb6 72. Qb2# Bb4 73. Qxb4# Bb6\n74. Qb2# Bb4 75. Qxb4# Bb6 76. Qb2# Bb4 77. Qxb4# Bb6 78. Qb2# Bb4",
          "neutral_tokens": {
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "e6",
            "Ne5",
            "Bd6",
            "Nxg4",
            "Nxg4",
            "h3",
            "Ne5",
            "O-O",
            "O-O",
            "Be3",
            "Qh4",
            "Be2",
            "f5",
            "exf5",
            "Qxa4",
            "Nxa4",
            "exf5",
            "Rac1",
            "f4",
            "Bd4",
            "f3",
            "Bxe5",
            "fxe2",
            "Rfe1",
            "Bxe5",
            "Rxe2",
            "Rae8",
            "Rce1",
            "Rf5",
            "g4",
            "Rg5",
            "Kg2",
            "Kf7",
            "f4",
            "Bxf4",
            "Rxe8",
            "Ra5",
            "R8e7+",
            "Kf6",
            "R1e6+",
            "Kg5",
            "Rxg7+",
            "Kh4",
            "Rxh7+",
            "Kg5",
            "Rh5#",
            "Bg5",
            "Nc3",
            "Rc5",
            "Ne4+",
            "Kf4",
            "Nxc5",
            "bxc5",
            "Rg6",
            "Be7",
            "Rf5+",
            "Ke4",
            "Re6+",
            "Kd4",
            "Rxe7",
            "c4",
            "Rxc7",
            "c3",
            "bxc3+",
            "Kxc3",
            "Rxa7",
            "Kb2",
            "c7",
            "Ka1",
            "c8=Q",
            "Kb2",
            "Rb7+",
            "Ka1",
            "Qc1+",
            "Kxa2",
            "Ra5#",
            "Bb6",
            "Rxb6#",
            "Bb4",
            "Rxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4",
            "Qxb4#",
            "Bb6",
            "Qb2#",
            "Bb4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f1c4",
          "positional_uci": "f1c4",
          "neutral_uci": "f1c4",
          "consensus_move": "f1c4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bxf3",
              "gxf3",
              "e6",
              "Bg5",
              "Be7",
              "Rd1",
              "Qc8",
              "Ba6",
              "Qb8",
              "Bb7",
              "O-O",
              "Bxa8",
              "Qxa8",
              "e5",
              "Nd5",
              "Nxd5",
              "Bxg5",
              "Nxc7",
              "Qb8",
              "Rd7",
              "Rd8",
              "Na6",
              "Qxe5+",
              "Kf1",
              "Rxd7",
              "cxd7",
              "Qd6",
              "Kg2",
              "h6",
              "Rd1",
              "Qe7",
              "d8=Q+",
              "Qxd8",
              "Rxd8+",
              "Bxd8",
              "Qe8+",
              "Kh7",
              "Qxd8",
              "Kg6",
              "Qd3+",
              "f5",
              "Nc7",
              "Kf6",
              "Qd6",
              "Kg5",
              "Nxe6+",
              "Kh5",
              "Nxg7+",
              "Kg5",
              "f4+",
              "Kg4",
              "Qg6+",
              "Kxf4",
              "Qxf5#",
              "e4",
              "Qg4+",
              "Kf3",
              "Qg6+",
              "Ke2",
              "Qg4+",
              "Kd2",
              "Qg6+",
              "Kc3",
              "Qg4+",
              "Kb4",
              "Qg6+",
              "Ka5",
              "Qg4+",
              "b5#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bxf3",
              "gxf3",
              "e6",
              "Bg5",
              "Be7",
              "Rd1",
              "Qc8",
              "Ba6",
              "Qb8",
              "Bb7",
              "O-O",
              "Bxa8",
              "Qxa8",
              "e5",
              "Nd5",
              "Nxd5",
              "Bxg5",
              "Nxc7",
              "Qb8",
              "Rd7",
              "Rd8",
              "Na6",
              "Qxe5+",
              "Kf1",
              "Rc8",
              "c7",
              "h6",
              "Qc6",
              "Qxb2",
              "Qb7",
              "Rf8",
              "c8=Q",
              "Rxc8",
              "Qxc8+",
              "Kh7",
              "Rxf7",
              "Qxa2",
              "Qb7",
              "Qa1+",
              "Kg2",
              "Qe5",
              "Qxa7",
              "Bf6",
              "Qxb6",
              "Qg5+",
              "Kf1",
              "Qc1+",
              "Ke2",
              "Qxh1",
              "Qxe6",
              "Qa1",
              "Qf5+",
              "Kg8",
              "Qd5",
              "Kh7",
              "Nc5",
              "Qb2+",
              "Kf1",
              "Qc1+",
              "Kg2",
              "Qg5+",
              "Qxg5",
              "hxg5",
              "Ne4",
              "Kg6",
              "Rb7",
              "Kf5",
              "Rb5+",
              "Kg6",
              "Kg3",
              "Kh6",
              "Kg4",
              "Kg6",
              "Nxg5",
              "Kh6",
              "Nf7+",
              "Kg6",
              "Nh8+",
              "Kh7",
              "Rh5+",
              "Kg8",
              "Kf5",
              "Kf8",
              "Ng6+",
              "Kf7",
              "Rh8",
              "Be7",
              "Nxe7",
              "Kxe7",
              "Kg6",
              "Ke6",
              "Kxg7",
              "Kf5",
              "h4",
              "Kf4",
              "h5",
              "Kxf3",
              "h6",
              "Kxf2",
              "h7",
              "Kf3",
              "Rf8+",
              "Kg4",
              "h8=Q",
              "Kg5",
              "Qh6+",
              "Kg4",
              "Rf4+",
              "Kg3",
              "Qh4+",
              "Kg2",
              "Rf2+",
              "Kg1",
              "Qh2#",
              "Qd4+",
              "Kg6",
              "Qg4+",
              "Kf6",
              "Qd4+",
              "Ke6",
              "Qe4+",
              "Kd6",
              "Qd4+",
              "Kc6",
              "Qc4+",
              "Kb6",
              "Qb4+",
              "Kc7",
              "Qc5+",
              "Kd7",
              "Qd5+",
              "Qd6",
              "Qb7+",
              "Ke6",
              "Qe4+",
              "Qe5",
              "Qc6+",
              "Kf5",
              "Qd7+",
              "Kg5",
              "Qd8+",
              "Kg4",
              "Qg8+"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e6",
              "Ne5",
              "Bd6",
              "Nxg4",
              "Nxg4",
              "h3",
              "Ne5",
              "O-O",
              "O-O",
              "Be3",
              "Qh4",
              "Be2",
              "f5",
              "exf5",
              "Qxa4",
              "Nxa4",
              "exf5",
              "Rac1",
              "f4",
              "Bd4",
              "f3",
              "Bxe5",
              "fxe2",
              "Rfe1",
              "Bxe5",
              "Rxe2",
              "Rae8",
              "Rce1",
              "Rf5",
              "g4",
              "Rg5",
              "Kg2",
              "Kf7",
              "f4",
              "Bxf4",
              "Rxe8",
              "Ra5",
              "R8e7+",
              "Kf6",
              "R1e6+",
              "Kg5",
              "Rxg7+",
              "Kh4",
              "Rxh7+",
              "Kg5",
              "Rh5#",
              "Bg5",
              "Nc3",
              "Rc5",
              "Ne4+",
              "Kf4",
              "Nxc5",
              "bxc5",
              "Rg6",
              "Be7",
              "Rf5+",
              "Ke4",
              "Re6+",
              "Kd4",
              "Rxe7",
              "c4",
              "Rxc7",
              "c3",
              "bxc3+",
              "Kxc3",
              "Rxa7",
              "Kb2",
              "c7",
              "Ka1",
              "c8=Q",
              "Kb2",
              "Rb7+",
              "Ka1",
              "Qc1+",
              "Kxa2",
              "Ra5#",
              "Bb6",
              "Rxb6#",
              "Bb4",
              "Rxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4",
              "Qxb4#",
              "Bb6",
              "Qb2#",
              "Bb4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 175,
            "completion_tokens": 288,
            "total_tokens": 463,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 175,
            "completion_tokens": 640,
            "total_tokens": 815,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 524,
          "total_completion_tokens": 1568,
          "total_tokens": 2092
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 175,
            "completion_tokens": 288,
            "total_tokens": 463,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 175,
            "completion_tokens": 640,
            "total_tokens": 815,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r2qkb1r/p1p1pppp/1pP2n2/8/Q1p1P1b1/2N2N2/PP3PPP/R1B1KB1R w KQkq - 0 9"
    },
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e5",
          "aggressive_uci": "e7e5",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6\n4. Ba4 Nf6 5. O-O Be7 6. Re1 b5\n7. Bb3 d6 8. c3 O-O 9. h3 Na5\n10. Bc2 c5 11. d4 Qc7 12. Nbd2 cxd4\n13. cxd4 Nc6 14. Nb3 a5 15. Be3 a4\n16. Nbd2 Bd7 17. Rc1 Qb7 18. Bb1 Rfc8\n19. Nf1 Bd8 20. Ng3 Bb6 21. Qd2 Ba5\n22. Qe2 Bxe1 23. Rxe1 h6 24. Qd2 Qa6\n25. Bxh6 gxh6 26. Qxh6 Nh7 27. Nh5 1-0\n27. Nh7 Nxd4 28. Nf6+ Kh8",
          "aggressive_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 236,
            "total_tokens": 350,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Qc7",
            "Nbd2",
            "cxd4",
            "cxd4",
            "Nc6",
            "Nb3",
            "a5",
            "Be3",
            "a4",
            "Nbd2",
            "Bd7",
            "Rc1",
            "Qb7",
            "Bb1",
            "Rfc8",
            "Nf1",
            "Bd8",
            "Ng3",
            "Bb6",
            "Qd2",
            "Ba5",
            "Qe2",
            "Bxe1",
            "Rxe1",
            "h6",
            "Qd2",
            "Qa6",
            "Bxh6",
            "gxh6",
            "Qxh6",
            "Nh7",
            "Nh5",
            "Nh7",
            "Nxd4",
            "Nf6+",
            "Kh8"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e5",
          "positional_uci": "e7e5",
          "positional_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7\n6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5\n11. d4 Qc7 12. Nbd2 cxd4 13. cxd4 Nc6 14. Nb3 a5\n15. Be3 a4 16. Nbd2 Bd7 17. Rc1 Qb7 18. Bb1 Rfc8\n19. Nf1 Bd8 20. Ng3 Bb6 21. Qd2 Ba5 22. Qe2 Bxe1\n23. Rxe1 Re8 24. Qd2 Rac8 25. Bg5 Kh8 26. Bxf6 gxf6\n27. Qh6 Rg8 28. Qxf6+ Rg7 29. Nh5 Rg8 30. Ng5 Be8\n31. Nxg7 Rxg7 32. dxe5 dxe5 33. Rd1 Qe7 34. Qxe7 Nxe7\n35. Rd8 Rg8 36. Nf3 f6 37. Bd3 Kg7 38. Rb8 Kf7\n39. Bxb5 Bxb5 40. Rxb5 Rc8 41. Rb4 Rc1+ 42. Kh2 Rc2\n43. Kg3 Ng6 44. Ne1 Re2 45. Nd3 Rd2 46. Nc5 Nf4\n47. Nxa4 Nd3 48. Rb7+ Kg6 49. f3 Nf4 50. Kh2 Rxg2+\n51. Kh1 Rf2 52. Nc3 Rxf3 53. a4 Rxh3+ 54. Kg1 Rd3\n55. a5 Rd6 56. Rb6 Rd7 57. a6 Ra7 58. Nb5 Ra8\n59. a7 Nd3 60. Rb8 Rxa7 61. Nxa7 Nc5 62. b4 Nxe4\n63. b5 Nd6 64. b6 Kf5 65. Rd8 Nb7 66. Rd7 Nc5\n67. Rxh7 Ke6 68. b7 Nxb7 69. Rxb7 Kd5 70. Kf2 f5\n71. Rb5+ Ke4 72. Nc6 Kf4 73. Rxe5 Kg4 74. Nd4 f4\n75. Rf5 Kh4 76. Kf3 Kh3 77. Rh5# e5 d5 Nf6",
          "positional_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 622,
            "total_tokens": 736,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d5",
            "Nf6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e5",
          "neutral_uci": "e7e5",
          "neutral_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6\n4. Ba4 Nf6 5. O-O Be7 6. Re1 b5\n7. Bb3 d6 8. c3 O-O 9. h3 Bb7\n10. d4 Re8 11. Nbd2 Bf8 12. a4 h6\n13. Bc2 exd4 14. cxd4 Nb4 15. Bb1 c5\n16. d5 Nd7 17. Ra3 c4 18. Nd4 Qf6\n19. N2f3 Nc5 20. axb5 Nbd3 21. Bxd3 Nxd3\n22. Re2 axb5 23. Nxb5 Rxa3 24. Nxa3 Ba6\n25. Qa4 Ra8 26. Be3 Nxb2\n27. Qc6 Rc8 28. Qxa6 c3 29. Qxc8 Nd3",
          "neutral_tokens": {
            "prompt_tokens": 113,
            "completion_tokens": 230,
            "total_tokens": 343,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Bb7",
            "d4",
            "Re8",
            "Nbd2",
            "Bf8",
            "a4",
            "h6",
            "Bc2",
            "exd4",
            "cxd4",
            "Nb4",
            "Bb1",
            "c5",
            "d5",
            "Nd7",
            "Ra3",
            "c4",
            "Nd4",
            "Qf6",
            "N2f3",
            "Nc5",
            "axb5",
            "Nbd3",
            "Bxd3",
            "Nxd3",
            "Re2",
            "axb5",
            "Nxb5",
            "Rxa3",
            "Nxa3",
            "Ba6",
            "Qa4",
            "Ra8",
            "Be3",
            "Nxb2",
            "Qc6",
            "Rc8",
            "Qxa6",
            "c3",
            "Qxc8",
            "Nd3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e7e5",
          "positional_uci": "e7e5",
          "neutral_uci": "e7e5",
          "consensus_move": "e7e5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Qc7",
              "Nbd2",
              "cxd4",
              "cxd4",
              "Nc6",
              "Nb3",
              "a5",
              "Be3",
              "a4",
              "Nbd2",
              "Bd7",
              "Rc1",
              "Qb7",
              "Bb1",
              "Rfc8",
              "Nf1",
              "Bd8",
              "Ng3",
              "Bb6",
              "Qd2",
              "Ba5",
              "Qe2",
              "Bxe1",
              "Rxe1",
              "h6",
              "Qd2",
              "Qa6",
              "Bxh6",
              "gxh6",
              "Qxh6",
              "Nh7",
              "Nh5",
              "Nh7",
              "Nxd4",
              "Nf6+",
              "Kh8"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "d5",
              "Nf6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Bb7",
              "d4",
              "Re8",
              "Nbd2",
              "Bf8",
              "a4",
              "h6",
              "Bc2",
              "exd4",
              "cxd4",
              "Nb4",
              "Bb1",
              "c5",
              "d5",
              "Nd7",
              "Ra3",
              "c4",
              "Nd4",
              "Qf6",
              "N2f3",
              "Nc5",
              "axb5",
              "Nbd3",
              "Bxd3",
              "Nxd3",
              "Re2",
              "axb5",
              "Nxb5",
              "Rxa3",
              "Nxa3",
              "Ba6",
              "Qa4",
              "Ra8",
              "Be3",
              "Nxb2",
              "Qc6",
              "Rc8",
              "Qxa6",
              "c3",
              "Qxc8",
              "Nd3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 114,
            "completion_tokens": 236,
            "total_tokens": 350,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 114,
            "completion_tokens": 622,
            "total_tokens": 736,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 113,
            "completion_tokens": 230,
            "total_tokens": 343,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 341,
          "total_completion_tokens": 1088,
          "total_tokens": 1429
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 114,
            "completion_tokens": 236,
            "total_tokens": 350,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 114,
            "completion_tokens": 622,
            "total_tokens": 736,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 113,
            "completion_tokens": 230,
            "total_tokens": 343,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1"
    }
  ],
  "white_errors": [],
  "black_errors": [],
  "white_plan_log": [],
  "black_plan_log": [],
  "timestamp": "2025-11-30T00:42:31.623707",
  "game_id": "meta-llama_llama-3.3-70b-instruct_vs_gpt-3.5-turbo-instruct_SC_2",
  "configuration": "SC"
}