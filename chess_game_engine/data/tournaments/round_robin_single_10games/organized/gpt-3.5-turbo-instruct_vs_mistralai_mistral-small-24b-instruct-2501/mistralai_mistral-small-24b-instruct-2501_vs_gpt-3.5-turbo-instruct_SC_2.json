{
  "result": "gpt-3.5-turbo-instruct wins by disqualification (mistralai/mistral-small-24b-instruct-2501 failed to produce a legal move)",
  "white_player": "mistralai/mistral-small-24b-instruct-2501",
  "black_player": "gpt-3.5-turbo-instruct",
  "white_score": 0.0,
  "black_score": 1.0,
  "total_moves": 14,
  "moves": [
    "e2e4",
    "e7e5",
    "g1f3",
    "b8c6",
    "f1c4",
    "g8f6",
    "b1c3",
    "f6e4",
    "c3e4",
    "d7d5",
    "c4d3",
    "d5e4",
    "e1g1",
    "e4d3"
  ],
  "game_history": [
    {
      "move_number": 1,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "e2e4",
      "move_san": "e4",
      "fen_after": "rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "move_number": 2,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "e7e5",
      "move_san": "e5",
      "fen_after": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "move_number": 3,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "g1f3",
      "move_san": "Nf3",
      "fen_after": "rnbqkbnr/pppp1ppp/8/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R b KQkq - 1 2"
    },
    {
      "move_number": 4,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "b8c6",
      "move_san": "Nc6",
      "fen_after": "r1bqkbnr/pppp1ppp/2n5/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3"
    },
    {
      "move_number": 5,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "f1c4",
      "move_san": "Bc4",
      "fen_after": "r1bqkbnr/pppp1ppp/2n5/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R b KQkq - 3 3"
    },
    {
      "move_number": 6,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "g8f6",
      "move_san": "Nf6",
      "fen_after": "r1bqkb1r/pppp1ppp/2n2n2/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R w KQkq - 4 4"
    },
    {
      "move_number": 7,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "b1c3",
      "move_san": "Nc3",
      "fen_after": "r1bqkb1r/pppp1ppp/2n2n2/4p3/2B1P3/2N2N2/PPPP1PPP/R1BQK2R b KQkq - 5 4"
    },
    {
      "move_number": 8,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "f6e4",
      "move_san": "Nxe4",
      "fen_after": "r1bqkb1r/pppp1ppp/2n5/4p3/2B1n3/2N2N2/PPPP1PPP/R1BQK2R w KQkq - 0 5"
    },
    {
      "move_number": 9,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "c3e4",
      "move_san": "Nxe4",
      "fen_after": "r1bqkb1r/pppp1ppp/2n5/4p3/2B1N3/5N2/PPPP1PPP/R1BQK2R b KQkq - 0 5"
    },
    {
      "move_number": 10,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "d7d5",
      "move_san": "d5",
      "fen_after": "r1bqkb1r/ppp2ppp/2n5/3pp3/2B1N3/5N2/PPPP1PPP/R1BQK2R w KQkq - 0 6"
    },
    {
      "move_number": 11,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "c4d3",
      "move_san": "Bd3",
      "fen_after": "r1bqkb1r/ppp2ppp/2n5/3pp3/4N3/3B1N2/PPPP1PPP/R1BQK2R b KQkq - 1 6"
    },
    {
      "move_number": 12,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "d5e4",
      "move_san": "dxe4",
      "fen_after": "r1bqkb1r/ppp2ppp/2n5/4p3/4p3/3B1N2/PPPP1PPP/R1BQK2R w KQkq - 0 7"
    },
    {
      "move_number": 13,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "e1g1",
      "move_san": "O-O",
      "fen_after": "r1bqkb1r/ppp2ppp/2n5/4p3/4p3/3B1N2/PPPP1PPP/R1BQ1RK1 b kq - 1 7"
    },
    {
      "move_number": 14,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "e4d3",
      "move_san": "exd3",
      "fen_after": "r1bqkb1r/ppp2ppp/2n5/4p3/8/3p1N2/PPPP1PPP/R1BQ1RK1 w kq - 0 8"
    }
  ],
  "final_fen": "r1bqkb1r/ppp2ppp/2n5/4p3/8/3p1N2/PPPP1PPP/R1BQ1RK1 w kq - 0 8",
  "white_tokens": {
    "prompt_tokens": 2879,
    "completion_tokens": 4594,
    "total_tokens": 7473
  },
  "black_tokens": {
    "prompt_tokens": 3250,
    "completion_tokens": 12034,
    "total_tokens": 15284
  },
  "white_token_log": [
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 113,
          "total_tokens": 227,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 215,
          "total_tokens": 329,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 166,
          "total_tokens": 279,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 341,
        "total_completion_tokens": 494,
        "total_tokens": 835
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 113,
          "total_tokens": 227,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 215,
          "total_tokens": 329,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 166,
          "total_tokens": 279,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 3,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 120,
          "completion_tokens": 640,
          "total_tokens": 760,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 120,
          "completion_tokens": 640,
          "total_tokens": 760,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 119,
          "completion_tokens": 640,
          "total_tokens": 759,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 359,
        "total_completion_tokens": 1920,
        "total_tokens": 2279
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 120,
          "completion_tokens": 640,
          "total_tokens": 760,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 120,
          "completion_tokens": 640,
          "total_tokens": 760,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 119,
          "completion_tokens": 640,
          "total_tokens": 759,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 5,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 129,
          "completion_tokens": 95,
          "total_tokens": 224,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 129,
          "completion_tokens": 41,
          "total_tokens": 170,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 128,
          "completion_tokens": 640,
          "total_tokens": 768,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 386,
        "total_completion_tokens": 776,
        "total_tokens": 1162
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 129,
          "completion_tokens": 95,
          "total_tokens": 224,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 129,
          "completion_tokens": 41,
          "total_tokens": 170,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 128,
          "completion_tokens": 640,
          "total_tokens": 768,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 7,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 137,
          "completion_tokens": 27,
          "total_tokens": 164,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 137,
          "completion_tokens": 132,
          "total_tokens": 269,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 136,
          "completion_tokens": 27,
          "total_tokens": 163,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 410,
        "total_completion_tokens": 186,
        "total_tokens": 596
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 137,
          "completion_tokens": 27,
          "total_tokens": 164,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 137,
          "completion_tokens": 132,
          "total_tokens": 269,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 136,
          "completion_tokens": 27,
          "total_tokens": 163,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 9,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 146,
          "completion_tokens": 36,
          "total_tokens": 182,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 146,
          "completion_tokens": 110,
          "total_tokens": 256,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 145,
          "completion_tokens": 39,
          "total_tokens": 184,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 437,
        "total_completion_tokens": 185,
        "total_tokens": 622
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 146,
          "completion_tokens": 36,
          "total_tokens": 182,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 146,
          "completion_tokens": 110,
          "total_tokens": 256,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 145,
          "completion_tokens": 39,
          "total_tokens": 184,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 11,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 154,
          "completion_tokens": 81,
          "total_tokens": 235,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 154,
          "completion_tokens": 68,
          "total_tokens": 222,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 153,
          "completion_tokens": 81,
          "total_tokens": 234,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 461,
        "total_completion_tokens": 230,
        "total_tokens": 691
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 154,
          "completion_tokens": 81,
          "total_tokens": 235,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 154,
          "completion_tokens": 68,
          "total_tokens": 222,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 153,
          "completion_tokens": 81,
          "total_tokens": 234,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 13,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 162,
          "completion_tokens": 55,
          "total_tokens": 217,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 162,
          "completion_tokens": 640,
          "total_tokens": 802,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 161,
          "completion_tokens": 108,
          "total_tokens": 269,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 485,
        "total_completion_tokens": 803,
        "total_tokens": 1288
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 162,
          "completion_tokens": 55,
          "total_tokens": 217,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 162,
          "completion_tokens": 640,
          "total_tokens": 802,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 161,
          "completion_tokens": 108,
          "total_tokens": 269,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    }
  ],
  "black_token_log": [
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 640,
          "total_tokens": 749,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 329,
        "total_completion_tokens": 1920,
        "total_tokens": 2249
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 640,
          "total_tokens": 749,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 2,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 429,
          "total_tokens": 543,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 498,
          "total_tokens": 612,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 640,
          "total_tokens": 753,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 341,
        "total_completion_tokens": 1567,
        "total_tokens": 1908
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 429,
          "total_tokens": 543,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 498,
          "total_tokens": 612,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 640,
          "total_tokens": 753,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 4,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 122,
          "completion_tokens": 258,
          "total_tokens": 380,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 122,
          "completion_tokens": 640,
          "total_tokens": 762,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 121,
          "completion_tokens": 640,
          "total_tokens": 761,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 365,
        "total_completion_tokens": 1538,
        "total_tokens": 1903
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 122,
          "completion_tokens": 258,
          "total_tokens": 380,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 122,
          "completion_tokens": 640,
          "total_tokens": 762,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 121,
          "completion_tokens": 640,
          "total_tokens": 761,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 6,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 131,
          "completion_tokens": 640,
          "total_tokens": 771,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 131,
          "completion_tokens": 640,
          "total_tokens": 771,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 130,
          "completion_tokens": 174,
          "total_tokens": 304,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 392,
        "total_completion_tokens": 1454,
        "total_tokens": 1846
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 131,
          "completion_tokens": 640,
          "total_tokens": 771,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 131,
          "completion_tokens": 640,
          "total_tokens": 771,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 130,
          "completion_tokens": 174,
          "total_tokens": 304,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 8,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 140,
          "completion_tokens": 640,
          "total_tokens": 780,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 140,
          "completion_tokens": 640,
          "total_tokens": 780,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 139,
          "completion_tokens": 292,
          "total_tokens": 431,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 419,
        "total_completion_tokens": 1572,
        "total_tokens": 1991
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 140,
          "completion_tokens": 640,
          "total_tokens": 780,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 140,
          "completion_tokens": 640,
          "total_tokens": 780,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 139,
          "completion_tokens": 292,
          "total_tokens": 431,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 10,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 640,
          "total_tokens": 789,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 388,
          "total_tokens": 537,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 640,
          "total_tokens": 788,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 446,
        "total_completion_tokens": 1668,
        "total_tokens": 2114
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 640,
          "total_tokens": 789,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 388,
          "total_tokens": 537,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 640,
          "total_tokens": 788,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 12,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 640,
          "total_tokens": 796,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 365,
          "total_tokens": 521,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 640,
          "total_tokens": 795,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 467,
        "total_completion_tokens": 1645,
        "total_tokens": 2112
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 640,
          "total_tokens": 796,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 365,
          "total_tokens": 521,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 640,
          "total_tokens": 795,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 14,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 164,
          "completion_tokens": 156,
          "total_tokens": 320,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 164,
          "completion_tokens": 182,
          "total_tokens": 346,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 163,
          "completion_tokens": 332,
          "total_tokens": 495,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 491,
        "total_completion_tokens": 670,
        "total_tokens": 1161
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 164,
          "completion_tokens": 156,
          "total_tokens": 320,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 164,
          "completion_tokens": 182,
          "total_tokens": 346,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 163,
          "completion_tokens": 332,
          "total_tokens": 495,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    }
  ],
  "white_responses": [
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4\n\n1. d4",
          "aggressive_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4\n\n1. d4",
          "positional_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": null,
          "neutral_response": "1. d4\n\n1. d4",
          "neutral_tokens": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 353,
          "total_completion_tokens": 27,
          "total_tokens": 380
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 2,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4\n\n1. d4",
          "aggressive_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4\n\n1. d4",
          "positional_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": null,
          "neutral_response": "1. d4\n\n1. d4",
          "neutral_tokens": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 353,
          "total_completion_tokens": 27,
          "total_tokens": 380
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 2,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4\n\n1. d4",
          "aggressive_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4\n\n1. d4",
          "positional_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": null,
          "neutral_response": "1. d4\n\n1. d4",
          "neutral_tokens": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 353,
          "total_completion_tokens": 27,
          "total_tokens": 380
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": "e2e4",
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Bc5\n4. c3 Nf6\n5. d4 exd4\n6. cxd4 Bb4+\n7. Nc3 Nxe4\n8. O-O Bxc3\n9. d5 Bxa1\n10. Qxa1 Nc5\n11. Qd4 Ncd7\n12. Qe5 Nc6\n13. Qf5\n\nQd7",
          "aggressive_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 113,
            "total_tokens": 227,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bc4",
            "Bc5",
            "c3",
            "Nf6",
            "d4",
            "exd4",
            "cxd4",
            "Bb4+",
            "Nc3",
            "Nxe4",
            "O-O",
            "Bxc3",
            "d5",
            "Bxa1",
            "Qxa1",
            "Nc5",
            "Qd4",
            "Ncd7",
            "Qe5",
            "Nc6",
            "Qf5",
            "Qd7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bb5 a6\n4. Ba4 Nf6\n5. O-O Be7\n6. Re1 b5\n7. Bb3 d6\n8. c3 O-O\n9. h3 Na5\n10. Bc2 c5\n11. d4 Nc6\n12. Nbd2 cxd4\n13. cxd4 Nxd4\n14. Nxd4 Qxd4\n15. Nf3 Qd7\n16. Bg5 Qd8\n17. Qd3 Qd7\n18. Qxd7 Bxd7\n19. Bxe7 Nxe7\n20. Rc1 Nc6\n21. Rfd1 Rfc8\n22. Rxc8 Rxc8\n23. Rc1 Rxc1\n24. Bxc1\n\nNf5",
          "positional_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 215,
            "total_tokens": 329,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "Nxd4",
            "Nxd4",
            "Qxd4",
            "Nf3",
            "Qd7",
            "Bg5",
            "Qd8",
            "Qd3",
            "Qd7",
            "Qxd7",
            "Bxd7",
            "Bxe7",
            "Nxe7",
            "Rc1",
            "Nc6",
            "Rfd1",
            "Rfc8",
            "Rxc8",
            "Rxc8",
            "Rc1",
            "Rxc1",
            "Bxc1",
            "Nf5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": "e2e4",
          "neutral_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Bc5\n4. c3 Nf6\n5. d4 exd4\n6. cxd4 Bb4+\n7. Nc3 Nxe4\n8. O-O Bxc3\n9. d5 Bxa1\n10. Qxa1 Nc5\n11. Qd4 Ncd7\n12. Nxe5 Nxe5\n13. Qxe5 O-O\n14. Qh5 Qf6\n15. Qxf7+ Kh8\n16. Qf5+ Qg6\n17. Qxg6+ hxg6\n18. Bg5\n\n18...",
          "neutral_tokens": {
            "prompt_tokens": 113,
            "completion_tokens": 166,
            "total_tokens": 279,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bc4",
            "Bc5",
            "c3",
            "Nf6",
            "d4",
            "exd4",
            "cxd4",
            "Bb4+",
            "Nc3",
            "Nxe4",
            "O-O",
            "Bxc3",
            "d5",
            "Bxa1",
            "Qxa1",
            "Nc5",
            "Qd4",
            "Ncd7",
            "Nxe5",
            "Nxe5",
            "Qxe5",
            "O-O",
            "Qh5",
            "Qf6",
            "Qxf7+",
            "Kh8",
            "Qf5+",
            "Qg6",
            "Qxg6+",
            "hxg6",
            "Bg5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e2e4",
          "positional_uci": "e2e4",
          "neutral_uci": "e2e4",
          "consensus_move": "e2e4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bc4",
              "Bc5",
              "c3",
              "Nf6",
              "d4",
              "exd4",
              "cxd4",
              "Bb4+",
              "Nc3",
              "Nxe4",
              "O-O",
              "Bxc3",
              "d5",
              "Bxa1",
              "Qxa1",
              "Nc5",
              "Qd4",
              "Ncd7",
              "Qe5",
              "Nc6",
              "Qf5",
              "Qd7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "Nxd4",
              "Nxd4",
              "Qxd4",
              "Nf3",
              "Qd7",
              "Bg5",
              "Qd8",
              "Qd3",
              "Qd7",
              "Qxd7",
              "Bxd7",
              "Bxe7",
              "Nxe7",
              "Rc1",
              "Nc6",
              "Rfd1",
              "Rfc8",
              "Rxc8",
              "Rxc8",
              "Rc1",
              "Rxc1",
              "Bxc1",
              "Nf5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bc4",
              "Bc5",
              "c3",
              "Nf6",
              "d4",
              "exd4",
              "cxd4",
              "Bb4+",
              "Nc3",
              "Nxe4",
              "O-O",
              "Bxc3",
              "d5",
              "Bxa1",
              "Qxa1",
              "Nc5",
              "Qd4",
              "Ncd7",
              "Nxe5",
              "Nxe5",
              "Qxe5",
              "O-O",
              "Qh5",
              "Qf6",
              "Qxf7+",
              "Kh8",
              "Qf5+",
              "Qg6",
              "Qxg6+",
              "hxg6",
              "Bg5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 114,
            "completion_tokens": 113,
            "total_tokens": 227,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 114,
            "completion_tokens": 215,
            "total_tokens": 329,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 113,
            "completion_tokens": 166,
            "total_tokens": 279,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 341,
          "total_completion_tokens": 494,
          "total_tokens": 835
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 114,
            "completion_tokens": 113,
            "total_tokens": 227,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 114,
            "completion_tokens": 215,
            "total_tokens": 329,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 113,
            "completion_tokens": 166,
            "total_tokens": 279,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 3,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf3",
          "aggressive_uci": "g1f3",
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Bc5\n4. c3 Nf6\n5. d4 exd4\n6. cxd4 Bb4+\n7. Nc3 Nxe4\n8. O-O Bxc3\n9. d5 Bxa1\n10. Qxa1 Nc5\n11. Qd4 Qf6\n12. Qxc5 Qxc5\n13. Nxe5 Qd6\n14. Nc4 O-O\n15. Be3 Qd7\n16. Bxc5 Qxc5\n17. Nxb7 Qxb7\n18. Rc1 Qd7\n19. Rc7 Qd8\n20. Rc8+ Rxc8\n21. Bxc8 Qxc8\n22. Rc1 Qd8\n23. Rc7 Qd6\n24. Rc8+ Rxc8\n25. Bxc8 Qxc8\n26. Rc1 Qd8\n27. Rc7 Qd6\n28. Rc8+ Rxc8\n29. Bxc8 Qxc8\n30. Rc1 Qd8\n31. Rc7 Qd6\n32. Rc8+ Rxc8\n33. Bxc8 Qxc8\n34. Rc1 Qd8\n35. Rc7 Qd6\n36. Rc8+ Rxc8\n37. Bxc8 Qxc8\n38. Rc1 Qd8\n39. Rc7 Qd6\n40. Rc8+ Rxc8\n41. Bxc8 Qxc8\n42. Rc1 Qd8\n43. Rc7 Qd6\n44. Rc8+ Rxc8\n45. Bxc8 Qxc8\n46. Rc1 Qd8\n47. Rc7 Qd6\n48. Rc8+ Rxc8\n49. Bxc8 Qxc8\n50. Rc1 Qd8\n51. Rc7 Qd6\n52. Rc8+ Rxc8\n53. Bxc8 Qxc8\n54. Rc1 Qd8\n55. Rc7 Qd6\n56. Rc8+ Rxc8\n57. Bxc8 Qxc8\n58. Rc1 Qd8\n59. Rc7 Qd6\n60. Rc8+ Rxc8\n61. Bxc8 Qxc8\n62. Rc1 Qd8\n63. Rc7 Qd6\n64. Rc8+ Rxc8\n65. Bxc8 Q",
          "aggressive_tokens": {
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nc6",
            "Bc4",
            "Bc5",
            "c3",
            "Nf6",
            "d4",
            "exd4",
            "cxd4",
            "Bb4+",
            "Nc3",
            "Nxe4",
            "O-O",
            "Bxc3",
            "d5",
            "Bxa1",
            "Qxa1",
            "Nc5",
            "Qd4",
            "Qf6",
            "Qxc5",
            "Qxc5",
            "Nxe5",
            "Qd6",
            "Nc4",
            "O-O",
            "Be3",
            "Qd7",
            "Bxc5",
            "Qxc5",
            "Nxb7",
            "Qxb7",
            "Rc1",
            "Qd7",
            "Rc7",
            "Qd8",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf3",
          "positional_uci": "g1f3",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5 11. d4 Nc6 12. Nbd2 cxd4 13. cxd4 exd4 14. Nxd4 Nxd4 15. Qxd4 Bc5 16. Qd2 Bxd4 17. Bxd4 Qxd4 18. Qxd4 Nxd4 19. Rxd4 Rfd8 20. Rd2 Rxd2 21. Nxd2 Rd8 22. Nf3 Rd4 23. Kf1 Kf8 24. Ke2 Ke7 25. Kd3 Kd6 26. Nd2 Rc4 27. Nc4+ Kc5 28. Nc3+ Kd5 29. Nc4+ Kc5 30. Nc3+ Kd5 31. Nc4+ Kc5 32. Nc3+ Kd5 33. Nc4+ Kc5 34. Nc3+ Kd5 35. Nc4+ Kc5 36. Nc3+ Kd5 37. Nc4+ Kc5 38. Nc3+ Kd5 39. Nc4+ Kc5 40. Nc3+ Kd5 41. Nc4+ Kc5 42. Nc3+ Kd5 43. Nc4+ Kc5 44. Nc3+ Kd5 45. Nc4+ Kc5 46. Nc3+ Kd5 47. Nc4+ Kc5 48. Nc3+ Kd5 49. Nc4+ Kc5 50. Nc3+ Kd5 51. Nc4+ Kc5 52. Nc3+ Kd5 53. Nc4+ Kc5 54. Nc3+ Kd5 55. Nc4+ Kc5 56. Nc3+ Kd5 57. Nc4+ Kc5 58. Nc3+ Kd5 59. Nc4+ Kc5 60. Nc3+ Kd5 61. Nc4+ Kc5 62. Nc3+ Kd5 63. Nc4+ Kc5 64. N",
          "positional_tokens": {
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "Rd4",
            "Kf1",
            "Kf8",
            "Ke2",
            "Ke7",
            "Kd3",
            "Kd6",
            "Nd2",
            "Rc4",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf3",
          "neutral_uci": "g1f3",
          "neutral_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bb5 a6\n4. Ba4 Nf6\n5. O-O Be7\n6. Re1 b5\n7. Bb3 d6\n8. c3 O-O\n9. h3 Na5\n10. Bc2 c5\n11. d4 Nc6\n12. Nbd2 cxd4\n13. cxd4 exd4\n14. Nxd4 Bg4\n15. f3 Bxf3\n16. Qxf3 Nxd4\n17. Bxd4 Qxd4\n18. Qxd4 Nxd4\n19. Bxd4 Bc5\n20. Bxc5 dxc5\n21. Rf2 Rfd8\n22. Rc2 Rxd1\n23. Rxd1 Rd8\n24. Rxd8+ Kxd8\n25. Kf2 Ke7\n26. Ke3 Kd6\n27. Kd4 Kc6\n28. Kc4 Kb6\n29. Kb4 Kc6\n30. Kc4 Kb6\n31. Kb4 Kc6\n32. Kc4 Kb6\n33. Kb4 Kc6\n34. Kc4 Kb6\n35. Kb4 Kc6\n36. Kc4 Kb6\n37. Kb4 Kc6\n38. Kc4 Kb6\n39. Kb4 Kc6\n40. Kc4 Kb6\n41. Kb4 Kc6\n42. Kc4 Kb6\n43. Kb4 Kc6\n44. Kc4 Kb6\n45. Kb4 Kc6\n46. Kc4 Kb6\n47. Kb4 Kc6\n48. Kc4 Kb6\n49. Kb4 Kc6\n50. Kc4 Kb6\n51. Kb4 Kc6\n52. Kc4 Kb6\n53. Kb4 Kc6\n54. Kc4 Kb6\n55. Kb4 Kc6\n56. Kc4 Kb6\n57. Kb4 Kc6\n58. Kc4 Kb6\n59. Kb4 Kc6\n60. Kc4 Kb6\n61. Kb4 Kc6\n62. Kc4 Kb6\n63. Kb4 Kc6\n64. Kc4 Kb6\n65. Kb4 Kc6\n66. Kc4 Kb6\n67. Kb4 Kc6",
          "neutral_tokens": {
            "prompt_tokens": 119,
            "completion_tokens": 640,
            "total_tokens": 759,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "exd4",
            "Nxd4",
            "Bg4",
            "f3",
            "Bxf3",
            "Qxf3",
            "Nxd4",
            "Bxd4",
            "Qxd4",
            "Qxd4",
            "Nxd4",
            "Bxd4",
            "Bc5",
            "Bxc5",
            "dxc5",
            "Rf2",
            "Rfd8",
            "Rc2",
            "Rxd1",
            "Rxd1",
            "Rd8",
            "Rxd8+",
            "Kxd8",
            "Kf2",
            "Ke7",
            "Ke3",
            "Kd6",
            "Kd4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g1f3",
          "positional_uci": "g1f3",
          "neutral_uci": "g1f3",
          "consensus_move": "g1f3",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nc6",
              "Bc4",
              "Bc5",
              "c3",
              "Nf6",
              "d4",
              "exd4",
              "cxd4",
              "Bb4+",
              "Nc3",
              "Nxe4",
              "O-O",
              "Bxc3",
              "d5",
              "Bxa1",
              "Qxa1",
              "Nc5",
              "Qd4",
              "Qf6",
              "Qxc5",
              "Qxc5",
              "Nxe5",
              "Qd6",
              "Nc4",
              "O-O",
              "Be3",
              "Qd7",
              "Bxc5",
              "Qxc5",
              "Nxb7",
              "Qxb7",
              "Rc1",
              "Qd7",
              "Rc7",
              "Qd8",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Rd4",
              "Kf1",
              "Kf8",
              "Ke2",
              "Ke7",
              "Kd3",
              "Kd6",
              "Nd2",
              "Rc4",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "exd4",
              "Nxd4",
              "Bg4",
              "f3",
              "Bxf3",
              "Qxf3",
              "Nxd4",
              "Bxd4",
              "Qxd4",
              "Qxd4",
              "Nxd4",
              "Bxd4",
              "Bc5",
              "Bxc5",
              "dxc5",
              "Rf2",
              "Rfd8",
              "Rc2",
              "Rxd1",
              "Rxd1",
              "Rd8",
              "Rxd8+",
              "Kxd8",
              "Kf2",
              "Ke7",
              "Ke3",
              "Kd6",
              "Kd4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 119,
            "completion_tokens": 640,
            "total_tokens": 759,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 359,
          "total_completion_tokens": 1920,
          "total_tokens": 2279
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 119,
            "completion_tokens": 640,
            "total_tokens": 759,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 5,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bc4",
          "aggressive_uci": "f1c4",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Bc5 4. c3 Nf6 5. d4 exd4 6. cxd4 Bb4+ 7. Nc3 Nxe4 8. O-O Bxc3 9. d5 Bxa1 10. Qxa1+ Nc5 11. Qd4\n\nNxd5",
          "aggressive_tokens": {
            "prompt_tokens": 129,
            "completion_tokens": 95,
            "total_tokens": 224,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "c3",
            "Nf6",
            "d4",
            "exd4",
            "cxd4",
            "Bb4+",
            "Nc3",
            "Nxe4",
            "O-O",
            "Bxc3",
            "d5",
            "Bxa1",
            "Qxa1+",
            "Nc5",
            "Qd4",
            "Nxd5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bc4",
          "positional_uci": "f1c4",
          "positional_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4\n\n1. e4 e5 2. Nf3 Nc6 3. Bc4",
          "positional_tokens": {
            "prompt_tokens": 129,
            "completion_tokens": 41,
            "total_tokens": 170,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bb5",
          "neutral_uci": "f1b5",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5 11. d4 Nc6 12. Nbd2 cxd4 13. cxd4 exd4 14. Nxd4 Bc5 15. Nxc6 Bxc6 16. e5 dxe5 17. Nxe5 Bxe5 18. Rxe5 Qd6 19. Qf3 Qxd4 20. Qxd5+ Kh8 21. Qf5+ Qf6 22. Qxf6+ gxf6 23. Rxe7 Rg8 24. Re8+ Rg8 25. Rxf8+ Rxf8 26. Bxf8 Kg7 27. Bg5+ Kg8 28. Bxf6 Kg7 29. Bg5+ Kg8 30. Bxf8 Kg7 31. Bg5+ Kg8 32. Bxf8 Kg7 33. Bg5+ Kg8 34. Bxf8 Kg7 35. Bg5+ Kg8 36. Bxf8 Kg7 37. Bg5+ Kg8 38. Bxf8 Kg7 39. Bg5+ Kg8 40. Bxf8 Kg7 41. Bg5+ Kg8 42. Bxf8 Kg7 43. Bg5+ Kg8 44. Bxf8 Kg7 45. Bg5+ Kg8 46. Bxf8 Kg7 47. Bg5+ Kg8 48. Bxf8 Kg7 49. Bg5+ Kg8 50. Bxf8 Kg7 51. Bg5+ Kg8 52. Bxf8 Kg7 53. Bg5+ Kg8 54. Bxf8 Kg7 55. Bg5+ Kg8 56. Bxf8 Kg7 57. Bg5+ Kg8 58. Bxf8 Kg7 59. Bg5+ Kg8 60. Bxf8 Kg7 61. Bg5+ Kg8 62. Bxf8 Kg7 63. Bg5+ Kg8 64. Bxf8 Kg7 65. Bg",
          "neutral_tokens": {
            "prompt_tokens": 128,
            "completion_tokens": 640,
            "total_tokens": 768,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "exd4",
            "Nxd4",
            "Bc5",
            "Nxc6",
            "Bxc6",
            "e5",
            "dxe5",
            "Nxe5",
            "Bxe5",
            "Rxe5",
            "Qd6",
            "Qf3",
            "Qxd4",
            "Qxd5+",
            "Kh8",
            "Qf5+",
            "Qf6",
            "Qxf6+",
            "gxf6",
            "Rxe7",
            "Rg8",
            "Re8+",
            "Rg8",
            "Rxf8+",
            "Rxf8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf6",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f1c4",
          "positional_uci": "f1c4",
          "neutral_uci": "f1b5",
          "consensus_move": "f1c4",
          "source_agents": [
            "aggressive",
            "positional"
          ],
          "selected_agent": "aggressive"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bc5",
              "c3",
              "Nf6",
              "d4",
              "exd4",
              "cxd4",
              "Bb4+",
              "Nc3",
              "Nxe4",
              "O-O",
              "Bxc3",
              "d5",
              "Bxa1",
              "Qxa1+",
              "Nc5",
              "Qd4",
              "Nxd5"
            ],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "exd4",
              "Nxd4",
              "Bc5",
              "Nxc6",
              "Bxc6",
              "e5",
              "dxe5",
              "Nxe5",
              "Bxe5",
              "Rxe5",
              "Qd6",
              "Qf3",
              "Qxd4",
              "Qxd5+",
              "Kh8",
              "Qf5+",
              "Qf6",
              "Qxf6+",
              "gxf6",
              "Rxe7",
              "Rg8",
              "Re8+",
              "Rg8",
              "Rxf8+",
              "Rxf8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf6",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 129,
            "completion_tokens": 95,
            "total_tokens": 224,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 129,
            "completion_tokens": 41,
            "total_tokens": 170,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 128,
            "completion_tokens": 640,
            "total_tokens": 768,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 386,
          "total_completion_tokens": 776,
          "total_tokens": 1162
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 129,
            "completion_tokens": 95,
            "total_tokens": 224,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 129,
            "completion_tokens": 41,
            "total_tokens": 170,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 128,
            "completion_tokens": 640,
            "total_tokens": 768,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bqkbnr/pppp1ppp/2n5/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3"
    },
    {
      "turn": 7,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc3",
          "aggressive_uci": "b1c3",
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n\nNc3",
          "aggressive_tokens": {
            "prompt_tokens": 137,
            "completion_tokens": 27,
            "total_tokens": 164,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Ng5",
          "positional_uci": "f3g5",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Na5 6. Bb5+ c6 7. dxc6 bxc6 8. Be2 h6 9. Nf3 e4 10. Ne5 Qd4 11. Nc3\n\nQd3+ 12. Kf1 0-0-0 13. d4\n\nNc4 14. Bxc4 Qxc4 15. Nxc4",
          "positional_tokens": {
            "prompt_tokens": 137,
            "completion_tokens": 132,
            "total_tokens": 269,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d5",
            "exd5",
            "Na5",
            "Bb5+",
            "c6",
            "dxc6",
            "bxc6",
            "Be2",
            "h6",
            "Nf3",
            "e4",
            "Ne5",
            "Qd4",
            "Nc3",
            "Qd3+",
            "Kf1",
            "d4",
            "Nc4",
            "Bxc4",
            "Qxc4",
            "Nxc4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc3",
          "neutral_uci": "b1c3",
          "neutral_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n\nNc3",
          "neutral_tokens": {
            "prompt_tokens": 136,
            "completion_tokens": 27,
            "total_tokens": 163,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b1c3",
          "positional_uci": "f3g5",
          "neutral_uci": "b1c3",
          "consensus_move": "b1c3",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [
              "d5",
              "exd5",
              "Na5",
              "Bb5+",
              "c6",
              "dxc6",
              "bxc6",
              "Be2",
              "h6",
              "Nf3",
              "e4",
              "Ne5",
              "Qd4",
              "Nc3",
              "Qd3+",
              "Kf1",
              "d4",
              "Nc4",
              "Bxc4",
              "Qxc4",
              "Nxc4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 137,
            "completion_tokens": 27,
            "total_tokens": 164,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 137,
            "completion_tokens": 132,
            "total_tokens": 269,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 136,
            "completion_tokens": 27,
            "total_tokens": 163,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 410,
          "total_completion_tokens": 186,
          "total_tokens": 596
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 137,
            "completion_tokens": 27,
            "total_tokens": 164,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 137,
            "completion_tokens": 132,
            "total_tokens": 269,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 136,
            "completion_tokens": 27,
            "total_tokens": 163,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/pppp1ppp/2n2n2/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R w KQkq - 4 4"
    },
    {
      "turn": 9,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc3",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Nc3 Nxe4\n\nNc3",
          "aggressive_tokens": {
            "prompt_tokens": 146,
            "completion_tokens": 36,
            "total_tokens": 182,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nxe4",
          "positional_uci": "c3e4",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. Bxe4 Nxe4 8. Qh5 Nd6 9. Qxe5+ Be7 10. 0-0 0-0 11. Re1\n\nQd7 12. Qxd7 13. Nxd7",
          "positional_tokens": {
            "prompt_tokens": 146,
            "completion_tokens": 110,
            "total_tokens": 256,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qh5",
            "Nd6",
            "Qxe5+",
            "Be7",
            "Re1",
            "Qd7",
            "Qxd7",
            "Nxd7"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc3",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Nc3 Nxe4\n\nNc3 Nc6",
          "neutral_tokens": {
            "prompt_tokens": 145,
            "completion_tokens": 39,
            "total_tokens": 184,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": "c3e4",
          "neutral_uci": null,
          "consensus_move": "c3e4",
          "source_agents": [
            "positional"
          ],
          "selected_agent": "positional"
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [
              "Qh5",
              "Nd6",
              "Qxe5+",
              "Be7",
              "Re1",
              "Qd7",
              "Qxd7",
              "Nxd7"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 146,
            "completion_tokens": 36,
            "total_tokens": 182,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 146,
            "completion_tokens": 110,
            "total_tokens": 256,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 145,
            "completion_tokens": 39,
            "total_tokens": 184,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 437,
          "total_completion_tokens": 185,
          "total_tokens": 622
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 146,
            "completion_tokens": 36,
            "total_tokens": 182,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 146,
            "completion_tokens": 110,
            "total_tokens": 256,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 145,
            "completion_tokens": 39,
            "total_tokens": 184,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/pppp1ppp/2n5/4p3/2B1n3/2N2N2/PPPP1PPP/R1BQK2R w KQkq - 0 5"
    },
    {
      "turn": 11,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bd3",
          "aggressive_uci": "c4d3",
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Nc3 Nxe4\n5. Nxe4 d5\n6. Bd3 Bc5\n7. O-O O-O\n8. c3 Nf6\n9. d4 Bb6\n10. Nc3\n\nNf6",
          "aggressive_tokens": {
            "prompt_tokens": 154,
            "completion_tokens": 81,
            "total_tokens": 235,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "O-O",
            "O-O",
            "c3",
            "Nf6",
            "d4",
            "Bb6",
            "Nc3",
            "Nf6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bd3",
          "positional_uci": "c4d3",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 Be7 7. O-O O-O 8. c3\n\nNf6 9. d4",
          "positional_tokens": {
            "prompt_tokens": 154,
            "completion_tokens": 68,
            "total_tokens": 222,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Be7",
            "O-O",
            "O-O",
            "c3",
            "Nf6",
            "d4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bd3",
          "neutral_uci": "c4d3",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 Be7 7. O-O O-O 8. c4\n\nc4 Nxd4 9. Nxd4 dxc4 10. Bxc4",
          "neutral_tokens": {
            "prompt_tokens": 153,
            "completion_tokens": 81,
            "total_tokens": 234,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Be7",
            "O-O",
            "O-O",
            "c4",
            "c4",
            "Nxd4",
            "Nxd4",
            "dxc4",
            "Bxc4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "c4d3",
          "positional_uci": "c4d3",
          "neutral_uci": "c4d3",
          "consensus_move": "c4d3",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bc5",
              "O-O",
              "O-O",
              "c3",
              "Nf6",
              "d4",
              "Bb6",
              "Nc3",
              "Nf6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Be7",
              "O-O",
              "O-O",
              "c3",
              "Nf6",
              "d4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Be7",
              "O-O",
              "O-O",
              "c4",
              "c4",
              "Nxd4",
              "Nxd4",
              "dxc4",
              "Bxc4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 154,
            "completion_tokens": 81,
            "total_tokens": 235,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 154,
            "completion_tokens": 68,
            "total_tokens": 222,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 153,
            "completion_tokens": 81,
            "total_tokens": 234,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 461,
          "total_completion_tokens": 230,
          "total_tokens": 691
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 154,
            "completion_tokens": 81,
            "total_tokens": 235,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 154,
            "completion_tokens": 68,
            "total_tokens": 222,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 153,
            "completion_tokens": 81,
            "total_tokens": 234,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/3pp3/2B1N3/5N2/PPPP1PPP/R1BQK2R w KQkq - 0 6"
    },
    {
      "turn": 13,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bxe4",
          "aggressive_uci": "d3e4",
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Nc3 Nxe4\n5. Nxe4 d5\n6. Bd3 dxe4\n\nBxe4 Nc5",
          "aggressive_tokens": {
            "prompt_tokens": 162,
            "completion_tokens": 55,
            "total_tokens": 217,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "O-O",
          "positional_uci": "e1g1",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7",
          "positional_tokens": {
            "prompt_tokens": 162,
            "completion_tokens": 640,
            "total_tokens": 802,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "O-O",
          "neutral_uci": "e1g1",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O\n\ne4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O\n\nO-O",
          "neutral_tokens": {
            "prompt_tokens": 161,
            "completion_tokens": 108,
            "total_tokens": 269,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d3e4",
          "positional_uci": "e1g1",
          "neutral_uci": "e1g1",
          "consensus_move": "e1g1",
          "source_agents": [
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nc5"
            ],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 162,
            "completion_tokens": 55,
            "total_tokens": 217,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 162,
            "completion_tokens": 640,
            "total_tokens": 802,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 161,
            "completion_tokens": 108,
            "total_tokens": 269,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 485,
          "total_completion_tokens": 803,
          "total_tokens": 1288
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 162,
            "completion_tokens": 55,
            "total_tokens": 217,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 162,
            "completion_tokens": 640,
            "total_tokens": 802,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 161,
            "completion_tokens": 108,
            "total_tokens": 269,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/4p3/4p3/3B1N2/PPPP1PPP/R1BQK2R w KQkq - 0 7"
    },
    {
      "turn": 15,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qxd3",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O exd3 8. Qxd3 Qxd3 9. Rxd3\n\nQxd3 Rxd3",
          "aggressive_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 78,
            "total_tokens": 248,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Rxd3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qxd3",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O exd3 8. Qxd3 Bc5 9. Qe3+ Be7 10. Qxe7+ Kxe7 11. Bg5+ Kf8 12. Bxc6+ bxc6 13. Nxd5\n\nNxd5",
          "positional_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 117,
            "total_tokens": 287,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "Qe3+",
            "Be7",
            "Qxe7+",
            "Kxe7",
            "Bg5+",
            "Kf8",
            "Bxc6+",
            "bxc6",
            "Nxd5",
            "Nxd5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qxd3",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O exd3 8. Qxd3 Bc5 9. Qe3+ Be6 10. Qxe6+ fxe6\n\nQxd3+ Qxd3+",
          "neutral_tokens": {
            "prompt_tokens": 169,
            "completion_tokens": 93,
            "total_tokens": 262,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Rxd3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bc5",
              "Qe3+",
              "Be7",
              "Qxe7+",
              "Kxe7",
              "Bg5+",
              "Kf8",
              "Bxc6+",
              "bxc6",
              "Nxd5",
              "Nxd5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 170,
            "completion_tokens": 78,
            "total_tokens": 248,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 170,
            "completion_tokens": 117,
            "total_tokens": 287,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 169,
            "completion_tokens": 93,
            "total_tokens": 262,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 509,
          "total_completion_tokens": 288,
          "total_tokens": 797
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 170,
            "completion_tokens": 78,
            "total_tokens": 248,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 170,
            "completion_tokens": 117,
            "total_tokens": 287,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 169,
            "completion_tokens": 93,
            "total_tokens": 262,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/4p3/8/3p1N2/PPPP1PPP/R1BQ1RK1 w kq - 0 8"
    },
    {
      "turn": 15,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qxd3",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Nc3 Nxe4\n5. Nxe4 d5\n6. Bd3 dxe4\n7. O-O exd3\n8. Qxd3 Bc5\n9. Qe3+ Kf8\n10. Qe3+ Kf8\n\nQe3+ Kf8 Qe3+",
          "aggressive_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 97,
            "total_tokens": 267,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "Qe3+",
            "Kf8",
            "Qe3+",
            "Kf8",
            "Qe3+",
            "Kf8",
            "Qe3+"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qxd3",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O exd3 8. Qxd3 Nc6 9. Nc3 Be7 10. Be3\n\nQxd3 Nc6 9. Nc3 Be7 10. Be3",
          "positional_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 100,
            "total_tokens": 270,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc6",
            "Nc3",
            "Be7",
            "Be3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qxd3",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O exd3 8. Qxd3 Nxd4 9. Qxd4 Bg4 10. Qxd8+ Kxd8 11. Nxe5\n\nNxe5 12. Bxe5+ Kc8 13. Bxc6+",
          "neutral_tokens": {
            "prompt_tokens": 169,
            "completion_tokens": 115,
            "total_tokens": 284,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxd4",
            "Qxd4",
            "Bg4",
            "Qxd8+",
            "Kxd8",
            "Nxe5",
            "Nxe5",
            "Bxe5+",
            "Kc8",
            "Bxc6+"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bc5",
              "Qe3+",
              "Kf8",
              "Qe3+",
              "Kf8",
              "Qe3+",
              "Kf8",
              "Qe3+"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nc6",
              "Nc3",
              "Be7",
              "Be3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nxd4",
              "Qxd4",
              "Bg4",
              "Qxd8+",
              "Kxd8",
              "Nxe5",
              "Nxe5",
              "Bxe5+",
              "Kc8",
              "Bxc6+"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 170,
            "completion_tokens": 97,
            "total_tokens": 267,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 170,
            "completion_tokens": 100,
            "total_tokens": 270,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 169,
            "completion_tokens": 115,
            "total_tokens": 284,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 509,
          "total_completion_tokens": 312,
          "total_tokens": 821
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 170,
            "completion_tokens": 97,
            "total_tokens": 267,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 170,
            "completion_tokens": 100,
            "total_tokens": 270,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 169,
            "completion_tokens": 115,
            "total_tokens": 284,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/4p3/8/3p1N2/PPPP1PPP/R1BQ1RK1 w kq - 0 8"
    },
    {
      "turn": 15,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qxd3",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Nc3 Nxe4\n5. Nxe4 d5\n6. Bd3 dxe4\n7. O-O exd3\n8. Qxd3 Bc5\n9. Qe3+ Kf8\n10. Qe3+ Kf8\n\nQe3+ Kf8 Qe3+",
          "aggressive_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 97,
            "total_tokens": 267,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "Qe3+",
            "Kf8",
            "Qe3+",
            "Kf8",
            "Qe3+",
            "Kf8",
            "Qe3+"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qxd3",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O exd3 8. Qxd3 Bc5 9. Qe3+ Be6 10. Qxe6+ Qe7\n\nQe3+ Be6 10. Qxe6+ Qe7",
          "positional_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 102,
            "total_tokens": 272,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "Qe3+",
            "Be6",
            "Qxe6+",
            "Qe7",
            "Qe3+",
            "Be6",
            "Qxe6+",
            "Qe7"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qxd3",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O exd3 8. Qxd3 Bc5 9. Qe3+ Kf8 10. Qxe5\n\nQxe5+ Kg8 11. Qe3+",
          "neutral_tokens": {
            "prompt_tokens": 169,
            "completion_tokens": 97,
            "total_tokens": 266,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "Qe3+",
            "Kf8",
            "Qxe5",
            "Qxe5+",
            "Kg8",
            "Qe3+"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bc5",
              "Qe3+",
              "Kf8",
              "Qe3+",
              "Kf8",
              "Qe3+",
              "Kf8",
              "Qe3+"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bc5",
              "Qe3+",
              "Be6",
              "Qxe6+",
              "Qe7",
              "Qe3+",
              "Be6",
              "Qxe6+",
              "Qe7"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bc5",
              "Qe3+",
              "Kf8",
              "Qxe5",
              "Qxe5+",
              "Kg8",
              "Qe3+"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 170,
            "completion_tokens": 97,
            "total_tokens": 267,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 170,
            "completion_tokens": 102,
            "total_tokens": 272,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 169,
            "completion_tokens": 97,
            "total_tokens": 266,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 509,
          "total_completion_tokens": 296,
          "total_tokens": 805
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 170,
            "completion_tokens": 97,
            "total_tokens": 267,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 170,
            "completion_tokens": 102,
            "total_tokens": 272,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 169,
            "completion_tokens": 97,
            "total_tokens": 266,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/4p3/8/3p1N2/PPPP1PPP/R1BQ1RK1 w kq - 0 8"
    }
  ],
  "black_responses": [
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": "d2d4",
          "aggressive_response": "d4 d5 2. Nf3 Nf6 3. Bg5 Bf5 4. e3 e6 5. Bd3 Bxd3 6. Qxd3 Be7 7. O-O O-O 8. Nbd2 Nbd7 9. c4 c6 10. Rac1 Rc8 11. cxd5 exd5 12. a3 Re8 13. b4 a6 14. Nb3 Ne4 15. Bxe7 Qxe7 16. Nc5 Ndxc5 17. bxc5 f5 18. Rb1 Rc7 19. Rb6 Rf8 20. Rfb1 Rf7 21. Ne5 Rf6 22. f3 Ng5 23. h4 Nf7 24. f4 Nxe5 25. fxe5 Rf7 26. g3 g5 27. hxg5 Qxg5 28. Kf2 Rg7 29. Rg1 h5 30. Rbb1 Rcf7 31. Qe2 Qg4 32. Qxg4 fxg4+ 33. Ke2 Rf3 34. Rbf1 Rgf7 35. Rxf3 Rxf3 36. Rg2 Kf7 37. Rf2 Rxf2+ 38. Kxf2 Ke6 39. Ke2 Kf5 40. Kd3 Ke6 41. e4 dxe4+ 42. Kxe4 a5 43. a4 Ke7 44. Kf5 Kf7 45. Kg5 Ke6 46. Kxh5 Kd5 47. Kxg4 Kxd4 48. e6 Kxc5 49. e7 Kb4 50. e8=Q Kxa4 51. Qe4+ Kb5 52. Kf3 a4 53. g4 a3 54. g5 a2 55. Qe5+ Ka4 56. g6 b5 57. g7 b4 58. g8=Q b3 59. Qa8+ Kb4 60. Qaa5+ Kc4 61. Qec3# 1-0 d4 d5 Nf3 Nf6 Bg5 Bf5 e3 e6 Bd3 Bxd3 Qxd3 Be7 O-O O-O Nbd2 c6 Rac1 Rc8 cxd5 exd5 a3 Re8 b4 a6 Nb3 Ne4 Bxe7 Qxe7 Nc5 Ndxc5 bxc5 f5 Rb1 Rc7 Rb6 Rf8 Ne5 Rf6 f3 Ng5 h4 Nf7 f4 Nxe5 fxe5 Rf7 g3 g5 hxg5 Qxg5 Kf2 Rg7 Rg",
          "aggressive_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Nf3",
            "Nf6",
            "Bg5",
            "Bf5",
            "e3",
            "e6",
            "Bd3",
            "Bxd3",
            "Qxd3",
            "Be7",
            "O-O",
            "O-O",
            "Nbd2",
            "c6",
            "Rac1",
            "Rc8",
            "cxd5",
            "exd5",
            "a3",
            "Re8",
            "b4",
            "a6",
            "Nb3",
            "Ne4",
            "Bxe7",
            "Qxe7",
            "Nc5",
            "Ndxc5",
            "bxc5",
            "f5",
            "Rb1",
            "Rc7",
            "Rb6",
            "Rf8",
            "Ne5",
            "Rf6",
            "f3",
            "Ng5",
            "h4",
            "Nf7",
            "f4",
            "Nxe5",
            "fxe5",
            "Rf7",
            "g3",
            "g5",
            "hxg5",
            "Qxg5",
            "Kf2",
            "Rg7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "Game 1\n1. e4 e5\n2. Nf3 Nc6\n3. Bb5 a6\n4. Bxc6 dxc6\n5. O-O Bg4\n6. h3 Bh5\n7. d3 Bc5\n8. Be3 Bxe3\n9. fxe3 Nf6\n10. Nbd2 O-O\n11. Qe1 Re8\n12. Qg3 Qd6\n13. Nc4 Qc5\n14. Nfxe5 b5\n15. d4 Qe7\n16. Nxc6 Qxe4\n17. Rxf6 bxc4\n18. Ne5 Qxc2\n19. Raf1 Re7\n20. R6f2 Qe4\n21. Rf4 Qd5\n22. Qg5 f6\n23. Qxh5 fxe5\n24. Rf5 g6\n25. Qg5 Rae8\n26. Rf6 Re6\n27. Rf7 exd4\n28. Qh6 Qxg2+\n29. Kxg2 Rxe3\n30. Qg7# e4 Nf6 Rxf6#\n\n*Game 1\n1. e4 e5\n2. Nf3 Nc6\n3. Bb5 a6\n4. Bxc6 dxc6\n5. O-O Bg4\n6. h3 Bh5\n7. d3 Bc5\n8. Be3 Bxe3\n9. fxe3 Nf6\n10. Nbd2 O-O\n11. Qe1 Re8\n12. Qg3 Qd6\n13. Nc4 Qc5\n14. Nfxe5 b5\n15. d4 Qe7\n16. Nxc6 Qxe4\n17. Rxf6 bxc4\n18. Ne5 Qxc2\n19. Raf1 Re7\n20. R6f2 Qe4\n21. Rf4 Qd5\n22. Qg5 f6\n23. Qxh5 fxe5\n24. Rf5 g6\n25. Qg5 Rae8\n26. Rf6 Re6\n27. Rf7 exd4\n28. Qh6 Qxg2+\n29. Kxg2 Rxe3\n30. Qg7# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 R",
          "positional_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": "d2d4",
          "neutral_response": "d4 d5\nd4 d5 Nf3 Nf6 Bf4 Bf5 e3 e6 c4 c6 Nc3 Nbd7 Be2 Be7 O-O O-O Rc1 Rc8 Qb3 Qb6 c5 Qxb3 axb3 a6 b4 Nh5 Bg3 Nxg3 hxg3 Bf6 Nd2 e5 dxe5 Nxe5 Nb3 Nd3 Bxd3 Bxd3 Rfd1 Rfe8 Nd4 Bg6 Na4 Re7 Nb6 Rce8 b3 h6 Kf1 Bh5 Rd2 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke",
          "neutral_tokens": {
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Nf3",
            "Nf6",
            "Bf4",
            "Bf5",
            "e3",
            "e6",
            "c4",
            "c6",
            "Nc3",
            "Nbd7",
            "Be2",
            "Be7",
            "O-O",
            "O-O",
            "Rc1",
            "Rc8",
            "Qb3",
            "Qb6",
            "c5",
            "Qxb3",
            "axb3",
            "a6",
            "b4",
            "Nh5",
            "Bg3",
            "Nxg3",
            "hxg3",
            "Bf6",
            "Nd2",
            "e5",
            "dxe5",
            "Nxe5",
            "Nb3",
            "Nd3",
            "Bxd3",
            "Bxd3",
            "Rfd1",
            "Rfe8",
            "Nd4",
            "Bg6",
            "Na4",
            "Re7",
            "Nb6",
            "Rce8",
            "b3",
            "h6",
            "Kf1",
            "Bh5",
            "Rd2",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d2d4",
          "positional_uci": "e2e4",
          "neutral_uci": "d2d4",
          "consensus_move": "d2d4",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d5",
              "Nf3",
              "Nf6",
              "Bg5",
              "Bf5",
              "e3",
              "e6",
              "Bd3",
              "Bxd3",
              "Qxd3",
              "Be7",
              "O-O",
              "O-O",
              "Nbd2",
              "c6",
              "Rac1",
              "Rc8",
              "cxd5",
              "exd5",
              "a3",
              "Re8",
              "b4",
              "a6",
              "Nb3",
              "Ne4",
              "Bxe7",
              "Qxe7",
              "Nc5",
              "Ndxc5",
              "bxc5",
              "f5",
              "Rb1",
              "Rc7",
              "Rb6",
              "Rf8",
              "Ne5",
              "Rf6",
              "f3",
              "Ng5",
              "h4",
              "Nf7",
              "f4",
              "Nxe5",
              "fxe5",
              "Rf7",
              "g3",
              "g5",
              "hxg5",
              "Qxg5",
              "Kf2",
              "Rg7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "d5",
              "Nf3",
              "Nf6",
              "Bf4",
              "Bf5",
              "e3",
              "e6",
              "c4",
              "c6",
              "Nc3",
              "Nbd7",
              "Be2",
              "Be7",
              "O-O",
              "O-O",
              "Rc1",
              "Rc8",
              "Qb3",
              "Qb6",
              "c5",
              "Qxb3",
              "axb3",
              "a6",
              "b4",
              "Nh5",
              "Bg3",
              "Nxg3",
              "hxg3",
              "Bf6",
              "Nd2",
              "e5",
              "dxe5",
              "Nxe5",
              "Nb3",
              "Nd3",
              "Bxd3",
              "Bxd3",
              "Rfd1",
              "Rfe8",
              "Nd4",
              "Bg6",
              "Na4",
              "Re7",
              "Nb6",
              "Rce8",
              "b3",
              "h6",
              "Kf1",
              "Bh5",
              "Rd2",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 329,
          "total_completion_tokens": 1920,
          "total_tokens": 2249
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e5",
          "aggressive_uci": "e7e5",
          "aggressive_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7\n6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5\n11. d4 Qc7 12. Nbd2 cxd4 13. cxd4 Nc6 14. Nb3 a5\n15. Be3 a4 16. Nbd2 Bd7 17. Rc1 Qb7 18. Bb1 Rfc8\n19. Nf1 Bd8 20. Ng3 Bb6 21. Qd2 Ba5 22. Qe2 Bxe1\n23. Rxe1 Re8 24. Qd2 Rad8 25. Bg5 Ne7 26. Bxf6 gxf6\n27. Qh6 Ng6 28. Nh5 Re6 29. Qg7# d5 e4 Nxe4\n30. Bxe4 dxe4 31. Rxe4 Qxe4 32. Ng5 Qe1+ 33. Kh2 fxg5\n34. Nf6+ Rxf6 35. Qxf6 Re8 36. Qxg5 Qxf2 37. h4 Qf4+\n38. Qxf4 Nxf4 39. Kg3 Nd3 40. Kf3 Nxb2 41. g4 Nc4\n42. Kf4 b4 43. g5 b3 44. axb3 axb3 45. h5 b2\n46. g6 b1=Q 47. gxh7+ Kxh7 48. h6 Qf5+ 49. Kg3 Rg8+\n50. Kh4 Qg4# e5 Qg5# e6 Qg6#",
          "aggressive_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 429,
            "total_tokens": 543,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qg5#",
            "e6",
            "Qg6#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e5",
          "positional_uci": "e7e5",
          "positional_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7\n6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5\n11. d4 Qc7 12. Nbd2 cxd4 13. cxd4 Nc6 14. Nb3 a5\n15. Be3 a4 16. Nbd2 Bd7 17. Rc1 Qb7 18. Bb1 Rfc8\n19. Nf1 Bd8 20. Ng3 Bb6 21. Qd2 Ba5 22. Qe2 Bxe1\n23. Rxe1 Re8 24. Qd2 Rac8 25. Bg5 Kh8 26. Bxf6 gxf6\n27. Qh6 Rg8 28. Qxf6+ Rg7 29. Nh5 Rg8 30. Ng5 Be8\n31. Nxg7 Rxg7 32. dxe5 dxe5 33. Rd1 Qe7 34. Qxe7 Nxe7\n35. Rd8 Rg8 36. Nf3 f6 37. Bd3 Kg7 38. Rb8 Bf7\n39. Rxb5 Bxa2 40. Ra5 Bb3 41. Nd2 Nc6 42. Ra6 Nb4\n43. Ra7+ Kg6 44. Bc4 Bxc4 45. Nxc4 Nd3 46. Rxa4 Rc8\n47. b3 Nc5 48. Rb4 Nxe4 49. Nxe5+ Kf5 50. Nc4 Rd8\n51. Ne3+ Ke5 52. Rb5+ Ke6 53. Rb6+ Ke5 54. Ng4+ Kf5\n55. f3 Rd1+ 56. Kh2 Nd6 57. Ne3+ Ke5 58. Nxd1 1-0\n58... Kd5 59. Rxd6+ Kxd6 60. Kg3 Kc5",
          "positional_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 498,
            "total_tokens": 612,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Qc7",
            "Nbd2",
            "cxd4",
            "cxd4",
            "Nc6",
            "Nb3",
            "a5",
            "Be3",
            "a4",
            "Nbd2",
            "Bd7",
            "Rc1",
            "Qb7",
            "Bb1",
            "Rfc8",
            "Nf1",
            "Bd8",
            "Ng3",
            "Bb6",
            "Qd2",
            "Ba5",
            "Qe2",
            "Bxe1",
            "Rxe1",
            "Re8",
            "Qd2",
            "Rac8",
            "Bg5",
            "Kh8",
            "Bxf6",
            "gxf6",
            "Qh6",
            "Rg8",
            "Qxf6+",
            "Rg7",
            "Nh5",
            "Rg8",
            "Ng5",
            "Be8",
            "Nxg7",
            "Rxg7",
            "dxe5",
            "dxe5",
            "Rd1",
            "Qe7",
            "Qxe7",
            "Nxe7",
            "Rd8",
            "Rg8",
            "Nf3",
            "f6",
            "Bd3",
            "Kg7",
            "Rb8",
            "Bf7",
            "Rxb5",
            "Bxa2",
            "Ra5",
            "Bb3",
            "Nd2",
            "Nc6",
            "Ra6",
            "Nb4",
            "Ra7+",
            "Kg6",
            "Bc4",
            "Bxc4",
            "Nxc4",
            "Nd3",
            "Rxa4",
            "Rc8",
            "b3",
            "Nc5",
            "Rb4",
            "Nxe4",
            "Nxe5+",
            "Kf5",
            "Nc4",
            "Rd8",
            "Ne3+",
            "Ke5",
            "Rb5+",
            "Ke6",
            "Rb6+",
            "Ke5",
            "Ng4+",
            "Kf5",
            "f3",
            "Rd1+",
            "Kh2",
            "Nd6",
            "Ne3+",
            "Ke5",
            "Nxd1",
            "Kd5",
            "Rxd6+",
            "Kxd6",
            "Kg3",
            "Kc5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e5",
          "neutral_uci": "e7e5",
          "neutral_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6\n4. Ba4 Nf6 5. O-O Be7 6. Re1 b5\n7. Bb3 d6 8. c3 O-O 9. h3 Bb7\n10. d4 Re8 11. Nbd2 Bf8 12. a4 h6\n13. Bc2 exd4 14. cxd4 Nb4 15. Bb1 c5\n16. d5 Nd7 17. Ra3 f5 18. Rae3 fxe4\n19. Nxe4 Nxd5 20. Ba2 c4 21. Nxd6 Rxe3\n22. Nxb7 Rxe1+ 23. Nxe1 Qe7 24. Qxd5+ Kh8\n25. Nf3 Nf6 26. Qc6 Ra7 27. Na5 Qd8\n28. Bd2 Rd7 29. Bc3 Rd1+ 30. Kh2 Bd6+\n31. g3 Bc7 32. Nb7 Qd3 33. Bxf6 Qf1\n34. Qe8+ Kh7 35. Qe4+ Kh8 36. Qe8+ Kh7\n37. Qe4+ Kh8 38. Qe8+ Kh7 39. Qe4+ Kh8 40. Qe8+ Kh7 41. Qe4+ Kh8 42. Qe8+ Kh7 43. Qe4+ Kh8 44. Qe8+ Kh7 45. Qe4+ Kh8 46. Qe8+ Kh7 47. Qe4+ Kh8 48. Qe8+ Kh7 49. Qe4+ Kh8 50. Qe8+ Kh7 51. Qe4+ Kh8 52. Qe8+ Kh7 53. Qe4+ Kh8 54. Qe8+ Kh7 55. Qe4+ Kh8 56. Qe8+ Kh7 57. Qe4+ Kh8 58. Qe8+ Kh7 59. Qe4+ Kh8 60. Qe8+ Kh7 61. Qe4+ Kh8 62. Qe8+ Kh7 63. Qe4+ Kh8 64. Qe8+ Kh7 65. Qe4+ Kh8 66. Qe8+ Kh7 67. Qe4+ Kh8 68. Qe8+ Kh7 69. Qe4+ Kh8 70. Qe8+ Kh7 71. Qe4+ Kh8 72. Qe8+ Kh7 73. Qe4+ Kh8 74. Qe8+ Kh7 75. Qe4+ Kh",
          "neutral_tokens": {
            "prompt_tokens": 113,
            "completion_tokens": 640,
            "total_tokens": 753,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Bb7",
            "d4",
            "Re8",
            "Nbd2",
            "Bf8",
            "a4",
            "h6",
            "Bc2",
            "exd4",
            "cxd4",
            "Nb4",
            "Bb1",
            "c5",
            "d5",
            "Nd7",
            "Ra3",
            "f5",
            "Rae3",
            "fxe4",
            "Nxe4",
            "Nxd5",
            "Ba2",
            "c4",
            "Nxd6",
            "Rxe3",
            "Nxb7",
            "Rxe1+",
            "Nxe1",
            "Qe7",
            "Qxd5+",
            "Kh8",
            "Nf3",
            "Nf6",
            "Qc6",
            "Ra7",
            "Na5",
            "Qd8",
            "Bd2",
            "Rd7",
            "Bc3",
            "Rd1+",
            "Kh2",
            "Bd6+",
            "g3",
            "Bc7",
            "Nb7",
            "Qd3",
            "Bxf6",
            "Qf1",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e7e5",
          "positional_uci": "e7e5",
          "neutral_uci": "e7e5",
          "consensus_move": "e7e5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qg5#",
              "e6",
              "Qg6#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Qc7",
              "Nbd2",
              "cxd4",
              "cxd4",
              "Nc6",
              "Nb3",
              "a5",
              "Be3",
              "a4",
              "Nbd2",
              "Bd7",
              "Rc1",
              "Qb7",
              "Bb1",
              "Rfc8",
              "Nf1",
              "Bd8",
              "Ng3",
              "Bb6",
              "Qd2",
              "Ba5",
              "Qe2",
              "Bxe1",
              "Rxe1",
              "Re8",
              "Qd2",
              "Rac8",
              "Bg5",
              "Kh8",
              "Bxf6",
              "gxf6",
              "Qh6",
              "Rg8",
              "Qxf6+",
              "Rg7",
              "Nh5",
              "Rg8",
              "Ng5",
              "Be8",
              "Nxg7",
              "Rxg7",
              "dxe5",
              "dxe5",
              "Rd1",
              "Qe7",
              "Qxe7",
              "Nxe7",
              "Rd8",
              "Rg8",
              "Nf3",
              "f6",
              "Bd3",
              "Kg7",
              "Rb8",
              "Bf7",
              "Rxb5",
              "Bxa2",
              "Ra5",
              "Bb3",
              "Nd2",
              "Nc6",
              "Ra6",
              "Nb4",
              "Ra7+",
              "Kg6",
              "Bc4",
              "Bxc4",
              "Nxc4",
              "Nd3",
              "Rxa4",
              "Rc8",
              "b3",
              "Nc5",
              "Rb4",
              "Nxe4",
              "Nxe5+",
              "Kf5",
              "Nc4",
              "Rd8",
              "Ne3+",
              "Ke5",
              "Rb5+",
              "Ke6",
              "Rb6+",
              "Ke5",
              "Ng4+",
              "Kf5",
              "f3",
              "Rd1+",
              "Kh2",
              "Nd6",
              "Ne3+",
              "Ke5",
              "Nxd1",
              "Kd5",
              "Rxd6+",
              "Kxd6",
              "Kg3",
              "Kc5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Bb7",
              "d4",
              "Re8",
              "Nbd2",
              "Bf8",
              "a4",
              "h6",
              "Bc2",
              "exd4",
              "cxd4",
              "Nb4",
              "Bb1",
              "c5",
              "d5",
              "Nd7",
              "Ra3",
              "f5",
              "Rae3",
              "fxe4",
              "Nxe4",
              "Nxd5",
              "Ba2",
              "c4",
              "Nxd6",
              "Rxe3",
              "Nxb7",
              "Rxe1+",
              "Nxe1",
              "Qe7",
              "Qxd5+",
              "Kh8",
              "Nf3",
              "Nf6",
              "Qc6",
              "Ra7",
              "Na5",
              "Qd8",
              "Bd2",
              "Rd7",
              "Bc3",
              "Rd1+",
              "Kh2",
              "Bd6+",
              "g3",
              "Bc7",
              "Nb7",
              "Qd3",
              "Bxf6",
              "Qf1",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 114,
            "completion_tokens": 429,
            "total_tokens": 543,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 114,
            "completion_tokens": 498,
            "total_tokens": 612,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 113,
            "completion_tokens": 640,
            "total_tokens": 753,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 341,
          "total_completion_tokens": 1567,
          "total_tokens": 1908
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 114,
            "completion_tokens": 429,
            "total_tokens": 543,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 114,
            "completion_tokens": 498,
            "total_tokens": 612,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 113,
            "completion_tokens": 640,
            "total_tokens": 753,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 4,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc6",
          "aggressive_uci": "b8c6",
          "aggressive_response": "2... Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 O-O 8. c3 d5\n9. exd5 Nxd5 10. Nxe5 Nxe5 11. Rxe5 c6 12. d4 Bd6 13. Re1 Qh4 14. g3 Qh3\n15. Be3 Bg4 16. Qd3 Rae8 17. Nd2 Re6 18. Qf1 Qh5 19. a4 Rfe8 20. axb5 axb5\n21. Bxd5 Qxd5 22. Qg2 Qh5 23. Qxc6 Bxg3 24. Qg2 Bf4 25. Ra8 Bb8 26. Kh1\n26... Rg6 27. Bf4 Rxe1+ 28. Nf1 Rxf1+ 29. Qxf1 Bf3+ 30. Qg2 Bxg2+ 31. Kg1\n31... Qd1#",
          "aggressive_tokens": {
            "prompt_tokens": 122,
            "completion_tokens": 258,
            "total_tokens": 380,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "O-O",
            "c3",
            "d5",
            "exd5",
            "Nxd5",
            "Nxe5",
            "Nxe5",
            "Rxe5",
            "c6",
            "d4",
            "Bd6",
            "Re1",
            "Qh4",
            "g3",
            "Qh3",
            "Be3",
            "Bg4",
            "Qd3",
            "Rae8",
            "Nd2",
            "Re6",
            "Qf1",
            "Qh5",
            "a4",
            "Rfe8",
            "axb5",
            "axb5",
            "Bxd5",
            "Qxd5",
            "Qg2",
            "Qh5",
            "Qxc6",
            "Bxg3",
            "Qg2",
            "Bf4",
            "Ra8",
            "Bb8",
            "Kh1",
            "Rg6",
            "Bf4",
            "Rxe1+",
            "Nf1",
            "Rxf1+",
            "Qxf1",
            "Bf3+",
            "Qg2",
            "Bxg2+",
            "Kg1",
            "Qd1#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nc6",
          "positional_uci": "b8c6",
          "positional_response": "2... Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 O-O 8. c3 d5\n9. exd5 Nxd5 10. Nxe5 Nxe5 11. Rxe5 c6 12. d4 Bd6 13. Re1 Qh4 14. g3 Qh3\n15. Be3 Bg4 16. Qd3 Rae8 17. Nd2 Re6 18. a4 Qh5 19. axb5 axb5 20. Ra6 Bb8\n21. Qf1 Rfe8 22. Bxd5 Qxd5 23. Qg2 Qh5 24. Rxc6 Bh3 25. Qf3 Bg4 26. Qg2 Bh3\n27. Qf3 Bg4 28. Qg2 Bh3 29. Qf3 Bg4 30. Qg2 Bh3 31. Qf3 Bg4 32. Qg2 Bh3\n33. Qf3 Bg4 34. Qg2 Bh3 35. Qf3 Bg4 36. Qg2 Bh3 37. Qf3 Bg4 38. Qg2 Bh3\n39. Qf3 Bg4 40. Qg2 Bh3 41. Qf3 Bg4 42. Qg2 Bh3 43. Qf3 Bg4 44. Qg2 Bh3\n45. Qf3 Bg4 46. Qg2 Bh3 47. Qf3 Bg4 48. Qg2 Bh3 49. Qf3 Bg4 50. Qg2 Bh3\n51. Qf3 Bg4 52. Qg2 Bh3 53. Qf3 Bg4 54. Qg2 Bh3 55. Qf3 Bg4 56. Qg2 Bh3\n57. Qf3 Bg4 58. Qg2 Bh3 59. Qf3 Bg4 60. Qg2 Bh3 61. Qf3 Bg4 62. Qg2 Bh3\n63. Qf3 Bg4 64. Qg2 Bh3 65. Qf3 Bg4 66. Qg2 Bh3 67. Qf3 Bg4 68. Qg2 Bh3\n69. Qf3 Bg4 70. Qg2 Bh3 71. Qf3 Bg4 72. Qg2 Bh3 73. Qf3 Bg4 74. Qg2 Bh3\n75. Qf3 Bg4 76. Qg2 Bh3 77. Qf3 Bg4 78. Qg2",
          "positional_tokens": {
            "prompt_tokens": 122,
            "completion_tokens": 640,
            "total_tokens": 762,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "O-O",
            "c3",
            "d5",
            "exd5",
            "Nxd5",
            "Nxe5",
            "Nxe5",
            "Rxe5",
            "c6",
            "d4",
            "Bd6",
            "Re1",
            "Qh4",
            "g3",
            "Qh3",
            "Be3",
            "Bg4",
            "Qd3",
            "Rae8",
            "Nd2",
            "Re6",
            "a4",
            "Qh5",
            "axb5",
            "axb5",
            "Ra6",
            "Bb8",
            "Qf1",
            "Rfe8",
            "Bxd5",
            "Qxd5",
            "Qg2",
            "Qh5",
            "Rxc6",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc6",
          "neutral_uci": "b8c6",
          "neutral_response": "2... Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 O-O 8. c3 d5\n9. exd5 Nxd5 10. Nxe5 Nxe5 11. Rxe5 c6 12. d4 Bd6 13. Re1 Qh4 14. g3 Qh3\n15. Be3 Bg4 16. Qd3 Rae8 17. Nd2 Re6 18. Qf1 Qh5 19. a4 Rfe8 20. axb5 axb5\n21. Bxd5 Qxd5 22. Qg2 Qh5 23. Qxc6 Bxg3 24. Qg2 Bf4 25. Ra8 Bb8 26. Kh1\n26... Bh3 27. Qf3 Bg4 28. Qg2 Bh3 29. Qf3 Bg4 30. Qg2 Bh3 31. Qf3 Bg4\n32. Qg2 Bh3 33. Qf3 Bg4 34. Qg2 Bh3 35. Qf3 Bg4 36. Qg2 Bh3 37. Qf3 Bg4\n38. Qg2 Bh3 39. Qf3 Bg4 40. Qg2 Bh3 41. Qf3 Bg4 42. Qg2 Bh3 43. Qf3 Bg4\n44. Qg2 Bh3 45. Qf3 Bg4 46. Qg2 Bh3 47. Qf3 Bg4 48. Qg2 Bh3 49. Qf3 Bg4\n50. Qg2 Bh3 51. Qf3 Bg4 52. Qg2 Bh3 53. Qf3 Bg4 54. Qg2 Bh3 55. Qf3 Bg4\n56. Qg2 Bh3 57. Qf3 Bg4 58. Qg2 Bh3 59. Qf3 Bg4 60. Qg2 Bh3 61. Qf3 Bg4\n62. Qg2 Bh3 63. Qf3 Bg4 64. Qg2 Bh3 65. Qf3 Bg4 66. Qg2 Bh3 67. Qf3 Bg4\n68. Qg2 Bh3 69. Qf3 Bg4 70. Qg2 Bh3 71. Qf3 Bg4 72. Qg2 Bh3 73. Qf3 Bg4\n74. Qg2 Bh3 75. Qf3 Bg4 76. Qg2 Bh3 77. Qf3 Bg4 78",
          "neutral_tokens": {
            "prompt_tokens": 121,
            "completion_tokens": 640,
            "total_tokens": 761,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "O-O",
            "c3",
            "d5",
            "exd5",
            "Nxd5",
            "Nxe5",
            "Nxe5",
            "Rxe5",
            "c6",
            "d4",
            "Bd6",
            "Re1",
            "Qh4",
            "g3",
            "Qh3",
            "Be3",
            "Bg4",
            "Qd3",
            "Rae8",
            "Nd2",
            "Re6",
            "Qf1",
            "Qh5",
            "a4",
            "Rfe8",
            "axb5",
            "axb5",
            "Bxd5",
            "Qxd5",
            "Qg2",
            "Qh5",
            "Qxc6",
            "Bxg3",
            "Qg2",
            "Bf4",
            "Ra8",
            "Bb8",
            "Kh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b8c6",
          "positional_uci": "b8c6",
          "neutral_uci": "b8c6",
          "consensus_move": "b8c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "O-O",
              "c3",
              "d5",
              "exd5",
              "Nxd5",
              "Nxe5",
              "Nxe5",
              "Rxe5",
              "c6",
              "d4",
              "Bd6",
              "Re1",
              "Qh4",
              "g3",
              "Qh3",
              "Be3",
              "Bg4",
              "Qd3",
              "Rae8",
              "Nd2",
              "Re6",
              "Qf1",
              "Qh5",
              "a4",
              "Rfe8",
              "axb5",
              "axb5",
              "Bxd5",
              "Qxd5",
              "Qg2",
              "Qh5",
              "Qxc6",
              "Bxg3",
              "Qg2",
              "Bf4",
              "Ra8",
              "Bb8",
              "Kh1",
              "Rg6",
              "Bf4",
              "Rxe1+",
              "Nf1",
              "Rxf1+",
              "Qxf1",
              "Bf3+",
              "Qg2",
              "Bxg2+",
              "Kg1",
              "Qd1#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "O-O",
              "c3",
              "d5",
              "exd5",
              "Nxd5",
              "Nxe5",
              "Nxe5",
              "Rxe5",
              "c6",
              "d4",
              "Bd6",
              "Re1",
              "Qh4",
              "g3",
              "Qh3",
              "Be3",
              "Bg4",
              "Qd3",
              "Rae8",
              "Nd2",
              "Re6",
              "a4",
              "Qh5",
              "axb5",
              "axb5",
              "Ra6",
              "Bb8",
              "Qf1",
              "Rfe8",
              "Bxd5",
              "Qxd5",
              "Qg2",
              "Qh5",
              "Rxc6",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "O-O",
              "c3",
              "d5",
              "exd5",
              "Nxd5",
              "Nxe5",
              "Nxe5",
              "Rxe5",
              "c6",
              "d4",
              "Bd6",
              "Re1",
              "Qh4",
              "g3",
              "Qh3",
              "Be3",
              "Bg4",
              "Qd3",
              "Rae8",
              "Nd2",
              "Re6",
              "Qf1",
              "Qh5",
              "a4",
              "Rfe8",
              "axb5",
              "axb5",
              "Bxd5",
              "Qxd5",
              "Qg2",
              "Qh5",
              "Qxc6",
              "Bxg3",
              "Qg2",
              "Bf4",
              "Ra8",
              "Bb8",
              "Kh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 122,
            "completion_tokens": 258,
            "total_tokens": 380,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 122,
            "completion_tokens": 640,
            "total_tokens": 762,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 121,
            "completion_tokens": 640,
            "total_tokens": 761,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 365,
          "total_completion_tokens": 1538,
          "total_tokens": 1903
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 122,
            "completion_tokens": 258,
            "total_tokens": 380,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 122,
            "completion_tokens": 640,
            "total_tokens": 762,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 121,
            "completion_tokens": 640,
            "total_tokens": 761,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R b KQkq - 1 2"
    },
    {
      "turn": 6,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf6",
          "aggressive_uci": "g8f6",
          "aggressive_response": "3... Nf6 4. d3 Bc5 5. O-O O-O 6. Nc3 d6 7. Bg5 Be6 8. Nd5 Bxd5 9. Bxd5 h6 10. Bh4 g5 11. Bg3 Nxd5 12. exd5 Ne7 13. c4 f5 14. h3 f4 15. Bh2 Nf5 16. a3 a5 17. Qd2 Qf6 18. b4 axb4 19. axb4 Rxa1 20. Rxa1 e4 21. dxe4 Qxa1+ 22. Ne1 Ng3 23. bxc5 Nxe4 24. Qe2 Re8 25. cxd6 Nxd6 26. Qf1 Rxe1 27. Qxe1 Qxe1# 1-0\n3... Nf6 4. d3 Bc5 5. O-O O-O 6. Nc3 d6 7. Bg5 Be6 8. Nd5 Bxd5 9. Bxd5 h6 10. Bh4 g5 11. Bg3 Nxd5 12. exd5 Ne7 13. c4 f5 14. h3 f4 15. Bh2 Nf5 16. a3 a5 17. Qd2 Qf6 18. b4 axb4 19. axb4 Rxa1 20. Rxa1 e4 21. dxe4 Qxa1+ 22. Ne1 Ng3 23. bxc5 Nxe4 24. Qe2 Re8 25. cxd6 Nxd6 26. Qf1 Rxe1 27. Qxe1 Qxe1# 1-0 Nf6 d3 Bc5 O-O O-O Nc3 d6 Bg5 Be6 Nd5 Bxd5 Bh4 g5 Bg3 Nxd5 exd5 Ne7 c4 f5 h3 f4 Bh2 Nf5 a3 a5 Qd2 Qf6 b4 axb4 axb4 Rxa1 Rxa1 e4 dxe4 Qxe4 Re8 Qe2 c6 bxc5 dxc5 Qb2 Nd4 Nxd4 cxd4 Qxb7 Re7 Qc8+ Kg7 Qxc5 Qe5 Ra6 f3 g3 Qe1+ Kh2 Qxf2+ Kh1 Qg2# 1-0 Nf6 d3 Bc5 O-O O-O Nc3 d6 Bg5 Be6 Nd5 Bxd5 Bh4 g5 Bg3 Nxd5 exd5 Ne7 c4 f5 h3 f4 Bh2 Nf5 a3 a5 Qd2 Qf6 b4 axb4 axb4 R",
          "aggressive_tokens": {
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d3",
            "Bc5",
            "O-O",
            "O-O",
            "Nc3",
            "d6",
            "Bg5",
            "Be6",
            "Nd5",
            "Bxd5",
            "Bh4",
            "g5",
            "Bg3",
            "Nxd5",
            "exd5",
            "Ne7",
            "c4",
            "f5",
            "h3",
            "f4",
            "Bh2",
            "Nf5",
            "a3",
            "a5",
            "Qd2",
            "Qf6",
            "b4",
            "axb4",
            "axb4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf6",
          "positional_uci": "g8f6",
          "positional_response": "3... Nf6 4. d3 Bc5 5. O-O O-O 6. Nc3 d6 7. Bg5 Be6 8. Nd5 Bxd5 9. Bxd5 h6 10. Bh4 g5 11. Bg3 Nxd5 12. exd5 Ne7 13. c4 f5 14. h3 f4 15. Bh2 Nf5 16. Nd2 Qe8 17. Ne4 Bd4 18. Rb1 Qg6 19. Kh1 h5 20. g4 hxg4 21. Qxg4 Kf7 22. Nxg5+ Ke7 23. Ne6 Qxg4 24. hxg4 Rh8 25. gxf5 Rh3 26. Kg2 Rah8 27. Rh1 f3+ 28. Kg1 Rg8+ 29. Kf1 Rg2 30. Nxd4 exd4 31. Re1+ Kf7 32. Re4 Rhxh2 33. Rxh2 Rxh2 34. Kg1 Rg2+ 35. Kf1 Rh2 36. Kg1 Rg2+ 37. Kf1 Rh2 38. Kg1 Rg2+ 39. Kf1 Rh2 40. Kg1 Rg2+ 41. Kf1 Rh2 42. Kg1 Rg2+ 43. Kf1 Rh2 44. Kg1 Rg2+ 45. Kf1 Rh2 46. Kg1 Rg2+ 47. Kf1 Rh2 48. Kg1 Rg2+ 49. Kf1 Rh2 50. Kg1 Rg2+ 51. Kf1 Rh2 52. Kg1 Rg2+ 53. Kf1 Rh2 54. Kg1 Rg2+ 55. Kf1 Rh2 56. Kg1 Rg2+ 57. Kf1 Rh2 58. Kg1 Rg2+ 59. Kf1 Rh2 60. Kg1 Rg2+ 61. Kf1 Rh2 62. Kg1 Rg2+ 63. Kf1 Rh2 64. Kg1 Rg2+ 65. Kf1 Rh2 66. Kg1 Rg2+ 67. Kf1 Rh2 68. Kg1 Rg2+ 69. Kf1 Rh2 70. Kg1 Rg2+ 71. Kf1 Rh2 72. Kg1 Rg2+ 73. Kf1 Rh2 74. Kg1 Rg2+ 75. Kf1 Rh2 76. Kg1 Rg2+ 77. Kf1 Rh2 78. Kg1 Rg2+ 79. Kf1",
          "positional_tokens": {
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d3",
            "Bc5",
            "O-O",
            "O-O",
            "Nc3",
            "d6",
            "Bg5",
            "Be6",
            "Nd5",
            "Bxd5",
            "Bxd5",
            "h6",
            "Bh4",
            "g5",
            "Bg3",
            "Nxd5",
            "exd5",
            "Ne7",
            "c4",
            "f5",
            "h3",
            "f4",
            "Bh2",
            "Nf5",
            "Nd2",
            "Qe8",
            "Ne4",
            "Bd4",
            "Rb1",
            "Qg6",
            "Kh1",
            "h5",
            "g4",
            "hxg4",
            "Qxg4",
            "Kf7",
            "Nxg5+",
            "Ke7",
            "Ne6",
            "Qxg4",
            "hxg4",
            "Rh8",
            "gxf5",
            "Rh3",
            "Kg2",
            "Rah8",
            "Rh1",
            "f3+",
            "Kg1",
            "Rg8+",
            "Kf1",
            "Rg2",
            "Nxd4",
            "exd4",
            "Re1+",
            "Kf7",
            "Re4",
            "Rhxh2",
            "Rxh2",
            "Rxh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf6",
          "neutral_uci": "g8f6",
          "neutral_response": "3... Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. Qf3+ Ke6 8. Nc3 Ne7 9. d4 c6 10. O-O b5 11. Bb3 a5 12. Re1 a4 13. Rxe5+ Kd6 14. Bxd5 Nxd5 15. Nxd5 cxd5 16. Qxd5+ Kc7 17. Qxa8 Qxd4 18. Qa5+ Kd6 19. Re1 Bd7 20. Be3 Qxb2 21. Rad1+ Kc6 22. Qb6# e4 Nf6 Qc3#",
          "neutral_tokens": {
            "prompt_tokens": 130,
            "completion_tokens": 174,
            "total_tokens": 304,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qc3#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g8f6",
          "positional_uci": "g8f6",
          "neutral_uci": "g8f6",
          "consensus_move": "g8f6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d3",
              "Bc5",
              "O-O",
              "O-O",
              "Nc3",
              "d6",
              "Bg5",
              "Be6",
              "Nd5",
              "Bxd5",
              "Bh4",
              "g5",
              "Bg3",
              "Nxd5",
              "exd5",
              "Ne7",
              "c4",
              "f5",
              "h3",
              "f4",
              "Bh2",
              "Nf5",
              "a3",
              "a5",
              "Qd2",
              "Qf6",
              "b4",
              "axb4",
              "axb4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "d3",
              "Bc5",
              "O-O",
              "O-O",
              "Nc3",
              "d6",
              "Bg5",
              "Be6",
              "Nd5",
              "Bxd5",
              "Bxd5",
              "h6",
              "Bh4",
              "g5",
              "Bg3",
              "Nxd5",
              "exd5",
              "Ne7",
              "c4",
              "f5",
              "h3",
              "f4",
              "Bh2",
              "Nf5",
              "Nd2",
              "Qe8",
              "Ne4",
              "Bd4",
              "Rb1",
              "Qg6",
              "Kh1",
              "h5",
              "g4",
              "hxg4",
              "Qxg4",
              "Kf7",
              "Nxg5+",
              "Ke7",
              "Ne6",
              "Qxg4",
              "hxg4",
              "Rh8",
              "gxf5",
              "Rh3",
              "Kg2",
              "Rah8",
              "Rh1",
              "f3+",
              "Kg1",
              "Rg8+",
              "Kf1",
              "Rg2",
              "Nxd4",
              "exd4",
              "Re1+",
              "Kf7",
              "Re4",
              "Rhxh2",
              "Rxh2",
              "Rxh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qc3#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 130,
            "completion_tokens": 174,
            "total_tokens": 304,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 392,
          "total_completion_tokens": 1454,
          "total_tokens": 1846
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 130,
            "completion_tokens": 174,
            "total_tokens": 304,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkbnr/pppp1ppp/2n5/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R b KQkq - 3 3"
    },
    {
      "turn": 8,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nxe4",
          "aggressive_uci": "f6e4",
          "aggressive_response": "4... Nxe4 5. Nxe4 d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. Re1 Bd6 11. d4 O-O 12. dxe5 Nxe5 13. g4 Nxf3+ 14. Qxf3 Bg6 15. Qxb7 Qh4 16. Qg2 Bxc2 17. Be3 Rab8 18. b3 Be5 19. Rac1 Bxc3 20. Rxc2 Bxe1 21. Rxc7 Rfd8 22. Qf3 Qf6 23. Qe2 Ba5 24. Rxa7 Bb6 25. Bxb6 Rxb6 26. Re7 Re6 27. Rxe6 Qxe6 28. Qc2 Qe1+ 29. Kg2 Qd2 30. Qb1 Qd5+ 31. Kg1 Qd1+ 32. Qxd1 Rxd1+ 33. Kg2 Ra1 34. a4 Rb1 35. a5 Rxb3 36. a6 Ra3 37. a7 Rxa7 38. f4 Ra3 39. h4 Kf8 40. g5 Ke7 41. f5 Kd6 42. h5 Ke5 43. g6 fxg6 44. fxg6 hxg6 45. hxg6 Kf4 46. Kf2 Ra2+ 47. Ke1 Ke3 48. Kd1 Kd3 49. Kc1 Kc3 50. Kb1 Rh2 51. Ka1 Kb3 52. Kb1 Rh1# e4 d5 Nf3 Nc6 Bc4 Nf6 Nc3 Nxe4 Nxe4 d5 Bxd5 Qxd5 Nc6 Qd8 O-O Bg4 h3 Bh5 Re1 Bd6 d4 O-O dxe5 Nxe5 g4 Nxf3 Qxf3 Bg6 Qxb7 Qh4 Qg2 Bxc2 Be3 Rab8 b3 Be5 Rac1 Bxc3 Rxc2 Bxe1 Rxc7 Rfd8 Qf3 Qf6 Qe2 Ba5 Rxa7 Bb6 Bxb6 Rxb6 Re7 Re6 Rxe6 Qxe6 Qc2 Qe1+ Kg2 Qd2 Qb1 Qd5+ Kg1 Qd1+ Qxd1 Rxd1+ Kg2 Rd2 a4 Rb2 a5 Rxb3 a6 Ra3 a7 Rxa7 f4 Ra3 h4 Kf8 g5 Ke7 f5 Kd6 h5 Ke5 g6 fxg6 fxg6 hxg6 Kg2 Kf4 Kf2",
          "aggressive_tokens": {
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Bxd5",
            "Qxd5",
            "Nc6",
            "Qd8",
            "O-O",
            "Bg4",
            "h3",
            "Bh5",
            "Re1",
            "Bd6",
            "d4",
            "O-O",
            "dxe5",
            "Nxe5",
            "g4",
            "Nxf3",
            "Qxf3",
            "Bg6",
            "Qxb7",
            "Qh4",
            "Qg2",
            "Bxc2",
            "Be3",
            "Rab8",
            "b3",
            "Be5",
            "Rac1",
            "Bxc3",
            "Rxc2",
            "Bxe1",
            "Rxc7",
            "Rfd8",
            "Qf3",
            "Qf6",
            "Qe2",
            "Ba5",
            "Rxa7",
            "Bb6",
            "Bxb6",
            "Rxb6",
            "Re7",
            "Re6",
            "Rxe6",
            "Qxe6",
            "Qc2",
            "Qe1+",
            "Kg2",
            "Qd2",
            "Qb1",
            "Qd5+",
            "Kg1",
            "Qd1+",
            "Qxd1",
            "Rxd1+",
            "Kg2",
            "Rd2",
            "a4",
            "Rb2",
            "a5",
            "Rxb3",
            "a6",
            "Ra3",
            "a7",
            "Rxa7",
            "f4",
            "Ra3",
            "h4",
            "Kf8",
            "g5",
            "Ke7",
            "f5",
            "Kd6",
            "h5",
            "Ke5",
            "g6",
            "fxg6",
            "fxg6",
            "hxg6",
            "Kg2",
            "Kf4",
            "Kf2"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nxe4",
          "positional_uci": "f6e4",
          "positional_response": "4... Nxe4 5. Nxe4 d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. Re1 Bd6 11. d4 O-O 12. dxe5 Nxe5 13. g4 Nxf3+ 14. Qxf3 Bg6 15. Qxb7 Qh4 16. Qg2 Bxc2 17. Be3 Rab8 18. b3 Be5 19. Rac1 Bxc3 20. Rxc2 Bxe1 21. Rxc7 Rbc8 22. Rxa7 Rc2 23. a4 Re8 24. Ra8 Rcc8 25. Rxc8 Rxc8 26. Qb7 Qd8 27. b4 Rb8 28. Qe4 Bxb4 29. Bf4 Bd6 30. Be3 Qe7 31. Qc6 Qe5 32. Kf1 Rb1+ 33. Ke2 Rb2+ 34. Kf1 Qh2 35. Qe8+ Bf8 36. Qe4 Qxh3+ 37. Ke1 Bb4+ 38. Kd1 Qf1# 39. Qe1# Bxe1 40. Kxe1 Qe2# 41. Bxe2 Rxe2+ 42. Kf1 Rxe3 43. fxe3 Kf8 44. a5 Ke8 45. a6 Kd8 46. a7 Kc8 47. a8=Q+ Kc7 48. Qa7+ Kd6 49. Qxf7 Ke5 50. Qxg7+ Ke4 51. Qxh7+ Kxe3 52. g5 Kf4 53. g6 Kg5 54. g7 Kf6 55. g8=Q Ke5 56. Qg5+ Kd4 57. Qhh4+ Kd3 58. Qgg3+ Kd2 59. Qhh2+ Kd1 60. Qd3+ Kc1 61. Qhc2# e5 d5 e6 dxe6 fxe6 Qxe6 Qd8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# B",
          "positional_tokens": {
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Bxd5",
            "Qxd5",
            "Nc3",
            "Qd8",
            "O-O",
            "Bg4",
            "h3",
            "Bh5",
            "Re1",
            "Bd6",
            "d4",
            "O-O",
            "dxe5",
            "Nxe5",
            "g4",
            "Nxf3+",
            "Qxf3",
            "Bg6",
            "Qxb7",
            "Qh4",
            "Qg2",
            "Bxc2",
            "Be3",
            "Rab8",
            "b3",
            "Be5",
            "Rac1",
            "Bxc3",
            "Rxc2",
            "Bxe1",
            "Rxc7",
            "Rbc8",
            "Rxa7",
            "Rc2",
            "a4",
            "Re8",
            "Ra8",
            "Rcc8",
            "Rxc8",
            "Rxc8",
            "Qb7",
            "Qd8",
            "b4",
            "Rb8",
            "Qe4",
            "Bxb4",
            "Bf4",
            "Bd6",
            "Be3",
            "Qe7",
            "Qc6",
            "Qe5",
            "Kf1",
            "Rb1+",
            "Ke2",
            "Rb2+",
            "Kf1",
            "Qh2",
            "Qe8+",
            "Bf8",
            "Qe4",
            "Qxh3+",
            "Ke1",
            "Bb4+",
            "Kd1",
            "Qf1#",
            "Qe1#",
            "Bxe1",
            "Kxe1",
            "Qe2#",
            "Bxe2",
            "Rxe2+",
            "Kf1",
            "Rxe3",
            "fxe3",
            "Kf8",
            "a5",
            "Ke8",
            "a6",
            "Kd8",
            "a7",
            "Kc8",
            "a8=Q+",
            "Kc7",
            "Qa7+",
            "Kd6",
            "Qxf7",
            "Ke5",
            "Qxg7+",
            "Ke4",
            "Qxh7+",
            "Kxe3",
            "g5",
            "Kf4",
            "g6",
            "Kg5",
            "g7",
            "Kf6",
            "g8=Q",
            "Ke5",
            "Qg5+",
            "Kd4",
            "Qhh4+",
            "Kd3",
            "Qgg3+",
            "Kd2",
            "Qhh2+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qhc2#",
            "e5",
            "d5",
            "e6",
            "dxe6",
            "fxe6",
            "Qxe6",
            "Qd8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nxe4",
          "neutral_uci": "f6e4",
          "neutral_response": "4... Nxe4 5. Nxe4 d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. Re1 Bd6 11. d4 O-O 12. dxe5 Nxe5 13. g4 Nxf3+ 14. Qxf3 Bg6 15. Qxb7 Qh4 16. Qg2 Bxc2 17. Be3 Rab8 18. b3 Be5 19. Rac1 Bxc3 20. Rxc2 Bxe1 21. Rxc7 Rbc8 22. Rxa7 Rc2 23. a4 Rfc8 24. a5 Bxf2+ 25. Bxf2 Rxf2 26. Qxf2 Rc1+ 27. Kg2 Qxf2+ 28. Kxf2 Rc2+ 29. Kg3 Rc3+ 30. Kh4 Rxb3 31. Ra8+ Rb8 32. Rxb8# Rxb8 33. a6 Ra8 34. a7 Rxa7 35. Kg3 Ra3+ 36. Kh4 f6 37. Kh5 Rxh3# Rxe1 g6+ Kh7 g5#",
          "neutral_tokens": {
            "prompt_tokens": 139,
            "completion_tokens": 292,
            "total_tokens": 431,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d5",
            "Bxd5",
            "Qxd5",
            "Nc3",
            "Qd8",
            "O-O",
            "Bg4",
            "h3",
            "Bh5",
            "Re1",
            "Bd6",
            "d4",
            "O-O",
            "dxe5",
            "Nxe5",
            "g4",
            "Nxf3+",
            "Qxf3",
            "Bg6",
            "Qxb7",
            "Qh4",
            "Qg2",
            "Bxc2",
            "Be3",
            "Rab8",
            "b3",
            "Be5",
            "Rac1",
            "Bxc3",
            "Rxc2",
            "Bxe1",
            "Rxc7",
            "Rbc8",
            "Rxa7",
            "Rc2",
            "a4",
            "Rfc8",
            "a5",
            "Bxf2+",
            "Bxf2",
            "Rxf2",
            "Qxf2",
            "Rc1+",
            "Kg2",
            "Qxf2+",
            "Kxf2",
            "Rc2+",
            "Kg3",
            "Rc3+",
            "Kh4",
            "Rxb3",
            "Ra8+",
            "Rb8",
            "Rxb8#",
            "Rxb8",
            "a6",
            "Ra8",
            "a7",
            "Rxa7",
            "Kg3",
            "Ra3+",
            "Kh4",
            "f6",
            "Kh5",
            "Rxh3#",
            "Rxe1",
            "g6+",
            "Kh7",
            "g5#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f6e4",
          "positional_uci": "f6e4",
          "neutral_uci": "f6e4",
          "consensus_move": "f6e4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d5",
              "Bxd5",
              "Qxd5",
              "Nc6",
              "Qd8",
              "O-O",
              "Bg4",
              "h3",
              "Bh5",
              "Re1",
              "Bd6",
              "d4",
              "O-O",
              "dxe5",
              "Nxe5",
              "g4",
              "Nxf3",
              "Qxf3",
              "Bg6",
              "Qxb7",
              "Qh4",
              "Qg2",
              "Bxc2",
              "Be3",
              "Rab8",
              "b3",
              "Be5",
              "Rac1",
              "Bxc3",
              "Rxc2",
              "Bxe1",
              "Rxc7",
              "Rfd8",
              "Qf3",
              "Qf6",
              "Qe2",
              "Ba5",
              "Rxa7",
              "Bb6",
              "Bxb6",
              "Rxb6",
              "Re7",
              "Re6",
              "Rxe6",
              "Qxe6",
              "Qc2",
              "Qe1+",
              "Kg2",
              "Qd2",
              "Qb1",
              "Qd5+",
              "Kg1",
              "Qd1+",
              "Qxd1",
              "Rxd1+",
              "Kg2",
              "Rd2",
              "a4",
              "Rb2",
              "a5",
              "Rxb3",
              "a6",
              "Ra3",
              "a7",
              "Rxa7",
              "f4",
              "Ra3",
              "h4",
              "Kf8",
              "g5",
              "Ke7",
              "f5",
              "Kd6",
              "h5",
              "Ke5",
              "g6",
              "fxg6",
              "fxg6",
              "hxg6",
              "Kg2",
              "Kf4",
              "Kf2"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "d5",
              "Bxd5",
              "Qxd5",
              "Nc3",
              "Qd8",
              "O-O",
              "Bg4",
              "h3",
              "Bh5",
              "Re1",
              "Bd6",
              "d4",
              "O-O",
              "dxe5",
              "Nxe5",
              "g4",
              "Nxf3+",
              "Qxf3",
              "Bg6",
              "Qxb7",
              "Qh4",
              "Qg2",
              "Bxc2",
              "Be3",
              "Rab8",
              "b3",
              "Be5",
              "Rac1",
              "Bxc3",
              "Rxc2",
              "Bxe1",
              "Rxc7",
              "Rbc8",
              "Rxa7",
              "Rc2",
              "a4",
              "Re8",
              "Ra8",
              "Rcc8",
              "Rxc8",
              "Rxc8",
              "Qb7",
              "Qd8",
              "b4",
              "Rb8",
              "Qe4",
              "Bxb4",
              "Bf4",
              "Bd6",
              "Be3",
              "Qe7",
              "Qc6",
              "Qe5",
              "Kf1",
              "Rb1+",
              "Ke2",
              "Rb2+",
              "Kf1",
              "Qh2",
              "Qe8+",
              "Bf8",
              "Qe4",
              "Qxh3+",
              "Ke1",
              "Bb4+",
              "Kd1",
              "Qf1#",
              "Qe1#",
              "Bxe1",
              "Kxe1",
              "Qe2#",
              "Bxe2",
              "Rxe2+",
              "Kf1",
              "Rxe3",
              "fxe3",
              "Kf8",
              "a5",
              "Ke8",
              "a6",
              "Kd8",
              "a7",
              "Kc8",
              "a8=Q+",
              "Kc7",
              "Qa7+",
              "Kd6",
              "Qxf7",
              "Ke5",
              "Qxg7+",
              "Ke4",
              "Qxh7+",
              "Kxe3",
              "g5",
              "Kf4",
              "g6",
              "Kg5",
              "g7",
              "Kf6",
              "g8=Q",
              "Ke5",
              "Qg5+",
              "Kd4",
              "Qhh4+",
              "Kd3",
              "Qgg3+",
              "Kd2",
              "Qhh2+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qhc2#",
              "e5",
              "d5",
              "e6",
              "dxe6",
              "fxe6",
              "Qxe6",
              "Qd8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "d5",
              "Bxd5",
              "Qxd5",
              "Nc3",
              "Qd8",
              "O-O",
              "Bg4",
              "h3",
              "Bh5",
              "Re1",
              "Bd6",
              "d4",
              "O-O",
              "dxe5",
              "Nxe5",
              "g4",
              "Nxf3+",
              "Qxf3",
              "Bg6",
              "Qxb7",
              "Qh4",
              "Qg2",
              "Bxc2",
              "Be3",
              "Rab8",
              "b3",
              "Be5",
              "Rac1",
              "Bxc3",
              "Rxc2",
              "Bxe1",
              "Rxc7",
              "Rbc8",
              "Rxa7",
              "Rc2",
              "a4",
              "Rfc8",
              "a5",
              "Bxf2+",
              "Bxf2",
              "Rxf2",
              "Qxf2",
              "Rc1+",
              "Kg2",
              "Qxf2+",
              "Kxf2",
              "Rc2+",
              "Kg3",
              "Rc3+",
              "Kh4",
              "Rxb3",
              "Ra8+",
              "Rb8",
              "Rxb8#",
              "Rxb8",
              "a6",
              "Ra8",
              "a7",
              "Rxa7",
              "Kg3",
              "Ra3+",
              "Kh4",
              "f6",
              "Kh5",
              "Rxh3#",
              "Rxe1",
              "g6+",
              "Kh7",
              "g5#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 139,
            "completion_tokens": 292,
            "total_tokens": 431,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 419,
          "total_completion_tokens": 1572,
          "total_tokens": 1991
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 139,
            "completion_tokens": 292,
            "total_tokens": 431,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/pppp1ppp/2n2n2/4p3/2B1P3/2N2N2/PPPP1PPP/R1BQK2R b KQkq - 5 4"
    },
    {
      "turn": 10,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d5",
          "aggressive_uci": "d7d5",
          "aggressive_response": "5... d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. d3 Nd4 11. g4 Nxf3+ 12. Qxf3 Bg6 13. Qxb7 Bd6 14. Qc6+ Kf8 15. Be3 h5 16. g5 Rb8 17. b3 Qc8 18. Kg2 Bf5 19. Rh1 Rb4 20. Bc5 Bxc5 21. Qxc5+ Kg8 22. Qxb4 Qa8+ 23. f3 Kh7 24. Ne4 Rb8 25. Qe7 Qc6 26. Rac1 Re8 27. Qxf7 Bg6 28. Qc4 Qd7 29. Rhf1 Rf8 30. Rce1 Rf4 31. Qc5 Rh4 32. Rh1 Qf5 33. Qxc7 Rf4 34. Ref1 h4 35. Qxa7 Bh5 36. Qe3 Qg6 37. Rhg1 Qf5 38. Kh2 Bxf3 39. Rxf3 Rxf3 40. g6+ Kg8 41. Qg5 Qxh3# 0-1 1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. d3 Nd4 11. g4 Nxf3+ 12. Qxf3 Bg6 13. Qxb7 Bd6 14. Qc6+ Kf8 15. Be3 h5 16. g5 Rb8 17. b3 Qc8 18. Kg2 Bf5 19. Rh1 Rb4 20. Bc5 Bxc5 21. Qxc5+ Kg8 22. Qxb4 Qa8+ 23. f3 Kh7 24. Ne4 Rb8 25. Qe7 Qc6 26. Rac1 Re8 27. Qxf7 Bg6 28. Qc4 Qd7 29. Rhf1 Rf8 30. Rce1 Rf4 31. Qc5 Rh4 32. Rh1 Qf5 33. Qxc7 Rf4 34. Ref1 h4 35. Qxa7 Bh5 36. Qe3 Qg6 37. Rhg1 Qf5 38. Kh2 Bxf3 39. Rxf3 Rxf3",
          "aggressive_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bxd5",
            "Qxd5",
            "Nc3",
            "Qd8",
            "O-O",
            "Bg4",
            "h3",
            "Bh5",
            "d3",
            "Nd4",
            "g4",
            "Nxf3+",
            "Qxf3",
            "Bg6",
            "Qxb7",
            "Bd6",
            "Qc6+",
            "Kf8",
            "Be3",
            "h5",
            "g5",
            "Rb8",
            "b3",
            "Qc8",
            "Kg2",
            "Bf5",
            "Rh1",
            "Rb4",
            "Bc5",
            "Bxc5",
            "Qxc5+",
            "Kg8",
            "Qxb4",
            "Qa8+",
            "f3",
            "Kh7",
            "Ne4",
            "Rb8",
            "Qe7",
            "Qc6",
            "Rac1",
            "Re8",
            "Qxf7",
            "Bg6",
            "Qc4",
            "Qd7",
            "Rhf1",
            "Rf8",
            "Rce1",
            "Rf4",
            "Qc5",
            "Rh4",
            "Rh1",
            "Qf5",
            "Qxc7",
            "Rf4",
            "Ref1",
            "h4",
            "Qxa7",
            "Bh5",
            "Qe3",
            "Qg6",
            "Rhg1",
            "Qf5",
            "Kh2",
            "Bxf3",
            "Rxf3",
            "Rxf3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d5",
          "positional_uci": "d7d5",
          "positional_response": "5... d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. g4 Bg6 11. d3 Bc5 12. Be3 Bxe3 13. fxe3 O-O 14. e4 Nd4 15. Nxe5 Qh4 16. Kg2 Rae8 17. Nxg6 fxg6 18. Rxf8+ Rxf8 19. Qd2 Rf3 20. Rh1 Qg3# 0-1\n1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. g4 Bg6 11. d3 Bc5 12. Be3 Bxe3 13. fxe3 O-O 14. e4 Nd4 15. Nxe5 Qh4 16. Kg2 Rae8 17. Nxg6 fxg6 18. Rxf8+ Rxf8 19. Qd2 Rf3 20. Rh1 Qg3# 0-1 d5 Qxd5 Nc3 Qd8 O-O Bg4 h3 Bh5 g4 Bg6 d3 Bc5 Be3 Bxe3 fxe3 O-O e4 Nd4 Nxe5 Qh4 Kg2 Rae8 Nxg6 fxg6 Rxf8+ Rxf8 Qd2 Rf3 Rh1 Qg3# 0-1",
          "positional_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 388,
            "total_tokens": 537,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qxd5",
            "Nc3",
            "Qd8",
            "O-O",
            "Bg4",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "d3",
            "Bc5",
            "Be3",
            "Bxe3",
            "fxe3",
            "O-O",
            "e4",
            "Nd4",
            "Nxe5",
            "Qh4",
            "Kg2",
            "Rae8",
            "Nxg6",
            "fxg6",
            "Rxf8+",
            "Rxf8",
            "Qd2",
            "Rf3",
            "Rh1",
            "Qg3#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d5",
          "neutral_uci": "d7d5",
          "neutral_response": "5... d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. Re1 Bd6 11. d3 O-O 12. Ne4 Nd4 13. g4 Nxf3+ 14. Qxf3 Bg6 15. Nxd6 cxd6 16. Qxb7 Qh4 17. Qg2 f5 18. gxf5 Bxf5 19. Re3 Rac8 20. c3 Rf6 21. Rg3 Rg6 22. Rxg6 Bxg6 23. Qg3 Qh5 24. Be3 Bxd3 25. Bxa7 Be4 26. Re1 Ba8 27. Qg4 Qe8 28. Rd1 Rd8 29. Bb6 Rb8 30. Rxd6 Qb5 31. Qe6+ Kh8 32. Rd8+ Rxd8 33. Bxd8 Qb8 34. Qe8# d5 35. Qf8# d4 36. cxd4 exd4 37. Qe7 d3 38. Qf8# d2 39. Qe7 d1=Q+ 40. Kh2 Qh1+ 41. Kg3 Qbf3+ 42. Kh4 Qhxh3+ 43. Kg5 Qhg4# d5 44. Qf8# d4 45. Qe7 d3 46. Qf8# d2 47. Qe7 d1=Q 48. Qf8# d5 49. Qe7 d4 50. Qf8# d3 51. Qe7 d2 52. Qf8# d5 53. Qe7 d4 54. Qf8# d3 55. Qe7 d2 56. Qf8# d5 57. Qe7 d4 58. Qf8# d3 59. Qe7 d2 60. Qf8# d5 61. Qe7 d4 62. Qf8# d3 63. Qe7 d2 64. Qf8# d5 65. Qe7 d4 66. Qf8# d3 67. Qe7 d2 68. Qf8# d5 69. Qe7 d4 70. Qf8# d3 71. Qe7 d2 72. Qf8# d5 73. Qe7 d4 74. Qf8# d3 75. Qe7 d2 76. Qf8# d5 77. Qe7 d4 78. Qf8# d3 79. Qe7 d",
          "neutral_tokens": {
            "prompt_tokens": 148,
            "completion_tokens": 640,
            "total_tokens": 788,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Qe7",
            "d4",
            "Qf8#",
            "d3",
            "Qe7"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d7d5",
          "positional_uci": "d7d5",
          "neutral_uci": "d7d5",
          "consensus_move": "d7d5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bxd5",
              "Qxd5",
              "Nc3",
              "Qd8",
              "O-O",
              "Bg4",
              "h3",
              "Bh5",
              "d3",
              "Nd4",
              "g4",
              "Nxf3+",
              "Qxf3",
              "Bg6",
              "Qxb7",
              "Bd6",
              "Qc6+",
              "Kf8",
              "Be3",
              "h5",
              "g5",
              "Rb8",
              "b3",
              "Qc8",
              "Kg2",
              "Bf5",
              "Rh1",
              "Rb4",
              "Bc5",
              "Bxc5",
              "Qxc5+",
              "Kg8",
              "Qxb4",
              "Qa8+",
              "f3",
              "Kh7",
              "Ne4",
              "Rb8",
              "Qe7",
              "Qc6",
              "Rac1",
              "Re8",
              "Qxf7",
              "Bg6",
              "Qc4",
              "Qd7",
              "Rhf1",
              "Rf8",
              "Rce1",
              "Rf4",
              "Qc5",
              "Rh4",
              "Rh1",
              "Qf5",
              "Qxc7",
              "Rf4",
              "Ref1",
              "h4",
              "Qxa7",
              "Bh5",
              "Qe3",
              "Qg6",
              "Rhg1",
              "Qf5",
              "Kh2",
              "Bxf3",
              "Rxf3",
              "Rxf3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qxd5",
              "Nc3",
              "Qd8",
              "O-O",
              "Bg4",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "d3",
              "Bc5",
              "Be3",
              "Bxe3",
              "fxe3",
              "O-O",
              "e4",
              "Nd4",
              "Nxe5",
              "Qh4",
              "Kg2",
              "Rae8",
              "Nxg6",
              "fxg6",
              "Rxf8+",
              "Rxf8",
              "Qd2",
              "Rf3",
              "Rh1",
              "Qg3#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qe7",
              "d4",
              "Qf8#",
              "d3",
              "Qe7"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 149,
            "completion_tokens": 388,
            "total_tokens": 537,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 148,
            "completion_tokens": 640,
            "total_tokens": 788,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 446,
          "total_completion_tokens": 1668,
          "total_tokens": 2114
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 149,
            "completion_tokens": 388,
            "total_tokens": 537,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 148,
            "completion_tokens": 640,
            "total_tokens": 788,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bqkb1r/pppp1ppp/2n5/4p3/2B1N3/5N2/PPPP1PPP/R1BQK2R b KQkq - 0 5"
    },
    {
      "turn": 12,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "dxe4",
          "aggressive_uci": "d5e4",
          "aggressive_response": "6... dxe4 7. Bxe4 Bd6 8. O-O O-O 9. d3 Bg4 10. h3 Bh5 11. g4 Bg6 12. Bxg6 hxg6 13. Be3 f5 14. gxf5 gxf5 15. Ng5 Qe8 16. Kh1 f4 17. Bd2 Rf5 18. Rg1 Be7 19. Ne6 Bf6 20. Nxc7 Qd7 21. Nxa8 Rh5 22. Qg4 Rxh3+ 23. Kg2 Qxg4+ 24. Kf1 Qf3 25. Nc7 Rh2 26. Be1 Nd4 27. Ne6 Qe2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2",
          "aggressive_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bxe4",
            "Bd6",
            "O-O",
            "O-O",
            "d3",
            "Bg4",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Bxg6",
            "hxg6",
            "Be3",
            "f5",
            "gxf5",
            "gxf5",
            "Ng5",
            "Qe8",
            "Kh1",
            "f4",
            "Bd2",
            "Rf5",
            "Rg1",
            "Be7",
            "Ne6",
            "Bf6",
            "Nxc7",
            "Qd7",
            "Nxa8",
            "Rh5",
            "Qg4",
            "Rxh3+",
            "Kg2",
            "Qxg4+",
            "Kf1",
            "Qf3",
            "Nc7",
            "Rh2",
            "Be1",
            "Nd4",
            "Ne6",
            "Qe2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "dxe4",
          "positional_uci": "d5e4",
          "positional_response": "6... dxe4 7. Bxe4 Bd6 8. O-O O-O 9. d3 Bg4 10. h3 Bh5 11. g4 Bg6 12. Bxg6 hxg6 13. Be3 f5 14. gxf5 gxf5 15. Ng5 Qe8 16. Kh1 f4 17. Bd2 Nd4 18. c3 Ne6 19. Nxe6 Qxe6 20. Qg4 Qd5+ 21. Kh2 e4 22. Rg1 Rf7 23. dxe4 Qxd2 24. Rg2 f3+ 25. Rg3 Qxf2+ 26. Kh1 Bxg3 27. Rg1 Qh2# 0-1\n28. Qxh2 Bxh2 29. Kxh2 f2 30. Rf1 Rd8 31. Kg2 Rd2 32. b4 Rxa2 33. c4 Re2 34. c5 Re1 35. Rxf2 Rxf2+ 36. Kxf2 Rxe4 37. b5 Rc4 38. c6 bxc6 39. bxc6 Rxc6 40. Ke3 a5 41. Kd4 a4 42. Kd5 Rc1 43. Kd4 a3 44. Kd3 a2 45. Kd2 a1=Q 46. Kd3 Qc3+ 47. Ke4 Re1+ 48. Kd5 Re5# 0-1",
          "positional_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 365,
            "total_tokens": 521,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qxd2",
            "Rg2",
            "f3+",
            "Rg3",
            "Qxf2+",
            "Kh1",
            "Bxg3",
            "Rg1",
            "Qh2#",
            "Qxh2",
            "Bxh2",
            "Kxh2",
            "f2",
            "Rf1",
            "Rd8",
            "Kg2",
            "Rd2",
            "b4",
            "Rxa2",
            "c4",
            "Re2",
            "c5",
            "Re1",
            "Rxf2",
            "Rxf2+",
            "Kxf2",
            "Rxe4",
            "b5",
            "Rc4",
            "c6",
            "bxc6",
            "bxc6",
            "Rxc6",
            "Ke3",
            "a5",
            "Kd4",
            "a4",
            "Kd5",
            "Rc1",
            "Kd4",
            "a3",
            "Kd3",
            "a2",
            "Kd2",
            "a1=Q",
            "Kd3",
            "Qc3+",
            "Ke4",
            "Re1+",
            "Kd5",
            "Re5#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Re8",
          "neutral_uci": null,
          "neutral_response": "Re8\n7. O-O Bg4 8. h3 Bh5 9. Ng3 Bg6 10. Bxg6 hxg6 11. d3 Qd7 12. Re1 O-O-O\n13. c3 f6 14. b4 g5 15. b5 Ne7 16. Qa4 Kb8 17. Be3 Nc8 18. Rab1 Bd6\n19. Rb3 g6 20. Reb1 Rh8 21. c4 d4 22. Bd2 g4 23. hxg4 Qxg4 24. Bb4 Bxb4\n25. Qxb4 Rh7 26. c5 Rdh8 27. Kf1 Rh1+ 28. Nxh1 Rxh1+ 29. Ke2 Qxg2\n30. Rxh1 Qxh1 31. Rb1 Qh5 32. c6 b6 33. Qc4 Nd6 34. Qg8+ Nc8 35. Qe6 Nd6\n36. Qxf6 e4 37. dxe4 Nxe4 38. Qd8# e4 Qxf3+ Kd2 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+",
          "neutral_tokens": {
            "prompt_tokens": 155,
            "completion_tokens": 640,
            "total_tokens": 795,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "O-O",
            "Bg4",
            "h3",
            "Bh5",
            "Ng3",
            "Bg6",
            "Bxg6",
            "hxg6",
            "d3",
            "Qd7",
            "Re1",
            "O-O-O",
            "c3",
            "f6",
            "b4",
            "g5",
            "b5",
            "Ne7",
            "Qa4",
            "Kb8",
            "Be3",
            "Nc8",
            "Rab1",
            "Bd6",
            "Rb3",
            "g6",
            "Reb1",
            "Rh8",
            "c4",
            "d4",
            "Bd2",
            "g4",
            "hxg4",
            "Qxg4",
            "Bb4",
            "Bxb4",
            "Qxb4",
            "Rh7",
            "c5",
            "Rdh8",
            "Kf1",
            "Rh1+",
            "Nxh1",
            "Rxh1+",
            "Ke2",
            "Qxg2",
            "Rxh1",
            "Qxh1",
            "Rb1",
            "Qh5",
            "c6",
            "b6",
            "Qc4",
            "Nd6",
            "Qg8+",
            "Nc8",
            "Qe6",
            "Nd6",
            "Qxf6",
            "e4",
            "dxe4",
            "Nxe4",
            "Qd8#",
            "e4",
            "Qxf3+",
            "Kd2",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d5e4",
          "positional_uci": "d5e4",
          "neutral_uci": null,
          "consensus_move": "d5e4",
          "source_agents": [
            "aggressive",
            "positional"
          ],
          "selected_agent": "aggressive"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bxe4",
              "Bd6",
              "O-O",
              "O-O",
              "d3",
              "Bg4",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Bxg6",
              "hxg6",
              "Be3",
              "f5",
              "gxf5",
              "gxf5",
              "Ng5",
              "Qe8",
              "Kh1",
              "f4",
              "Bd2",
              "Rf5",
              "Rg1",
              "Be7",
              "Ne6",
              "Bf6",
              "Nxc7",
              "Qd7",
              "Nxa8",
              "Rh5",
              "Qg4",
              "Rxh3+",
              "Kg2",
              "Qxg4+",
              "Kf1",
              "Qf3",
              "Nc7",
              "Rh2",
              "Be1",
              "Nd4",
              "Ne6",
              "Qe2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qxd2",
              "Rg2",
              "f3+",
              "Rg3",
              "Qxf2+",
              "Kh1",
              "Bxg3",
              "Rg1",
              "Qh2#",
              "Qxh2",
              "Bxh2",
              "Kxh2",
              "f2",
              "Rf1",
              "Rd8",
              "Kg2",
              "Rd2",
              "b4",
              "Rxa2",
              "c4",
              "Re2",
              "c5",
              "Re1",
              "Rxf2",
              "Rxf2+",
              "Kxf2",
              "Rxe4",
              "b5",
              "Rc4",
              "c6",
              "bxc6",
              "bxc6",
              "Rxc6",
              "Ke3",
              "a5",
              "Kd4",
              "a4",
              "Kd5",
              "Rc1",
              "Kd4",
              "a3",
              "Kd3",
              "a2",
              "Kd2",
              "a1=Q",
              "Kd3",
              "Qc3+",
              "Ke4",
              "Re1+",
              "Kd5",
              "Re5#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "O-O",
              "Bg4",
              "h3",
              "Bh5",
              "Ng3",
              "Bg6",
              "Bxg6",
              "hxg6",
              "d3",
              "Qd7",
              "Re1",
              "O-O-O",
              "c3",
              "f6",
              "b4",
              "g5",
              "b5",
              "Ne7",
              "Qa4",
              "Kb8",
              "Be3",
              "Nc8",
              "Rab1",
              "Bd6",
              "Rb3",
              "g6",
              "Reb1",
              "Rh8",
              "c4",
              "d4",
              "Bd2",
              "g4",
              "hxg4",
              "Qxg4",
              "Bb4",
              "Bxb4",
              "Qxb4",
              "Rh7",
              "c5",
              "Rdh8",
              "Kf1",
              "Rh1+",
              "Nxh1",
              "Rxh1+",
              "Ke2",
              "Qxg2",
              "Rxh1",
              "Qxh1",
              "Rb1",
              "Qh5",
              "c6",
              "b6",
              "Qc4",
              "Nd6",
              "Qg8+",
              "Nc8",
              "Qe6",
              "Nd6",
              "Qxf6",
              "e4",
              "dxe4",
              "Nxe4",
              "Qd8#",
              "e4",
              "Qxf3+",
              "Kd2",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 156,
            "completion_tokens": 365,
            "total_tokens": 521,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 155,
            "completion_tokens": 640,
            "total_tokens": 795,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 467,
          "total_completion_tokens": 1645,
          "total_tokens": 2112
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 156,
            "completion_tokens": 365,
            "total_tokens": 521,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 155,
            "completion_tokens": 640,
            "total_tokens": 795,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/3pp3/4N3/3B1N2/PPPP1PPP/R1BQK2R b KQkq - 1 6"
    },
    {
      "turn": 14,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "exd3",
          "aggressive_uci": "e4d3",
          "aggressive_response": "7... exd3 8. cxd3 Qxd3 9. Re1 Bg4 10. Re3 Qd5 11. h3 Bxf3 12. Rxf3 O-O-O 13. d3 e4 14. Re3 exd3 15. Bd2 Bc5 16. Rf3 Rhe8 17. Rc1 Re2 18. a3 Nd4 19. Rg3 Nb3 20. Rc3 Rxd2 21. Qxb3 Bxf2+ 22. Kh2 Bxg3+ 23. Kxg3 Rxg2+ 24. Kh4 Qg5# d4 d5 dxe6",
          "aggressive_tokens": {
            "prompt_tokens": 164,
            "completion_tokens": 156,
            "total_tokens": 320,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bd2",
            "Bc5",
            "Rf3",
            "Rhe8",
            "Rc1",
            "Re2",
            "a3",
            "Nd4",
            "Rg3",
            "Nb3",
            "Rc3",
            "Rxd2",
            "Qxb3",
            "Bxf2+",
            "Kh2",
            "Bxg3+",
            "Kxg3",
            "Rxg2+",
            "Kh4",
            "Qg5#",
            "d4",
            "d5",
            "dxe6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "exd3",
          "positional_uci": "e4d3",
          "positional_response": "7... exd3 8. cxd3 Qxd3 9. Re1 Bg4 10. Re3 Qd5 11. h3 Bxf3 12. Rxf3 O-O-O 13. d3 Nd4 14. Re3 Bc5 15. Re4 f5 16. Re1 Rhe8 17. Be3 f4 18. Bxd4 Qxd4 19. Qg4+ Kb8 20. Qxg7 Qxf2+ 21. Kh1 Rg8 22. Rf1 Qxf1+ 23. Rxf1 Rxg7 24. Kh2 Rxd3 25. Re1 Rd2 26. Rxe5 Rgxg2+ 27. Kh1 Rh2# Rg8 Qxg1#",
          "positional_tokens": {
            "prompt_tokens": 164,
            "completion_tokens": 182,
            "total_tokens": 346,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "cxd3",
            "Qxd3",
            "Re1",
            "Bg4",
            "Re3",
            "Qd5",
            "h3",
            "Bxf3",
            "Rxf3",
            "O-O-O",
            "d3",
            "Nd4",
            "Re3",
            "Bc5",
            "Re4",
            "f5",
            "Re1",
            "Rhe8",
            "Be3",
            "f4",
            "Bxd4",
            "Qxd4",
            "Qg4+",
            "Kb8",
            "Qxg7",
            "Qxf2+",
            "Kh1",
            "Rg8",
            "Rf1",
            "Qxf1+",
            "Rxf1",
            "Rxg7",
            "Kh2",
            "Rxd3",
            "Re1",
            "Rd2",
            "Rxe5",
            "Rgxg2+",
            "Kh1",
            "Rh2#",
            "Rg8",
            "Qxg1#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "exd3",
          "neutral_uci": "e4d3",
          "neutral_response": "7... exd3 8. cxd3 Qxd3 9. Re1 Bg4 10. Re3 Qd5 11. Qe1 Bxf3 12. Rxf3 O-O-O 13. d3 Nd4 14. Re3 Nc2 15. Qc3 Nxa1 16. Rxe5 Qxd3 17. Qxd3 Rxd3 18. Re1 Nc2 19. Rf1 Bc5 20. Bf4 Re8 21. h3 Re1 22. Rxe1 Nxe1 23. Kf1 Nc2 24. Ke2 Rd7 25. a3 Nd4+ 26. Kf1 Ne6 27. Bg3 Rd2 28. b4 Bd4 29. Ke1 Ra2 30. f4 Rxa3 31. Bh2 Bc3+ 32. Kf2 Bxb4 33. f5 Nd4 34. Be5 Nxf5 35. g4 Bc5+ 36. Kg2 Ne3+ 37. Kf3 Nc4+ 38. Ke4 Nxe5 39. Kxe5 Rxh3 40. Kd5 Bd6 41. g5 a5 42. Kc4 a4 43. Kb5 a3 44. Kc4 a2 45. Kd5 a1=Q 46. Ke4 Qe5# Qe5#",
          "neutral_tokens": {
            "prompt_tokens": 163,
            "completion_tokens": 332,
            "total_tokens": 495,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "cxd3",
            "Qxd3",
            "Re1",
            "Bg4",
            "Re3",
            "Qd5",
            "Qe1",
            "Bxf3",
            "Rxf3",
            "O-O-O",
            "d3",
            "Nd4",
            "Re3",
            "Nc2",
            "Qc3",
            "Nxa1",
            "Rxe5",
            "Qxd3",
            "Qxd3",
            "Rxd3",
            "Re1",
            "Nc2",
            "Rf1",
            "Bc5",
            "Bf4",
            "Re8",
            "h3",
            "Re1",
            "Rxe1",
            "Nxe1",
            "Kf1",
            "Nc2",
            "Ke2",
            "Rd7",
            "a3",
            "Nd4+",
            "Kf1",
            "Ne6",
            "Bg3",
            "Rd2",
            "b4",
            "Bd4",
            "Ke1",
            "Ra2",
            "f4",
            "Rxa3",
            "Bh2",
            "Bc3+",
            "Kf2",
            "Bxb4",
            "f5",
            "Nd4",
            "Be5",
            "Nxf5",
            "g4",
            "Bc5+",
            "Kg2",
            "Ne3+",
            "Kf3",
            "Nc4+",
            "Ke4",
            "Nxe5",
            "Kxe5",
            "Rxh3",
            "Kd5",
            "Bd6",
            "g5",
            "a5",
            "Kc4",
            "a4",
            "Kb5",
            "a3",
            "Kc4",
            "a2",
            "Kd5",
            "a1=Q",
            "Ke4",
            "Qe5#",
            "Qe5#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e4d3",
          "positional_uci": "e4d3",
          "neutral_uci": "e4d3",
          "consensus_move": "e4d3",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bd2",
              "Bc5",
              "Rf3",
              "Rhe8",
              "Rc1",
              "Re2",
              "a3",
              "Nd4",
              "Rg3",
              "Nb3",
              "Rc3",
              "Rxd2",
              "Qxb3",
              "Bxf2+",
              "Kh2",
              "Bxg3+",
              "Kxg3",
              "Rxg2+",
              "Kh4",
              "Qg5#",
              "d4",
              "d5",
              "dxe6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "cxd3",
              "Qxd3",
              "Re1",
              "Bg4",
              "Re3",
              "Qd5",
              "h3",
              "Bxf3",
              "Rxf3",
              "O-O-O",
              "d3",
              "Nd4",
              "Re3",
              "Bc5",
              "Re4",
              "f5",
              "Re1",
              "Rhe8",
              "Be3",
              "f4",
              "Bxd4",
              "Qxd4",
              "Qg4+",
              "Kb8",
              "Qxg7",
              "Qxf2+",
              "Kh1",
              "Rg8",
              "Rf1",
              "Qxf1+",
              "Rxf1",
              "Rxg7",
              "Kh2",
              "Rxd3",
              "Re1",
              "Rd2",
              "Rxe5",
              "Rgxg2+",
              "Kh1",
              "Rh2#",
              "Rg8",
              "Qxg1#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "cxd3",
              "Qxd3",
              "Re1",
              "Bg4",
              "Re3",
              "Qd5",
              "Qe1",
              "Bxf3",
              "Rxf3",
              "O-O-O",
              "d3",
              "Nd4",
              "Re3",
              "Nc2",
              "Qc3",
              "Nxa1",
              "Rxe5",
              "Qxd3",
              "Qxd3",
              "Rxd3",
              "Re1",
              "Nc2",
              "Rf1",
              "Bc5",
              "Bf4",
              "Re8",
              "h3",
              "Re1",
              "Rxe1",
              "Nxe1",
              "Kf1",
              "Nc2",
              "Ke2",
              "Rd7",
              "a3",
              "Nd4+",
              "Kf1",
              "Ne6",
              "Bg3",
              "Rd2",
              "b4",
              "Bd4",
              "Ke1",
              "Ra2",
              "f4",
              "Rxa3",
              "Bh2",
              "Bc3+",
              "Kf2",
              "Bxb4",
              "f5",
              "Nd4",
              "Be5",
              "Nxf5",
              "g4",
              "Bc5+",
              "Kg2",
              "Ne3+",
              "Kf3",
              "Nc4+",
              "Ke4",
              "Nxe5",
              "Kxe5",
              "Rxh3",
              "Kd5",
              "Bd6",
              "g5",
              "a5",
              "Kc4",
              "a4",
              "Kb5",
              "a3",
              "Kc4",
              "a2",
              "Kd5",
              "a1=Q",
              "Ke4",
              "Qe5#",
              "Qe5#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 164,
            "completion_tokens": 156,
            "total_tokens": 320,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 164,
            "completion_tokens": 182,
            "total_tokens": 346,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 163,
            "completion_tokens": 332,
            "total_tokens": 495,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 491,
          "total_completion_tokens": 670,
          "total_tokens": 1161
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 164,
            "completion_tokens": 156,
            "total_tokens": 320,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 164,
            "completion_tokens": 182,
            "total_tokens": 346,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 163,
            "completion_tokens": 332,
            "total_tokens": 495,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/4p3/4p3/3B1N2/PPPP1PPP/R1BQ1RK1 b kq - 1 7"
    }
  ],
  "white_errors": [],
  "black_errors": [],
  "white_plan_log": [],
  "black_plan_log": [],
  "timestamp": "2025-11-30T00:06:31.761247",
  "game_id": "mistralai_mistral-small-24b-instruct-2501_vs_gpt-3.5-turbo-instruct_SC_2",
  "configuration": "SC"
}