{
  "result": "gpt-3.5-turbo-instruct wins by disqualification (mistralai/mistral-small-24b-instruct-2501 failed to produce a legal move)",
  "white_player": "mistralai/mistral-small-24b-instruct-2501",
  "black_player": "gpt-3.5-turbo-instruct",
  "white_score": 0.0,
  "black_score": 1.0,
  "total_moves": 20,
  "moves": [
    "e2e4",
    "e7e5",
    "g1f3",
    "b8c6",
    "f1c4",
    "g8f6",
    "f3g5",
    "d7d5",
    "e4d5",
    "f6d5",
    "g5f7",
    "e8f7",
    "d2d4",
    "c6d4",
    "d1f3",
    "f7e6",
    "f3e3",
    "d4c2",
    "e1d2",
    "c2e3"
  ],
  "game_history": [
    {
      "move_number": 1,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "e2e4",
      "move_san": "e4",
      "fen_after": "rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "move_number": 2,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "e7e5",
      "move_san": "e5",
      "fen_after": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "move_number": 3,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "g1f3",
      "move_san": "Nf3",
      "fen_after": "rnbqkbnr/pppp1ppp/8/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R b KQkq - 1 2"
    },
    {
      "move_number": 4,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "b8c6",
      "move_san": "Nc6",
      "fen_after": "r1bqkbnr/pppp1ppp/2n5/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3"
    },
    {
      "move_number": 5,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "f1c4",
      "move_san": "Bc4",
      "fen_after": "r1bqkbnr/pppp1ppp/2n5/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R b KQkq - 3 3"
    },
    {
      "move_number": 6,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "g8f6",
      "move_san": "Nf6",
      "fen_after": "r1bqkb1r/pppp1ppp/2n2n2/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R w KQkq - 4 4"
    },
    {
      "move_number": 7,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "f3g5",
      "move_san": "Ng5",
      "fen_after": "r1bqkb1r/pppp1ppp/2n2n2/4p1N1/2B1P3/8/PPPP1PPP/RNBQK2R b KQkq - 5 4"
    },
    {
      "move_number": 8,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "d7d5",
      "move_san": "d5",
      "fen_after": "r1bqkb1r/ppp2ppp/2n2n2/3pp1N1/2B1P3/8/PPPP1PPP/RNBQK2R w KQkq - 0 5"
    },
    {
      "move_number": 9,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "e4d5",
      "move_san": "exd5",
      "fen_after": "r1bqkb1r/ppp2ppp/2n2n2/3Pp1N1/2B5/8/PPPP1PPP/RNBQK2R b KQkq - 0 5"
    },
    {
      "move_number": 10,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "f6d5",
      "move_san": "Nxd5",
      "fen_after": "r1bqkb1r/ppp2ppp/2n5/3np1N1/2B5/8/PPPP1PPP/RNBQK2R w KQkq - 0 6"
    },
    {
      "move_number": 11,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "g5f7",
      "move_san": "Nxf7",
      "fen_after": "r1bqkb1r/ppp2Npp/2n5/3np3/2B5/8/PPPP1PPP/RNBQK2R b KQkq - 0 6"
    },
    {
      "move_number": 12,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "e8f7",
      "move_san": "Kxf7",
      "fen_after": "r1bq1b1r/ppp2kpp/2n5/3np3/2B5/8/PPPP1PPP/RNBQK2R w KQ - 0 7"
    },
    {
      "move_number": 13,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "d2d4",
      "move_san": "d4",
      "fen_after": "r1bq1b1r/ppp2kpp/2n5/3np3/2BP4/8/PPP2PPP/RNBQK2R b KQ - 0 7"
    },
    {
      "move_number": 14,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "c6d4",
      "move_san": "Nxd4",
      "fen_after": "r1bq1b1r/ppp2kpp/8/3np3/2Bn4/8/PPP2PPP/RNBQK2R w KQ - 0 8"
    },
    {
      "move_number": 15,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "d1f3",
      "move_san": "Qf3+",
      "fen_after": "r1bq1b1r/ppp2kpp/8/3np3/2Bn4/5Q2/PPP2PPP/RNB1K2R b KQ - 1 8"
    },
    {
      "move_number": 16,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "f7e6",
      "move_san": "Ke6",
      "fen_after": "r1bq1b1r/ppp3pp/4k3/3np3/2Bn4/5Q2/PPP2PPP/RNB1K2R w KQ - 2 9"
    },
    {
      "move_number": 17,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "f3e3",
      "move_san": "Qe3",
      "fen_after": "r1bq1b1r/ppp3pp/4k3/3np3/2Bn4/4Q3/PPP2PPP/RNB1K2R b KQ - 3 9"
    },
    {
      "move_number": 18,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "d4c2",
      "move_san": "Nxc2+",
      "fen_after": "r1bq1b1r/ppp3pp/4k3/3np3/2B5/4Q3/PPn2PPP/RNB1K2R w KQ - 0 10"
    },
    {
      "move_number": 19,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "e1d2",
      "move_san": "Kd2",
      "fen_after": "r1bq1b1r/ppp3pp/4k3/3np3/2B5/4Q3/PPnK1PPP/RNB4R b - - 1 10"
    },
    {
      "move_number": 20,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "c2e3",
      "move_san": "Nxe3",
      "fen_after": "r1bq1b1r/ppp3pp/4k3/3np3/2B5/4n3/PP1K1PPP/RNB4R w - - 0 11"
    }
  ],
  "final_fen": "r1bq1b1r/ppp3pp/4k3/3np3/2B5/4n3/PP1K1PPP/RNB4R w - - 0 11",
  "white_tokens": {
    "prompt_tokens": 7360,
    "completion_tokens": 10330,
    "total_tokens": 17690
  },
  "black_tokens": {
    "prompt_tokens": 8132,
    "completion_tokens": 28811,
    "total_tokens": 36943
  },
  "white_token_log": [
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 113,
          "total_tokens": 227,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 215,
          "total_tokens": 329,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 166,
          "total_tokens": 279,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 341,
        "total_completion_tokens": 494,
        "total_tokens": 835
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 113,
          "total_tokens": 227,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 215,
          "total_tokens": 329,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 166,
          "total_tokens": 279,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 3,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 120,
          "completion_tokens": 640,
          "total_tokens": 760,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 120,
          "completion_tokens": 640,
          "total_tokens": 760,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 119,
          "completion_tokens": 640,
          "total_tokens": 759,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 359,
        "total_completion_tokens": 1920,
        "total_tokens": 2279
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 120,
          "completion_tokens": 640,
          "total_tokens": 760,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 120,
          "completion_tokens": 640,
          "total_tokens": 760,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 119,
          "completion_tokens": 640,
          "total_tokens": 759,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 5,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 129,
          "completion_tokens": 95,
          "total_tokens": 224,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 129,
          "completion_tokens": 41,
          "total_tokens": 170,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 128,
          "completion_tokens": 640,
          "total_tokens": 768,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 386,
        "total_completion_tokens": 776,
        "total_tokens": 1162
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 129,
          "completion_tokens": 95,
          "total_tokens": 224,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 129,
          "completion_tokens": 41,
          "total_tokens": 170,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 128,
          "completion_tokens": 640,
          "total_tokens": 768,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 7,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 137,
          "completion_tokens": 27,
          "total_tokens": 164,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 137,
          "completion_tokens": 132,
          "total_tokens": 269,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 136,
          "completion_tokens": 27,
          "total_tokens": 163,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 410,
        "total_completion_tokens": 186,
        "total_tokens": 596
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 137,
          "completion_tokens": 27,
          "total_tokens": 164,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 137,
          "completion_tokens": 132,
          "total_tokens": 269,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 136,
          "completion_tokens": 27,
          "total_tokens": 163,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 9,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 146,
          "completion_tokens": 36,
          "total_tokens": 182,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 146,
          "completion_tokens": 110,
          "total_tokens": 256,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 145,
          "completion_tokens": 39,
          "total_tokens": 184,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 437,
        "total_completion_tokens": 185,
        "total_tokens": 622
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 146,
          "completion_tokens": 36,
          "total_tokens": 182,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 146,
          "completion_tokens": 110,
          "total_tokens": 256,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 145,
          "completion_tokens": 39,
          "total_tokens": 184,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 11,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 154,
          "completion_tokens": 81,
          "total_tokens": 235,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 154,
          "completion_tokens": 68,
          "total_tokens": 222,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 153,
          "completion_tokens": 81,
          "total_tokens": 234,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 461,
        "total_completion_tokens": 230,
        "total_tokens": 691
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 154,
          "completion_tokens": 81,
          "total_tokens": 235,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 154,
          "completion_tokens": 68,
          "total_tokens": 222,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 153,
          "completion_tokens": 81,
          "total_tokens": 234,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 13,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 162,
          "completion_tokens": 55,
          "total_tokens": 217,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 162,
          "completion_tokens": 640,
          "total_tokens": 802,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 161,
          "completion_tokens": 108,
          "total_tokens": 269,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 485,
        "total_completion_tokens": 803,
        "total_tokens": 1288
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 162,
          "completion_tokens": 55,
          "total_tokens": 217,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 162,
          "completion_tokens": 640,
          "total_tokens": 802,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 161,
          "completion_tokens": 108,
          "total_tokens": 269,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 223,
          "total_tokens": 337,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 640,
          "total_tokens": 753,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 341,
        "total_completion_tokens": 1503,
        "total_tokens": 1844
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 223,
          "total_tokens": 337,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 640,
          "total_tokens": 753,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 3,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 120,
          "completion_tokens": 174,
          "total_tokens": 294,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 120,
          "completion_tokens": 163,
          "total_tokens": 283,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 119,
          "completion_tokens": 640,
          "total_tokens": 759,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 359,
        "total_completion_tokens": 977,
        "total_tokens": 1336
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 120,
          "completion_tokens": 174,
          "total_tokens": 294,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 120,
          "completion_tokens": 163,
          "total_tokens": 283,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 119,
          "completion_tokens": 640,
          "total_tokens": 759,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 5,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 129,
          "completion_tokens": 105,
          "total_tokens": 234,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 129,
          "completion_tokens": 174,
          "total_tokens": 303,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 128,
          "completion_tokens": 95,
          "total_tokens": 223,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 386,
        "total_completion_tokens": 374,
        "total_tokens": 760
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 129,
          "completion_tokens": 105,
          "total_tokens": 234,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 129,
          "completion_tokens": 174,
          "total_tokens": 303,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 128,
          "completion_tokens": 95,
          "total_tokens": 223,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 7,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 137,
          "completion_tokens": 27,
          "total_tokens": 164,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 137,
          "completion_tokens": 75,
          "total_tokens": 212,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 136,
          "completion_tokens": 115,
          "total_tokens": 251,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 410,
        "total_completion_tokens": 217,
        "total_tokens": 627
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 137,
          "completion_tokens": 27,
          "total_tokens": 164,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 137,
          "completion_tokens": 75,
          "total_tokens": 212,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 136,
          "completion_tokens": 115,
          "total_tokens": 251,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 9,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 144,
          "completion_tokens": 34,
          "total_tokens": 178,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 144,
          "completion_tokens": 59,
          "total_tokens": 203,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 143,
          "completion_tokens": 640,
          "total_tokens": 783,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 431,
        "total_completion_tokens": 733,
        "total_tokens": 1164
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 144,
          "completion_tokens": 34,
          "total_tokens": 178,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 144,
          "completion_tokens": 59,
          "total_tokens": 203,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 143,
          "completion_tokens": 640,
          "total_tokens": 783,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 11,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 153,
          "completion_tokens": 70,
          "total_tokens": 223,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 153,
          "completion_tokens": 70,
          "total_tokens": 223,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 152,
          "completion_tokens": 70,
          "total_tokens": 222,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 458,
        "total_completion_tokens": 210,
        "total_tokens": 668
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 153,
          "completion_tokens": 70,
          "total_tokens": 223,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 153,
          "completion_tokens": 70,
          "total_tokens": 223,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 152,
          "completion_tokens": 70,
          "total_tokens": 222,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 13,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 162,
          "completion_tokens": 90,
          "total_tokens": 252,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 162,
          "completion_tokens": 70,
          "total_tokens": 232,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 161,
          "completion_tokens": 70,
          "total_tokens": 231,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 485,
        "total_completion_tokens": 230,
        "total_tokens": 715
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 162,
          "completion_tokens": 90,
          "total_tokens": 252,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 162,
          "completion_tokens": 70,
          "total_tokens": 232,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 161,
          "completion_tokens": 70,
          "total_tokens": 231,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 15,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 170,
          "completion_tokens": 71,
          "total_tokens": 241,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 170,
          "completion_tokens": 71,
          "total_tokens": 241,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 169,
          "completion_tokens": 61,
          "total_tokens": 230,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 509,
        "total_completion_tokens": 203,
        "total_tokens": 712
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 170,
          "completion_tokens": 71,
          "total_tokens": 241,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 170,
          "completion_tokens": 71,
          "total_tokens": 241,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 169,
          "completion_tokens": 61,
          "total_tokens": 230,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 17,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 179,
          "completion_tokens": 120,
          "total_tokens": 299,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 179,
          "completion_tokens": 101,
          "total_tokens": 280,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 178,
          "completion_tokens": 95,
          "total_tokens": 273,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 536,
        "total_completion_tokens": 316,
        "total_tokens": 852
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 179,
          "completion_tokens": 120,
          "total_tokens": 299,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 179,
          "completion_tokens": 101,
          "total_tokens": 280,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 178,
          "completion_tokens": 95,
          "total_tokens": 273,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 19,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 189,
          "completion_tokens": 640,
          "total_tokens": 829,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 189,
          "completion_tokens": 167,
          "total_tokens": 356,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 188,
          "completion_tokens": 166,
          "total_tokens": 354,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 566,
        "total_completion_tokens": 973,
        "total_tokens": 1539
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 189,
          "completion_tokens": 640,
          "total_tokens": 829,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 189,
          "completion_tokens": 167,
          "total_tokens": 356,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 188,
          "completion_tokens": 166,
          "total_tokens": 354,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    }
  ],
  "black_token_log": [
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 640,
          "total_tokens": 749,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 329,
        "total_completion_tokens": 1920,
        "total_tokens": 2249
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 640,
          "total_tokens": 749,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 2,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 429,
          "total_tokens": 543,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 498,
          "total_tokens": 612,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 640,
          "total_tokens": 753,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 341,
        "total_completion_tokens": 1567,
        "total_tokens": 1908
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 429,
          "total_tokens": 543,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 498,
          "total_tokens": 612,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 640,
          "total_tokens": 753,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 4,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 122,
          "completion_tokens": 258,
          "total_tokens": 380,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 122,
          "completion_tokens": 640,
          "total_tokens": 762,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 121,
          "completion_tokens": 640,
          "total_tokens": 761,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 365,
        "total_completion_tokens": 1538,
        "total_tokens": 1903
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 122,
          "completion_tokens": 258,
          "total_tokens": 380,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 122,
          "completion_tokens": 640,
          "total_tokens": 762,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 121,
          "completion_tokens": 640,
          "total_tokens": 761,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 6,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 131,
          "completion_tokens": 640,
          "total_tokens": 771,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 131,
          "completion_tokens": 640,
          "total_tokens": 771,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 130,
          "completion_tokens": 174,
          "total_tokens": 304,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 392,
        "total_completion_tokens": 1454,
        "total_tokens": 1846
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 131,
          "completion_tokens": 640,
          "total_tokens": 771,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 131,
          "completion_tokens": 640,
          "total_tokens": 771,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 130,
          "completion_tokens": 174,
          "total_tokens": 304,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 8,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 140,
          "completion_tokens": 640,
          "total_tokens": 780,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 140,
          "completion_tokens": 640,
          "total_tokens": 780,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 139,
          "completion_tokens": 292,
          "total_tokens": 431,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 419,
        "total_completion_tokens": 1572,
        "total_tokens": 1991
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 140,
          "completion_tokens": 640,
          "total_tokens": 780,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 140,
          "completion_tokens": 640,
          "total_tokens": 780,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 139,
          "completion_tokens": 292,
          "total_tokens": 431,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 10,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 640,
          "total_tokens": 789,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 388,
          "total_tokens": 537,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 640,
          "total_tokens": 788,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 446,
        "total_completion_tokens": 1668,
        "total_tokens": 2114
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 640,
          "total_tokens": 789,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 388,
          "total_tokens": 537,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 640,
          "total_tokens": 788,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 12,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 640,
          "total_tokens": 796,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 365,
          "total_tokens": 521,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 640,
          "total_tokens": 795,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 467,
        "total_completion_tokens": 1645,
        "total_tokens": 2112
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 640,
          "total_tokens": 796,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 365,
          "total_tokens": 521,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 640,
          "total_tokens": 795,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 14,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 164,
          "completion_tokens": 156,
          "total_tokens": 320,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 164,
          "completion_tokens": 182,
          "total_tokens": 346,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 163,
          "completion_tokens": 332,
          "total_tokens": 495,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 491,
        "total_completion_tokens": 670,
        "total_tokens": 1161
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 164,
          "completion_tokens": 156,
          "total_tokens": 320,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 164,
          "completion_tokens": 182,
          "total_tokens": 346,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 163,
          "completion_tokens": 332,
          "total_tokens": 495,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 640,
          "total_tokens": 749,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 329,
        "total_completion_tokens": 1920,
        "total_tokens": 2249
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 640,
          "total_tokens": 749,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 2,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 345,
          "total_tokens": 458,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 341,
        "total_completion_tokens": 1625,
        "total_tokens": 1966
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 345,
          "total_tokens": 458,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 4,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 122,
          "completion_tokens": 362,
          "total_tokens": 484,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 122,
          "completion_tokens": 640,
          "total_tokens": 762,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 121,
          "completion_tokens": 640,
          "total_tokens": 761,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 365,
        "total_completion_tokens": 1642,
        "total_tokens": 2007
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 122,
          "completion_tokens": 362,
          "total_tokens": 484,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 122,
          "completion_tokens": 640,
          "total_tokens": 762,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 121,
          "completion_tokens": 640,
          "total_tokens": 761,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 6,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 131,
          "completion_tokens": 640,
          "total_tokens": 771,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 131,
          "completion_tokens": 640,
          "total_tokens": 771,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 130,
          "completion_tokens": 640,
          "total_tokens": 770,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 392,
        "total_completion_tokens": 1920,
        "total_tokens": 2312
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 131,
          "completion_tokens": 640,
          "total_tokens": 771,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 131,
          "completion_tokens": 640,
          "total_tokens": 771,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 130,
          "completion_tokens": 640,
          "total_tokens": 770,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 8,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 139,
          "completion_tokens": 640,
          "total_tokens": 779,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 139,
          "completion_tokens": 640,
          "total_tokens": 779,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 138,
          "completion_tokens": 640,
          "total_tokens": 778,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 416,
        "total_completion_tokens": 1920,
        "total_tokens": 2336
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 139,
          "completion_tokens": 640,
          "total_tokens": 779,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 139,
          "completion_tokens": 640,
          "total_tokens": 779,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 138,
          "completion_tokens": 640,
          "total_tokens": 778,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 10,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 147,
          "completion_tokens": 174,
          "total_tokens": 321,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 147,
          "completion_tokens": 215,
          "total_tokens": 362,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 146,
          "completion_tokens": 640,
          "total_tokens": 786,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 440,
        "total_completion_tokens": 1029,
        "total_tokens": 1469
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 147,
          "completion_tokens": 174,
          "total_tokens": 321,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 147,
          "completion_tokens": 215,
          "total_tokens": 362,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 146,
          "completion_tokens": 640,
          "total_tokens": 786,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 12,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 178,
          "total_tokens": 334,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 640,
          "total_tokens": 796,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 640,
          "total_tokens": 795,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 467,
        "total_completion_tokens": 1458,
        "total_tokens": 1925
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 178,
          "total_tokens": 334,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 640,
          "total_tokens": 796,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 640,
          "total_tokens": 795,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 14,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 164,
          "completion_tokens": 505,
          "total_tokens": 669,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 164,
          "completion_tokens": 640,
          "total_tokens": 804,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 163,
          "completion_tokens": 640,
          "total_tokens": 803,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 491,
        "total_completion_tokens": 1785,
        "total_tokens": 2276
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 164,
          "completion_tokens": 505,
          "total_tokens": 669,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 164,
          "completion_tokens": 640,
          "total_tokens": 804,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 163,
          "completion_tokens": 640,
          "total_tokens": 803,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 16,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 174,
          "completion_tokens": 640,
          "total_tokens": 814,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 174,
          "completion_tokens": 640,
          "total_tokens": 814,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 173,
          "completion_tokens": 640,
          "total_tokens": 813,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 521,
        "total_completion_tokens": 1920,
        "total_tokens": 2441
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 174,
          "completion_tokens": 640,
          "total_tokens": 814,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 174,
          "completion_tokens": 640,
          "total_tokens": 814,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 173,
          "completion_tokens": 640,
          "total_tokens": 813,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 18,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 182,
          "completion_tokens": 94,
          "total_tokens": 276,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 182,
          "completion_tokens": 183,
          "total_tokens": 365,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 181,
          "completion_tokens": 640,
          "total_tokens": 821,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 545,
        "total_completion_tokens": 917,
        "total_tokens": 1462
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 182,
          "completion_tokens": 94,
          "total_tokens": 276,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 182,
          "completion_tokens": 183,
          "total_tokens": 365,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 181,
          "completion_tokens": 640,
          "total_tokens": 821,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 20,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 192,
          "completion_tokens": 490,
          "total_tokens": 682,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 192,
          "completion_tokens": 72,
          "total_tokens": 264,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 191,
          "completion_tokens": 79,
          "total_tokens": 270,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 575,
        "total_completion_tokens": 641,
        "total_tokens": 1216
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 192,
          "completion_tokens": 490,
          "total_tokens": 682,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 192,
          "completion_tokens": 72,
          "total_tokens": 264,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 191,
          "completion_tokens": 79,
          "total_tokens": 270,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    }
  ],
  "white_responses": [
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4\n\n1. d4",
          "aggressive_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4\n\n1. d4",
          "positional_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": null,
          "neutral_response": "1. d4\n\n1. d4",
          "neutral_tokens": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 353,
          "total_completion_tokens": 27,
          "total_tokens": 380
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 2,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4\n\n1. d4",
          "aggressive_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4\n\n1. d4",
          "positional_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": null,
          "neutral_response": "1. d4\n\n1. d4",
          "neutral_tokens": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 353,
          "total_completion_tokens": 27,
          "total_tokens": 380
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 2,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4\n\n1. d4",
          "aggressive_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4\n\n1. d4",
          "positional_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": null,
          "neutral_response": "1. d4\n\n1. d4",
          "neutral_tokens": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 353,
          "total_completion_tokens": 27,
          "total_tokens": 380
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": "e2e4",
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Bc5\n4. c3 Nf6\n5. d4 exd4\n6. cxd4 Bb4+\n7. Nc3 Nxe4\n8. O-O Bxc3\n9. d5 Bxa1\n10. Qxa1 Nc5\n11. Qd4 Ncd7\n12. Qe5 Nc6\n13. Qf5\n\nQd7",
          "aggressive_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 113,
            "total_tokens": 227,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bc4",
            "Bc5",
            "c3",
            "Nf6",
            "d4",
            "exd4",
            "cxd4",
            "Bb4+",
            "Nc3",
            "Nxe4",
            "O-O",
            "Bxc3",
            "d5",
            "Bxa1",
            "Qxa1",
            "Nc5",
            "Qd4",
            "Ncd7",
            "Qe5",
            "Nc6",
            "Qf5",
            "Qd7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bb5 a6\n4. Ba4 Nf6\n5. O-O Be7\n6. Re1 b5\n7. Bb3 d6\n8. c3 O-O\n9. h3 Na5\n10. Bc2 c5\n11. d4 Nc6\n12. Nbd2 cxd4\n13. cxd4 Nxd4\n14. Nxd4 Qxd4\n15. Nf3 Qd7\n16. Bg5 Qd8\n17. Qd3 Qd7\n18. Qxd7 Bxd7\n19. Bxe7 Nxe7\n20. Rc1 Nc6\n21. Rfd1 Rfc8\n22. Rxc8 Rxc8\n23. Rc1 Rxc1\n24. Bxc1\n\nNf5",
          "positional_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 215,
            "total_tokens": 329,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "Nxd4",
            "Nxd4",
            "Qxd4",
            "Nf3",
            "Qd7",
            "Bg5",
            "Qd8",
            "Qd3",
            "Qd7",
            "Qxd7",
            "Bxd7",
            "Bxe7",
            "Nxe7",
            "Rc1",
            "Nc6",
            "Rfd1",
            "Rfc8",
            "Rxc8",
            "Rxc8",
            "Rc1",
            "Rxc1",
            "Bxc1",
            "Nf5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": "e2e4",
          "neutral_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Bc5\n4. c3 Nf6\n5. d4 exd4\n6. cxd4 Bb4+\n7. Nc3 Nxe4\n8. O-O Bxc3\n9. d5 Bxa1\n10. Qxa1 Nc5\n11. Qd4 Ncd7\n12. Nxe5 Nxe5\n13. Qxe5 O-O\n14. Qh5 Qf6\n15. Qxf7+ Kh8\n16. Qf5+ Qg6\n17. Qxg6+ hxg6\n18. Bg5\n\n18...",
          "neutral_tokens": {
            "prompt_tokens": 113,
            "completion_tokens": 166,
            "total_tokens": 279,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bc4",
            "Bc5",
            "c3",
            "Nf6",
            "d4",
            "exd4",
            "cxd4",
            "Bb4+",
            "Nc3",
            "Nxe4",
            "O-O",
            "Bxc3",
            "d5",
            "Bxa1",
            "Qxa1",
            "Nc5",
            "Qd4",
            "Ncd7",
            "Nxe5",
            "Nxe5",
            "Qxe5",
            "O-O",
            "Qh5",
            "Qf6",
            "Qxf7+",
            "Kh8",
            "Qf5+",
            "Qg6",
            "Qxg6+",
            "hxg6",
            "Bg5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e2e4",
          "positional_uci": "e2e4",
          "neutral_uci": "e2e4",
          "consensus_move": "e2e4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bc4",
              "Bc5",
              "c3",
              "Nf6",
              "d4",
              "exd4",
              "cxd4",
              "Bb4+",
              "Nc3",
              "Nxe4",
              "O-O",
              "Bxc3",
              "d5",
              "Bxa1",
              "Qxa1",
              "Nc5",
              "Qd4",
              "Ncd7",
              "Qe5",
              "Nc6",
              "Qf5",
              "Qd7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "Nxd4",
              "Nxd4",
              "Qxd4",
              "Nf3",
              "Qd7",
              "Bg5",
              "Qd8",
              "Qd3",
              "Qd7",
              "Qxd7",
              "Bxd7",
              "Bxe7",
              "Nxe7",
              "Rc1",
              "Nc6",
              "Rfd1",
              "Rfc8",
              "Rxc8",
              "Rxc8",
              "Rc1",
              "Rxc1",
              "Bxc1",
              "Nf5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bc4",
              "Bc5",
              "c3",
              "Nf6",
              "d4",
              "exd4",
              "cxd4",
              "Bb4+",
              "Nc3",
              "Nxe4",
              "O-O",
              "Bxc3",
              "d5",
              "Bxa1",
              "Qxa1",
              "Nc5",
              "Qd4",
              "Ncd7",
              "Nxe5",
              "Nxe5",
              "Qxe5",
              "O-O",
              "Qh5",
              "Qf6",
              "Qxf7+",
              "Kh8",
              "Qf5+",
              "Qg6",
              "Qxg6+",
              "hxg6",
              "Bg5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 114,
            "completion_tokens": 113,
            "total_tokens": 227,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 114,
            "completion_tokens": 215,
            "total_tokens": 329,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 113,
            "completion_tokens": 166,
            "total_tokens": 279,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 341,
          "total_completion_tokens": 494,
          "total_tokens": 835
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 114,
            "completion_tokens": 113,
            "total_tokens": 227,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 114,
            "completion_tokens": 215,
            "total_tokens": 329,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 113,
            "completion_tokens": 166,
            "total_tokens": 279,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 3,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf3",
          "aggressive_uci": "g1f3",
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Bc5\n4. c3 Nf6\n5. d4 exd4\n6. cxd4 Bb4+\n7. Nc3 Nxe4\n8. O-O Bxc3\n9. d5 Bxa1\n10. Qxa1 Nc5\n11. Qd4 Qf6\n12. Qxc5 Qxc5\n13. Nxe5 Qd6\n14. Nc4 O-O\n15. Be3 Qd7\n16. Bxc5 Qxc5\n17. Nxb7 Qxb7\n18. Rc1 Qd7\n19. Rc7 Qd8\n20. Rc8+ Rxc8\n21. Bxc8 Qxc8\n22. Rc1 Qd8\n23. Rc7 Qd6\n24. Rc8+ Rxc8\n25. Bxc8 Qxc8\n26. Rc1 Qd8\n27. Rc7 Qd6\n28. Rc8+ Rxc8\n29. Bxc8 Qxc8\n30. Rc1 Qd8\n31. Rc7 Qd6\n32. Rc8+ Rxc8\n33. Bxc8 Qxc8\n34. Rc1 Qd8\n35. Rc7 Qd6\n36. Rc8+ Rxc8\n37. Bxc8 Qxc8\n38. Rc1 Qd8\n39. Rc7 Qd6\n40. Rc8+ Rxc8\n41. Bxc8 Qxc8\n42. Rc1 Qd8\n43. Rc7 Qd6\n44. Rc8+ Rxc8\n45. Bxc8 Qxc8\n46. Rc1 Qd8\n47. Rc7 Qd6\n48. Rc8+ Rxc8\n49. Bxc8 Qxc8\n50. Rc1 Qd8\n51. Rc7 Qd6\n52. Rc8+ Rxc8\n53. Bxc8 Qxc8\n54. Rc1 Qd8\n55. Rc7 Qd6\n56. Rc8+ Rxc8\n57. Bxc8 Qxc8\n58. Rc1 Qd8\n59. Rc7 Qd6\n60. Rc8+ Rxc8\n61. Bxc8 Qxc8\n62. Rc1 Qd8\n63. Rc7 Qd6\n64. Rc8+ Rxc8\n65. Bxc8 Q",
          "aggressive_tokens": {
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nc6",
            "Bc4",
            "Bc5",
            "c3",
            "Nf6",
            "d4",
            "exd4",
            "cxd4",
            "Bb4+",
            "Nc3",
            "Nxe4",
            "O-O",
            "Bxc3",
            "d5",
            "Bxa1",
            "Qxa1",
            "Nc5",
            "Qd4",
            "Qf6",
            "Qxc5",
            "Qxc5",
            "Nxe5",
            "Qd6",
            "Nc4",
            "O-O",
            "Be3",
            "Qd7",
            "Bxc5",
            "Qxc5",
            "Nxb7",
            "Qxb7",
            "Rc1",
            "Qd7",
            "Rc7",
            "Qd8",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf3",
          "positional_uci": "g1f3",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5 11. d4 Nc6 12. Nbd2 cxd4 13. cxd4 exd4 14. Nxd4 Nxd4 15. Qxd4 Bc5 16. Qd2 Bxd4 17. Bxd4 Qxd4 18. Qxd4 Nxd4 19. Rxd4 Rfd8 20. Rd2 Rxd2 21. Nxd2 Rd8 22. Nf3 Rd4 23. Kf1 Kf8 24. Ke2 Ke7 25. Kd3 Kd6 26. Nd2 Rc4 27. Nc4+ Kc5 28. Nc3+ Kd5 29. Nc4+ Kc5 30. Nc3+ Kd5 31. Nc4+ Kc5 32. Nc3+ Kd5 33. Nc4+ Kc5 34. Nc3+ Kd5 35. Nc4+ Kc5 36. Nc3+ Kd5 37. Nc4+ Kc5 38. Nc3+ Kd5 39. Nc4+ Kc5 40. Nc3+ Kd5 41. Nc4+ Kc5 42. Nc3+ Kd5 43. Nc4+ Kc5 44. Nc3+ Kd5 45. Nc4+ Kc5 46. Nc3+ Kd5 47. Nc4+ Kc5 48. Nc3+ Kd5 49. Nc4+ Kc5 50. Nc3+ Kd5 51. Nc4+ Kc5 52. Nc3+ Kd5 53. Nc4+ Kc5 54. Nc3+ Kd5 55. Nc4+ Kc5 56. Nc3+ Kd5 57. Nc4+ Kc5 58. Nc3+ Kd5 59. Nc4+ Kc5 60. Nc3+ Kd5 61. Nc4+ Kc5 62. Nc3+ Kd5 63. Nc4+ Kc5 64. N",
          "positional_tokens": {
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "Rd4",
            "Kf1",
            "Kf8",
            "Ke2",
            "Ke7",
            "Kd3",
            "Kd6",
            "Nd2",
            "Rc4",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf3",
          "neutral_uci": "g1f3",
          "neutral_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bb5 a6\n4. Ba4 Nf6\n5. O-O Be7\n6. Re1 b5\n7. Bb3 d6\n8. c3 O-O\n9. h3 Na5\n10. Bc2 c5\n11. d4 Nc6\n12. Nbd2 cxd4\n13. cxd4 exd4\n14. Nxd4 Bg4\n15. f3 Bxf3\n16. Qxf3 Nxd4\n17. Bxd4 Qxd4\n18. Qxd4 Nxd4\n19. Bxd4 Bc5\n20. Bxc5 dxc5\n21. Rf2 Rfd8\n22. Rc2 Rxd1\n23. Rxd1 Rd8\n24. Rxd8+ Kxd8\n25. Kf2 Ke7\n26. Ke3 Kd6\n27. Kd4 Kc6\n28. Kc4 Kb6\n29. Kb4 Kc6\n30. Kc4 Kb6\n31. Kb4 Kc6\n32. Kc4 Kb6\n33. Kb4 Kc6\n34. Kc4 Kb6\n35. Kb4 Kc6\n36. Kc4 Kb6\n37. Kb4 Kc6\n38. Kc4 Kb6\n39. Kb4 Kc6\n40. Kc4 Kb6\n41. Kb4 Kc6\n42. Kc4 Kb6\n43. Kb4 Kc6\n44. Kc4 Kb6\n45. Kb4 Kc6\n46. Kc4 Kb6\n47. Kb4 Kc6\n48. Kc4 Kb6\n49. Kb4 Kc6\n50. Kc4 Kb6\n51. Kb4 Kc6\n52. Kc4 Kb6\n53. Kb4 Kc6\n54. Kc4 Kb6\n55. Kb4 Kc6\n56. Kc4 Kb6\n57. Kb4 Kc6\n58. Kc4 Kb6\n59. Kb4 Kc6\n60. Kc4 Kb6\n61. Kb4 Kc6\n62. Kc4 Kb6\n63. Kb4 Kc6\n64. Kc4 Kb6\n65. Kb4 Kc6\n66. Kc4 Kb6\n67. Kb4 Kc6",
          "neutral_tokens": {
            "prompt_tokens": 119,
            "completion_tokens": 640,
            "total_tokens": 759,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "exd4",
            "Nxd4",
            "Bg4",
            "f3",
            "Bxf3",
            "Qxf3",
            "Nxd4",
            "Bxd4",
            "Qxd4",
            "Qxd4",
            "Nxd4",
            "Bxd4",
            "Bc5",
            "Bxc5",
            "dxc5",
            "Rf2",
            "Rfd8",
            "Rc2",
            "Rxd1",
            "Rxd1",
            "Rd8",
            "Rxd8+",
            "Kxd8",
            "Kf2",
            "Ke7",
            "Ke3",
            "Kd6",
            "Kd4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g1f3",
          "positional_uci": "g1f3",
          "neutral_uci": "g1f3",
          "consensus_move": "g1f3",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nc6",
              "Bc4",
              "Bc5",
              "c3",
              "Nf6",
              "d4",
              "exd4",
              "cxd4",
              "Bb4+",
              "Nc3",
              "Nxe4",
              "O-O",
              "Bxc3",
              "d5",
              "Bxa1",
              "Qxa1",
              "Nc5",
              "Qd4",
              "Qf6",
              "Qxc5",
              "Qxc5",
              "Nxe5",
              "Qd6",
              "Nc4",
              "O-O",
              "Be3",
              "Qd7",
              "Bxc5",
              "Qxc5",
              "Nxb7",
              "Qxb7",
              "Rc1",
              "Qd7",
              "Rc7",
              "Qd8",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Rd4",
              "Kf1",
              "Kf8",
              "Ke2",
              "Ke7",
              "Kd3",
              "Kd6",
              "Nd2",
              "Rc4",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "exd4",
              "Nxd4",
              "Bg4",
              "f3",
              "Bxf3",
              "Qxf3",
              "Nxd4",
              "Bxd4",
              "Qxd4",
              "Qxd4",
              "Nxd4",
              "Bxd4",
              "Bc5",
              "Bxc5",
              "dxc5",
              "Rf2",
              "Rfd8",
              "Rc2",
              "Rxd1",
              "Rxd1",
              "Rd8",
              "Rxd8+",
              "Kxd8",
              "Kf2",
              "Ke7",
              "Ke3",
              "Kd6",
              "Kd4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 119,
            "completion_tokens": 640,
            "total_tokens": 759,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 359,
          "total_completion_tokens": 1920,
          "total_tokens": 2279
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 119,
            "completion_tokens": 640,
            "total_tokens": 759,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 5,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bc4",
          "aggressive_uci": "f1c4",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Bc5 4. c3 Nf6 5. d4 exd4 6. cxd4 Bb4+ 7. Nc3 Nxe4 8. O-O Bxc3 9. d5 Bxa1 10. Qxa1+ Nc5 11. Qd4\n\nNxd5",
          "aggressive_tokens": {
            "prompt_tokens": 129,
            "completion_tokens": 95,
            "total_tokens": 224,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "c3",
            "Nf6",
            "d4",
            "exd4",
            "cxd4",
            "Bb4+",
            "Nc3",
            "Nxe4",
            "O-O",
            "Bxc3",
            "d5",
            "Bxa1",
            "Qxa1+",
            "Nc5",
            "Qd4",
            "Nxd5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bc4",
          "positional_uci": "f1c4",
          "positional_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4\n\n1. e4 e5 2. Nf3 Nc6 3. Bc4",
          "positional_tokens": {
            "prompt_tokens": 129,
            "completion_tokens": 41,
            "total_tokens": 170,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bb5",
          "neutral_uci": "f1b5",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5 11. d4 Nc6 12. Nbd2 cxd4 13. cxd4 exd4 14. Nxd4 Bc5 15. Nxc6 Bxc6 16. e5 dxe5 17. Nxe5 Bxe5 18. Rxe5 Qd6 19. Qf3 Qxd4 20. Qxd5+ Kh8 21. Qf5+ Qf6 22. Qxf6+ gxf6 23. Rxe7 Rg8 24. Re8+ Rg8 25. Rxf8+ Rxf8 26. Bxf8 Kg7 27. Bg5+ Kg8 28. Bxf6 Kg7 29. Bg5+ Kg8 30. Bxf8 Kg7 31. Bg5+ Kg8 32. Bxf8 Kg7 33. Bg5+ Kg8 34. Bxf8 Kg7 35. Bg5+ Kg8 36. Bxf8 Kg7 37. Bg5+ Kg8 38. Bxf8 Kg7 39. Bg5+ Kg8 40. Bxf8 Kg7 41. Bg5+ Kg8 42. Bxf8 Kg7 43. Bg5+ Kg8 44. Bxf8 Kg7 45. Bg5+ Kg8 46. Bxf8 Kg7 47. Bg5+ Kg8 48. Bxf8 Kg7 49. Bg5+ Kg8 50. Bxf8 Kg7 51. Bg5+ Kg8 52. Bxf8 Kg7 53. Bg5+ Kg8 54. Bxf8 Kg7 55. Bg5+ Kg8 56. Bxf8 Kg7 57. Bg5+ Kg8 58. Bxf8 Kg7 59. Bg5+ Kg8 60. Bxf8 Kg7 61. Bg5+ Kg8 62. Bxf8 Kg7 63. Bg5+ Kg8 64. Bxf8 Kg7 65. Bg",
          "neutral_tokens": {
            "prompt_tokens": 128,
            "completion_tokens": 640,
            "total_tokens": 768,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "exd4",
            "Nxd4",
            "Bc5",
            "Nxc6",
            "Bxc6",
            "e5",
            "dxe5",
            "Nxe5",
            "Bxe5",
            "Rxe5",
            "Qd6",
            "Qf3",
            "Qxd4",
            "Qxd5+",
            "Kh8",
            "Qf5+",
            "Qf6",
            "Qxf6+",
            "gxf6",
            "Rxe7",
            "Rg8",
            "Re8+",
            "Rg8",
            "Rxf8+",
            "Rxf8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf6",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f1c4",
          "positional_uci": "f1c4",
          "neutral_uci": "f1b5",
          "consensus_move": "f1c4",
          "source_agents": [
            "aggressive",
            "positional"
          ],
          "selected_agent": "aggressive"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bc5",
              "c3",
              "Nf6",
              "d4",
              "exd4",
              "cxd4",
              "Bb4+",
              "Nc3",
              "Nxe4",
              "O-O",
              "Bxc3",
              "d5",
              "Bxa1",
              "Qxa1+",
              "Nc5",
              "Qd4",
              "Nxd5"
            ],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "exd4",
              "Nxd4",
              "Bc5",
              "Nxc6",
              "Bxc6",
              "e5",
              "dxe5",
              "Nxe5",
              "Bxe5",
              "Rxe5",
              "Qd6",
              "Qf3",
              "Qxd4",
              "Qxd5+",
              "Kh8",
              "Qf5+",
              "Qf6",
              "Qxf6+",
              "gxf6",
              "Rxe7",
              "Rg8",
              "Re8+",
              "Rg8",
              "Rxf8+",
              "Rxf8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf6",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 129,
            "completion_tokens": 95,
            "total_tokens": 224,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 129,
            "completion_tokens": 41,
            "total_tokens": 170,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 128,
            "completion_tokens": 640,
            "total_tokens": 768,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 386,
          "total_completion_tokens": 776,
          "total_tokens": 1162
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 129,
            "completion_tokens": 95,
            "total_tokens": 224,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 129,
            "completion_tokens": 41,
            "total_tokens": 170,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 128,
            "completion_tokens": 640,
            "total_tokens": 768,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bqkbnr/pppp1ppp/2n5/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3"
    },
    {
      "turn": 7,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc3",
          "aggressive_uci": "b1c3",
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n\nNc3",
          "aggressive_tokens": {
            "prompt_tokens": 137,
            "completion_tokens": 27,
            "total_tokens": 164,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Ng5",
          "positional_uci": "f3g5",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Na5 6. Bb5+ c6 7. dxc6 bxc6 8. Be2 h6 9. Nf3 e4 10. Ne5 Qd4 11. Nc3\n\nQd3+ 12. Kf1 0-0-0 13. d4\n\nNc4 14. Bxc4 Qxc4 15. Nxc4",
          "positional_tokens": {
            "prompt_tokens": 137,
            "completion_tokens": 132,
            "total_tokens": 269,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d5",
            "exd5",
            "Na5",
            "Bb5+",
            "c6",
            "dxc6",
            "bxc6",
            "Be2",
            "h6",
            "Nf3",
            "e4",
            "Ne5",
            "Qd4",
            "Nc3",
            "Qd3+",
            "Kf1",
            "d4",
            "Nc4",
            "Bxc4",
            "Qxc4",
            "Nxc4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc3",
          "neutral_uci": "b1c3",
          "neutral_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n\nNc3",
          "neutral_tokens": {
            "prompt_tokens": 136,
            "completion_tokens": 27,
            "total_tokens": 163,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b1c3",
          "positional_uci": "f3g5",
          "neutral_uci": "b1c3",
          "consensus_move": "b1c3",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [
              "d5",
              "exd5",
              "Na5",
              "Bb5+",
              "c6",
              "dxc6",
              "bxc6",
              "Be2",
              "h6",
              "Nf3",
              "e4",
              "Ne5",
              "Qd4",
              "Nc3",
              "Qd3+",
              "Kf1",
              "d4",
              "Nc4",
              "Bxc4",
              "Qxc4",
              "Nxc4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 137,
            "completion_tokens": 27,
            "total_tokens": 164,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 137,
            "completion_tokens": 132,
            "total_tokens": 269,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 136,
            "completion_tokens": 27,
            "total_tokens": 163,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 410,
          "total_completion_tokens": 186,
          "total_tokens": 596
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 137,
            "completion_tokens": 27,
            "total_tokens": 164,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 137,
            "completion_tokens": 132,
            "total_tokens": 269,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 136,
            "completion_tokens": 27,
            "total_tokens": 163,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/pppp1ppp/2n2n2/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R w KQkq - 4 4"
    },
    {
      "turn": 9,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc3",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Nc3 Nxe4\n\nNc3",
          "aggressive_tokens": {
            "prompt_tokens": 146,
            "completion_tokens": 36,
            "total_tokens": 182,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nxe4",
          "positional_uci": "c3e4",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. Bxe4 Nxe4 8. Qh5 Nd6 9. Qxe5+ Be7 10. 0-0 0-0 11. Re1\n\nQd7 12. Qxd7 13. Nxd7",
          "positional_tokens": {
            "prompt_tokens": 146,
            "completion_tokens": 110,
            "total_tokens": 256,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qh5",
            "Nd6",
            "Qxe5+",
            "Be7",
            "Re1",
            "Qd7",
            "Qxd7",
            "Nxd7"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc3",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Nc3 Nxe4\n\nNc3 Nc6",
          "neutral_tokens": {
            "prompt_tokens": 145,
            "completion_tokens": 39,
            "total_tokens": 184,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": "c3e4",
          "neutral_uci": null,
          "consensus_move": "c3e4",
          "source_agents": [
            "positional"
          ],
          "selected_agent": "positional"
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [
              "Qh5",
              "Nd6",
              "Qxe5+",
              "Be7",
              "Re1",
              "Qd7",
              "Qxd7",
              "Nxd7"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 146,
            "completion_tokens": 36,
            "total_tokens": 182,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 146,
            "completion_tokens": 110,
            "total_tokens": 256,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 145,
            "completion_tokens": 39,
            "total_tokens": 184,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 437,
          "total_completion_tokens": 185,
          "total_tokens": 622
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 146,
            "completion_tokens": 36,
            "total_tokens": 182,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 146,
            "completion_tokens": 110,
            "total_tokens": 256,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 145,
            "completion_tokens": 39,
            "total_tokens": 184,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/pppp1ppp/2n5/4p3/2B1n3/2N2N2/PPPP1PPP/R1BQK2R w KQkq - 0 5"
    },
    {
      "turn": 11,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bd3",
          "aggressive_uci": "c4d3",
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Nc3 Nxe4\n5. Nxe4 d5\n6. Bd3 Bc5\n7. O-O O-O\n8. c3 Nf6\n9. d4 Bb6\n10. Nc3\n\nNf6",
          "aggressive_tokens": {
            "prompt_tokens": 154,
            "completion_tokens": 81,
            "total_tokens": 235,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "O-O",
            "O-O",
            "c3",
            "Nf6",
            "d4",
            "Bb6",
            "Nc3",
            "Nf6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bd3",
          "positional_uci": "c4d3",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 Be7 7. O-O O-O 8. c3\n\nNf6 9. d4",
          "positional_tokens": {
            "prompt_tokens": 154,
            "completion_tokens": 68,
            "total_tokens": 222,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Be7",
            "O-O",
            "O-O",
            "c3",
            "Nf6",
            "d4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bd3",
          "neutral_uci": "c4d3",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 Be7 7. O-O O-O 8. c4\n\nc4 Nxd4 9. Nxd4 dxc4 10. Bxc4",
          "neutral_tokens": {
            "prompt_tokens": 153,
            "completion_tokens": 81,
            "total_tokens": 234,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Be7",
            "O-O",
            "O-O",
            "c4",
            "c4",
            "Nxd4",
            "Nxd4",
            "dxc4",
            "Bxc4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "c4d3",
          "positional_uci": "c4d3",
          "neutral_uci": "c4d3",
          "consensus_move": "c4d3",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bc5",
              "O-O",
              "O-O",
              "c3",
              "Nf6",
              "d4",
              "Bb6",
              "Nc3",
              "Nf6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Be7",
              "O-O",
              "O-O",
              "c3",
              "Nf6",
              "d4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Be7",
              "O-O",
              "O-O",
              "c4",
              "c4",
              "Nxd4",
              "Nxd4",
              "dxc4",
              "Bxc4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 154,
            "completion_tokens": 81,
            "total_tokens": 235,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 154,
            "completion_tokens": 68,
            "total_tokens": 222,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 153,
            "completion_tokens": 81,
            "total_tokens": 234,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 461,
          "total_completion_tokens": 230,
          "total_tokens": 691
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 154,
            "completion_tokens": 81,
            "total_tokens": 235,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 154,
            "completion_tokens": 68,
            "total_tokens": 222,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 153,
            "completion_tokens": 81,
            "total_tokens": 234,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/3pp3/2B1N3/5N2/PPPP1PPP/R1BQK2R w KQkq - 0 6"
    },
    {
      "turn": 13,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bxe4",
          "aggressive_uci": "d3e4",
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Nc3 Nxe4\n5. Nxe4 d5\n6. Bd3 dxe4\n\nBxe4 Nc5",
          "aggressive_tokens": {
            "prompt_tokens": 162,
            "completion_tokens": 55,
            "total_tokens": 217,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "O-O",
          "positional_uci": "e1g1",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7",
          "positional_tokens": {
            "prompt_tokens": 162,
            "completion_tokens": 640,
            "total_tokens": 802,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "O-O",
          "neutral_uci": "e1g1",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O\n\ne4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O\n\nO-O",
          "neutral_tokens": {
            "prompt_tokens": 161,
            "completion_tokens": 108,
            "total_tokens": 269,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d3e4",
          "positional_uci": "e1g1",
          "neutral_uci": "e1g1",
          "consensus_move": "e1g1",
          "source_agents": [
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nc5"
            ],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 162,
            "completion_tokens": 55,
            "total_tokens": 217,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 162,
            "completion_tokens": 640,
            "total_tokens": 802,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 161,
            "completion_tokens": 108,
            "total_tokens": 269,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 485,
          "total_completion_tokens": 803,
          "total_tokens": 1288
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 162,
            "completion_tokens": 55,
            "total_tokens": 217,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 162,
            "completion_tokens": 640,
            "total_tokens": 802,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 161,
            "completion_tokens": 108,
            "total_tokens": 269,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/4p3/4p3/3B1N2/PPPP1PPP/R1BQK2R w KQkq - 0 7"
    },
    {
      "turn": 15,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qxd3",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O exd3 8. Qxd3 Qxd3 9. Rxd3\n\nQxd3 Rxd3",
          "aggressive_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 78,
            "total_tokens": 248,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Rxd3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qxd3",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O exd3 8. Qxd3 Bc5 9. Qe3+ Be7 10. Qxe7+ Kxe7 11. Bg5+ Kf8 12. Bxc6+ bxc6 13. Nxd5\n\nNxd5",
          "positional_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 117,
            "total_tokens": 287,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "Qe3+",
            "Be7",
            "Qxe7+",
            "Kxe7",
            "Bg5+",
            "Kf8",
            "Bxc6+",
            "bxc6",
            "Nxd5",
            "Nxd5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qxd3",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O exd3 8. Qxd3 Bc5 9. Qe3+ Be6 10. Qxe6+ fxe6\n\nQxd3+ Qxd3+",
          "neutral_tokens": {
            "prompt_tokens": 169,
            "completion_tokens": 93,
            "total_tokens": 262,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Rxd3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bc5",
              "Qe3+",
              "Be7",
              "Qxe7+",
              "Kxe7",
              "Bg5+",
              "Kf8",
              "Bxc6+",
              "bxc6",
              "Nxd5",
              "Nxd5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 170,
            "completion_tokens": 78,
            "total_tokens": 248,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 170,
            "completion_tokens": 117,
            "total_tokens": 287,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 169,
            "completion_tokens": 93,
            "total_tokens": 262,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 509,
          "total_completion_tokens": 288,
          "total_tokens": 797
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 170,
            "completion_tokens": 78,
            "total_tokens": 248,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 170,
            "completion_tokens": 117,
            "total_tokens": 287,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 169,
            "completion_tokens": 93,
            "total_tokens": 262,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/4p3/8/3p1N2/PPPP1PPP/R1BQ1RK1 w kq - 0 8"
    },
    {
      "turn": 15,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qxd3",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Nc3 Nxe4\n5. Nxe4 d5\n6. Bd3 dxe4\n7. O-O exd3\n8. Qxd3 Bc5\n9. Qe3+ Kf8\n10. Qe3+ Kf8\n\nQe3+ Kf8 Qe3+",
          "aggressive_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 97,
            "total_tokens": 267,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "Qe3+",
            "Kf8",
            "Qe3+",
            "Kf8",
            "Qe3+",
            "Kf8",
            "Qe3+"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qxd3",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O exd3 8. Qxd3 Nc6 9. Nc3 Be7 10. Be3\n\nQxd3 Nc6 9. Nc3 Be7 10. Be3",
          "positional_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 100,
            "total_tokens": 270,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc6",
            "Nc3",
            "Be7",
            "Be3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qxd3",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O exd3 8. Qxd3 Nxd4 9. Qxd4 Bg4 10. Qxd8+ Kxd8 11. Nxe5\n\nNxe5 12. Bxe5+ Kc8 13. Bxc6+",
          "neutral_tokens": {
            "prompt_tokens": 169,
            "completion_tokens": 115,
            "total_tokens": 284,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxd4",
            "Qxd4",
            "Bg4",
            "Qxd8+",
            "Kxd8",
            "Nxe5",
            "Nxe5",
            "Bxe5+",
            "Kc8",
            "Bxc6+"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bc5",
              "Qe3+",
              "Kf8",
              "Qe3+",
              "Kf8",
              "Qe3+",
              "Kf8",
              "Qe3+"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nc6",
              "Nc3",
              "Be7",
              "Be3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nxd4",
              "Qxd4",
              "Bg4",
              "Qxd8+",
              "Kxd8",
              "Nxe5",
              "Nxe5",
              "Bxe5+",
              "Kc8",
              "Bxc6+"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 170,
            "completion_tokens": 97,
            "total_tokens": 267,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 170,
            "completion_tokens": 100,
            "total_tokens": 270,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 169,
            "completion_tokens": 115,
            "total_tokens": 284,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 509,
          "total_completion_tokens": 312,
          "total_tokens": 821
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 170,
            "completion_tokens": 97,
            "total_tokens": 267,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 170,
            "completion_tokens": 100,
            "total_tokens": 270,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 169,
            "completion_tokens": 115,
            "total_tokens": 284,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/4p3/8/3p1N2/PPPP1PPP/R1BQ1RK1 w kq - 0 8"
    },
    {
      "turn": 15,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qxd3",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Nc3 Nxe4\n5. Nxe4 d5\n6. Bd3 dxe4\n7. O-O exd3\n8. Qxd3 Bc5\n9. Qe3+ Kf8\n10. Qe3+ Kf8\n\nQe3+ Kf8 Qe3+",
          "aggressive_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 97,
            "total_tokens": 267,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "Qe3+",
            "Kf8",
            "Qe3+",
            "Kf8",
            "Qe3+",
            "Kf8",
            "Qe3+"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qxd3",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O exd3 8. Qxd3 Bc5 9. Qe3+ Be6 10. Qxe6+ Qe7\n\nQe3+ Be6 10. Qxe6+ Qe7",
          "positional_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 102,
            "total_tokens": 272,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "Qe3+",
            "Be6",
            "Qxe6+",
            "Qe7",
            "Qe3+",
            "Be6",
            "Qxe6+",
            "Qe7"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qxd3",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O exd3 8. Qxd3 Bc5 9. Qe3+ Kf8 10. Qxe5\n\nQxe5+ Kg8 11. Qe3+",
          "neutral_tokens": {
            "prompt_tokens": 169,
            "completion_tokens": 97,
            "total_tokens": 266,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "Qe3+",
            "Kf8",
            "Qxe5",
            "Qxe5+",
            "Kg8",
            "Qe3+"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bc5",
              "Qe3+",
              "Kf8",
              "Qe3+",
              "Kf8",
              "Qe3+",
              "Kf8",
              "Qe3+"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bc5",
              "Qe3+",
              "Be6",
              "Qxe6+",
              "Qe7",
              "Qe3+",
              "Be6",
              "Qxe6+",
              "Qe7"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bc5",
              "Qe3+",
              "Kf8",
              "Qxe5",
              "Qxe5+",
              "Kg8",
              "Qe3+"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 170,
            "completion_tokens": 97,
            "total_tokens": 267,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 170,
            "completion_tokens": 102,
            "total_tokens": 272,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 169,
            "completion_tokens": 97,
            "total_tokens": 266,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 509,
          "total_completion_tokens": 296,
          "total_tokens": 805
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 170,
            "completion_tokens": 97,
            "total_tokens": 267,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 170,
            "completion_tokens": 102,
            "total_tokens": 272,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 169,
            "completion_tokens": 97,
            "total_tokens": 266,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/4p3/8/3p1N2/PPPP1PPP/R1BQ1RK1 w kq - 0 8"
    },
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4\n\n1. d4",
          "aggressive_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4\n\n1. d4",
          "positional_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": null,
          "neutral_response": "1. d4\n\n1. d4",
          "neutral_tokens": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 353,
          "total_completion_tokens": 27,
          "total_tokens": 380
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 2,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4\n\n1. d4",
          "aggressive_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4\n\n1. d4",
          "positional_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": null,
          "neutral_response": "1. d4\n\n1. d4",
          "neutral_tokens": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 353,
          "total_completion_tokens": 27,
          "total_tokens": 380
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 2,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4\n\n1. d4",
          "aggressive_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4\n\n1. d4",
          "positional_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": null,
          "neutral_response": "1. d4\n\n1. d4",
          "neutral_tokens": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 353,
          "total_completion_tokens": 27,
          "total_tokens": 380
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": "e2e4",
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bb5 a6\n4. Ba4 Nf6\n5. O-O Be7\n6. Re1 b5\n7. Bb3 d6\n8. c3 O-O\n9. h3 Na5\n10. Bc2 c5\n11. d4 Nc6\n12. Nbd2 cxd4\n13. cxd4 exd4\n14. Nxd4 Bb7\n15. Nxc6 Bxc6\n16. Bg5 h6\n17. Bh4 g5\n18. Bg3 Kh7\n19. Qd3+ f5\n20. exf6+ Kg6\n21. fxe7 Qxe7\n22. Qf5+ Kg7\n23. Qf7+ Kh8\n24. Qf8# 1-0\n\nQf8# 1-0",
          "aggressive_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 223,
            "total_tokens": 337,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "exd4",
            "Nxd4",
            "Bb7",
            "Nxc6",
            "Bxc6",
            "Bg5",
            "h6",
            "Bh4",
            "g5",
            "Bg3",
            "Kh7",
            "Qd3+",
            "f5",
            "exf6+",
            "Kg6",
            "fxe7",
            "Qxe7",
            "Qf5+",
            "Kg7",
            "Qf7+",
            "Kh8",
            "Qf8#",
            "Qf8#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bb5 a6\n4. Ba4 Nf6\n5. O-O Be7\n6. Re1 b5\n7. Bb3 d6\n8. c3 O-O\n9. h3 Na5\n10. Bc2 c5\n11. d4 Nc6\n12. Nbd2 cxd4\n13. cxd4 exd4\n14. Nxd4 Nxd4\n15. Qxd4 Bc5\n16. Qd2 Be6\n17. Nf3 Qd7\n18. Be3 Rad8\n19. Qc2 Rfe8\n20. Rc1 Rc8\n21. Rcd1 Qe7\n22. Qd2 Qd7\n23. Qc2 Qe7\n24. Qd2 Qd7\n25. Qc2 Qe7\n26. Qd2 Qd7\n27. Qc2 Qe7\n28. Qd2 Qd7\n29. Qc2 Qe7\n30. Qd2 Qd7\n31. Qc2 Qe7\n32. Qd2 Qd7\n33. Qc2 Qe7\n34. Qd2 Qd7\n35. Qc2 Qe7\n36. Qd2 Qd7\n37. Qc2 Qe7\n38. Qd2 Qd7\n39. Qc2 Qe7\n40. Qd2 Qd7\n41. Qc2 Qe7\n42. Qd2 Qd7\n43. Qc2 Qe7\n44. Qd2 Qd7\n45. Qc2 Qe7\n46. Qd2 Qd7\n47. Qc2 Qe7\n48. Qd2 Qd7\n49. Qc2 Qe7\n50. Qd2 Qd7\n51. Qc2 Qe7\n52. Qd2 Qd7\n53. Qc2 Qe7\n54. Qd2 Qd7\n55. Qc2 Qe7\n56. Qd2 Qd7\n57. Qc2 Qe7\n58. Qd2 Qd7\n59. Qc2 Qe7\n60. Qd2 Qd7\n61. Qc2 Qe7\n62. Qd2 Qd7\n63. Qc2 Qe7\n64. Qd2 Qd7\n65. Qc2 Qe7\n66. Qd2 Qd7\n67. Qc2 Qe7",
          "positional_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "exd4",
            "Nxd4",
            "Nxd4",
            "Qxd4",
            "Bc5",
            "Qd2",
            "Be6",
            "Nf3",
            "Qd7",
            "Be3",
            "Rad8",
            "Qc2",
            "Rfe8",
            "Rc1",
            "Rc8",
            "Rcd1",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": "e2e4",
          "neutral_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bb5 a6\n4. Ba4 Nf6\n5. O-O Be7\n6. Re1 b5\n7. Bb3 d6\n8. c3 O-O\n9. h3 Na5\n10. Bc2 c5\n11. d4 Nc6\n12. Nbd2 cxd4\n13. cxd4 exd4\n14. Nxd4 Nxd4\n15. Qxd4 Bc5\n16. Qd3 Bxd4\n17. Bxd4 Qxd4\n18. Qxd4 Nxd4\n19. Bxd4 Bg4\n20. f3 Be6\n21. Kf2 Kf8\n22. Ke3 Ke7\n23. Kd3 Kd6\n24. Kc3 Kc6\n25. Kb3 Kb6\n26. Ka3 Ka5\n27. Kxa5 Kxa5\n28. Kb3 Kb5\n29. Kc3 Kc5\n30. Kd3 Kd5\n31. Ke3 Ke5\n32. Kf3 Kf5\n33. Kg3 Kg5\n34. Kf3 Kf5\n35. Kg3 Kg5\n36. Kf3 Kf5\n37. Kg3 Kg5\n38. Kf3 Kf5\n39. Kg3 Kg5\n40. Kf3 Kf5\n41. Kg3 Kg5\n42. Kf3 Kf5\n43. Kg3 Kg5\n44. Kf3 Kf5\n45. Kg3 Kg5\n46. Kf3 Kf5\n47. Kg3 Kg5\n48. Kf3 Kf5\n49. Kg3 Kg5\n50. Kf3 Kf5\n51. Kg3 Kg5\n52. Kf3 Kf5\n53. Kg3 Kg5\n54. Kf3 Kf5\n55. Kg3 Kg5\n56. Kf3 Kf5\n57. Kg3 Kg5\n58. Kf3 Kf5\n59. Kg3 Kg5\n60. Kf3 Kf5\n61. Kg3 Kg5\n62. Kf3 Kf5\n63. Kg3 Kg5\n64. Kf3 Kf5\n65. Kg3 Kg5\n66. Kf3 Kf5\n67. Kg3 Kg5\n68. K",
          "neutral_tokens": {
            "prompt_tokens": 113,
            "completion_tokens": 640,
            "total_tokens": 753,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "exd4",
            "Nxd4",
            "Nxd4",
            "Qxd4",
            "Bc5",
            "Qd3",
            "Bxd4",
            "Bxd4",
            "Qxd4",
            "Qxd4",
            "Nxd4",
            "Bxd4",
            "Bg4",
            "f3",
            "Be6",
            "Kf2",
            "Kf8",
            "Ke3",
            "Ke7",
            "Kd3",
            "Kd6",
            "Kc3",
            "Kc6",
            "Kb3",
            "Kb6",
            "Ka3",
            "Ka5",
            "Kxa5",
            "Kxa5",
            "Kb3",
            "Kb5",
            "Kc3",
            "Kc5",
            "Kd3",
            "Kd5",
            "Ke3",
            "Ke5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e2e4",
          "positional_uci": "e2e4",
          "neutral_uci": "e2e4",
          "consensus_move": "e2e4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "exd4",
              "Nxd4",
              "Bb7",
              "Nxc6",
              "Bxc6",
              "Bg5",
              "h6",
              "Bh4",
              "g5",
              "Bg3",
              "Kh7",
              "Qd3+",
              "f5",
              "exf6+",
              "Kg6",
              "fxe7",
              "Qxe7",
              "Qf5+",
              "Kg7",
              "Qf7+",
              "Kh8",
              "Qf8#",
              "Qf8#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "exd4",
              "Nxd4",
              "Nxd4",
              "Qxd4",
              "Bc5",
              "Qd2",
              "Be6",
              "Nf3",
              "Qd7",
              "Be3",
              "Rad8",
              "Qc2",
              "Rfe8",
              "Rc1",
              "Rc8",
              "Rcd1",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "exd4",
              "Nxd4",
              "Nxd4",
              "Qxd4",
              "Bc5",
              "Qd3",
              "Bxd4",
              "Bxd4",
              "Qxd4",
              "Qxd4",
              "Nxd4",
              "Bxd4",
              "Bg4",
              "f3",
              "Be6",
              "Kf2",
              "Kf8",
              "Ke3",
              "Ke7",
              "Kd3",
              "Kd6",
              "Kc3",
              "Kc6",
              "Kb3",
              "Kb6",
              "Ka3",
              "Ka5",
              "Kxa5",
              "Kxa5",
              "Kb3",
              "Kb5",
              "Kc3",
              "Kc5",
              "Kd3",
              "Kd5",
              "Ke3",
              "Ke5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 114,
            "completion_tokens": 223,
            "total_tokens": 337,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 113,
            "completion_tokens": 640,
            "total_tokens": 753,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 341,
          "total_completion_tokens": 1503,
          "total_tokens": 1844
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 114,
            "completion_tokens": 223,
            "total_tokens": 337,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 113,
            "completion_tokens": 640,
            "total_tokens": 753,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 3,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf3",
          "aggressive_uci": "g1f3",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5 11. d4 Nc6 12. Nbd2 cxd4 13. cxd4 exd4 14. Nxd4 Bg4 15. f3 Bxf3 16. Qxf3 Nxd4 17. Bxd4 Qxd4 18. Qxd4 Nxd4 19. Nc4 Nc6 20. Bb3\n\nBd7",
          "aggressive_tokens": {
            "prompt_tokens": 120,
            "completion_tokens": 174,
            "total_tokens": 294,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "exd4",
            "Nxd4",
            "Bg4",
            "f3",
            "Bxf3",
            "Qxf3",
            "Nxd4",
            "Bxd4",
            "Qxd4",
            "Qxd4",
            "Nxd4",
            "Nc4",
            "Nc6",
            "Bb3",
            "Bd7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf3",
          "positional_uci": "g1f3",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5 11. d4 Nc6 12. Nbd2 cxd4 13. cxd4 exd4 14. Nxd4 Nxd4 15. Qxd4 Bc5 16. Qd2 Be6 17. Nf3 Rc8 18. Bb3 Qd7 19. Qf4\n\nNf6",
          "positional_tokens": {
            "prompt_tokens": 120,
            "completion_tokens": 163,
            "total_tokens": 283,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Rc8",
            "Bb3",
            "Qd7",
            "Qf4",
            "Nf6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf3",
          "neutral_uci": "g1f3",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5 11. d4 Nc6 12. Nbd2 cxd4 13. cxd4 exd4 14. Nxd4 Nxd4 15. Qxd4 Bc5 16. Qd2 Bxd4 17. Bxd4 Qxd4 18. Qxd4 Nxd4 19. Rxd4 Rfd8 20. Rxd8+ Rxd8 21. Kf1 Kf8 22. Ke2 Ke7 23. Kd3 Kd6 24. Kc4 Kc6 25. Kb4 Kb6 26. Kxa4 Kxa5 27. Kb3 Kb5 28. Kc3 Kc5 29. Kd3 Kd5 30. Ke3 Ke5 31. Kf3 Kf5 32. Kg3 Kg5 33. Kh3 Kh5 34. Kg3 Kg5 35. Kh3 Kh5 36. Kg3 Kg5 37. Kh3 Kh5 38. Kg3 Kg5 39. Kh3 Kh5 40. Kg3 Kg5 41. Kh3 Kh5 42. Kg3 Kg5 43. Kh3 Kh5 44. Kg3 Kg5 45. Kh3 Kh5 46. Kg3 Kg5 47. Kh3 Kh5 48. Kg3 Kg5 49. Kh3 Kh5 50. Kg3 Kg5 51. Kh3 Kh5 52. Kg3 Kg5 53. Kh3 Kh5 54. Kg3 Kg5 55. Kh3 Kh5 56. Kg3 Kg5 57. Kh3 Kh5 58. Kg3 Kg5 59. Kh3 Kh5 60. Kg3 Kg5 61. Kh3 Kh5 62. Kg3 Kg5 63. Kh3 Kh5 64. Kg3 Kg5 65. Kh3 Kh5 66. Kg3 Kg5 67. Kh3 Kh5 68. Kg3 Kg5 69. Kh3 Kh5 70. Kg3 Kg5 71. Kh3 Kh5",
          "neutral_tokens": {
            "prompt_tokens": 119,
            "completion_tokens": 640,
            "total_tokens": 759,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "exd4",
            "Nxd4",
            "Nxd4",
            "Qxd4",
            "Bc5",
            "Qd2",
            "Bxd4",
            "Bxd4",
            "Qxd4",
            "Qxd4",
            "Nxd4",
            "Rxd4",
            "Rfd8",
            "Rxd8+",
            "Rxd8",
            "Kf1",
            "Kf8",
            "Ke2",
            "Ke7",
            "Kd3",
            "Kd6",
            "Kc4",
            "Kc6",
            "Kb4",
            "Kb6",
            "Kxa4",
            "Kxa5",
            "Kb3",
            "Kb5",
            "Kc3",
            "Kc5",
            "Kd3",
            "Kd5",
            "Ke3",
            "Ke5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g1f3",
          "positional_uci": "g1f3",
          "neutral_uci": "g1f3",
          "consensus_move": "g1f3",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "exd4",
              "Nxd4",
              "Bg4",
              "f3",
              "Bxf3",
              "Qxf3",
              "Nxd4",
              "Bxd4",
              "Qxd4",
              "Qxd4",
              "Nxd4",
              "Nc4",
              "Nc6",
              "Bb3",
              "Bd7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Rc8",
              "Bb3",
              "Qd7",
              "Qf4",
              "Nf6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "exd4",
              "Nxd4",
              "Nxd4",
              "Qxd4",
              "Bc5",
              "Qd2",
              "Bxd4",
              "Bxd4",
              "Qxd4",
              "Qxd4",
              "Nxd4",
              "Rxd4",
              "Rfd8",
              "Rxd8+",
              "Rxd8",
              "Kf1",
              "Kf8",
              "Ke2",
              "Ke7",
              "Kd3",
              "Kd6",
              "Kc4",
              "Kc6",
              "Kb4",
              "Kb6",
              "Kxa4",
              "Kxa5",
              "Kb3",
              "Kb5",
              "Kc3",
              "Kc5",
              "Kd3",
              "Kd5",
              "Ke3",
              "Ke5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 120,
            "completion_tokens": 174,
            "total_tokens": 294,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 120,
            "completion_tokens": 163,
            "total_tokens": 283,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 119,
            "completion_tokens": 640,
            "total_tokens": 759,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 359,
          "total_completion_tokens": 977,
          "total_tokens": 1336
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 120,
            "completion_tokens": 174,
            "total_tokens": 294,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 120,
            "completion_tokens": 163,
            "total_tokens": 283,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 119,
            "completion_tokens": 640,
            "total_tokens": 759,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 5,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bc4",
          "aggressive_uci": "f1c4",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Bc5 4. c3 Nf6 5. d4 exd4 6. cxd4 Bb4+ 7. Nc3 Nxe4 8. O-O Bxc3 9. d5 Bxa1 10. Qxa1+ Nc5 11. Qd4 Ncd7 12. Nxe5\n\nNf6",
          "aggressive_tokens": {
            "prompt_tokens": 129,
            "completion_tokens": 105,
            "total_tokens": 234,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "c3",
            "Nf6",
            "d4",
            "exd4",
            "cxd4",
            "Bb4+",
            "Nc3",
            "Nxe4",
            "O-O",
            "Bxc3",
            "d5",
            "Bxa1",
            "Qxa1+",
            "Nc5",
            "Qd4",
            "Ncd7",
            "Nxe5",
            "Nf6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bb5",
          "positional_uci": "f1b5",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5 11. d4 Nc6 12. Nbd2 cxd4 13. cxd4 exd4 14. Nxd4 Nxd4 15. Qxd4 Bc5 16. Qd3 Bxd4 17. Bxd4 Qxd4 18. Rxd4 Nxd4 19. Rc1 Nf5 20. Rc7\n\nRc8",
          "positional_tokens": {
            "prompt_tokens": 129,
            "completion_tokens": 174,
            "total_tokens": 303,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "exd4",
            "Nxd4",
            "Nxd4",
            "Qxd4",
            "Bc5",
            "Qd3",
            "Bxd4",
            "Bxd4",
            "Qxd4",
            "Rxd4",
            "Nxd4",
            "Rc1",
            "Nf5",
            "Rc7",
            "Rc8"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bc4",
          "neutral_uci": "f1c4",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Bc5 4. c3 Nf6 5. d4 exd4 6. cxd4 Bb4+ 7. Nc3 Nxe4 8. O-O Bxc3 9. d5 Bxa1 10. Qxa1+ Nc6 11. Qxa7\n\nNxd5",
          "neutral_tokens": {
            "prompt_tokens": 128,
            "completion_tokens": 95,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "c3",
            "Nf6",
            "d4",
            "exd4",
            "cxd4",
            "Bb4+",
            "Nc3",
            "Nxe4",
            "O-O",
            "Bxc3",
            "d5",
            "Bxa1",
            "Qxa1+",
            "Nc6",
            "Qxa7",
            "Nxd5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f1c4",
          "positional_uci": "f1b5",
          "neutral_uci": "f1c4",
          "consensus_move": "f1c4",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bc5",
              "c3",
              "Nf6",
              "d4",
              "exd4",
              "cxd4",
              "Bb4+",
              "Nc3",
              "Nxe4",
              "O-O",
              "Bxc3",
              "d5",
              "Bxa1",
              "Qxa1+",
              "Nc5",
              "Qd4",
              "Ncd7",
              "Nxe5",
              "Nf6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "exd4",
              "Nxd4",
              "Nxd4",
              "Qxd4",
              "Bc5",
              "Qd3",
              "Bxd4",
              "Bxd4",
              "Qxd4",
              "Rxd4",
              "Nxd4",
              "Rc1",
              "Nf5",
              "Rc7",
              "Rc8"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bc5",
              "c3",
              "Nf6",
              "d4",
              "exd4",
              "cxd4",
              "Bb4+",
              "Nc3",
              "Nxe4",
              "O-O",
              "Bxc3",
              "d5",
              "Bxa1",
              "Qxa1+",
              "Nc6",
              "Qxa7",
              "Nxd5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 129,
            "completion_tokens": 105,
            "total_tokens": 234,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 129,
            "completion_tokens": 174,
            "total_tokens": 303,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 128,
            "completion_tokens": 95,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 386,
          "total_completion_tokens": 374,
          "total_tokens": 760
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 129,
            "completion_tokens": 105,
            "total_tokens": 234,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 129,
            "completion_tokens": 174,
            "total_tokens": 303,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 128,
            "completion_tokens": 95,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkbnr/pppp1ppp/2n5/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3"
    },
    {
      "turn": 7,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc3",
          "aggressive_uci": "b1c3",
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n\nNc3",
          "aggressive_tokens": {
            "prompt_tokens": 137,
            "completion_tokens": 27,
            "total_tokens": 164,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Ng5",
          "positional_uci": "f3g5",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Na5 6. Bb5+ c6 7. d4 Bd6 8. Nf3 0-0 9. 0-0\n\nNc5",
          "positional_tokens": {
            "prompt_tokens": 137,
            "completion_tokens": 75,
            "total_tokens": 212,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d5",
            "exd5",
            "Na5",
            "Bb5+",
            "c6",
            "d4",
            "Bd6",
            "Nf3",
            "Nc5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Ng5",
          "neutral_uci": "f3g5",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Na5 6. Bb5+ c6 7. dxc6 bxc6 8. Be2 h6 9. Nf3 e4 10. Ne5 Qd6 11. O-O\n\nQxd5 12. Nc3 13. Nxd5 14. Nxd5\n\nNxd5",
          "neutral_tokens": {
            "prompt_tokens": 136,
            "completion_tokens": 115,
            "total_tokens": 251,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d5",
            "exd5",
            "Na5",
            "Bb5+",
            "c6",
            "dxc6",
            "bxc6",
            "Be2",
            "h6",
            "Nf3",
            "e4",
            "Ne5",
            "Qd6",
            "O-O",
            "Qxd5",
            "Nc3",
            "Nxd5",
            "Nxd5",
            "Nxd5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b1c3",
          "positional_uci": "f3g5",
          "neutral_uci": "f3g5",
          "consensus_move": "f3g5",
          "source_agents": [
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [
              "d5",
              "exd5",
              "Na5",
              "Bb5+",
              "c6",
              "d4",
              "Bd6",
              "Nf3",
              "Nc5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "d5",
              "exd5",
              "Na5",
              "Bb5+",
              "c6",
              "dxc6",
              "bxc6",
              "Be2",
              "h6",
              "Nf3",
              "e4",
              "Ne5",
              "Qd6",
              "O-O",
              "Qxd5",
              "Nc3",
              "Nxd5",
              "Nxd5",
              "Nxd5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 137,
            "completion_tokens": 27,
            "total_tokens": 164,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 137,
            "completion_tokens": 75,
            "total_tokens": 212,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 136,
            "completion_tokens": 115,
            "total_tokens": 251,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 410,
          "total_completion_tokens": 217,
          "total_tokens": 627
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 137,
            "completion_tokens": 27,
            "total_tokens": 164,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 137,
            "completion_tokens": 75,
            "total_tokens": 212,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 136,
            "completion_tokens": 115,
            "total_tokens": 251,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/pppp1ppp/2n2n2/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R w KQkq - 4 4"
    },
    {
      "turn": 9,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nxe4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Ng5 d5\n\nNxe4",
          "aggressive_tokens": {
            "prompt_tokens": 144,
            "completion_tokens": 34,
            "total_tokens": 178,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "exd5",
          "positional_uci": "e4d5",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. Bc5\n\nNc6 Bc5",
          "positional_tokens": {
            "prompt_tokens": 144,
            "completion_tokens": 59,
            "total_tokens": 203,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxd5",
            "Nxf7",
            "Kxf7",
            "Bc5",
            "Nc6",
            "Bc5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "exd5",
          "neutral_uci": "e4d5",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. Bc7+ Ke8 8. Qf3+ Kd7 9. Qf7+ Ke6 10. Qf5+ Kd7 11. Qf7+ Ke6 12. Qf5+ Kd7 13. Qf7+ Ke6 14. Qf5+ Kd7 15. Qf7+ Ke6 16. Qf5+ Kd7 17. Qf7+ Ke6 18. Qf5+ Kd7 19. Qf7+ Ke6 20. Qf5+ Kd7 21. Qf7+ Ke6 22. Qf5+ Kd7 23. Qf7+ Ke6 24. Qf5+ Kd7 25. Qf7+ Ke6 26. Qf5+ Kd7 27. Qf7+ Ke6 28. Qf5+ Kd7 29. Qf7+ Ke6 30. Qf5+ Kd7 31. Qf7+ Ke6 32. Qf5+ Kd7 33. Qf7+ Ke6 34. Qf5+ Kd7 35. Qf7+ Ke6 36. Qf5+ Kd7 37. Qf7+ Ke6 38. Qf5+ Kd7 39. Qf7+ Ke6 40. Qf5+ Kd7 41. Qf7+ Ke6 42. Qf5+ Kd7 43. Qf7+ Ke6 44. Qf5+ Kd7 45. Qf7+ Ke6 46. Qf5+ Kd7 47. Qf7+ Ke6 48. Qf5+ Kd7 49. Qf7+ Ke6 50. Qf5+ Kd7 51. Qf7+ Ke6 52. Qf5+ Kd7 53. Qf7+ Ke6 54. Qf5+ Kd7 55. Qf7+ Ke6 56. Qf5+ Kd7 57. Qf7+ Ke6 58. Qf5+ Kd7 59. Qf7+ Ke6 60. Qf5+ Kd7 61. Qf7+ Ke6 62. Qf5+ Kd7 63. Qf7+",
          "neutral_tokens": {
            "prompt_tokens": 143,
            "completion_tokens": 640,
            "total_tokens": 783,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nxd5",
            "Nxf7",
            "Kxf7",
            "Bc7+",
            "Ke8",
            "Qf3+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": "e4d5",
          "neutral_uci": "e4d5",
          "consensus_move": "e4d5",
          "source_agents": [
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [
              "Nxd5",
              "Nxf7",
              "Kxf7",
              "Bc5",
              "Nc6",
              "Bc5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nxd5",
              "Nxf7",
              "Kxf7",
              "Bc7+",
              "Ke8",
              "Qf3+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 144,
            "completion_tokens": 34,
            "total_tokens": 178,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 144,
            "completion_tokens": 59,
            "total_tokens": 203,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 143,
            "completion_tokens": 640,
            "total_tokens": 783,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 431,
          "total_completion_tokens": 733,
          "total_tokens": 1164
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 144,
            "completion_tokens": 34,
            "total_tokens": 178,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 144,
            "completion_tokens": 59,
            "total_tokens": 203,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 143,
            "completion_tokens": 640,
            "total_tokens": 783,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n2n2/3pp1N1/2B1P3/8/PPPP1PPP/RNBQK2R w KQkq - 0 5"
    },
    {
      "turn": 11,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nxf7",
          "aggressive_uci": "g5f7",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. Qf3+ Ke6 8. Nc3\n\nNc3 Qd6",
          "aggressive_tokens": {
            "prompt_tokens": 153,
            "completion_tokens": 70,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kxf7",
            "Qf3+",
            "Ke6",
            "Nc3",
            "Nc3",
            "Qd6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nxf7",
          "positional_uci": "g5f7",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. Qf3+ Ke6 8. Nc3\n\nNc3 Qd6",
          "positional_tokens": {
            "prompt_tokens": 153,
            "completion_tokens": 70,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kxf7",
            "Qf3+",
            "Ke6",
            "Nc3",
            "Nc3",
            "Qd6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nxf7",
          "neutral_uci": "g5f7",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. Qf3+ Ke6 8. Nc3\n\nNc3 Qd6",
          "neutral_tokens": {
            "prompt_tokens": 152,
            "completion_tokens": 70,
            "total_tokens": 222,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kxf7",
            "Qf3+",
            "Ke6",
            "Nc3",
            "Nc3",
            "Qd6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g5f7",
          "positional_uci": "g5f7",
          "neutral_uci": "g5f7",
          "consensus_move": "g5f7",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Kxf7",
              "Qf3+",
              "Ke6",
              "Nc3",
              "Nc3",
              "Qd6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Kxf7",
              "Qf3+",
              "Ke6",
              "Nc3",
              "Nc3",
              "Qd6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Kxf7",
              "Qf3+",
              "Ke6",
              "Nc3",
              "Nc3",
              "Qd6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 153,
            "completion_tokens": 70,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 153,
            "completion_tokens": 70,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 152,
            "completion_tokens": 70,
            "total_tokens": 222,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 458,
          "total_completion_tokens": 210,
          "total_tokens": 668
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 153,
            "completion_tokens": 70,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 153,
            "completion_tokens": 70,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 152,
            "completion_tokens": 70,
            "total_tokens": 222,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/3np1N1/2B5/8/PPPP1PPP/RNBQK2R w KQkq - 0 6"
    },
    {
      "turn": 13,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qf3+",
          "aggressive_uci": "d1f3",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. Qf3+ Ke6 8. Nc3 Qd7 9. O-O\n\nQd7 7. Qf3+ Ke6 8. Nc3",
          "aggressive_tokens": {
            "prompt_tokens": 162,
            "completion_tokens": 90,
            "total_tokens": 252,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Ke6",
            "Nc3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": "d2d4",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7\n\nBc4+ Kg8 7. d4 Nxc3 8. bxc3",
          "positional_tokens": {
            "prompt_tokens": 162,
            "completion_tokens": 70,
            "total_tokens": 232,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxc3",
            "bxc3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": "d2d4",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7\n\nBc7+ Kg8 7. d4 Nf6 8. Qf3",
          "neutral_tokens": {
            "prompt_tokens": 161,
            "completion_tokens": 70,
            "total_tokens": 231,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "Qf3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d1f3",
          "positional_uci": "d2d4",
          "neutral_uci": "d2d4",
          "consensus_move": "d2d4",
          "source_agents": [
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Ke6",
              "Nc3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nxc3",
              "bxc3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf6",
              "Qf3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 162,
            "completion_tokens": 90,
            "total_tokens": 252,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 162,
            "completion_tokens": 70,
            "total_tokens": 232,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 161,
            "completion_tokens": 70,
            "total_tokens": 231,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 485,
          "total_completion_tokens": 230,
          "total_tokens": 715
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 162,
            "completion_tokens": 90,
            "total_tokens": 252,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 162,
            "completion_tokens": 70,
            "total_tokens": 232,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 161,
            "completion_tokens": 70,
            "total_tokens": 231,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bq1b1r/ppp2kpp/2n5/3np3/2B5/8/PPPP1PPP/RNBQK2R w KQ - 0 7"
    },
    {
      "turn": 15,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qf3+",
          "aggressive_uci": "d1f3",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4\n\nBc5+ Kg8 8. Qf3+",
          "aggressive_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 71,
            "total_tokens": 241,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qf3+",
          "positional_uci": "d1f3",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4\n\nBc5+ Kg8 8. Qf3+",
          "positional_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 71,
            "total_tokens": 241,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bc5+",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4\n\nBc5+",
          "neutral_tokens": {
            "prompt_tokens": 169,
            "completion_tokens": 61,
            "total_tokens": 230,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d1f3",
          "positional_uci": "d1f3",
          "neutral_uci": null,
          "consensus_move": "d1f3",
          "source_agents": [
            "aggressive",
            "positional"
          ],
          "selected_agent": "aggressive"
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 170,
            "completion_tokens": 71,
            "total_tokens": 241,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 170,
            "completion_tokens": 71,
            "total_tokens": 241,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 169,
            "completion_tokens": 61,
            "total_tokens": 230,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 509,
          "total_completion_tokens": 203,
          "total_tokens": 712
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 170,
            "completion_tokens": 71,
            "total_tokens": 241,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 170,
            "completion_tokens": 71,
            "total_tokens": 241,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 169,
            "completion_tokens": 61,
            "total_tokens": 230,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bq1b1r/ppp2kpp/8/3np3/2Bn4/8/PPP2PPP/RNBQK2R w KQ - 0 8"
    },
    {
      "turn": 17,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qe3+",
          "aggressive_uci": "f3e3",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3+ Kf7 10. Qf3+ Ke6 11. Qe3+ Kf7 12. Qf3+ Ke6\n\nQe3+ Kf7 Qf3+ Ke6",
          "aggressive_tokens": {
            "prompt_tokens": 179,
            "completion_tokens": 120,
            "total_tokens": 299,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kf7",
            "Qf3+",
            "Ke6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qf3+",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6\n\nQe3+ Kf5 9. Qf3+ Kg6 10. Qf5+ Kh6 11. Qf7+",
          "positional_tokens": {
            "prompt_tokens": 179,
            "completion_tokens": 101,
            "total_tokens": 280,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kg6",
            "Qf5+",
            "Kh6",
            "Qf7+"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Ke7",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6\n\nKe7 Qxd5+ Kf7 Qf5+ Kg6 Qg5+ Kh6 Qf5+ Kg6",
          "neutral_tokens": {
            "prompt_tokens": 178,
            "completion_tokens": 95,
            "total_tokens": 273,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qxd5+",
            "Kf7",
            "Qf5+",
            "Kg6",
            "Qg5+",
            "Kh6",
            "Qf5+",
            "Kg6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f3e3",
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": "f3e3",
          "source_agents": [
            "aggressive"
          ],
          "selected_agent": "aggressive"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Kf7",
              "Qf3+",
              "Ke6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Kg6",
              "Qf5+",
              "Kh6",
              "Qf7+"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qxd5+",
              "Kf7",
              "Qf5+",
              "Kg6",
              "Qg5+",
              "Kh6",
              "Qf5+",
              "Kg6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 179,
            "completion_tokens": 120,
            "total_tokens": 299,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 179,
            "completion_tokens": 101,
            "total_tokens": 280,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 178,
            "completion_tokens": 95,
            "total_tokens": 273,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 536,
          "total_completion_tokens": 316,
          "total_tokens": 852
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 179,
            "completion_tokens": 120,
            "total_tokens": 299,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 179,
            "completion_tokens": 101,
            "total_tokens": 280,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 178,
            "completion_tokens": 95,
            "total_tokens": 273,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bq1b1r/ppp3pp/4k3/3np3/2Bn4/5Q2/PPP2PPP/RNB1K2R w KQ - 2 9"
    },
    {
      "turn": 19,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Kd2",
          "aggressive_uci": "e1d2",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxa1 11. Qe4+ Kf7 12. Qf4+ Kg6 13. Qg4+ Kf7 14. Qf4+ Kg6 15. Qg4+ Kf7 16. Qf4+ Kg6 17. Qg4+ Kf7 18. Qf4+ Kg6 19. Qg4+ Kf7 20. Qf4+ Kg6 21. Qg4+ Kf7 22. Qf4+ Kg6 23. Qg4+ Kf7 24. Qf4+ Kg6 25. Qg4+ Kf7 26. Qf4+ Kg6 27. Qg4+ Kf7 28. Qf4+ Kg6 29. Qg4+ Kf7 30. Qf4+ Kg6 31. Qg4+ Kf7 32. Qf4+ Kg6 33. Qg4+ Kf7 34. Qf4+ Kg6 35. Qg4+ Kf7 36. Qf4+ Kg6 37. Qg4+ Kf7 38. Qf4+ Kg6 39. Qg4+ Kf7 40. Qf4+ Kg6 41. Qg4+ Kf7 42. Qf4+ Kg6 43. Qg4+ Kf7 44. Qf4+ Kg6 45. Qg4+ Kf7 46. Qf4+ Kg6 47. Qg4+ Kf7 48. Qf4+ Kg6 49. Qg4+ Kf7 50. Qf4+ Kg6 51. Qg4+ Kf7 52. Qf4+ Kg6 53. Qg4+ Kf7 54. Qf4+ Kg6 55. Qg4+ Kf7 56. Qf4+ Kg6 57. Qg4+ Kf7 58. Qf4+ Kg6 59. Qg4+ Kf7 60. Qf4+ Kg6 61. Q",
          "aggressive_tokens": {
            "prompt_tokens": 189,
            "completion_tokens": 640,
            "total_tokens": 829,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nxa1",
            "Qe4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Kd1",
          "positional_uci": "e1d1",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd1 Nd4 11. Qe2+ Kf7 12. Qe5+ Kg6 13. Qf5+ Kh6 14. Qf4+ Kg6 15. Qf5+ Kh6 16. Qf4+ Kg6\n\nQf5+ Kg6 Qf4+ Kg6 Qf5+",
          "positional_tokens": {
            "prompt_tokens": 189,
            "completion_tokens": 167,
            "total_tokens": 356,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nd4",
            "Qe2+",
            "Kf7",
            "Qe5+",
            "Kg6",
            "Qf5+",
            "Kh6",
            "Qf4+",
            "Kg6",
            "Qf5+",
            "Kh6",
            "Qf4+",
            "Kg6",
            "Qf5+",
            "Kg6",
            "Qf4+",
            "Kg6",
            "Qf5+"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Kd2",
          "neutral_uci": "e1d2",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxa1 11. Qe4+ Kf7 12. Qf5+ Ke8 13. Qe6+ Kd8 14. Qd6+ Kc8 15. Qc6+ Kb8 16. Qb6+ Ka8 17. Qa6# 1-0\n\nQa6#",
          "neutral_tokens": {
            "prompt_tokens": 188,
            "completion_tokens": 166,
            "total_tokens": 354,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxa1",
            "Qe4+",
            "Kf7",
            "Qf5+",
            "Ke8",
            "Qe6+",
            "Kd8",
            "Qd6+",
            "Kc8",
            "Qc6+",
            "Kb8",
            "Qb6+",
            "Ka8",
            "Qa6#",
            "Qa6#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e1d2",
          "positional_uci": "e1d1",
          "neutral_uci": "e1d2",
          "consensus_move": "e1d2",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nxa1",
              "Qe4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nd4",
              "Qe2+",
              "Kf7",
              "Qe5+",
              "Kg6",
              "Qf5+",
              "Kh6",
              "Qf4+",
              "Kg6",
              "Qf5+",
              "Kh6",
              "Qf4+",
              "Kg6",
              "Qf5+",
              "Kg6",
              "Qf4+",
              "Kg6",
              "Qf5+"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nxa1",
              "Qe4+",
              "Kf7",
              "Qf5+",
              "Ke8",
              "Qe6+",
              "Kd8",
              "Qd6+",
              "Kc8",
              "Qc6+",
              "Kb8",
              "Qb6+",
              "Ka8",
              "Qa6#",
              "Qa6#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 189,
            "completion_tokens": 640,
            "total_tokens": 829,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 189,
            "completion_tokens": 167,
            "total_tokens": 356,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 188,
            "completion_tokens": 166,
            "total_tokens": 354,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 566,
          "total_completion_tokens": 973,
          "total_tokens": 1539
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 18,
            "turn_number": 10,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 189,
            "completion_tokens": 640,
            "total_tokens": 829,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 18,
            "turn_number": 10,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 189,
            "completion_tokens": 167,
            "total_tokens": 356,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 18,
            "turn_number": 10,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 188,
            "completion_tokens": 166,
            "total_tokens": 354,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bq1b1r/ppp3pp/4k3/3np3/2B5/4Q3/PPn2PPP/RNB1K2R w KQ - 0 10"
    },
    {
      "turn": 21,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bxc2+",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxe3\n\nBxc2+ Kxc2 Qd5+",
          "aggressive_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 97,
            "total_tokens": 296,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kxc2",
            "Qd5+"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Kxd1",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxe3\n\nKxd1 Qd3+",
          "positional_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 93,
            "total_tokens": 292,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qd3+"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Kxd1",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxe3\n\nKxd1 Qf7+",
          "neutral_tokens": {
            "prompt_tokens": 198,
            "completion_tokens": 93,
            "total_tokens": 291,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qf7+"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Kxc2",
              "Qd5+"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qd3+"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qf7+"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 199,
            "completion_tokens": 97,
            "total_tokens": 296,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 199,
            "completion_tokens": 93,
            "total_tokens": 292,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 198,
            "completion_tokens": 93,
            "total_tokens": 291,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 596,
          "total_completion_tokens": 283,
          "total_tokens": 879
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 20,
            "turn_number": 11,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 199,
            "completion_tokens": 97,
            "total_tokens": 296,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 20,
            "turn_number": 11,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 199,
            "completion_tokens": 93,
            "total_tokens": 292,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 20,
            "turn_number": 11,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 198,
            "completion_tokens": 93,
            "total_tokens": 291,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bq1b1r/ppp3pp/4k3/3np3/2B5/4n3/PP1K1PPP/RNB4R w - - 0 11"
    },
    {
      "turn": 21,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "Kd2xc3",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxe3\n\nKd2xc3",
          "aggressive_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 91,
            "total_tokens": 290,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Kxd1",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxe3\n\nKxd1 Qd3+",
          "positional_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 93,
            "total_tokens": 292,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qd3+"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Kxd1",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxe3\n\nKxd1 Qd8+",
          "neutral_tokens": {
            "prompt_tokens": 198,
            "completion_tokens": 93,
            "total_tokens": 291,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qd8+"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [
              "Qd3+"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qd8+"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 199,
            "completion_tokens": 91,
            "total_tokens": 290,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 199,
            "completion_tokens": 93,
            "total_tokens": 292,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 198,
            "completion_tokens": 93,
            "total_tokens": 291,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 596,
          "total_completion_tokens": 277,
          "total_tokens": 873
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 20,
            "turn_number": 11,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 199,
            "completion_tokens": 91,
            "total_tokens": 290,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 20,
            "turn_number": 11,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 199,
            "completion_tokens": 93,
            "total_tokens": 292,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 20,
            "turn_number": 11,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 198,
            "completion_tokens": 93,
            "total_tokens": 291,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bq1b1r/ppp3pp/4k3/3np3/2B5/4n3/PP1K1PPP/RNB4R w - - 0 11"
    },
    {
      "turn": 21,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "Kd2xc3",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxe3\n\nKd2xc3 Qd8-f6",
          "aggressive_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 96,
            "total_tokens": 295,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qd8",
            "f6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Kd2xc3",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxe3\n\nKd2xc3 Qd8-f6",
          "positional_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 96,
            "total_tokens": 295,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qd8",
            "f6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Ke6",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxe3\n\nKe6 Qxc2+ Kd6",
          "neutral_tokens": {
            "prompt_tokens": 198,
            "completion_tokens": 95,
            "total_tokens": 293,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qxc2+",
            "Kd6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qd8",
              "f6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qd8",
              "f6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qxc2+",
              "Kd6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 199,
            "completion_tokens": 96,
            "total_tokens": 295,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 199,
            "completion_tokens": 96,
            "total_tokens": 295,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 198,
            "completion_tokens": 95,
            "total_tokens": 293,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 596,
          "total_completion_tokens": 287,
          "total_tokens": 883
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 20,
            "turn_number": 11,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 199,
            "completion_tokens": 96,
            "total_tokens": 295,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 20,
            "turn_number": 11,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 199,
            "completion_tokens": 96,
            "total_tokens": 295,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 20,
            "turn_number": 11,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 198,
            "completion_tokens": 95,
            "total_tokens": 293,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bq1b1r/ppp3pp/4k3/3np3/2B5/4n3/PP1K1PPP/RNB4R w - - 0 11"
    }
  ],
  "black_responses": [
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": "d2d4",
          "aggressive_response": "d4 d5 2. Nf3 Nf6 3. Bg5 Bf5 4. e3 e6 5. Bd3 Bxd3 6. Qxd3 Be7 7. O-O O-O 8. Nbd2 Nbd7 9. c4 c6 10. Rac1 Rc8 11. cxd5 exd5 12. a3 Re8 13. b4 a6 14. Nb3 Ne4 15. Bxe7 Qxe7 16. Nc5 Ndxc5 17. bxc5 f5 18. Rb1 Rc7 19. Rb6 Rf8 20. Rfb1 Rf7 21. Ne5 Rf6 22. f3 Ng5 23. h4 Nf7 24. f4 Nxe5 25. fxe5 Rf7 26. g3 g5 27. hxg5 Qxg5 28. Kf2 Rg7 29. Rg1 h5 30. Rbb1 Rcf7 31. Qe2 Qg4 32. Qxg4 fxg4+ 33. Ke2 Rf3 34. Rbf1 Rgf7 35. Rxf3 Rxf3 36. Rg2 Kf7 37. Rf2 Rxf2+ 38. Kxf2 Ke6 39. Ke2 Kf5 40. Kd3 Ke6 41. e4 dxe4+ 42. Kxe4 a5 43. a4 Ke7 44. Kf5 Kf7 45. Kg5 Ke6 46. Kxh5 Kd5 47. Kxg4 Kxd4 48. e6 Kxc5 49. e7 Kb4 50. e8=Q Kxa4 51. Qe4+ Kb5 52. Kf3 a4 53. g4 a3 54. g5 a2 55. Qe5+ Ka4 56. g6 b5 57. g7 b4 58. g8=Q b3 59. Qa8+ Kb4 60. Qaa5+ Kc4 61. Qec3# 1-0 d4 d5 Nf3 Nf6 Bg5 Bf5 e3 e6 Bd3 Bxd3 Qxd3 Be7 O-O O-O Nbd2 c6 Rac1 Rc8 cxd5 exd5 a3 Re8 b4 a6 Nb3 Ne4 Bxe7 Qxe7 Nc5 Ndxc5 bxc5 f5 Rb1 Rc7 Rb6 Rf8 Ne5 Rf6 f3 Ng5 h4 Nf7 f4 Nxe5 fxe5 Rf7 g3 g5 hxg5 Qxg5 Kf2 Rg7 Rg",
          "aggressive_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Nf3",
            "Nf6",
            "Bg5",
            "Bf5",
            "e3",
            "e6",
            "Bd3",
            "Bxd3",
            "Qxd3",
            "Be7",
            "O-O",
            "O-O",
            "Nbd2",
            "c6",
            "Rac1",
            "Rc8",
            "cxd5",
            "exd5",
            "a3",
            "Re8",
            "b4",
            "a6",
            "Nb3",
            "Ne4",
            "Bxe7",
            "Qxe7",
            "Nc5",
            "Ndxc5",
            "bxc5",
            "f5",
            "Rb1",
            "Rc7",
            "Rb6",
            "Rf8",
            "Ne5",
            "Rf6",
            "f3",
            "Ng5",
            "h4",
            "Nf7",
            "f4",
            "Nxe5",
            "fxe5",
            "Rf7",
            "g3",
            "g5",
            "hxg5",
            "Qxg5",
            "Kf2",
            "Rg7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "Game 1\n1. e4 e5\n2. Nf3 Nc6\n3. Bb5 a6\n4. Bxc6 dxc6\n5. O-O Bg4\n6. h3 Bh5\n7. d3 Bc5\n8. Be3 Bxe3\n9. fxe3 Nf6\n10. Nbd2 O-O\n11. Qe1 Re8\n12. Qg3 Qd6\n13. Nc4 Qc5\n14. Nfxe5 b5\n15. d4 Qe7\n16. Nxc6 Qxe4\n17. Rxf6 bxc4\n18. Ne5 Qxc2\n19. Raf1 Re7\n20. R6f2 Qe4\n21. Rf4 Qd5\n22. Qg5 f6\n23. Qxh5 fxe5\n24. Rf5 g6\n25. Qg5 Rae8\n26. Rf6 Re6\n27. Rf7 exd4\n28. Qh6 Qxg2+\n29. Kxg2 Rxe3\n30. Qg7# e4 Nf6 Rxf6#\n\n*Game 1\n1. e4 e5\n2. Nf3 Nc6\n3. Bb5 a6\n4. Bxc6 dxc6\n5. O-O Bg4\n6. h3 Bh5\n7. d3 Bc5\n8. Be3 Bxe3\n9. fxe3 Nf6\n10. Nbd2 O-O\n11. Qe1 Re8\n12. Qg3 Qd6\n13. Nc4 Qc5\n14. Nfxe5 b5\n15. d4 Qe7\n16. Nxc6 Qxe4\n17. Rxf6 bxc4\n18. Ne5 Qxc2\n19. Raf1 Re7\n20. R6f2 Qe4\n21. Rf4 Qd5\n22. Qg5 f6\n23. Qxh5 fxe5\n24. Rf5 g6\n25. Qg5 Rae8\n26. Rf6 Re6\n27. Rf7 exd4\n28. Qh6 Qxg2+\n29. Kxg2 Rxe3\n30. Qg7# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 R",
          "positional_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": "d2d4",
          "neutral_response": "d4 d5\nd4 d5 Nf3 Nf6 Bf4 Bf5 e3 e6 c4 c6 Nc3 Nbd7 Be2 Be7 O-O O-O Rc1 Rc8 Qb3 Qb6 c5 Qxb3 axb3 a6 b4 Nh5 Bg3 Nxg3 hxg3 Bf6 Nd2 e5 dxe5 Nxe5 Nb3 Nd3 Bxd3 Bxd3 Rfd1 Rfe8 Nd4 Bg6 Na4 Re7 Nb6 Rce8 b3 h6 Kf1 Bh5 Rd2 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke",
          "neutral_tokens": {
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Nf3",
            "Nf6",
            "Bf4",
            "Bf5",
            "e3",
            "e6",
            "c4",
            "c6",
            "Nc3",
            "Nbd7",
            "Be2",
            "Be7",
            "O-O",
            "O-O",
            "Rc1",
            "Rc8",
            "Qb3",
            "Qb6",
            "c5",
            "Qxb3",
            "axb3",
            "a6",
            "b4",
            "Nh5",
            "Bg3",
            "Nxg3",
            "hxg3",
            "Bf6",
            "Nd2",
            "e5",
            "dxe5",
            "Nxe5",
            "Nb3",
            "Nd3",
            "Bxd3",
            "Bxd3",
            "Rfd1",
            "Rfe8",
            "Nd4",
            "Bg6",
            "Na4",
            "Re7",
            "Nb6",
            "Rce8",
            "b3",
            "h6",
            "Kf1",
            "Bh5",
            "Rd2",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d2d4",
          "positional_uci": "e2e4",
          "neutral_uci": "d2d4",
          "consensus_move": "d2d4",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d5",
              "Nf3",
              "Nf6",
              "Bg5",
              "Bf5",
              "e3",
              "e6",
              "Bd3",
              "Bxd3",
              "Qxd3",
              "Be7",
              "O-O",
              "O-O",
              "Nbd2",
              "c6",
              "Rac1",
              "Rc8",
              "cxd5",
              "exd5",
              "a3",
              "Re8",
              "b4",
              "a6",
              "Nb3",
              "Ne4",
              "Bxe7",
              "Qxe7",
              "Nc5",
              "Ndxc5",
              "bxc5",
              "f5",
              "Rb1",
              "Rc7",
              "Rb6",
              "Rf8",
              "Ne5",
              "Rf6",
              "f3",
              "Ng5",
              "h4",
              "Nf7",
              "f4",
              "Nxe5",
              "fxe5",
              "Rf7",
              "g3",
              "g5",
              "hxg5",
              "Qxg5",
              "Kf2",
              "Rg7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "d5",
              "Nf3",
              "Nf6",
              "Bf4",
              "Bf5",
              "e3",
              "e6",
              "c4",
              "c6",
              "Nc3",
              "Nbd7",
              "Be2",
              "Be7",
              "O-O",
              "O-O",
              "Rc1",
              "Rc8",
              "Qb3",
              "Qb6",
              "c5",
              "Qxb3",
              "axb3",
              "a6",
              "b4",
              "Nh5",
              "Bg3",
              "Nxg3",
              "hxg3",
              "Bf6",
              "Nd2",
              "e5",
              "dxe5",
              "Nxe5",
              "Nb3",
              "Nd3",
              "Bxd3",
              "Bxd3",
              "Rfd1",
              "Rfe8",
              "Nd4",
              "Bg6",
              "Na4",
              "Re7",
              "Nb6",
              "Rce8",
              "b3",
              "h6",
              "Kf1",
              "Bh5",
              "Rd2",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 329,
          "total_completion_tokens": 1920,
          "total_tokens": 2249
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e5",
          "aggressive_uci": "e7e5",
          "aggressive_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7\n6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5\n11. d4 Qc7 12. Nbd2 cxd4 13. cxd4 Nc6 14. Nb3 a5\n15. Be3 a4 16. Nbd2 Bd7 17. Rc1 Qb7 18. Bb1 Rfc8\n19. Nf1 Bd8 20. Ng3 Bb6 21. Qd2 Ba5 22. Qe2 Bxe1\n23. Rxe1 Re8 24. Qd2 Rad8 25. Bg5 Ne7 26. Bxf6 gxf6\n27. Qh6 Ng6 28. Nh5 Re6 29. Qg7# d5 e4 Nxe4\n30. Bxe4 dxe4 31. Rxe4 Qxe4 32. Ng5 Qe1+ 33. Kh2 fxg5\n34. Nf6+ Rxf6 35. Qxf6 Re8 36. Qxg5 Qxf2 37. h4 Qf4+\n38. Qxf4 Nxf4 39. Kg3 Nd3 40. Kf3 Nxb2 41. g4 Nc4\n42. Kf4 b4 43. g5 b3 44. axb3 axb3 45. h5 b2\n46. g6 b1=Q 47. gxh7+ Kxh7 48. h6 Qf5+ 49. Kg3 Rg8+\n50. Kh4 Qg4# e5 Qg5# e6 Qg6#",
          "aggressive_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 429,
            "total_tokens": 543,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qg5#",
            "e6",
            "Qg6#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e5",
          "positional_uci": "e7e5",
          "positional_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7\n6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5\n11. d4 Qc7 12. Nbd2 cxd4 13. cxd4 Nc6 14. Nb3 a5\n15. Be3 a4 16. Nbd2 Bd7 17. Rc1 Qb7 18. Bb1 Rfc8\n19. Nf1 Bd8 20. Ng3 Bb6 21. Qd2 Ba5 22. Qe2 Bxe1\n23. Rxe1 Re8 24. Qd2 Rac8 25. Bg5 Kh8 26. Bxf6 gxf6\n27. Qh6 Rg8 28. Qxf6+ Rg7 29. Nh5 Rg8 30. Ng5 Be8\n31. Nxg7 Rxg7 32. dxe5 dxe5 33. Rd1 Qe7 34. Qxe7 Nxe7\n35. Rd8 Rg8 36. Nf3 f6 37. Bd3 Kg7 38. Rb8 Bf7\n39. Rxb5 Bxa2 40. Ra5 Bb3 41. Nd2 Nc6 42. Ra6 Nb4\n43. Ra7+ Kg6 44. Bc4 Bxc4 45. Nxc4 Nd3 46. Rxa4 Rc8\n47. b3 Nc5 48. Rb4 Nxe4 49. Nxe5+ Kf5 50. Nc4 Rd8\n51. Ne3+ Ke5 52. Rb5+ Ke6 53. Rb6+ Ke5 54. Ng4+ Kf5\n55. f3 Rd1+ 56. Kh2 Nd6 57. Ne3+ Ke5 58. Nxd1 1-0\n58... Kd5 59. Rxd6+ Kxd6 60. Kg3 Kc5",
          "positional_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 498,
            "total_tokens": 612,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Qc7",
            "Nbd2",
            "cxd4",
            "cxd4",
            "Nc6",
            "Nb3",
            "a5",
            "Be3",
            "a4",
            "Nbd2",
            "Bd7",
            "Rc1",
            "Qb7",
            "Bb1",
            "Rfc8",
            "Nf1",
            "Bd8",
            "Ng3",
            "Bb6",
            "Qd2",
            "Ba5",
            "Qe2",
            "Bxe1",
            "Rxe1",
            "Re8",
            "Qd2",
            "Rac8",
            "Bg5",
            "Kh8",
            "Bxf6",
            "gxf6",
            "Qh6",
            "Rg8",
            "Qxf6+",
            "Rg7",
            "Nh5",
            "Rg8",
            "Ng5",
            "Be8",
            "Nxg7",
            "Rxg7",
            "dxe5",
            "dxe5",
            "Rd1",
            "Qe7",
            "Qxe7",
            "Nxe7",
            "Rd8",
            "Rg8",
            "Nf3",
            "f6",
            "Bd3",
            "Kg7",
            "Rb8",
            "Bf7",
            "Rxb5",
            "Bxa2",
            "Ra5",
            "Bb3",
            "Nd2",
            "Nc6",
            "Ra6",
            "Nb4",
            "Ra7+",
            "Kg6",
            "Bc4",
            "Bxc4",
            "Nxc4",
            "Nd3",
            "Rxa4",
            "Rc8",
            "b3",
            "Nc5",
            "Rb4",
            "Nxe4",
            "Nxe5+",
            "Kf5",
            "Nc4",
            "Rd8",
            "Ne3+",
            "Ke5",
            "Rb5+",
            "Ke6",
            "Rb6+",
            "Ke5",
            "Ng4+",
            "Kf5",
            "f3",
            "Rd1+",
            "Kh2",
            "Nd6",
            "Ne3+",
            "Ke5",
            "Nxd1",
            "Kd5",
            "Rxd6+",
            "Kxd6",
            "Kg3",
            "Kc5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e5",
          "neutral_uci": "e7e5",
          "neutral_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6\n4. Ba4 Nf6 5. O-O Be7 6. Re1 b5\n7. Bb3 d6 8. c3 O-O 9. h3 Bb7\n10. d4 Re8 11. Nbd2 Bf8 12. a4 h6\n13. Bc2 exd4 14. cxd4 Nb4 15. Bb1 c5\n16. d5 Nd7 17. Ra3 f5 18. Rae3 fxe4\n19. Nxe4 Nxd5 20. Ba2 c4 21. Nxd6 Rxe3\n22. Nxb7 Rxe1+ 23. Nxe1 Qe7 24. Qxd5+ Kh8\n25. Nf3 Nf6 26. Qc6 Ra7 27. Na5 Qd8\n28. Bd2 Rd7 29. Bc3 Rd1+ 30. Kh2 Bd6+\n31. g3 Bc7 32. Nb7 Qd3 33. Bxf6 Qf1\n34. Qe8+ Kh7 35. Qe4+ Kh8 36. Qe8+ Kh7\n37. Qe4+ Kh8 38. Qe8+ Kh7 39. Qe4+ Kh8 40. Qe8+ Kh7 41. Qe4+ Kh8 42. Qe8+ Kh7 43. Qe4+ Kh8 44. Qe8+ Kh7 45. Qe4+ Kh8 46. Qe8+ Kh7 47. Qe4+ Kh8 48. Qe8+ Kh7 49. Qe4+ Kh8 50. Qe8+ Kh7 51. Qe4+ Kh8 52. Qe8+ Kh7 53. Qe4+ Kh8 54. Qe8+ Kh7 55. Qe4+ Kh8 56. Qe8+ Kh7 57. Qe4+ Kh8 58. Qe8+ Kh7 59. Qe4+ Kh8 60. Qe8+ Kh7 61. Qe4+ Kh8 62. Qe8+ Kh7 63. Qe4+ Kh8 64. Qe8+ Kh7 65. Qe4+ Kh8 66. Qe8+ Kh7 67. Qe4+ Kh8 68. Qe8+ Kh7 69. Qe4+ Kh8 70. Qe8+ Kh7 71. Qe4+ Kh8 72. Qe8+ Kh7 73. Qe4+ Kh8 74. Qe8+ Kh7 75. Qe4+ Kh",
          "neutral_tokens": {
            "prompt_tokens": 113,
            "completion_tokens": 640,
            "total_tokens": 753,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Bb7",
            "d4",
            "Re8",
            "Nbd2",
            "Bf8",
            "a4",
            "h6",
            "Bc2",
            "exd4",
            "cxd4",
            "Nb4",
            "Bb1",
            "c5",
            "d5",
            "Nd7",
            "Ra3",
            "f5",
            "Rae3",
            "fxe4",
            "Nxe4",
            "Nxd5",
            "Ba2",
            "c4",
            "Nxd6",
            "Rxe3",
            "Nxb7",
            "Rxe1+",
            "Nxe1",
            "Qe7",
            "Qxd5+",
            "Kh8",
            "Nf3",
            "Nf6",
            "Qc6",
            "Ra7",
            "Na5",
            "Qd8",
            "Bd2",
            "Rd7",
            "Bc3",
            "Rd1+",
            "Kh2",
            "Bd6+",
            "g3",
            "Bc7",
            "Nb7",
            "Qd3",
            "Bxf6",
            "Qf1",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e7e5",
          "positional_uci": "e7e5",
          "neutral_uci": "e7e5",
          "consensus_move": "e7e5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qg5#",
              "e6",
              "Qg6#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Qc7",
              "Nbd2",
              "cxd4",
              "cxd4",
              "Nc6",
              "Nb3",
              "a5",
              "Be3",
              "a4",
              "Nbd2",
              "Bd7",
              "Rc1",
              "Qb7",
              "Bb1",
              "Rfc8",
              "Nf1",
              "Bd8",
              "Ng3",
              "Bb6",
              "Qd2",
              "Ba5",
              "Qe2",
              "Bxe1",
              "Rxe1",
              "Re8",
              "Qd2",
              "Rac8",
              "Bg5",
              "Kh8",
              "Bxf6",
              "gxf6",
              "Qh6",
              "Rg8",
              "Qxf6+",
              "Rg7",
              "Nh5",
              "Rg8",
              "Ng5",
              "Be8",
              "Nxg7",
              "Rxg7",
              "dxe5",
              "dxe5",
              "Rd1",
              "Qe7",
              "Qxe7",
              "Nxe7",
              "Rd8",
              "Rg8",
              "Nf3",
              "f6",
              "Bd3",
              "Kg7",
              "Rb8",
              "Bf7",
              "Rxb5",
              "Bxa2",
              "Ra5",
              "Bb3",
              "Nd2",
              "Nc6",
              "Ra6",
              "Nb4",
              "Ra7+",
              "Kg6",
              "Bc4",
              "Bxc4",
              "Nxc4",
              "Nd3",
              "Rxa4",
              "Rc8",
              "b3",
              "Nc5",
              "Rb4",
              "Nxe4",
              "Nxe5+",
              "Kf5",
              "Nc4",
              "Rd8",
              "Ne3+",
              "Ke5",
              "Rb5+",
              "Ke6",
              "Rb6+",
              "Ke5",
              "Ng4+",
              "Kf5",
              "f3",
              "Rd1+",
              "Kh2",
              "Nd6",
              "Ne3+",
              "Ke5",
              "Nxd1",
              "Kd5",
              "Rxd6+",
              "Kxd6",
              "Kg3",
              "Kc5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Bb7",
              "d4",
              "Re8",
              "Nbd2",
              "Bf8",
              "a4",
              "h6",
              "Bc2",
              "exd4",
              "cxd4",
              "Nb4",
              "Bb1",
              "c5",
              "d5",
              "Nd7",
              "Ra3",
              "f5",
              "Rae3",
              "fxe4",
              "Nxe4",
              "Nxd5",
              "Ba2",
              "c4",
              "Nxd6",
              "Rxe3",
              "Nxb7",
              "Rxe1+",
              "Nxe1",
              "Qe7",
              "Qxd5+",
              "Kh8",
              "Nf3",
              "Nf6",
              "Qc6",
              "Ra7",
              "Na5",
              "Qd8",
              "Bd2",
              "Rd7",
              "Bc3",
              "Rd1+",
              "Kh2",
              "Bd6+",
              "g3",
              "Bc7",
              "Nb7",
              "Qd3",
              "Bxf6",
              "Qf1",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 114,
            "completion_tokens": 429,
            "total_tokens": 543,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 114,
            "completion_tokens": 498,
            "total_tokens": 612,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 113,
            "completion_tokens": 640,
            "total_tokens": 753,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 341,
          "total_completion_tokens": 1567,
          "total_tokens": 1908
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 114,
            "completion_tokens": 429,
            "total_tokens": 543,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 114,
            "completion_tokens": 498,
            "total_tokens": 612,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 113,
            "completion_tokens": 640,
            "total_tokens": 753,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 4,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc6",
          "aggressive_uci": "b8c6",
          "aggressive_response": "2... Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 O-O 8. c3 d5\n9. exd5 Nxd5 10. Nxe5 Nxe5 11. Rxe5 c6 12. d4 Bd6 13. Re1 Qh4 14. g3 Qh3\n15. Be3 Bg4 16. Qd3 Rae8 17. Nd2 Re6 18. Qf1 Qh5 19. a4 Rfe8 20. axb5 axb5\n21. Bxd5 Qxd5 22. Qg2 Qh5 23. Qxc6 Bxg3 24. Qg2 Bf4 25. Ra8 Bb8 26. Kh1\n26... Rg6 27. Bf4 Rxe1+ 28. Nf1 Rxf1+ 29. Qxf1 Bf3+ 30. Qg2 Bxg2+ 31. Kg1\n31... Qd1#",
          "aggressive_tokens": {
            "prompt_tokens": 122,
            "completion_tokens": 258,
            "total_tokens": 380,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "O-O",
            "c3",
            "d5",
            "exd5",
            "Nxd5",
            "Nxe5",
            "Nxe5",
            "Rxe5",
            "c6",
            "d4",
            "Bd6",
            "Re1",
            "Qh4",
            "g3",
            "Qh3",
            "Be3",
            "Bg4",
            "Qd3",
            "Rae8",
            "Nd2",
            "Re6",
            "Qf1",
            "Qh5",
            "a4",
            "Rfe8",
            "axb5",
            "axb5",
            "Bxd5",
            "Qxd5",
            "Qg2",
            "Qh5",
            "Qxc6",
            "Bxg3",
            "Qg2",
            "Bf4",
            "Ra8",
            "Bb8",
            "Kh1",
            "Rg6",
            "Bf4",
            "Rxe1+",
            "Nf1",
            "Rxf1+",
            "Qxf1",
            "Bf3+",
            "Qg2",
            "Bxg2+",
            "Kg1",
            "Qd1#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nc6",
          "positional_uci": "b8c6",
          "positional_response": "2... Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 O-O 8. c3 d5\n9. exd5 Nxd5 10. Nxe5 Nxe5 11. Rxe5 c6 12. d4 Bd6 13. Re1 Qh4 14. g3 Qh3\n15. Be3 Bg4 16. Qd3 Rae8 17. Nd2 Re6 18. a4 Qh5 19. axb5 axb5 20. Ra6 Bb8\n21. Qf1 Rfe8 22. Bxd5 Qxd5 23. Qg2 Qh5 24. Rxc6 Bh3 25. Qf3 Bg4 26. Qg2 Bh3\n27. Qf3 Bg4 28. Qg2 Bh3 29. Qf3 Bg4 30. Qg2 Bh3 31. Qf3 Bg4 32. Qg2 Bh3\n33. Qf3 Bg4 34. Qg2 Bh3 35. Qf3 Bg4 36. Qg2 Bh3 37. Qf3 Bg4 38. Qg2 Bh3\n39. Qf3 Bg4 40. Qg2 Bh3 41. Qf3 Bg4 42. Qg2 Bh3 43. Qf3 Bg4 44. Qg2 Bh3\n45. Qf3 Bg4 46. Qg2 Bh3 47. Qf3 Bg4 48. Qg2 Bh3 49. Qf3 Bg4 50. Qg2 Bh3\n51. Qf3 Bg4 52. Qg2 Bh3 53. Qf3 Bg4 54. Qg2 Bh3 55. Qf3 Bg4 56. Qg2 Bh3\n57. Qf3 Bg4 58. Qg2 Bh3 59. Qf3 Bg4 60. Qg2 Bh3 61. Qf3 Bg4 62. Qg2 Bh3\n63. Qf3 Bg4 64. Qg2 Bh3 65. Qf3 Bg4 66. Qg2 Bh3 67. Qf3 Bg4 68. Qg2 Bh3\n69. Qf3 Bg4 70. Qg2 Bh3 71. Qf3 Bg4 72. Qg2 Bh3 73. Qf3 Bg4 74. Qg2 Bh3\n75. Qf3 Bg4 76. Qg2 Bh3 77. Qf3 Bg4 78. Qg2",
          "positional_tokens": {
            "prompt_tokens": 122,
            "completion_tokens": 640,
            "total_tokens": 762,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "O-O",
            "c3",
            "d5",
            "exd5",
            "Nxd5",
            "Nxe5",
            "Nxe5",
            "Rxe5",
            "c6",
            "d4",
            "Bd6",
            "Re1",
            "Qh4",
            "g3",
            "Qh3",
            "Be3",
            "Bg4",
            "Qd3",
            "Rae8",
            "Nd2",
            "Re6",
            "a4",
            "Qh5",
            "axb5",
            "axb5",
            "Ra6",
            "Bb8",
            "Qf1",
            "Rfe8",
            "Bxd5",
            "Qxd5",
            "Qg2",
            "Qh5",
            "Rxc6",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc6",
          "neutral_uci": "b8c6",
          "neutral_response": "2... Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 O-O 8. c3 d5\n9. exd5 Nxd5 10. Nxe5 Nxe5 11. Rxe5 c6 12. d4 Bd6 13. Re1 Qh4 14. g3 Qh3\n15. Be3 Bg4 16. Qd3 Rae8 17. Nd2 Re6 18. Qf1 Qh5 19. a4 Rfe8 20. axb5 axb5\n21. Bxd5 Qxd5 22. Qg2 Qh5 23. Qxc6 Bxg3 24. Qg2 Bf4 25. Ra8 Bb8 26. Kh1\n26... Bh3 27. Qf3 Bg4 28. Qg2 Bh3 29. Qf3 Bg4 30. Qg2 Bh3 31. Qf3 Bg4\n32. Qg2 Bh3 33. Qf3 Bg4 34. Qg2 Bh3 35. Qf3 Bg4 36. Qg2 Bh3 37. Qf3 Bg4\n38. Qg2 Bh3 39. Qf3 Bg4 40. Qg2 Bh3 41. Qf3 Bg4 42. Qg2 Bh3 43. Qf3 Bg4\n44. Qg2 Bh3 45. Qf3 Bg4 46. Qg2 Bh3 47. Qf3 Bg4 48. Qg2 Bh3 49. Qf3 Bg4\n50. Qg2 Bh3 51. Qf3 Bg4 52. Qg2 Bh3 53. Qf3 Bg4 54. Qg2 Bh3 55. Qf3 Bg4\n56. Qg2 Bh3 57. Qf3 Bg4 58. Qg2 Bh3 59. Qf3 Bg4 60. Qg2 Bh3 61. Qf3 Bg4\n62. Qg2 Bh3 63. Qf3 Bg4 64. Qg2 Bh3 65. Qf3 Bg4 66. Qg2 Bh3 67. Qf3 Bg4\n68. Qg2 Bh3 69. Qf3 Bg4 70. Qg2 Bh3 71. Qf3 Bg4 72. Qg2 Bh3 73. Qf3 Bg4\n74. Qg2 Bh3 75. Qf3 Bg4 76. Qg2 Bh3 77. Qf3 Bg4 78",
          "neutral_tokens": {
            "prompt_tokens": 121,
            "completion_tokens": 640,
            "total_tokens": 761,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "O-O",
            "c3",
            "d5",
            "exd5",
            "Nxd5",
            "Nxe5",
            "Nxe5",
            "Rxe5",
            "c6",
            "d4",
            "Bd6",
            "Re1",
            "Qh4",
            "g3",
            "Qh3",
            "Be3",
            "Bg4",
            "Qd3",
            "Rae8",
            "Nd2",
            "Re6",
            "Qf1",
            "Qh5",
            "a4",
            "Rfe8",
            "axb5",
            "axb5",
            "Bxd5",
            "Qxd5",
            "Qg2",
            "Qh5",
            "Qxc6",
            "Bxg3",
            "Qg2",
            "Bf4",
            "Ra8",
            "Bb8",
            "Kh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b8c6",
          "positional_uci": "b8c6",
          "neutral_uci": "b8c6",
          "consensus_move": "b8c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "O-O",
              "c3",
              "d5",
              "exd5",
              "Nxd5",
              "Nxe5",
              "Nxe5",
              "Rxe5",
              "c6",
              "d4",
              "Bd6",
              "Re1",
              "Qh4",
              "g3",
              "Qh3",
              "Be3",
              "Bg4",
              "Qd3",
              "Rae8",
              "Nd2",
              "Re6",
              "Qf1",
              "Qh5",
              "a4",
              "Rfe8",
              "axb5",
              "axb5",
              "Bxd5",
              "Qxd5",
              "Qg2",
              "Qh5",
              "Qxc6",
              "Bxg3",
              "Qg2",
              "Bf4",
              "Ra8",
              "Bb8",
              "Kh1",
              "Rg6",
              "Bf4",
              "Rxe1+",
              "Nf1",
              "Rxf1+",
              "Qxf1",
              "Bf3+",
              "Qg2",
              "Bxg2+",
              "Kg1",
              "Qd1#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "O-O",
              "c3",
              "d5",
              "exd5",
              "Nxd5",
              "Nxe5",
              "Nxe5",
              "Rxe5",
              "c6",
              "d4",
              "Bd6",
              "Re1",
              "Qh4",
              "g3",
              "Qh3",
              "Be3",
              "Bg4",
              "Qd3",
              "Rae8",
              "Nd2",
              "Re6",
              "a4",
              "Qh5",
              "axb5",
              "axb5",
              "Ra6",
              "Bb8",
              "Qf1",
              "Rfe8",
              "Bxd5",
              "Qxd5",
              "Qg2",
              "Qh5",
              "Rxc6",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "O-O",
              "c3",
              "d5",
              "exd5",
              "Nxd5",
              "Nxe5",
              "Nxe5",
              "Rxe5",
              "c6",
              "d4",
              "Bd6",
              "Re1",
              "Qh4",
              "g3",
              "Qh3",
              "Be3",
              "Bg4",
              "Qd3",
              "Rae8",
              "Nd2",
              "Re6",
              "Qf1",
              "Qh5",
              "a4",
              "Rfe8",
              "axb5",
              "axb5",
              "Bxd5",
              "Qxd5",
              "Qg2",
              "Qh5",
              "Qxc6",
              "Bxg3",
              "Qg2",
              "Bf4",
              "Ra8",
              "Bb8",
              "Kh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 122,
            "completion_tokens": 258,
            "total_tokens": 380,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 122,
            "completion_tokens": 640,
            "total_tokens": 762,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 121,
            "completion_tokens": 640,
            "total_tokens": 761,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 365,
          "total_completion_tokens": 1538,
          "total_tokens": 1903
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 122,
            "completion_tokens": 258,
            "total_tokens": 380,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 122,
            "completion_tokens": 640,
            "total_tokens": 762,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 121,
            "completion_tokens": 640,
            "total_tokens": 761,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R b KQkq - 1 2"
    },
    {
      "turn": 6,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf6",
          "aggressive_uci": "g8f6",
          "aggressive_response": "3... Nf6 4. d3 Bc5 5. O-O O-O 6. Nc3 d6 7. Bg5 Be6 8. Nd5 Bxd5 9. Bxd5 h6 10. Bh4 g5 11. Bg3 Nxd5 12. exd5 Ne7 13. c4 f5 14. h3 f4 15. Bh2 Nf5 16. a3 a5 17. Qd2 Qf6 18. b4 axb4 19. axb4 Rxa1 20. Rxa1 e4 21. dxe4 Qxa1+ 22. Ne1 Ng3 23. bxc5 Nxe4 24. Qe2 Re8 25. cxd6 Nxd6 26. Qf1 Rxe1 27. Qxe1 Qxe1# 1-0\n3... Nf6 4. d3 Bc5 5. O-O O-O 6. Nc3 d6 7. Bg5 Be6 8. Nd5 Bxd5 9. Bxd5 h6 10. Bh4 g5 11. Bg3 Nxd5 12. exd5 Ne7 13. c4 f5 14. h3 f4 15. Bh2 Nf5 16. a3 a5 17. Qd2 Qf6 18. b4 axb4 19. axb4 Rxa1 20. Rxa1 e4 21. dxe4 Qxa1+ 22. Ne1 Ng3 23. bxc5 Nxe4 24. Qe2 Re8 25. cxd6 Nxd6 26. Qf1 Rxe1 27. Qxe1 Qxe1# 1-0 Nf6 d3 Bc5 O-O O-O Nc3 d6 Bg5 Be6 Nd5 Bxd5 Bh4 g5 Bg3 Nxd5 exd5 Ne7 c4 f5 h3 f4 Bh2 Nf5 a3 a5 Qd2 Qf6 b4 axb4 axb4 Rxa1 Rxa1 e4 dxe4 Qxe4 Re8 Qe2 c6 bxc5 dxc5 Qb2 Nd4 Nxd4 cxd4 Qxb7 Re7 Qc8+ Kg7 Qxc5 Qe5 Ra6 f3 g3 Qe1+ Kh2 Qxf2+ Kh1 Qg2# 1-0 Nf6 d3 Bc5 O-O O-O Nc3 d6 Bg5 Be6 Nd5 Bxd5 Bh4 g5 Bg3 Nxd5 exd5 Ne7 c4 f5 h3 f4 Bh2 Nf5 a3 a5 Qd2 Qf6 b4 axb4 axb4 R",
          "aggressive_tokens": {
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d3",
            "Bc5",
            "O-O",
            "O-O",
            "Nc3",
            "d6",
            "Bg5",
            "Be6",
            "Nd5",
            "Bxd5",
            "Bh4",
            "g5",
            "Bg3",
            "Nxd5",
            "exd5",
            "Ne7",
            "c4",
            "f5",
            "h3",
            "f4",
            "Bh2",
            "Nf5",
            "a3",
            "a5",
            "Qd2",
            "Qf6",
            "b4",
            "axb4",
            "axb4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf6",
          "positional_uci": "g8f6",
          "positional_response": "3... Nf6 4. d3 Bc5 5. O-O O-O 6. Nc3 d6 7. Bg5 Be6 8. Nd5 Bxd5 9. Bxd5 h6 10. Bh4 g5 11. Bg3 Nxd5 12. exd5 Ne7 13. c4 f5 14. h3 f4 15. Bh2 Nf5 16. Nd2 Qe8 17. Ne4 Bd4 18. Rb1 Qg6 19. Kh1 h5 20. g4 hxg4 21. Qxg4 Kf7 22. Nxg5+ Ke7 23. Ne6 Qxg4 24. hxg4 Rh8 25. gxf5 Rh3 26. Kg2 Rah8 27. Rh1 f3+ 28. Kg1 Rg8+ 29. Kf1 Rg2 30. Nxd4 exd4 31. Re1+ Kf7 32. Re4 Rhxh2 33. Rxh2 Rxh2 34. Kg1 Rg2+ 35. Kf1 Rh2 36. Kg1 Rg2+ 37. Kf1 Rh2 38. Kg1 Rg2+ 39. Kf1 Rh2 40. Kg1 Rg2+ 41. Kf1 Rh2 42. Kg1 Rg2+ 43. Kf1 Rh2 44. Kg1 Rg2+ 45. Kf1 Rh2 46. Kg1 Rg2+ 47. Kf1 Rh2 48. Kg1 Rg2+ 49. Kf1 Rh2 50. Kg1 Rg2+ 51. Kf1 Rh2 52. Kg1 Rg2+ 53. Kf1 Rh2 54. Kg1 Rg2+ 55. Kf1 Rh2 56. Kg1 Rg2+ 57. Kf1 Rh2 58. Kg1 Rg2+ 59. Kf1 Rh2 60. Kg1 Rg2+ 61. Kf1 Rh2 62. Kg1 Rg2+ 63. Kf1 Rh2 64. Kg1 Rg2+ 65. Kf1 Rh2 66. Kg1 Rg2+ 67. Kf1 Rh2 68. Kg1 Rg2+ 69. Kf1 Rh2 70. Kg1 Rg2+ 71. Kf1 Rh2 72. Kg1 Rg2+ 73. Kf1 Rh2 74. Kg1 Rg2+ 75. Kf1 Rh2 76. Kg1 Rg2+ 77. Kf1 Rh2 78. Kg1 Rg2+ 79. Kf1",
          "positional_tokens": {
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d3",
            "Bc5",
            "O-O",
            "O-O",
            "Nc3",
            "d6",
            "Bg5",
            "Be6",
            "Nd5",
            "Bxd5",
            "Bxd5",
            "h6",
            "Bh4",
            "g5",
            "Bg3",
            "Nxd5",
            "exd5",
            "Ne7",
            "c4",
            "f5",
            "h3",
            "f4",
            "Bh2",
            "Nf5",
            "Nd2",
            "Qe8",
            "Ne4",
            "Bd4",
            "Rb1",
            "Qg6",
            "Kh1",
            "h5",
            "g4",
            "hxg4",
            "Qxg4",
            "Kf7",
            "Nxg5+",
            "Ke7",
            "Ne6",
            "Qxg4",
            "hxg4",
            "Rh8",
            "gxf5",
            "Rh3",
            "Kg2",
            "Rah8",
            "Rh1",
            "f3+",
            "Kg1",
            "Rg8+",
            "Kf1",
            "Rg2",
            "Nxd4",
            "exd4",
            "Re1+",
            "Kf7",
            "Re4",
            "Rhxh2",
            "Rxh2",
            "Rxh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf6",
          "neutral_uci": "g8f6",
          "neutral_response": "3... Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. Qf3+ Ke6 8. Nc3 Ne7 9. d4 c6 10. O-O b5 11. Bb3 a5 12. Re1 a4 13. Rxe5+ Kd6 14. Bxd5 Nxd5 15. Nxd5 cxd5 16. Qxd5+ Kc7 17. Qxa8 Qxd4 18. Qa5+ Kd6 19. Re1 Bd7 20. Be3 Qxb2 21. Rad1+ Kc6 22. Qb6# e4 Nf6 Qc3#",
          "neutral_tokens": {
            "prompt_tokens": 130,
            "completion_tokens": 174,
            "total_tokens": 304,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qc3#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g8f6",
          "positional_uci": "g8f6",
          "neutral_uci": "g8f6",
          "consensus_move": "g8f6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d3",
              "Bc5",
              "O-O",
              "O-O",
              "Nc3",
              "d6",
              "Bg5",
              "Be6",
              "Nd5",
              "Bxd5",
              "Bh4",
              "g5",
              "Bg3",
              "Nxd5",
              "exd5",
              "Ne7",
              "c4",
              "f5",
              "h3",
              "f4",
              "Bh2",
              "Nf5",
              "a3",
              "a5",
              "Qd2",
              "Qf6",
              "b4",
              "axb4",
              "axb4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "d3",
              "Bc5",
              "O-O",
              "O-O",
              "Nc3",
              "d6",
              "Bg5",
              "Be6",
              "Nd5",
              "Bxd5",
              "Bxd5",
              "h6",
              "Bh4",
              "g5",
              "Bg3",
              "Nxd5",
              "exd5",
              "Ne7",
              "c4",
              "f5",
              "h3",
              "f4",
              "Bh2",
              "Nf5",
              "Nd2",
              "Qe8",
              "Ne4",
              "Bd4",
              "Rb1",
              "Qg6",
              "Kh1",
              "h5",
              "g4",
              "hxg4",
              "Qxg4",
              "Kf7",
              "Nxg5+",
              "Ke7",
              "Ne6",
              "Qxg4",
              "hxg4",
              "Rh8",
              "gxf5",
              "Rh3",
              "Kg2",
              "Rah8",
              "Rh1",
              "f3+",
              "Kg1",
              "Rg8+",
              "Kf1",
              "Rg2",
              "Nxd4",
              "exd4",
              "Re1+",
              "Kf7",
              "Re4",
              "Rhxh2",
              "Rxh2",
              "Rxh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qc3#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 130,
            "completion_tokens": 174,
            "total_tokens": 304,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 392,
          "total_completion_tokens": 1454,
          "total_tokens": 1846
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 130,
            "completion_tokens": 174,
            "total_tokens": 304,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkbnr/pppp1ppp/2n5/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R b KQkq - 3 3"
    },
    {
      "turn": 8,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nxe4",
          "aggressive_uci": "f6e4",
          "aggressive_response": "4... Nxe4 5. Nxe4 d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. Re1 Bd6 11. d4 O-O 12. dxe5 Nxe5 13. g4 Nxf3+ 14. Qxf3 Bg6 15. Qxb7 Qh4 16. Qg2 Bxc2 17. Be3 Rab8 18. b3 Be5 19. Rac1 Bxc3 20. Rxc2 Bxe1 21. Rxc7 Rfd8 22. Qf3 Qf6 23. Qe2 Ba5 24. Rxa7 Bb6 25. Bxb6 Rxb6 26. Re7 Re6 27. Rxe6 Qxe6 28. Qc2 Qe1+ 29. Kg2 Qd2 30. Qb1 Qd5+ 31. Kg1 Qd1+ 32. Qxd1 Rxd1+ 33. Kg2 Ra1 34. a4 Rb1 35. a5 Rxb3 36. a6 Ra3 37. a7 Rxa7 38. f4 Ra3 39. h4 Kf8 40. g5 Ke7 41. f5 Kd6 42. h5 Ke5 43. g6 fxg6 44. fxg6 hxg6 45. hxg6 Kf4 46. Kf2 Ra2+ 47. Ke1 Ke3 48. Kd1 Kd3 49. Kc1 Kc3 50. Kb1 Rh2 51. Ka1 Kb3 52. Kb1 Rh1# e4 d5 Nf3 Nc6 Bc4 Nf6 Nc3 Nxe4 Nxe4 d5 Bxd5 Qxd5 Nc6 Qd8 O-O Bg4 h3 Bh5 Re1 Bd6 d4 O-O dxe5 Nxe5 g4 Nxf3 Qxf3 Bg6 Qxb7 Qh4 Qg2 Bxc2 Be3 Rab8 b3 Be5 Rac1 Bxc3 Rxc2 Bxe1 Rxc7 Rfd8 Qf3 Qf6 Qe2 Ba5 Rxa7 Bb6 Bxb6 Rxb6 Re7 Re6 Rxe6 Qxe6 Qc2 Qe1+ Kg2 Qd2 Qb1 Qd5+ Kg1 Qd1+ Qxd1 Rxd1+ Kg2 Rd2 a4 Rb2 a5 Rxb3 a6 Ra3 a7 Rxa7 f4 Ra3 h4 Kf8 g5 Ke7 f5 Kd6 h5 Ke5 g6 fxg6 fxg6 hxg6 Kg2 Kf4 Kf2",
          "aggressive_tokens": {
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Bxd5",
            "Qxd5",
            "Nc6",
            "Qd8",
            "O-O",
            "Bg4",
            "h3",
            "Bh5",
            "Re1",
            "Bd6",
            "d4",
            "O-O",
            "dxe5",
            "Nxe5",
            "g4",
            "Nxf3",
            "Qxf3",
            "Bg6",
            "Qxb7",
            "Qh4",
            "Qg2",
            "Bxc2",
            "Be3",
            "Rab8",
            "b3",
            "Be5",
            "Rac1",
            "Bxc3",
            "Rxc2",
            "Bxe1",
            "Rxc7",
            "Rfd8",
            "Qf3",
            "Qf6",
            "Qe2",
            "Ba5",
            "Rxa7",
            "Bb6",
            "Bxb6",
            "Rxb6",
            "Re7",
            "Re6",
            "Rxe6",
            "Qxe6",
            "Qc2",
            "Qe1+",
            "Kg2",
            "Qd2",
            "Qb1",
            "Qd5+",
            "Kg1",
            "Qd1+",
            "Qxd1",
            "Rxd1+",
            "Kg2",
            "Rd2",
            "a4",
            "Rb2",
            "a5",
            "Rxb3",
            "a6",
            "Ra3",
            "a7",
            "Rxa7",
            "f4",
            "Ra3",
            "h4",
            "Kf8",
            "g5",
            "Ke7",
            "f5",
            "Kd6",
            "h5",
            "Ke5",
            "g6",
            "fxg6",
            "fxg6",
            "hxg6",
            "Kg2",
            "Kf4",
            "Kf2"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nxe4",
          "positional_uci": "f6e4",
          "positional_response": "4... Nxe4 5. Nxe4 d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. Re1 Bd6 11. d4 O-O 12. dxe5 Nxe5 13. g4 Nxf3+ 14. Qxf3 Bg6 15. Qxb7 Qh4 16. Qg2 Bxc2 17. Be3 Rab8 18. b3 Be5 19. Rac1 Bxc3 20. Rxc2 Bxe1 21. Rxc7 Rbc8 22. Rxa7 Rc2 23. a4 Re8 24. Ra8 Rcc8 25. Rxc8 Rxc8 26. Qb7 Qd8 27. b4 Rb8 28. Qe4 Bxb4 29. Bf4 Bd6 30. Be3 Qe7 31. Qc6 Qe5 32. Kf1 Rb1+ 33. Ke2 Rb2+ 34. Kf1 Qh2 35. Qe8+ Bf8 36. Qe4 Qxh3+ 37. Ke1 Bb4+ 38. Kd1 Qf1# 39. Qe1# Bxe1 40. Kxe1 Qe2# 41. Bxe2 Rxe2+ 42. Kf1 Rxe3 43. fxe3 Kf8 44. a5 Ke8 45. a6 Kd8 46. a7 Kc8 47. a8=Q+ Kc7 48. Qa7+ Kd6 49. Qxf7 Ke5 50. Qxg7+ Ke4 51. Qxh7+ Kxe3 52. g5 Kf4 53. g6 Kg5 54. g7 Kf6 55. g8=Q Ke5 56. Qg5+ Kd4 57. Qhh4+ Kd3 58. Qgg3+ Kd2 59. Qhh2+ Kd1 60. Qd3+ Kc1 61. Qhc2# e5 d5 e6 dxe6 fxe6 Qxe6 Qd8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# B",
          "positional_tokens": {
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Bxd5",
            "Qxd5",
            "Nc3",
            "Qd8",
            "O-O",
            "Bg4",
            "h3",
            "Bh5",
            "Re1",
            "Bd6",
            "d4",
            "O-O",
            "dxe5",
            "Nxe5",
            "g4",
            "Nxf3+",
            "Qxf3",
            "Bg6",
            "Qxb7",
            "Qh4",
            "Qg2",
            "Bxc2",
            "Be3",
            "Rab8",
            "b3",
            "Be5",
            "Rac1",
            "Bxc3",
            "Rxc2",
            "Bxe1",
            "Rxc7",
            "Rbc8",
            "Rxa7",
            "Rc2",
            "a4",
            "Re8",
            "Ra8",
            "Rcc8",
            "Rxc8",
            "Rxc8",
            "Qb7",
            "Qd8",
            "b4",
            "Rb8",
            "Qe4",
            "Bxb4",
            "Bf4",
            "Bd6",
            "Be3",
            "Qe7",
            "Qc6",
            "Qe5",
            "Kf1",
            "Rb1+",
            "Ke2",
            "Rb2+",
            "Kf1",
            "Qh2",
            "Qe8+",
            "Bf8",
            "Qe4",
            "Qxh3+",
            "Ke1",
            "Bb4+",
            "Kd1",
            "Qf1#",
            "Qe1#",
            "Bxe1",
            "Kxe1",
            "Qe2#",
            "Bxe2",
            "Rxe2+",
            "Kf1",
            "Rxe3",
            "fxe3",
            "Kf8",
            "a5",
            "Ke8",
            "a6",
            "Kd8",
            "a7",
            "Kc8",
            "a8=Q+",
            "Kc7",
            "Qa7+",
            "Kd6",
            "Qxf7",
            "Ke5",
            "Qxg7+",
            "Ke4",
            "Qxh7+",
            "Kxe3",
            "g5",
            "Kf4",
            "g6",
            "Kg5",
            "g7",
            "Kf6",
            "g8=Q",
            "Ke5",
            "Qg5+",
            "Kd4",
            "Qhh4+",
            "Kd3",
            "Qgg3+",
            "Kd2",
            "Qhh2+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qhc2#",
            "e5",
            "d5",
            "e6",
            "dxe6",
            "fxe6",
            "Qxe6",
            "Qd8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nxe4",
          "neutral_uci": "f6e4",
          "neutral_response": "4... Nxe4 5. Nxe4 d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. Re1 Bd6 11. d4 O-O 12. dxe5 Nxe5 13. g4 Nxf3+ 14. Qxf3 Bg6 15. Qxb7 Qh4 16. Qg2 Bxc2 17. Be3 Rab8 18. b3 Be5 19. Rac1 Bxc3 20. Rxc2 Bxe1 21. Rxc7 Rbc8 22. Rxa7 Rc2 23. a4 Rfc8 24. a5 Bxf2+ 25. Bxf2 Rxf2 26. Qxf2 Rc1+ 27. Kg2 Qxf2+ 28. Kxf2 Rc2+ 29. Kg3 Rc3+ 30. Kh4 Rxb3 31. Ra8+ Rb8 32. Rxb8# Rxb8 33. a6 Ra8 34. a7 Rxa7 35. Kg3 Ra3+ 36. Kh4 f6 37. Kh5 Rxh3# Rxe1 g6+ Kh7 g5#",
          "neutral_tokens": {
            "prompt_tokens": 139,
            "completion_tokens": 292,
            "total_tokens": 431,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d5",
            "Bxd5",
            "Qxd5",
            "Nc3",
            "Qd8",
            "O-O",
            "Bg4",
            "h3",
            "Bh5",
            "Re1",
            "Bd6",
            "d4",
            "O-O",
            "dxe5",
            "Nxe5",
            "g4",
            "Nxf3+",
            "Qxf3",
            "Bg6",
            "Qxb7",
            "Qh4",
            "Qg2",
            "Bxc2",
            "Be3",
            "Rab8",
            "b3",
            "Be5",
            "Rac1",
            "Bxc3",
            "Rxc2",
            "Bxe1",
            "Rxc7",
            "Rbc8",
            "Rxa7",
            "Rc2",
            "a4",
            "Rfc8",
            "a5",
            "Bxf2+",
            "Bxf2",
            "Rxf2",
            "Qxf2",
            "Rc1+",
            "Kg2",
            "Qxf2+",
            "Kxf2",
            "Rc2+",
            "Kg3",
            "Rc3+",
            "Kh4",
            "Rxb3",
            "Ra8+",
            "Rb8",
            "Rxb8#",
            "Rxb8",
            "a6",
            "Ra8",
            "a7",
            "Rxa7",
            "Kg3",
            "Ra3+",
            "Kh4",
            "f6",
            "Kh5",
            "Rxh3#",
            "Rxe1",
            "g6+",
            "Kh7",
            "g5#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f6e4",
          "positional_uci": "f6e4",
          "neutral_uci": "f6e4",
          "consensus_move": "f6e4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d5",
              "Bxd5",
              "Qxd5",
              "Nc6",
              "Qd8",
              "O-O",
              "Bg4",
              "h3",
              "Bh5",
              "Re1",
              "Bd6",
              "d4",
              "O-O",
              "dxe5",
              "Nxe5",
              "g4",
              "Nxf3",
              "Qxf3",
              "Bg6",
              "Qxb7",
              "Qh4",
              "Qg2",
              "Bxc2",
              "Be3",
              "Rab8",
              "b3",
              "Be5",
              "Rac1",
              "Bxc3",
              "Rxc2",
              "Bxe1",
              "Rxc7",
              "Rfd8",
              "Qf3",
              "Qf6",
              "Qe2",
              "Ba5",
              "Rxa7",
              "Bb6",
              "Bxb6",
              "Rxb6",
              "Re7",
              "Re6",
              "Rxe6",
              "Qxe6",
              "Qc2",
              "Qe1+",
              "Kg2",
              "Qd2",
              "Qb1",
              "Qd5+",
              "Kg1",
              "Qd1+",
              "Qxd1",
              "Rxd1+",
              "Kg2",
              "Rd2",
              "a4",
              "Rb2",
              "a5",
              "Rxb3",
              "a6",
              "Ra3",
              "a7",
              "Rxa7",
              "f4",
              "Ra3",
              "h4",
              "Kf8",
              "g5",
              "Ke7",
              "f5",
              "Kd6",
              "h5",
              "Ke5",
              "g6",
              "fxg6",
              "fxg6",
              "hxg6",
              "Kg2",
              "Kf4",
              "Kf2"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "d5",
              "Bxd5",
              "Qxd5",
              "Nc3",
              "Qd8",
              "O-O",
              "Bg4",
              "h3",
              "Bh5",
              "Re1",
              "Bd6",
              "d4",
              "O-O",
              "dxe5",
              "Nxe5",
              "g4",
              "Nxf3+",
              "Qxf3",
              "Bg6",
              "Qxb7",
              "Qh4",
              "Qg2",
              "Bxc2",
              "Be3",
              "Rab8",
              "b3",
              "Be5",
              "Rac1",
              "Bxc3",
              "Rxc2",
              "Bxe1",
              "Rxc7",
              "Rbc8",
              "Rxa7",
              "Rc2",
              "a4",
              "Re8",
              "Ra8",
              "Rcc8",
              "Rxc8",
              "Rxc8",
              "Qb7",
              "Qd8",
              "b4",
              "Rb8",
              "Qe4",
              "Bxb4",
              "Bf4",
              "Bd6",
              "Be3",
              "Qe7",
              "Qc6",
              "Qe5",
              "Kf1",
              "Rb1+",
              "Ke2",
              "Rb2+",
              "Kf1",
              "Qh2",
              "Qe8+",
              "Bf8",
              "Qe4",
              "Qxh3+",
              "Ke1",
              "Bb4+",
              "Kd1",
              "Qf1#",
              "Qe1#",
              "Bxe1",
              "Kxe1",
              "Qe2#",
              "Bxe2",
              "Rxe2+",
              "Kf1",
              "Rxe3",
              "fxe3",
              "Kf8",
              "a5",
              "Ke8",
              "a6",
              "Kd8",
              "a7",
              "Kc8",
              "a8=Q+",
              "Kc7",
              "Qa7+",
              "Kd6",
              "Qxf7",
              "Ke5",
              "Qxg7+",
              "Ke4",
              "Qxh7+",
              "Kxe3",
              "g5",
              "Kf4",
              "g6",
              "Kg5",
              "g7",
              "Kf6",
              "g8=Q",
              "Ke5",
              "Qg5+",
              "Kd4",
              "Qhh4+",
              "Kd3",
              "Qgg3+",
              "Kd2",
              "Qhh2+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qhc2#",
              "e5",
              "d5",
              "e6",
              "dxe6",
              "fxe6",
              "Qxe6",
              "Qd8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "d5",
              "Bxd5",
              "Qxd5",
              "Nc3",
              "Qd8",
              "O-O",
              "Bg4",
              "h3",
              "Bh5",
              "Re1",
              "Bd6",
              "d4",
              "O-O",
              "dxe5",
              "Nxe5",
              "g4",
              "Nxf3+",
              "Qxf3",
              "Bg6",
              "Qxb7",
              "Qh4",
              "Qg2",
              "Bxc2",
              "Be3",
              "Rab8",
              "b3",
              "Be5",
              "Rac1",
              "Bxc3",
              "Rxc2",
              "Bxe1",
              "Rxc7",
              "Rbc8",
              "Rxa7",
              "Rc2",
              "a4",
              "Rfc8",
              "a5",
              "Bxf2+",
              "Bxf2",
              "Rxf2",
              "Qxf2",
              "Rc1+",
              "Kg2",
              "Qxf2+",
              "Kxf2",
              "Rc2+",
              "Kg3",
              "Rc3+",
              "Kh4",
              "Rxb3",
              "Ra8+",
              "Rb8",
              "Rxb8#",
              "Rxb8",
              "a6",
              "Ra8",
              "a7",
              "Rxa7",
              "Kg3",
              "Ra3+",
              "Kh4",
              "f6",
              "Kh5",
              "Rxh3#",
              "Rxe1",
              "g6+",
              "Kh7",
              "g5#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 139,
            "completion_tokens": 292,
            "total_tokens": 431,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 419,
          "total_completion_tokens": 1572,
          "total_tokens": 1991
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 139,
            "completion_tokens": 292,
            "total_tokens": 431,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/pppp1ppp/2n2n2/4p3/2B1P3/2N2N2/PPPP1PPP/R1BQK2R b KQkq - 5 4"
    },
    {
      "turn": 10,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d5",
          "aggressive_uci": "d7d5",
          "aggressive_response": "5... d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. d3 Nd4 11. g4 Nxf3+ 12. Qxf3 Bg6 13. Qxb7 Bd6 14. Qc6+ Kf8 15. Be3 h5 16. g5 Rb8 17. b3 Qc8 18. Kg2 Bf5 19. Rh1 Rb4 20. Bc5 Bxc5 21. Qxc5+ Kg8 22. Qxb4 Qa8+ 23. f3 Kh7 24. Ne4 Rb8 25. Qe7 Qc6 26. Rac1 Re8 27. Qxf7 Bg6 28. Qc4 Qd7 29. Rhf1 Rf8 30. Rce1 Rf4 31. Qc5 Rh4 32. Rh1 Qf5 33. Qxc7 Rf4 34. Ref1 h4 35. Qxa7 Bh5 36. Qe3 Qg6 37. Rhg1 Qf5 38. Kh2 Bxf3 39. Rxf3 Rxf3 40. g6+ Kg8 41. Qg5 Qxh3# 0-1 1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. d3 Nd4 11. g4 Nxf3+ 12. Qxf3 Bg6 13. Qxb7 Bd6 14. Qc6+ Kf8 15. Be3 h5 16. g5 Rb8 17. b3 Qc8 18. Kg2 Bf5 19. Rh1 Rb4 20. Bc5 Bxc5 21. Qxc5+ Kg8 22. Qxb4 Qa8+ 23. f3 Kh7 24. Ne4 Rb8 25. Qe7 Qc6 26. Rac1 Re8 27. Qxf7 Bg6 28. Qc4 Qd7 29. Rhf1 Rf8 30. Rce1 Rf4 31. Qc5 Rh4 32. Rh1 Qf5 33. Qxc7 Rf4 34. Ref1 h4 35. Qxa7 Bh5 36. Qe3 Qg6 37. Rhg1 Qf5 38. Kh2 Bxf3 39. Rxf3 Rxf3",
          "aggressive_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bxd5",
            "Qxd5",
            "Nc3",
            "Qd8",
            "O-O",
            "Bg4",
            "h3",
            "Bh5",
            "d3",
            "Nd4",
            "g4",
            "Nxf3+",
            "Qxf3",
            "Bg6",
            "Qxb7",
            "Bd6",
            "Qc6+",
            "Kf8",
            "Be3",
            "h5",
            "g5",
            "Rb8",
            "b3",
            "Qc8",
            "Kg2",
            "Bf5",
            "Rh1",
            "Rb4",
            "Bc5",
            "Bxc5",
            "Qxc5+",
            "Kg8",
            "Qxb4",
            "Qa8+",
            "f3",
            "Kh7",
            "Ne4",
            "Rb8",
            "Qe7",
            "Qc6",
            "Rac1",
            "Re8",
            "Qxf7",
            "Bg6",
            "Qc4",
            "Qd7",
            "Rhf1",
            "Rf8",
            "Rce1",
            "Rf4",
            "Qc5",
            "Rh4",
            "Rh1",
            "Qf5",
            "Qxc7",
            "Rf4",
            "Ref1",
            "h4",
            "Qxa7",
            "Bh5",
            "Qe3",
            "Qg6",
            "Rhg1",
            "Qf5",
            "Kh2",
            "Bxf3",
            "Rxf3",
            "Rxf3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d5",
          "positional_uci": "d7d5",
          "positional_response": "5... d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. g4 Bg6 11. d3 Bc5 12. Be3 Bxe3 13. fxe3 O-O 14. e4 Nd4 15. Nxe5 Qh4 16. Kg2 Rae8 17. Nxg6 fxg6 18. Rxf8+ Rxf8 19. Qd2 Rf3 20. Rh1 Qg3# 0-1\n1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. g4 Bg6 11. d3 Bc5 12. Be3 Bxe3 13. fxe3 O-O 14. e4 Nd4 15. Nxe5 Qh4 16. Kg2 Rae8 17. Nxg6 fxg6 18. Rxf8+ Rxf8 19. Qd2 Rf3 20. Rh1 Qg3# 0-1 d5 Qxd5 Nc3 Qd8 O-O Bg4 h3 Bh5 g4 Bg6 d3 Bc5 Be3 Bxe3 fxe3 O-O e4 Nd4 Nxe5 Qh4 Kg2 Rae8 Nxg6 fxg6 Rxf8+ Rxf8 Qd2 Rf3 Rh1 Qg3# 0-1",
          "positional_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 388,
            "total_tokens": 537,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qxd5",
            "Nc3",
            "Qd8",
            "O-O",
            "Bg4",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "d3",
            "Bc5",
            "Be3",
            "Bxe3",
            "fxe3",
            "O-O",
            "e4",
            "Nd4",
            "Nxe5",
            "Qh4",
            "Kg2",
            "Rae8",
            "Nxg6",
            "fxg6",
            "Rxf8+",
            "Rxf8",
            "Qd2",
            "Rf3",
            "Rh1",
            "Qg3#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d5",
          "neutral_uci": "d7d5",
          "neutral_response": "5... d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. Re1 Bd6 11. d3 O-O 12. Ne4 Nd4 13. g4 Nxf3+ 14. Qxf3 Bg6 15. Nxd6 cxd6 16. Qxb7 Qh4 17. Qg2 f5 18. gxf5 Bxf5 19. Re3 Rac8 20. c3 Rf6 21. Rg3 Rg6 22. Rxg6 Bxg6 23. Qg3 Qh5 24. Be3 Bxd3 25. Bxa7 Be4 26. Re1 Ba8 27. Qg4 Qe8 28. Rd1 Rd8 29. Bb6 Rb8 30. Rxd6 Qb5 31. Qe6+ Kh8 32. Rd8+ Rxd8 33. Bxd8 Qb8 34. Qe8# d5 35. Qf8# d4 36. cxd4 exd4 37. Qe7 d3 38. Qf8# d2 39. Qe7 d1=Q+ 40. Kh2 Qh1+ 41. Kg3 Qbf3+ 42. Kh4 Qhxh3+ 43. Kg5 Qhg4# d5 44. Qf8# d4 45. Qe7 d3 46. Qf8# d2 47. Qe7 d1=Q 48. Qf8# d5 49. Qe7 d4 50. Qf8# d3 51. Qe7 d2 52. Qf8# d5 53. Qe7 d4 54. Qf8# d3 55. Qe7 d2 56. Qf8# d5 57. Qe7 d4 58. Qf8# d3 59. Qe7 d2 60. Qf8# d5 61. Qe7 d4 62. Qf8# d3 63. Qe7 d2 64. Qf8# d5 65. Qe7 d4 66. Qf8# d3 67. Qe7 d2 68. Qf8# d5 69. Qe7 d4 70. Qf8# d3 71. Qe7 d2 72. Qf8# d5 73. Qe7 d4 74. Qf8# d3 75. Qe7 d2 76. Qf8# d5 77. Qe7 d4 78. Qf8# d3 79. Qe7 d",
          "neutral_tokens": {
            "prompt_tokens": 148,
            "completion_tokens": 640,
            "total_tokens": 788,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Qe7",
            "d4",
            "Qf8#",
            "d3",
            "Qe7"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d7d5",
          "positional_uci": "d7d5",
          "neutral_uci": "d7d5",
          "consensus_move": "d7d5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bxd5",
              "Qxd5",
              "Nc3",
              "Qd8",
              "O-O",
              "Bg4",
              "h3",
              "Bh5",
              "d3",
              "Nd4",
              "g4",
              "Nxf3+",
              "Qxf3",
              "Bg6",
              "Qxb7",
              "Bd6",
              "Qc6+",
              "Kf8",
              "Be3",
              "h5",
              "g5",
              "Rb8",
              "b3",
              "Qc8",
              "Kg2",
              "Bf5",
              "Rh1",
              "Rb4",
              "Bc5",
              "Bxc5",
              "Qxc5+",
              "Kg8",
              "Qxb4",
              "Qa8+",
              "f3",
              "Kh7",
              "Ne4",
              "Rb8",
              "Qe7",
              "Qc6",
              "Rac1",
              "Re8",
              "Qxf7",
              "Bg6",
              "Qc4",
              "Qd7",
              "Rhf1",
              "Rf8",
              "Rce1",
              "Rf4",
              "Qc5",
              "Rh4",
              "Rh1",
              "Qf5",
              "Qxc7",
              "Rf4",
              "Ref1",
              "h4",
              "Qxa7",
              "Bh5",
              "Qe3",
              "Qg6",
              "Rhg1",
              "Qf5",
              "Kh2",
              "Bxf3",
              "Rxf3",
              "Rxf3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qxd5",
              "Nc3",
              "Qd8",
              "O-O",
              "Bg4",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "d3",
              "Bc5",
              "Be3",
              "Bxe3",
              "fxe3",
              "O-O",
              "e4",
              "Nd4",
              "Nxe5",
              "Qh4",
              "Kg2",
              "Rae8",
              "Nxg6",
              "fxg6",
              "Rxf8+",
              "Rxf8",
              "Qd2",
              "Rf3",
              "Rh1",
              "Qg3#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qe7",
              "d4",
              "Qf8#",
              "d3",
              "Qe7"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 149,
            "completion_tokens": 388,
            "total_tokens": 537,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 148,
            "completion_tokens": 640,
            "total_tokens": 788,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 446,
          "total_completion_tokens": 1668,
          "total_tokens": 2114
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 149,
            "completion_tokens": 388,
            "total_tokens": 537,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 148,
            "completion_tokens": 640,
            "total_tokens": 788,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bqkb1r/pppp1ppp/2n5/4p3/2B1N3/5N2/PPPP1PPP/R1BQK2R b KQkq - 0 5"
    },
    {
      "turn": 12,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "dxe4",
          "aggressive_uci": "d5e4",
          "aggressive_response": "6... dxe4 7. Bxe4 Bd6 8. O-O O-O 9. d3 Bg4 10. h3 Bh5 11. g4 Bg6 12. Bxg6 hxg6 13. Be3 f5 14. gxf5 gxf5 15. Ng5 Qe8 16. Kh1 f4 17. Bd2 Rf5 18. Rg1 Be7 19. Ne6 Bf6 20. Nxc7 Qd7 21. Nxa8 Rh5 22. Qg4 Rxh3+ 23. Kg2 Qxg4+ 24. Kf1 Qf3 25. Nc7 Rh2 26. Be1 Nd4 27. Ne6 Qe2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2",
          "aggressive_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bxe4",
            "Bd6",
            "O-O",
            "O-O",
            "d3",
            "Bg4",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Bxg6",
            "hxg6",
            "Be3",
            "f5",
            "gxf5",
            "gxf5",
            "Ng5",
            "Qe8",
            "Kh1",
            "f4",
            "Bd2",
            "Rf5",
            "Rg1",
            "Be7",
            "Ne6",
            "Bf6",
            "Nxc7",
            "Qd7",
            "Nxa8",
            "Rh5",
            "Qg4",
            "Rxh3+",
            "Kg2",
            "Qxg4+",
            "Kf1",
            "Qf3",
            "Nc7",
            "Rh2",
            "Be1",
            "Nd4",
            "Ne6",
            "Qe2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "dxe4",
          "positional_uci": "d5e4",
          "positional_response": "6... dxe4 7. Bxe4 Bd6 8. O-O O-O 9. d3 Bg4 10. h3 Bh5 11. g4 Bg6 12. Bxg6 hxg6 13. Be3 f5 14. gxf5 gxf5 15. Ng5 Qe8 16. Kh1 f4 17. Bd2 Nd4 18. c3 Ne6 19. Nxe6 Qxe6 20. Qg4 Qd5+ 21. Kh2 e4 22. Rg1 Rf7 23. dxe4 Qxd2 24. Rg2 f3+ 25. Rg3 Qxf2+ 26. Kh1 Bxg3 27. Rg1 Qh2# 0-1\n28. Qxh2 Bxh2 29. Kxh2 f2 30. Rf1 Rd8 31. Kg2 Rd2 32. b4 Rxa2 33. c4 Re2 34. c5 Re1 35. Rxf2 Rxf2+ 36. Kxf2 Rxe4 37. b5 Rc4 38. c6 bxc6 39. bxc6 Rxc6 40. Ke3 a5 41. Kd4 a4 42. Kd5 Rc1 43. Kd4 a3 44. Kd3 a2 45. Kd2 a1=Q 46. Kd3 Qc3+ 47. Ke4 Re1+ 48. Kd5 Re5# 0-1",
          "positional_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 365,
            "total_tokens": 521,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qxd2",
            "Rg2",
            "f3+",
            "Rg3",
            "Qxf2+",
            "Kh1",
            "Bxg3",
            "Rg1",
            "Qh2#",
            "Qxh2",
            "Bxh2",
            "Kxh2",
            "f2",
            "Rf1",
            "Rd8",
            "Kg2",
            "Rd2",
            "b4",
            "Rxa2",
            "c4",
            "Re2",
            "c5",
            "Re1",
            "Rxf2",
            "Rxf2+",
            "Kxf2",
            "Rxe4",
            "b5",
            "Rc4",
            "c6",
            "bxc6",
            "bxc6",
            "Rxc6",
            "Ke3",
            "a5",
            "Kd4",
            "a4",
            "Kd5",
            "Rc1",
            "Kd4",
            "a3",
            "Kd3",
            "a2",
            "Kd2",
            "a1=Q",
            "Kd3",
            "Qc3+",
            "Ke4",
            "Re1+",
            "Kd5",
            "Re5#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Re8",
          "neutral_uci": null,
          "neutral_response": "Re8\n7. O-O Bg4 8. h3 Bh5 9. Ng3 Bg6 10. Bxg6 hxg6 11. d3 Qd7 12. Re1 O-O-O\n13. c3 f6 14. b4 g5 15. b5 Ne7 16. Qa4 Kb8 17. Be3 Nc8 18. Rab1 Bd6\n19. Rb3 g6 20. Reb1 Rh8 21. c4 d4 22. Bd2 g4 23. hxg4 Qxg4 24. Bb4 Bxb4\n25. Qxb4 Rh7 26. c5 Rdh8 27. Kf1 Rh1+ 28. Nxh1 Rxh1+ 29. Ke2 Qxg2\n30. Rxh1 Qxh1 31. Rb1 Qh5 32. c6 b6 33. Qc4 Nd6 34. Qg8+ Nc8 35. Qe6 Nd6\n36. Qxf6 e4 37. dxe4 Nxe4 38. Qd8# e4 Qxf3+ Kd2 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+",
          "neutral_tokens": {
            "prompt_tokens": 155,
            "completion_tokens": 640,
            "total_tokens": 795,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "O-O",
            "Bg4",
            "h3",
            "Bh5",
            "Ng3",
            "Bg6",
            "Bxg6",
            "hxg6",
            "d3",
            "Qd7",
            "Re1",
            "O-O-O",
            "c3",
            "f6",
            "b4",
            "g5",
            "b5",
            "Ne7",
            "Qa4",
            "Kb8",
            "Be3",
            "Nc8",
            "Rab1",
            "Bd6",
            "Rb3",
            "g6",
            "Reb1",
            "Rh8",
            "c4",
            "d4",
            "Bd2",
            "g4",
            "hxg4",
            "Qxg4",
            "Bb4",
            "Bxb4",
            "Qxb4",
            "Rh7",
            "c5",
            "Rdh8",
            "Kf1",
            "Rh1+",
            "Nxh1",
            "Rxh1+",
            "Ke2",
            "Qxg2",
            "Rxh1",
            "Qxh1",
            "Rb1",
            "Qh5",
            "c6",
            "b6",
            "Qc4",
            "Nd6",
            "Qg8+",
            "Nc8",
            "Qe6",
            "Nd6",
            "Qxf6",
            "e4",
            "dxe4",
            "Nxe4",
            "Qd8#",
            "e4",
            "Qxf3+",
            "Kd2",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d5e4",
          "positional_uci": "d5e4",
          "neutral_uci": null,
          "consensus_move": "d5e4",
          "source_agents": [
            "aggressive",
            "positional"
          ],
          "selected_agent": "aggressive"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bxe4",
              "Bd6",
              "O-O",
              "O-O",
              "d3",
              "Bg4",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Bxg6",
              "hxg6",
              "Be3",
              "f5",
              "gxf5",
              "gxf5",
              "Ng5",
              "Qe8",
              "Kh1",
              "f4",
              "Bd2",
              "Rf5",
              "Rg1",
              "Be7",
              "Ne6",
              "Bf6",
              "Nxc7",
              "Qd7",
              "Nxa8",
              "Rh5",
              "Qg4",
              "Rxh3+",
              "Kg2",
              "Qxg4+",
              "Kf1",
              "Qf3",
              "Nc7",
              "Rh2",
              "Be1",
              "Nd4",
              "Ne6",
              "Qe2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qxd2",
              "Rg2",
              "f3+",
              "Rg3",
              "Qxf2+",
              "Kh1",
              "Bxg3",
              "Rg1",
              "Qh2#",
              "Qxh2",
              "Bxh2",
              "Kxh2",
              "f2",
              "Rf1",
              "Rd8",
              "Kg2",
              "Rd2",
              "b4",
              "Rxa2",
              "c4",
              "Re2",
              "c5",
              "Re1",
              "Rxf2",
              "Rxf2+",
              "Kxf2",
              "Rxe4",
              "b5",
              "Rc4",
              "c6",
              "bxc6",
              "bxc6",
              "Rxc6",
              "Ke3",
              "a5",
              "Kd4",
              "a4",
              "Kd5",
              "Rc1",
              "Kd4",
              "a3",
              "Kd3",
              "a2",
              "Kd2",
              "a1=Q",
              "Kd3",
              "Qc3+",
              "Ke4",
              "Re1+",
              "Kd5",
              "Re5#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "O-O",
              "Bg4",
              "h3",
              "Bh5",
              "Ng3",
              "Bg6",
              "Bxg6",
              "hxg6",
              "d3",
              "Qd7",
              "Re1",
              "O-O-O",
              "c3",
              "f6",
              "b4",
              "g5",
              "b5",
              "Ne7",
              "Qa4",
              "Kb8",
              "Be3",
              "Nc8",
              "Rab1",
              "Bd6",
              "Rb3",
              "g6",
              "Reb1",
              "Rh8",
              "c4",
              "d4",
              "Bd2",
              "g4",
              "hxg4",
              "Qxg4",
              "Bb4",
              "Bxb4",
              "Qxb4",
              "Rh7",
              "c5",
              "Rdh8",
              "Kf1",
              "Rh1+",
              "Nxh1",
              "Rxh1+",
              "Ke2",
              "Qxg2",
              "Rxh1",
              "Qxh1",
              "Rb1",
              "Qh5",
              "c6",
              "b6",
              "Qc4",
              "Nd6",
              "Qg8+",
              "Nc8",
              "Qe6",
              "Nd6",
              "Qxf6",
              "e4",
              "dxe4",
              "Nxe4",
              "Qd8#",
              "e4",
              "Qxf3+",
              "Kd2",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 156,
            "completion_tokens": 365,
            "total_tokens": 521,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 155,
            "completion_tokens": 640,
            "total_tokens": 795,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 467,
          "total_completion_tokens": 1645,
          "total_tokens": 2112
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 156,
            "completion_tokens": 365,
            "total_tokens": 521,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 155,
            "completion_tokens": 640,
            "total_tokens": 795,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/3pp3/4N3/3B1N2/PPPP1PPP/R1BQK2R b KQkq - 1 6"
    },
    {
      "turn": 14,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "exd3",
          "aggressive_uci": "e4d3",
          "aggressive_response": "7... exd3 8. cxd3 Qxd3 9. Re1 Bg4 10. Re3 Qd5 11. h3 Bxf3 12. Rxf3 O-O-O 13. d3 e4 14. Re3 exd3 15. Bd2 Bc5 16. Rf3 Rhe8 17. Rc1 Re2 18. a3 Nd4 19. Rg3 Nb3 20. Rc3 Rxd2 21. Qxb3 Bxf2+ 22. Kh2 Bxg3+ 23. Kxg3 Rxg2+ 24. Kh4 Qg5# d4 d5 dxe6",
          "aggressive_tokens": {
            "prompt_tokens": 164,
            "completion_tokens": 156,
            "total_tokens": 320,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bd2",
            "Bc5",
            "Rf3",
            "Rhe8",
            "Rc1",
            "Re2",
            "a3",
            "Nd4",
            "Rg3",
            "Nb3",
            "Rc3",
            "Rxd2",
            "Qxb3",
            "Bxf2+",
            "Kh2",
            "Bxg3+",
            "Kxg3",
            "Rxg2+",
            "Kh4",
            "Qg5#",
            "d4",
            "d5",
            "dxe6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "exd3",
          "positional_uci": "e4d3",
          "positional_response": "7... exd3 8. cxd3 Qxd3 9. Re1 Bg4 10. Re3 Qd5 11. h3 Bxf3 12. Rxf3 O-O-O 13. d3 Nd4 14. Re3 Bc5 15. Re4 f5 16. Re1 Rhe8 17. Be3 f4 18. Bxd4 Qxd4 19. Qg4+ Kb8 20. Qxg7 Qxf2+ 21. Kh1 Rg8 22. Rf1 Qxf1+ 23. Rxf1 Rxg7 24. Kh2 Rxd3 25. Re1 Rd2 26. Rxe5 Rgxg2+ 27. Kh1 Rh2# Rg8 Qxg1#",
          "positional_tokens": {
            "prompt_tokens": 164,
            "completion_tokens": 182,
            "total_tokens": 346,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "cxd3",
            "Qxd3",
            "Re1",
            "Bg4",
            "Re3",
            "Qd5",
            "h3",
            "Bxf3",
            "Rxf3",
            "O-O-O",
            "d3",
            "Nd4",
            "Re3",
            "Bc5",
            "Re4",
            "f5",
            "Re1",
            "Rhe8",
            "Be3",
            "f4",
            "Bxd4",
            "Qxd4",
            "Qg4+",
            "Kb8",
            "Qxg7",
            "Qxf2+",
            "Kh1",
            "Rg8",
            "Rf1",
            "Qxf1+",
            "Rxf1",
            "Rxg7",
            "Kh2",
            "Rxd3",
            "Re1",
            "Rd2",
            "Rxe5",
            "Rgxg2+",
            "Kh1",
            "Rh2#",
            "Rg8",
            "Qxg1#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "exd3",
          "neutral_uci": "e4d3",
          "neutral_response": "7... exd3 8. cxd3 Qxd3 9. Re1 Bg4 10. Re3 Qd5 11. Qe1 Bxf3 12. Rxf3 O-O-O 13. d3 Nd4 14. Re3 Nc2 15. Qc3 Nxa1 16. Rxe5 Qxd3 17. Qxd3 Rxd3 18. Re1 Nc2 19. Rf1 Bc5 20. Bf4 Re8 21. h3 Re1 22. Rxe1 Nxe1 23. Kf1 Nc2 24. Ke2 Rd7 25. a3 Nd4+ 26. Kf1 Ne6 27. Bg3 Rd2 28. b4 Bd4 29. Ke1 Ra2 30. f4 Rxa3 31. Bh2 Bc3+ 32. Kf2 Bxb4 33. f5 Nd4 34. Be5 Nxf5 35. g4 Bc5+ 36. Kg2 Ne3+ 37. Kf3 Nc4+ 38. Ke4 Nxe5 39. Kxe5 Rxh3 40. Kd5 Bd6 41. g5 a5 42. Kc4 a4 43. Kb5 a3 44. Kc4 a2 45. Kd5 a1=Q 46. Ke4 Qe5# Qe5#",
          "neutral_tokens": {
            "prompt_tokens": 163,
            "completion_tokens": 332,
            "total_tokens": 495,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "cxd3",
            "Qxd3",
            "Re1",
            "Bg4",
            "Re3",
            "Qd5",
            "Qe1",
            "Bxf3",
            "Rxf3",
            "O-O-O",
            "d3",
            "Nd4",
            "Re3",
            "Nc2",
            "Qc3",
            "Nxa1",
            "Rxe5",
            "Qxd3",
            "Qxd3",
            "Rxd3",
            "Re1",
            "Nc2",
            "Rf1",
            "Bc5",
            "Bf4",
            "Re8",
            "h3",
            "Re1",
            "Rxe1",
            "Nxe1",
            "Kf1",
            "Nc2",
            "Ke2",
            "Rd7",
            "a3",
            "Nd4+",
            "Kf1",
            "Ne6",
            "Bg3",
            "Rd2",
            "b4",
            "Bd4",
            "Ke1",
            "Ra2",
            "f4",
            "Rxa3",
            "Bh2",
            "Bc3+",
            "Kf2",
            "Bxb4",
            "f5",
            "Nd4",
            "Be5",
            "Nxf5",
            "g4",
            "Bc5+",
            "Kg2",
            "Ne3+",
            "Kf3",
            "Nc4+",
            "Ke4",
            "Nxe5",
            "Kxe5",
            "Rxh3",
            "Kd5",
            "Bd6",
            "g5",
            "a5",
            "Kc4",
            "a4",
            "Kb5",
            "a3",
            "Kc4",
            "a2",
            "Kd5",
            "a1=Q",
            "Ke4",
            "Qe5#",
            "Qe5#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e4d3",
          "positional_uci": "e4d3",
          "neutral_uci": "e4d3",
          "consensus_move": "e4d3",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bd2",
              "Bc5",
              "Rf3",
              "Rhe8",
              "Rc1",
              "Re2",
              "a3",
              "Nd4",
              "Rg3",
              "Nb3",
              "Rc3",
              "Rxd2",
              "Qxb3",
              "Bxf2+",
              "Kh2",
              "Bxg3+",
              "Kxg3",
              "Rxg2+",
              "Kh4",
              "Qg5#",
              "d4",
              "d5",
              "dxe6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "cxd3",
              "Qxd3",
              "Re1",
              "Bg4",
              "Re3",
              "Qd5",
              "h3",
              "Bxf3",
              "Rxf3",
              "O-O-O",
              "d3",
              "Nd4",
              "Re3",
              "Bc5",
              "Re4",
              "f5",
              "Re1",
              "Rhe8",
              "Be3",
              "f4",
              "Bxd4",
              "Qxd4",
              "Qg4+",
              "Kb8",
              "Qxg7",
              "Qxf2+",
              "Kh1",
              "Rg8",
              "Rf1",
              "Qxf1+",
              "Rxf1",
              "Rxg7",
              "Kh2",
              "Rxd3",
              "Re1",
              "Rd2",
              "Rxe5",
              "Rgxg2+",
              "Kh1",
              "Rh2#",
              "Rg8",
              "Qxg1#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "cxd3",
              "Qxd3",
              "Re1",
              "Bg4",
              "Re3",
              "Qd5",
              "Qe1",
              "Bxf3",
              "Rxf3",
              "O-O-O",
              "d3",
              "Nd4",
              "Re3",
              "Nc2",
              "Qc3",
              "Nxa1",
              "Rxe5",
              "Qxd3",
              "Qxd3",
              "Rxd3",
              "Re1",
              "Nc2",
              "Rf1",
              "Bc5",
              "Bf4",
              "Re8",
              "h3",
              "Re1",
              "Rxe1",
              "Nxe1",
              "Kf1",
              "Nc2",
              "Ke2",
              "Rd7",
              "a3",
              "Nd4+",
              "Kf1",
              "Ne6",
              "Bg3",
              "Rd2",
              "b4",
              "Bd4",
              "Ke1",
              "Ra2",
              "f4",
              "Rxa3",
              "Bh2",
              "Bc3+",
              "Kf2",
              "Bxb4",
              "f5",
              "Nd4",
              "Be5",
              "Nxf5",
              "g4",
              "Bc5+",
              "Kg2",
              "Ne3+",
              "Kf3",
              "Nc4+",
              "Ke4",
              "Nxe5",
              "Kxe5",
              "Rxh3",
              "Kd5",
              "Bd6",
              "g5",
              "a5",
              "Kc4",
              "a4",
              "Kb5",
              "a3",
              "Kc4",
              "a2",
              "Kd5",
              "a1=Q",
              "Ke4",
              "Qe5#",
              "Qe5#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 164,
            "completion_tokens": 156,
            "total_tokens": 320,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 164,
            "completion_tokens": 182,
            "total_tokens": 346,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 163,
            "completion_tokens": 332,
            "total_tokens": 495,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 491,
          "total_completion_tokens": 670,
          "total_tokens": 1161
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 164,
            "completion_tokens": 156,
            "total_tokens": 320,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 164,
            "completion_tokens": 182,
            "total_tokens": 346,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 163,
            "completion_tokens": 332,
            "total_tokens": 495,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/4p3/4p3/3B1N2/PPPP1PPP/R1BQ1RK1 b kq - 1 7"
    },
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": "d2d4",
          "aggressive_response": "d4 d5\nd4 d5 Nc3 Nf6 Bg5 e6 e4 dxe4 Nxe4 Be7 Nxf6+ Bxf6 Bxf6 Qxf6 Nf3 O-O Bd3 Nc6 O-O Rd8 c3 e5 dxe5 Nxe5 Nxe5 Qxe5 Bxh7+ Kxh7 Qxd8 b6 Rfe1 Qf5 Qh4+ Kg8 Re8#\nd4 d5 Nc3 Nf6 Bg5 e6 e4 dxe4 Nxe4 Be7 Nxf6+ Bxf6 Bxf6 Qxf6 Nf3 O-O Bd3 Nc6 O-O Rd8 c3 e5 dxe5 Nxe5 Nxe5 Qxe5 Bxh7+ Kxh7 Qxd8 b6 Rfe1 Qf5 Qh4+ Kg8 Re8# d4 d5 Nc3 Nf6 Bg5 e6 e4 dxe4 Nxe4 Be7 Nxf6+ Bxf6 Bxf6 Qxf6 Nf3 O-O Bd3 Nc6 O-O Rd8 c3 e5 dxe5 Nxe5 Nxe5 Qxe5 Bxh7+ Kxh7 Qxd8 b6 Rfe1 Qf5 Qh4+ Kg8 Re8# d4 d5 Nc3 Nf6 Bg5 e6 e4 dxe4 Nxe4 Be7 Nxf6+ Bxf6 Bxf6 Qxf6 Nf3 O-O Bd3 Nc6 O-O Rd8 c3 e5 dxe5 Nxe5 Nxe5 Qxe5 Bxh7+ Kxh7 Qxd8 b6 Rfe1 Qf5 Qh4+ Kg8 Re8# d4 d5 Nc3 Nf6 Bg5 e6 e4 dxe4 Nxe4 Be7 Nxf6+ Bxf6 Bxf6 Qxf6 Nf3 O-O Bd3 Nc6 O-O Rd8 c3 e5 dxe5 Nxe5 Nxe5 Qxe5 Bxh7+ Kxh7 Qxd8 b6 Rfe1 Qf5 Qh4+ Kg8 Re8# d4 d5 Nc3 Nf6 Bg5 e6 e4 dxe4 Nxe4 Be7 Nxf6+ Bxf6 Bxf6 Qxf6 Nf3 O-O Bd3 Nc6 O-O Rd8 c3 e5 dxe5 Nxe5 Nxe5 Qxe5 Bxh7+ Kxh7 Qxd8 b6 Rfe1 Qf5 Qh4+ Kg8 Re8# d4 d5 Nc3 Nf6 Bg5 e6 e4 dxe4 Nxe4 Be7 Nxf6+ Bxf6 Bxf6 Qxf6 Nf3 O-O Bd3 Nc6 O-O Rd8",
          "aggressive_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Nc3",
            "Nf6",
            "Bg5",
            "e6",
            "e4",
            "dxe4",
            "Nxe4",
            "Be7",
            "Nxf6+",
            "Bxf6",
            "Bxf6",
            "Qxf6",
            "Nf3",
            "O-O",
            "Bd3",
            "Nc6",
            "O-O",
            "Rd8"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "Game 1\n1. e4 e5\n2. Nf3 Nc6\n3. Bb5 Bc5\n4. O-O Nf6\n5. d3 d6\n6. Bg5 O-O\n7. Nc3 Bg4\n8. Nd5 Nd4\n9. Bc4 Bxf3\n10. gxf3 c6\n11. Nxf6+ gxf6\n12. Bh6 Re8\n13. f4 Kh8\n14. Bxf7 Re7\n15. Qh5 Nxc2\n16. Rac1 Nd4\n17. Kh1 Qd7\n18. Bc4 d5\n19. exd5 cxd5\n20. Bb3 Nxb3\n21. axb3 Bd4\n22. Rc2 Rg8\n23. Rfc1 Bb6\n24. fxe5 Rxe5\n25. Qh4 Qf5\n26. f4 Re3\n27. Rg2 Rxg2\n28. Kxg2 Re2+\n29. Kf1 Rf2+\n30. Ke1 Qe6+\n31. Kd1 Qe2#\n1. e4 e5\n2. Nf3 Nc6\n3. Bb5 Bc5\n4. O-O Nf6\n5. d3 d6\n6. Bg5 O-O\n7. Nc3 Bg4\n8. Nd5 Nd4\n9. Bc4 Bxf3\n10. gxf3 c6\n11. Nxf6+ gxf6\n12. Bh6 Re8\n13. f4 Kh8\n14. Bxf7 Re7\n15. Qh5 Nxc2\n16. Rac1 Nd4\n17. Kh1 Qd7\n18. Bc4 d5\n19. exd5 cxd5\n20. Bb3 Nxb3\n21. axb3 Bd4\n22. Rc2 Rg8\n23. Rfc1 Bb6\n24. fxe5 Rxe5\n25. Qh4 Qf5\n26. f4 Re3\n27. Rg2 Rxg2\n28. Kxg2 Re2+\n29. Kf1 Rf2+\n30. Ke1 Qe6+\n31. Kd1 Qe2# e4 e5 Nf3 Nc6 Bb5 Bc5 O-O Nf6 d3 d6 Bg5 O-O Nc3 Bg4 Nd5 Nd4 Bc4 Bxf3 gxf3 c6 Nxf6+ gxf6 Bh6 Re8 f4 Kh8 Bxf7 Re7 Qh5 Nxc2 Rc1 Nd4 Kh1 Qd7 Bc4 d5 exd5 cxd5 Bb3 Nxb3 axb3 Bd4 Rc2 Rg8 fxe5 Rxe5 Qh",
          "positional_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bb5",
            "Bc5",
            "O-O",
            "Nf6",
            "d3",
            "d6",
            "Bg5",
            "O-O",
            "Nc3",
            "Bg4",
            "Nd5",
            "Nd4",
            "Bc4",
            "Bxf3",
            "gxf3",
            "c6",
            "Nxf6+",
            "gxf6",
            "Bh6",
            "Re8",
            "f4",
            "Kh8",
            "Bxf7",
            "Re7",
            "Qh5",
            "Nxc2",
            "Rc1",
            "Nd4",
            "Kh1",
            "Qd7",
            "Bc4",
            "d5",
            "exd5",
            "cxd5",
            "Bb3",
            "Nxb3",
            "axb3",
            "Bd4",
            "Rc2",
            "Rg8",
            "fxe5",
            "Rxe5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": "d2d4",
          "neutral_response": "d4 d5 2. Nf3 Nf6 3. Bg5 Bf5 4. e3 e6 5. Bd3 Bxd3 6. Qxd3 Be7\n7. O-O O-O 8. Nbd2 Nbd7 9. c4 c6 10. Rac1 Rc8 11. cxd5 cxd5\n12. Rxc8 Qxc8 13. Nb3 Qc4 14. Qxc4 dxc4 15. Nbd2 b5 16. Rc1 Rc8\n17. Ne5 Nxe5 18. dxe5 Nd5 19. Bxe7 Nxe7 20. Ne4 Nc6 21. Nd6 Rb8\n22. f4 Nb4 23. a3 Nd3 24. Rc2 Kf8 25. Kf1 Ke7 26. Ke2 f6\n27. Ne4 fxe5 28. fxe5 Nxe5 29. Nc5 Rc8 30. Na6 Nd3 31. Nb4 Nxb4\n32. axb4 Kd6 33. Rd2+ Ke5 34. Rd7 a6 35. Rxg7 c3 36. bxc3 Rxc3\n37. Rxh7 Rc4 38. Ra7 Rxb4 39. Rxa6 Rb2+ 40. Kf3 b4 41. Ra5+ Kf6\n42. Rb5 b3 43. h4 Rb1 44. g4 b2 45. Ke4 Rg1 46. g5+ Kg6\n47. Rxb2 Rg4+ 48. Ke5 Rxh4 49. Rg2 Rh5 50. Kxe6 Rxg5 51. Rxg5+ Kxg5\n52. e4 Kg6 53. e5 Kg7 54. Ke7 Kg8 55. e6 Kg7 56. Kd8 Kf8\n57. e7+ Kf7 58. e8=Q+ Kf6 59. Qe4 Kg5 60. Qf3 Kg6 61. Qf4 Kg7\n62. Qf5 Kg8 63. Qf6 Kh7 64. Qg5 Kh8 65. Ke7 Kh7 66. Kf7 Kh8\nd4 d5 2. Nf3 Nf6 3. Bg5 Bf5 4. e3 e6 5. Bd3 Bxd3 6. Qxd3 Be7\n7. O-O O-O 8. Nbd2 Nbd7 9. c4 c6 10. Rac1 Rc8 11. cxd5 cxd5\n12. Rxc8 Qxc8 13. Nb3 Qc4 14. Qxc4",
          "neutral_tokens": {
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Nf3",
            "Nf6",
            "Bg5",
            "Bf5",
            "e3",
            "e6",
            "Bd3",
            "Bxd3",
            "Qxd3",
            "Be7",
            "O-O",
            "O-O",
            "Nbd2",
            "Nbd7",
            "c4",
            "c6",
            "Rac1",
            "Rc8",
            "cxd5",
            "cxd5",
            "Rxc8",
            "Qxc8",
            "Nb3",
            "Qc4",
            "Qxc4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d2d4",
          "positional_uci": "e2e4",
          "neutral_uci": "d2d4",
          "consensus_move": "d2d4",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d5",
              "Nc3",
              "Nf6",
              "Bg5",
              "e6",
              "e4",
              "dxe4",
              "Nxe4",
              "Be7",
              "Nxf6+",
              "Bxf6",
              "Bxf6",
              "Qxf6",
              "Nf3",
              "O-O",
              "Bd3",
              "Nc6",
              "O-O",
              "Rd8"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bb5",
              "Bc5",
              "O-O",
              "Nf6",
              "d3",
              "d6",
              "Bg5",
              "O-O",
              "Nc3",
              "Bg4",
              "Nd5",
              "Nd4",
              "Bc4",
              "Bxf3",
              "gxf3",
              "c6",
              "Nxf6+",
              "gxf6",
              "Bh6",
              "Re8",
              "f4",
              "Kh8",
              "Bxf7",
              "Re7",
              "Qh5",
              "Nxc2",
              "Rc1",
              "Nd4",
              "Kh1",
              "Qd7",
              "Bc4",
              "d5",
              "exd5",
              "cxd5",
              "Bb3",
              "Nxb3",
              "axb3",
              "Bd4",
              "Rc2",
              "Rg8",
              "fxe5",
              "Rxe5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "d5",
              "Nf3",
              "Nf6",
              "Bg5",
              "Bf5",
              "e3",
              "e6",
              "Bd3",
              "Bxd3",
              "Qxd3",
              "Be7",
              "O-O",
              "O-O",
              "Nbd2",
              "Nbd7",
              "c4",
              "c6",
              "Rac1",
              "Rc8",
              "cxd5",
              "cxd5",
              "Rxc8",
              "Qxc8",
              "Nb3",
              "Qc4",
              "Qxc4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 329,
          "total_completion_tokens": 1920,
          "total_tokens": 2249
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e5",
          "aggressive_uci": "e7e5",
          "aggressive_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6\n4. Ba4 Nf6 5. O-O Be7 6. Re1 b5\n7. Bb3 d6 8. c3 O-O 9. h3 Bb7\n10. d4 Re8 11. Nbd2 Bf8 12. a4 h6\n13. Bc2 exd4 14. cxd4 Nb4 15. Bb1 c5\n16. d5 Nd7 17. Ra3 c4 18. Nd4 Qf6\n19. N2f3 Nc5 20. axb5 Nbd3 21. Bxd3 Nxd3\n22. Re3 axb5 23. Nxb5 Rxa3 24. Nxa3 Ba6\n25. Qa4 Ra8 26. Nxc4 Nxc1 27. Qc6 Rc8\n28. Qxa6 Qd8 29. Rc3 Ne2+ 30. Kf1 Nxc3\n31. bxc3 Ra8 32. Qb5 Ra1+ 33. Ke2 Qa8\n34. Nfd2 Qa2 35. Qb3 Qa6 36. Qb6 Qa2\n37. Qb3 Qa6 38. Qb6 Qa2 39. Qb3 Qa6\n40. Qb6 Qa2 41. Qb3 Qa6 42. Qb6 Qa2\n43. Qb3 Qa6 44. Qb6 Qa2 45. Qb3 Qa6\n46. Qb6 Qa2 47. Qb3 Qa6 48. Qb6 Qa2\n49. Qb3 Qa6 50. Qb6 Qa2 51. Qb3 Qa6\n52. Qb6 Qa2 53. Qb3 Qa6 54. Qb6 Qa2\n55. Qb3 Qa6 56. Qb6 Qa2 57. Qb3 Qa6\n58. Qb6 Qa2 59. Qb3 Qa6 60. Qb6 Qa2\n61. Qb3 Qa6 62. Qb6 Qa2 63. Qb3 Qa6\n64. Qb6 Qa2 65. Qb3 Qa6 66. Qb6 Qa2\n67. Qb3 Qa6 68. Qb6 Qa2 69. Qb3 Qa6\n70. Qb6 Qa2 71. Qb3 Qa6 72. Qb6 Qa2\n73. Qb3 Qa6 74. Qb6 Qa2 75. Qb",
          "aggressive_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Bb7",
            "d4",
            "Re8",
            "Nbd2",
            "Bf8",
            "a4",
            "h6",
            "Bc2",
            "exd4",
            "cxd4",
            "Nb4",
            "Bb1",
            "c5",
            "d5",
            "Nd7",
            "Ra3",
            "c4",
            "Nd4",
            "Qf6",
            "N2f3",
            "Nc5",
            "axb5",
            "Nbd3",
            "Bxd3",
            "Nxd3",
            "Re3",
            "axb5",
            "Nxb5",
            "Rxa3",
            "Nxa3",
            "Ba6",
            "Qa4",
            "Ra8",
            "Nxc4",
            "Nxc1",
            "Qc6",
            "Rc8",
            "Qxa6",
            "Qd8",
            "Rc3",
            "Ne2+",
            "Kf1",
            "Nxc3",
            "bxc3",
            "Ra8",
            "Qb5",
            "Ra1+",
            "Ke2",
            "Qa8",
            "Nfd2",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e5",
          "positional_uci": "e7e5",
          "positional_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7\n6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5\n11. d4 Qc7 12. Nbd2 cxd4 13. cxd4 Nc6 14. Nb3 a5\n15. Be3 a4 16. Nbd2 Bd7 17. Rc1 Qb7 18. Bb1 Rfc8\n19. Nf1 Bd8 20. Ng3 Bb6 21. Qd2 Ba5 22. Qe2 Bxe1\n23. Rxe1 Re8 24. Qd2 Rac8 25. Bg5 Kh8 26. Bxf6 gxf6\n27. Qh6 Rg8 28. Qxf6+ Rg7 29. Nh5 Rg8 30. Ng5 Be8\n31. Nxg7 Rxg7 32. dxe5 dxe5 33. Rd1 Qe7 34. Qxe7 Nxe7\n35. Rd8 Rg8 36. Nf3 f6 37. Bd3 Kg7 38. Rb8 Bf7\n39. Rxb5 Bxa2 40. Rb7 Kf8 41. Ra7 Bb3 42. Nd2 Bd1\n43. Bc4 Rg5 44. Ra8+ Kg7 45. Re8 Nc6 46. Rg8+ Kh6\n47. Rxg5 Kxg5 48. Bb5 Nd4 49. Bd7 Kf4 50. g3+ Kg5\n51. h4+ Kg6 52. Kg2 Kf7 53. f4 Ke7 54. Bc8 Kd6\n55. Kf2 Kc5 56. Ke3 Kb4 57. Kd3 Bc2+ 58. Ke3 Bd1\n59. Kd3 Bc2+ 60. Ke3 Bd1 61. Kd3 Bc2+ 62. Ke3 Bd1\n63. Kd3 Bc2+ 64. Ke3 Bd1 65. Kd3 Bc2+ 66. Ke3 Bd1\n67. Kd3 Bc2+ 68. Ke3 Bd1 69. Kd3 Bc2+ 70. Ke3 Bd1\n71. Kd3 Bc2+ 72. Ke3 Bd1 73. Kd3 Bc2+ 74. Ke3 Bd1\n75. Kd3 Bc2+ 76. Ke3 Bd1 77. Kd3 Bc2+ 78. Ke3",
          "positional_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Qc7",
            "Nbd2",
            "cxd4",
            "cxd4",
            "Nc6",
            "Nb3",
            "a5",
            "Be3",
            "a4",
            "Nbd2",
            "Bd7",
            "Rc1",
            "Qb7",
            "Bb1",
            "Rfc8",
            "Nf1",
            "Bd8",
            "Ng3",
            "Bb6",
            "Qd2",
            "Ba5",
            "Qe2",
            "Bxe1",
            "Rxe1",
            "Re8",
            "Qd2",
            "Rac8",
            "Bg5",
            "Kh8",
            "Bxf6",
            "gxf6",
            "Qh6",
            "Rg8",
            "Qxf6+",
            "Rg7",
            "Nh5",
            "Rg8",
            "Ng5",
            "Be8",
            "Nxg7",
            "Rxg7",
            "dxe5",
            "dxe5",
            "Rd1",
            "Qe7",
            "Qxe7",
            "Nxe7",
            "Rd8",
            "Rg8",
            "Nf3",
            "f6",
            "Bd3",
            "Kg7",
            "Rb8",
            "Bf7",
            "Rxb5",
            "Bxa2",
            "Rb7",
            "Kf8",
            "Ra7",
            "Bb3",
            "Nd2",
            "Bd1",
            "Bc4",
            "Rg5",
            "Ra8+",
            "Kg7",
            "Re8",
            "Nc6",
            "Rg8+",
            "Kh6",
            "Rxg5",
            "Kxg5",
            "Bb5",
            "Nd4",
            "Bd7",
            "Kf4",
            "g3+",
            "Kg5",
            "h4+",
            "Kg6",
            "Kg2",
            "Kf7",
            "f4",
            "Ke7",
            "Bc8",
            "Kd6",
            "Kf2",
            "Kc5",
            "Ke3",
            "Kb4",
            "Kd3",
            "Bc2+",
            "Ke3",
            "Bd1",
            "Kd3",
            "Bc2+",
            "Ke3",
            "Bd1",
            "Kd3",
            "Bc2+",
            "Ke3",
            "Bd1",
            "Kd3",
            "Bc2+",
            "Ke3",
            "Bd1",
            "Kd3",
            "Bc2+",
            "Ke3",
            "Bd1",
            "Kd3",
            "Bc2+",
            "Ke3",
            "Bd1",
            "Kd3",
            "Bc2+",
            "Ke3",
            "Bd1",
            "Kd3",
            "Bc2+",
            "Ke3",
            "Bd1",
            "Kd3",
            "Bc2+",
            "Ke3",
            "Bd1",
            "Kd3",
            "Bc2+",
            "Ke3",
            "Bd1",
            "Kd3",
            "Bc2+",
            "Ke3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e5",
          "neutral_uci": "e7e5",
          "neutral_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6\n4. Ba4 Nf6 5. O-O Be7 6. Re1 b5\n7. Bb3 d6 8. c3 O-O 9. h3 Bb7\n10. d4 Re8 11. Nbd2 Bf8 12. a4 h6\n13. Bc2 exd4 14. cxd4 Nb4 15. Bb1 c5\n16. d5 Nd7 17. Ra3 f5 18. Rae3 fxe4\n19. Nxe4 Nxd5 20. Ba2 c4 21. Nxd6 Rxe3\n22. Nxb7 Rxe1+ 23. Nxe1 Qe7 24. Qxd5+ Kh8\n25. Nf3 Nf6 26. Qc6 Ra7 27. Na5 Qd8\n28. Bd2 Rd7 29. Bc3 Rd1+ 30. Kh2 Bd6+\n31. g3 Bc7 32. Nb7 Qd7 33. Qxa6 Qd5\n34. Kg2 Rd3 35. Qa8+ Kh7 36. Bb1 Qxf3+\n37. Kh2 Qxf2+ 38. Kh1 Qf1+ 39. Kh2 Bxg3#\n40. Kh1 Qxh3+ 41. Kg1 Qh2+ 42. Kf1 Qf2#",
          "neutral_tokens": {
            "prompt_tokens": 113,
            "completion_tokens": 345,
            "total_tokens": 458,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Bb7",
            "d4",
            "Re8",
            "Nbd2",
            "Bf8",
            "a4",
            "h6",
            "Bc2",
            "exd4",
            "cxd4",
            "Nb4",
            "Bb1",
            "c5",
            "d5",
            "Nd7",
            "Ra3",
            "f5",
            "Rae3",
            "fxe4",
            "Nxe4",
            "Nxd5",
            "Ba2",
            "c4",
            "Nxd6",
            "Rxe3",
            "Nxb7",
            "Rxe1+",
            "Nxe1",
            "Qe7",
            "Qxd5+",
            "Kh8",
            "Nf3",
            "Nf6",
            "Qc6",
            "Ra7",
            "Na5",
            "Qd8",
            "Bd2",
            "Rd7",
            "Bc3",
            "Rd1+",
            "Kh2",
            "Bd6+",
            "g3",
            "Bc7",
            "Nb7",
            "Qd7",
            "Qxa6",
            "Qd5",
            "Kg2",
            "Rd3",
            "Qa8+",
            "Kh7",
            "Bb1",
            "Qxf3+",
            "Kh2",
            "Qxf2+",
            "Kh1",
            "Qf1+",
            "Kh2",
            "Bxg3#",
            "Kh1",
            "Qxh3+",
            "Kg1",
            "Qh2+",
            "Kf1",
            "Qf2#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e7e5",
          "positional_uci": "e7e5",
          "neutral_uci": "e7e5",
          "consensus_move": "e7e5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Bb7",
              "d4",
              "Re8",
              "Nbd2",
              "Bf8",
              "a4",
              "h6",
              "Bc2",
              "exd4",
              "cxd4",
              "Nb4",
              "Bb1",
              "c5",
              "d5",
              "Nd7",
              "Ra3",
              "c4",
              "Nd4",
              "Qf6",
              "N2f3",
              "Nc5",
              "axb5",
              "Nbd3",
              "Bxd3",
              "Nxd3",
              "Re3",
              "axb5",
              "Nxb5",
              "Rxa3",
              "Nxa3",
              "Ba6",
              "Qa4",
              "Ra8",
              "Nxc4",
              "Nxc1",
              "Qc6",
              "Rc8",
              "Qxa6",
              "Qd8",
              "Rc3",
              "Ne2+",
              "Kf1",
              "Nxc3",
              "bxc3",
              "Ra8",
              "Qb5",
              "Ra1+",
              "Ke2",
              "Qa8",
              "Nfd2",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Qc7",
              "Nbd2",
              "cxd4",
              "cxd4",
              "Nc6",
              "Nb3",
              "a5",
              "Be3",
              "a4",
              "Nbd2",
              "Bd7",
              "Rc1",
              "Qb7",
              "Bb1",
              "Rfc8",
              "Nf1",
              "Bd8",
              "Ng3",
              "Bb6",
              "Qd2",
              "Ba5",
              "Qe2",
              "Bxe1",
              "Rxe1",
              "Re8",
              "Qd2",
              "Rac8",
              "Bg5",
              "Kh8",
              "Bxf6",
              "gxf6",
              "Qh6",
              "Rg8",
              "Qxf6+",
              "Rg7",
              "Nh5",
              "Rg8",
              "Ng5",
              "Be8",
              "Nxg7",
              "Rxg7",
              "dxe5",
              "dxe5",
              "Rd1",
              "Qe7",
              "Qxe7",
              "Nxe7",
              "Rd8",
              "Rg8",
              "Nf3",
              "f6",
              "Bd3",
              "Kg7",
              "Rb8",
              "Bf7",
              "Rxb5",
              "Bxa2",
              "Rb7",
              "Kf8",
              "Ra7",
              "Bb3",
              "Nd2",
              "Bd1",
              "Bc4",
              "Rg5",
              "Ra8+",
              "Kg7",
              "Re8",
              "Nc6",
              "Rg8+",
              "Kh6",
              "Rxg5",
              "Kxg5",
              "Bb5",
              "Nd4",
              "Bd7",
              "Kf4",
              "g3+",
              "Kg5",
              "h4+",
              "Kg6",
              "Kg2",
              "Kf7",
              "f4",
              "Ke7",
              "Bc8",
              "Kd6",
              "Kf2",
              "Kc5",
              "Ke3",
              "Kb4",
              "Kd3",
              "Bc2+",
              "Ke3",
              "Bd1",
              "Kd3",
              "Bc2+",
              "Ke3",
              "Bd1",
              "Kd3",
              "Bc2+",
              "Ke3",
              "Bd1",
              "Kd3",
              "Bc2+",
              "Ke3",
              "Bd1",
              "Kd3",
              "Bc2+",
              "Ke3",
              "Bd1",
              "Kd3",
              "Bc2+",
              "Ke3",
              "Bd1",
              "Kd3",
              "Bc2+",
              "Ke3",
              "Bd1",
              "Kd3",
              "Bc2+",
              "Ke3",
              "Bd1",
              "Kd3",
              "Bc2+",
              "Ke3",
              "Bd1",
              "Kd3",
              "Bc2+",
              "Ke3",
              "Bd1",
              "Kd3",
              "Bc2+",
              "Ke3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Bb7",
              "d4",
              "Re8",
              "Nbd2",
              "Bf8",
              "a4",
              "h6",
              "Bc2",
              "exd4",
              "cxd4",
              "Nb4",
              "Bb1",
              "c5",
              "d5",
              "Nd7",
              "Ra3",
              "f5",
              "Rae3",
              "fxe4",
              "Nxe4",
              "Nxd5",
              "Ba2",
              "c4",
              "Nxd6",
              "Rxe3",
              "Nxb7",
              "Rxe1+",
              "Nxe1",
              "Qe7",
              "Qxd5+",
              "Kh8",
              "Nf3",
              "Nf6",
              "Qc6",
              "Ra7",
              "Na5",
              "Qd8",
              "Bd2",
              "Rd7",
              "Bc3",
              "Rd1+",
              "Kh2",
              "Bd6+",
              "g3",
              "Bc7",
              "Nb7",
              "Qd7",
              "Qxa6",
              "Qd5",
              "Kg2",
              "Rd3",
              "Qa8+",
              "Kh7",
              "Bb1",
              "Qxf3+",
              "Kh2",
              "Qxf2+",
              "Kh1",
              "Qf1+",
              "Kh2",
              "Bxg3#",
              "Kh1",
              "Qxh3+",
              "Kg1",
              "Qh2+",
              "Kf1",
              "Qf2#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 113,
            "completion_tokens": 345,
            "total_tokens": 458,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 341,
          "total_completion_tokens": 1625,
          "total_tokens": 1966
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 113,
            "completion_tokens": 345,
            "total_tokens": 458,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 4,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc6",
          "aggressive_uci": "b8c6",
          "aggressive_response": "2... Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 O-O 8. c3 d5\n9. exd5 Nxd5 10. Nxe5 Nxe5 11. Rxe5 c6 12. d4 Bd6 13. Re1 Qh4 14. g3 Qh3\n15. Be3 Bg4 16. Qd3 Rae8 17. Nd2 Re6 18. Qf1 Qh5 19. a4 Rfe8 20. axb5 axb5\n21. Bxd5 Qxd5 22. Qg2 Qh5 23. Qxc6 Bxg3 24. Qg2 Bf4 25. Ra8 Bb8 26. Rea1 Rg6\n27. R1a6 Be6 28. Qxg6 Qxg6+ 29. Kf1 Qd3+ 30. Ke1 Bg4 31. f3 Qxe3+ 32. Kd1 Bxf3+\n33. Nxf3 Qxf3+ 34. Kc2 Qe2+ 35. Kb3 Qc4+ 36. Ka3 b4+ 37. cxb4 Re3+ 38. Ka4 Qb3+ 39. Kb5 Qd5+ 40. Ka4 Qd7+ 41. b5 Qxd4+ 42. b4 Qa1# Bb7 Rxa8# Rxa8#",
          "aggressive_tokens": {
            "prompt_tokens": 122,
            "completion_tokens": 362,
            "total_tokens": 484,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "O-O",
            "c3",
            "d5",
            "exd5",
            "Nxd5",
            "Nxe5",
            "Nxe5",
            "Rxe5",
            "c6",
            "d4",
            "Bd6",
            "Re1",
            "Qh4",
            "g3",
            "Qh3",
            "Be3",
            "Bg4",
            "Qd3",
            "Rae8",
            "Nd2",
            "Re6",
            "Qf1",
            "Qh5",
            "a4",
            "Rfe8",
            "axb5",
            "axb5",
            "Bxd5",
            "Qxd5",
            "Qg2",
            "Qh5",
            "Qxc6",
            "Bxg3",
            "Qg2",
            "Bf4",
            "Ra8",
            "Bb8",
            "Rea1",
            "Rg6",
            "R1a6",
            "Be6",
            "Qxg6",
            "Qxg6+",
            "Kf1",
            "Qd3+",
            "Ke1",
            "Bg4",
            "f3",
            "Qxe3+",
            "Kd1",
            "Bxf3+",
            "Nxf3",
            "Qxf3+",
            "Kc2",
            "Qe2+",
            "Kb3",
            "Qc4+",
            "Ka3",
            "b4+",
            "cxb4",
            "Re3+",
            "Ka4",
            "Qb3+",
            "Kb5",
            "Qd5+",
            "Ka4",
            "Qd7+",
            "b5",
            "Qxd4+",
            "b4",
            "Qa1#",
            "Bb7",
            "Rxa8#",
            "Rxa8#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nc6",
          "positional_uci": "b8c6",
          "positional_response": "2... Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 O-O 8. c3 d5\n9. exd5 Nxd5 10. Nxe5 Nxe5 11. Rxe5 c6 12. d4 Bd6 13. Re1 Qh4 14. g3 Qh3\n15. Be3 Bg4 16. Qd3 Rae8 17. Nd2 Re6 18. Qf1 Qh5 19. a4 Rfe8 20. axb5 axb5\n21. Bxd5 Qxd5 22. Qg2 Qh5 23. Ra6 Bf8 24. Raa1 h6 25. Ra6 Bh3 26. Qf3 Bg4\n27. Qg2 Bh3 28. Qf3 Bg4 29. Qg2 Bh3 30. Qf3 Bg4 31. Qg2 Bh3 32. Qf3 Bg4\n33. Qg2 Bh3 34. Qf3 Bg4 35. Qg2 Bh3 36. Qf3 Bg4 37. Qg2 Bh3 38. Qf3 Bg4\n39. Qg2 Bh3 40. Qf3 Bg4 41. Qg2 Bh3 42. Qf3 Bg4 43. Qg2 Bh3 44. Qf3 Bg4\n45. Qg2 Bh3 46. Qf3 Bg4 47. Qg2 Bh3 48. Qf3 Bg4 49. Qg2 Bh3 50. Qf3 Bg4\n51. Qg2 Bh3 52. Qf3 Bg4 53. Qg2 Bh3 54. Qf3 Bg4 55. Qg2 Bh3 56. Qf3 Bg4\n57. Qg2 Bh3 58. Qf3 Bg4 59. Qg2 Bh3 60. Qf3 Bg4 61. Qg2 Bh3 62. Qf3 Bg4\n63. Qg2 Bh3 64. Qf3 Bg4 65. Qg2 Bh3 66. Qf3 Bg4 67. Qg2 Bh3 68. Qf3 Bg4\n69. Qg2 Bh3 70. Qf3 Bg4 71. Qg2 Bh3 72. Qf3 Bg4 73. Qg2 Bh3 74. Qf3 Bg4\n75. Qg2 Bh3 76. Qf3 Bg4 77. Qg2 Bh3 78. Qf3 Bg",
          "positional_tokens": {
            "prompt_tokens": 122,
            "completion_tokens": 640,
            "total_tokens": 762,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "O-O",
            "c3",
            "d5",
            "exd5",
            "Nxd5",
            "Nxe5",
            "Nxe5",
            "Rxe5",
            "c6",
            "d4",
            "Bd6",
            "Re1",
            "Qh4",
            "g3",
            "Qh3",
            "Be3",
            "Bg4",
            "Qd3",
            "Rae8",
            "Nd2",
            "Re6",
            "Qf1",
            "Qh5",
            "a4",
            "Rfe8",
            "axb5",
            "axb5",
            "Bxd5",
            "Qxd5",
            "Qg2",
            "Qh5",
            "Ra6",
            "Bf8",
            "Raa1",
            "h6",
            "Ra6",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc6",
          "neutral_uci": "b8c6",
          "neutral_response": "2... Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 O-O 8. c3 d5\n9. exd5 Nxd5 10. Nxe5 Nxe5 11. Rxe5 c6 12. d4 Bd6 13. Re1 Qh4 14. g3 Qh3\n15. Be3 Bg4 16. Qd3 Rae8 17. Nd2 Re6 18. Qf1 Qh5 19. a4 Rfe8 20. axb5 axb5\n21. Bxd5 Qxd5 22. Qg2 Qh5 23. Ra6 Bf8 24. Rxc6 Bh3 25. Qh1 Qf5 26. Rxe6 Rxe6\n27. Qf3 Qc2 28. Qd1 Qxb2 29. Qb3 Qxb3 30. Nxb3 Rc6 31. Bd2 Be6 32. Na5 Ra6\n33. Nb7 Ra2 34. Be3 Rc2 35. Rc1 Rxc1+ 36. Bxc1 b4 37. cxb4 Bxb4 38. Nc5 Bd5\n39. Be3 f6 40. Nd3 Bd6 41. Bf4 Ba3 42. Bc1 Bd6 43. Bf4 Ba3 44. Bc1 Bd6\n45. Bf4 Ba3 46. Bc1 Bd6 47. Bf4 Ba3 48. Bc1 Bd6 49. Bf4 Ba3 50. Bc1 Bd6\n51. Bf4 Ba3 52. Bc1 Bd6 53. Bf4 Ba3 54. Bc1 Bd6 55. Bf4 Ba3 56. Bc1 Bd6\n57. Bf4 Ba3 58. Bc1 Bd6 59. Bf4 Ba3 60. Bc1 Bd6 61. Bf4 Ba3 62. Bc1 Bd6\n63. Bf4 Ba3 64. Bc1 Bd6 65. Bf4 Ba3 66. Bc1 Bd6 67. Bf4 Ba3 68. Bc1 Bd6\n69. Bf4 Ba3 70. Bc1 Bd6 71. Bf4 Ba3 72. Bc1 Bd6 73. Bf4 Ba3 74. Bc1 Bd6\n75. Bf4 Ba3 76. Bc1 Bd6 77. Bf4 Ba3 78. Bc1 Bd6 79. Bf4 Ba3 80. Bc1 Bd6\n81. Bf4 Ba3",
          "neutral_tokens": {
            "prompt_tokens": 121,
            "completion_tokens": 640,
            "total_tokens": 761,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "O-O",
            "c3",
            "d5",
            "exd5",
            "Nxd5",
            "Nxe5",
            "Nxe5",
            "Rxe5",
            "c6",
            "d4",
            "Bd6",
            "Re1",
            "Qh4",
            "g3",
            "Qh3",
            "Be3",
            "Bg4",
            "Qd3",
            "Rae8",
            "Nd2",
            "Re6",
            "Qf1",
            "Qh5",
            "a4",
            "Rfe8",
            "axb5",
            "axb5",
            "Bxd5",
            "Qxd5",
            "Qg2",
            "Qh5",
            "Ra6",
            "Bf8",
            "Rxc6",
            "Bh3",
            "Qh1",
            "Qf5",
            "Rxe6",
            "Rxe6",
            "Qf3",
            "Qc2",
            "Qd1",
            "Qxb2",
            "Qb3",
            "Qxb3",
            "Nxb3",
            "Rc6",
            "Bd2",
            "Be6",
            "Na5",
            "Ra6",
            "Nb7",
            "Ra2",
            "Be3",
            "Rc2",
            "Rc1",
            "Rxc1+",
            "Bxc1",
            "b4",
            "cxb4",
            "Bxb4",
            "Nc5",
            "Bd5",
            "Be3",
            "f6",
            "Nd3",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b8c6",
          "positional_uci": "b8c6",
          "neutral_uci": "b8c6",
          "consensus_move": "b8c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "O-O",
              "c3",
              "d5",
              "exd5",
              "Nxd5",
              "Nxe5",
              "Nxe5",
              "Rxe5",
              "c6",
              "d4",
              "Bd6",
              "Re1",
              "Qh4",
              "g3",
              "Qh3",
              "Be3",
              "Bg4",
              "Qd3",
              "Rae8",
              "Nd2",
              "Re6",
              "Qf1",
              "Qh5",
              "a4",
              "Rfe8",
              "axb5",
              "axb5",
              "Bxd5",
              "Qxd5",
              "Qg2",
              "Qh5",
              "Qxc6",
              "Bxg3",
              "Qg2",
              "Bf4",
              "Ra8",
              "Bb8",
              "Rea1",
              "Rg6",
              "R1a6",
              "Be6",
              "Qxg6",
              "Qxg6+",
              "Kf1",
              "Qd3+",
              "Ke1",
              "Bg4",
              "f3",
              "Qxe3+",
              "Kd1",
              "Bxf3+",
              "Nxf3",
              "Qxf3+",
              "Kc2",
              "Qe2+",
              "Kb3",
              "Qc4+",
              "Ka3",
              "b4+",
              "cxb4",
              "Re3+",
              "Ka4",
              "Qb3+",
              "Kb5",
              "Qd5+",
              "Ka4",
              "Qd7+",
              "b5",
              "Qxd4+",
              "b4",
              "Qa1#",
              "Bb7",
              "Rxa8#",
              "Rxa8#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "O-O",
              "c3",
              "d5",
              "exd5",
              "Nxd5",
              "Nxe5",
              "Nxe5",
              "Rxe5",
              "c6",
              "d4",
              "Bd6",
              "Re1",
              "Qh4",
              "g3",
              "Qh3",
              "Be3",
              "Bg4",
              "Qd3",
              "Rae8",
              "Nd2",
              "Re6",
              "Qf1",
              "Qh5",
              "a4",
              "Rfe8",
              "axb5",
              "axb5",
              "Bxd5",
              "Qxd5",
              "Qg2",
              "Qh5",
              "Ra6",
              "Bf8",
              "Raa1",
              "h6",
              "Ra6",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "O-O",
              "c3",
              "d5",
              "exd5",
              "Nxd5",
              "Nxe5",
              "Nxe5",
              "Rxe5",
              "c6",
              "d4",
              "Bd6",
              "Re1",
              "Qh4",
              "g3",
              "Qh3",
              "Be3",
              "Bg4",
              "Qd3",
              "Rae8",
              "Nd2",
              "Re6",
              "Qf1",
              "Qh5",
              "a4",
              "Rfe8",
              "axb5",
              "axb5",
              "Bxd5",
              "Qxd5",
              "Qg2",
              "Qh5",
              "Ra6",
              "Bf8",
              "Rxc6",
              "Bh3",
              "Qh1",
              "Qf5",
              "Rxe6",
              "Rxe6",
              "Qf3",
              "Qc2",
              "Qd1",
              "Qxb2",
              "Qb3",
              "Qxb3",
              "Nxb3",
              "Rc6",
              "Bd2",
              "Be6",
              "Na5",
              "Ra6",
              "Nb7",
              "Ra2",
              "Be3",
              "Rc2",
              "Rc1",
              "Rxc1+",
              "Bxc1",
              "b4",
              "cxb4",
              "Bxb4",
              "Nc5",
              "Bd5",
              "Be3",
              "f6",
              "Nd3",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 122,
            "completion_tokens": 362,
            "total_tokens": 484,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 122,
            "completion_tokens": 640,
            "total_tokens": 762,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 121,
            "completion_tokens": 640,
            "total_tokens": 761,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 365,
          "total_completion_tokens": 1642,
          "total_tokens": 2007
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 122,
            "completion_tokens": 362,
            "total_tokens": 484,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 122,
            "completion_tokens": 640,
            "total_tokens": 762,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 121,
            "completion_tokens": 640,
            "total_tokens": 761,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R b KQkq - 1 2"
    },
    {
      "turn": 6,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf6",
          "aggressive_uci": "g8f6",
          "aggressive_response": "3... Nf6 4. d3 Bc5 5. O-O O-O 6. Nc3 d6 7. Bg5 h6 8. Bh4 Bg4 9. h3 Bh5 10. g4 Bg6 11. Nd5 Nd4 12. Nxd4 Bxd4 13. c3 Bb6 14. Qf3 c6 15. Nxf6+ gxf6 16. Bxf6 Qd7 17. h4 Bd8 18. h5 Bxf6 19. hxg6 Kg7 20. gxf7 Bg5 21. Qf5 Qxf5 22. gxf5 Kf6 23. Kg2 Rad8 24. Be6 d5 25. Rad1 Rd6 26. Kf3 Rfd8 27. exd5 cxd5 28. Kg4 d4 29. c4 Rb6 30. b3 Ra6 31. a4 Rb6 32. Rb1 Bd2 33. Rh1 Rh8 34. Rh5 Bg5 35. Kf3 Rd6 36. Ke4 Rdd8 37. Rbh1 Rh7 38. Rg1 Rg7 39. Rg4 Rh7 40. f4 exf4 41. Rxf4 Bxf4 42. Kxf4 Rg7 43. Rxh6+ Ke7 44. f6+ Kxe6 45. fxg7+ Kxf7 46. Rh8 Rg8 47. Rxg8 Kxg8 48. Ke5 Kxg7 49. Kxd4 Kf7 50. Kd5 Ke7 51. d4 Kd7 52. b4 Kc7 53. c5 Kd7 54. b5 Kc7 55. a5 Kd7 56. c6+ bxc6+ 57. bxc6+ Kc7 58. Kc5 a6 59. d5 Kc8 60. d6 Kd8 61. Kb6 Kc8 62. Kxa6 Kb8 63. Kb6 Kc8 64. a6 Kb8 65. d7 Ka8 66. d8=Q# Kxd8 67. a7 Kc8 68. a8=Q# Kd7 69. c7 Kd6 70. c8=Q Ke5 71. Qac6 Kd4 72. Q8d7+ Ke5 73. Qce6+ Kf4 74. Qdf7+ Kg5 75. Qeg6+ Kh4 76. Qfh7# Kxg6 77. Qf7+ Kg5 78. Kc5 Kg4 79. K",
          "aggressive_tokens": {
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d3",
            "Bc5",
            "O-O",
            "O-O",
            "Nc3",
            "d6",
            "Bg5",
            "h6",
            "Bh4",
            "Bg4",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Nd5",
            "Nd4",
            "Nxd4",
            "Bxd4",
            "c3",
            "Bb6",
            "Qf3",
            "c6",
            "Nxf6+",
            "gxf6",
            "Bxf6",
            "Qd7",
            "h4",
            "Bd8",
            "h5",
            "Bxf6",
            "hxg6",
            "Kg7",
            "gxf7",
            "Bg5",
            "Qf5",
            "Qxf5",
            "gxf5",
            "Kf6",
            "Kg2",
            "Rad8",
            "Be6",
            "d5",
            "Rad1",
            "Rd6",
            "Kf3",
            "Rfd8",
            "exd5",
            "cxd5",
            "Kg4",
            "d4",
            "c4",
            "Rb6",
            "b3",
            "Ra6",
            "a4",
            "Rb6",
            "Rb1",
            "Bd2",
            "Rh1",
            "Rh8",
            "Rh5",
            "Bg5",
            "Kf3",
            "Rd6",
            "Ke4",
            "Rdd8",
            "Rbh1",
            "Rh7",
            "Rg1",
            "Rg7",
            "Rg4",
            "Rh7",
            "f4",
            "exf4",
            "Rxf4",
            "Bxf4",
            "Kxf4",
            "Rg7",
            "Rxh6+",
            "Ke7",
            "f6+",
            "Kxe6",
            "fxg7+",
            "Kxf7",
            "Rh8",
            "Rg8",
            "Rxg8",
            "Kxg8",
            "Ke5",
            "Kxg7",
            "Kxd4",
            "Kf7",
            "Kd5",
            "Ke7",
            "d4",
            "Kd7",
            "b4",
            "Kc7",
            "c5",
            "Kd7",
            "b5",
            "Kc7",
            "a5",
            "Kd7",
            "c6+",
            "bxc6+",
            "bxc6+",
            "Kc7",
            "Kc5",
            "a6",
            "d5",
            "Kc8",
            "d6",
            "Kd8",
            "Kb6",
            "Kc8",
            "Kxa6",
            "Kb8",
            "Kb6",
            "Kc8",
            "a6",
            "Kb8",
            "d7",
            "Ka8",
            "d8=Q#",
            "Kxd8",
            "a7",
            "Kc8",
            "a8=Q#",
            "Kd7",
            "c7",
            "Kd6",
            "c8=Q",
            "Ke5",
            "Qac6",
            "Kd4",
            "Q8d7+",
            "Ke5",
            "Qce6+",
            "Kf4",
            "Qdf7+",
            "Kg5",
            "Qeg6+",
            "Kh4",
            "Qfh7#",
            "Kxg6",
            "Qf7+",
            "Kg5",
            "Kc5",
            "Kg4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf6",
          "positional_uci": "g8f6",
          "positional_response": "3... Nf6 4. d3 Bc5 5. O-O O-O 6. Nc3 d6 7. Bg5 Be6 8. Nd5 Bxd5 9. Bxd5 h6 10. Bh4 g5 11. Bg3 Nxd5 12. exd5 Ne7 13. c4 f5 14. h3 f4 15. Bh2 Nf5 16. a3 a5 17. Qd2 Qf6 18. b4 axb4 19. axb4 Rxa1 20. Rxa1 e4 21. dxe4 Qxa1+ 22. Ne1 Ng3 23. bxc5 Nxe4 24. Qe2 Re8 25. cxd6 Nxd6 26. Qxe8+ Nxe8 27. Kf1 Nd6 28. c5 Ne4 29. Ke2 Qb2+ 30. Kf3 Nd2+ 31. Kg4 Qf6 32. h4 Kg7 33. hxg5 hxg5 34. Nf3 Nxf3 35. Kxf3 Qc3+ 36. Kg4 Kg6 37. g3 Qxc5 38. gxf4 Qxf2 39. Bg3 Qe2+ 40. Kh3 g4+ 41. Kh4 Kf5 42. Kh5 Qe8+ 43. Kh6 Qg6# 0-1\n1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. d3 Bc5 5. O-O O-O 6. Nc3 d6 7. Bg5 Be6 8. Nd5 Bxd5 9. Bxd5 h6 10. Bh4 g5 11. Bg3 Nxd5 12. exd5 Ne7 13. c4 f5 14. h3 f4 15. Bh2 Nf5 16. a3 a5 17. Qd2 Qf6 18. b4 axb4 19. axb4 Rxa1 20. Rxa1 e4 21. dxe4 Qxa1+ 22. Ne1 Ng3 23. bxc5 Nxe4 24. Qe2 Re8 25. cxd6 Nxd6 26. Qxe8+ Nxe8 27. Kf1 Nd6 28. c5 Ne4 29. Ke2 Qb2+ 30. Kf3 Nd2+ 31. Kg4 Qf6 32. h4 Kg7 33. hxg5 hxg5 34. Nf3 Nxf3 35. Kxf3 Qc3+ 36. Kg4 Kg6 37. g",
          "positional_tokens": {
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d3",
            "Bc5",
            "O-O",
            "O-O",
            "Nc3",
            "d6",
            "Bg5",
            "Be6",
            "Nd5",
            "Bxd5",
            "Bxd5",
            "h6",
            "Bh4",
            "g5",
            "Bg3",
            "Nxd5",
            "exd5",
            "Ne7",
            "c4",
            "f5",
            "h3",
            "f4",
            "Bh2",
            "Nf5",
            "a3",
            "a5",
            "Qd2",
            "Qf6",
            "b4",
            "axb4",
            "axb4",
            "Rxa1",
            "Rxa1",
            "e4",
            "dxe4",
            "Qxa1+",
            "Ne1",
            "Ng3",
            "bxc5",
            "Nxe4",
            "Qe2",
            "Re8",
            "cxd6",
            "Nxd6",
            "Qxe8+",
            "Nxe8",
            "Kf1",
            "Nd6",
            "c5",
            "Ne4",
            "Ke2",
            "Qb2+",
            "Kf3",
            "Nd2+",
            "Kg4",
            "Qf6",
            "h4",
            "Kg7",
            "hxg5",
            "hxg5",
            "Nf3",
            "Nxf3",
            "Kxf3",
            "Qc3+",
            "Kg4",
            "Kg6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf6",
          "neutral_uci": "g8f6",
          "neutral_response": "3... Nf6 4. d3 Bc5 5. O-O O-O 6. Nc3 d6 7. Bg5 h6 8. Bh4 g5 9. Nxg5 hxg5 10. Bxg5 Be6 11. Qf3 Kg7 12. Qg3 Nh5 13. Qh4 f6 14. Be3 Bxc4 15. dxc4 Rh8 16. Qg4+ Kf7 17. Nd5 Qc8 18. Qf3 Nd4 19. Bxd4 Bxd4 20. c3 Bb6 21. g4 Qg8 22. Kh1 Nf4 23. Nxf4 exf4 24. Qxf4 Qg5 25. Qg3 Rag8 26. f3 Rh4 27. Rad1 Rgh8 28. Rd5 Qe3 29. Rh5 R4xh5 30. gxh5 Rxh5 31. Qg4 Qg5 32. Qd7+ Kg6 33. Qe8+ Kh6 34. Qh8+ Kg6 35. Qg8+ Kh6 36. Qh8+ Kg6 37. Qg8+ Kh6 38. Qh8+ Kg6 39. Qg8+ Kh6 40. Qh8+ Kg6 41. Qg8+ Kh6 42. Qh8+ Kg6 43. Qg8+ Kh6 44. Qh8+ Kg6 45. Qg8+ Kh6 46. Qh8+ Kg6 47. Qg8+ Kh6 48. Qh8+ Kg6 49. Qg8+ Kh6 50. Qh8+ Kg6 51. Qg8+ Kh6 52. Qh8+ Kg6 53. Qg8+ Kh6 54. Qh8+ Kg6 55. Qg8+ Kh6 56. Qh8+ Kg6 57. Qg8+ Kh6 58. Qh8+ Kg6 59. Qg8+ Kh6 60. Qh8+ Kg6 61. Qg8+ Kh6 62. Qh8+ Kg6 63. Qg8+ Kh6 64. Qh8+ Kg6 65. Qg8+ Kh6 66. Qh8+ Kg6 67. Qg8+ Kh6 68. Qh8+ Kg6 69. Qg8+ Kh6 70. Qh8+ Kg6 71. Qg8+ Kh6 72. Qh8+ Kg6 73. Qg8+ Kh6 74. Qh8+ Kg6 75. Qg8+ Kh6 76. Qh8",
          "neutral_tokens": {
            "prompt_tokens": 130,
            "completion_tokens": 640,
            "total_tokens": 770,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d3",
            "Bc5",
            "O-O",
            "O-O",
            "Nc3",
            "d6",
            "Bg5",
            "h6",
            "Bh4",
            "g5",
            "Nxg5",
            "hxg5",
            "Bxg5",
            "Be6",
            "Qf3",
            "Kg7",
            "Qg3",
            "Nh5",
            "Qh4",
            "f6",
            "Be3",
            "Bxc4",
            "dxc4",
            "Rh8",
            "Qg4+",
            "Kf7",
            "Nd5",
            "Qc8",
            "Qf3",
            "Nd4",
            "Bxd4",
            "Bxd4",
            "c3",
            "Bb6",
            "g4",
            "Qg8",
            "Kh1",
            "Nf4",
            "Nxf4",
            "exf4",
            "Qxf4",
            "Qg5",
            "Qg3",
            "Rag8",
            "f3",
            "Rh4",
            "Rad1",
            "Rgh8",
            "Rd5",
            "Qe3",
            "Rh5",
            "R4xh5",
            "gxh5",
            "Rxh5",
            "Qg4",
            "Qg5",
            "Qd7+",
            "Kg6",
            "Qe8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g8f6",
          "positional_uci": "g8f6",
          "neutral_uci": "g8f6",
          "consensus_move": "g8f6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d3",
              "Bc5",
              "O-O",
              "O-O",
              "Nc3",
              "d6",
              "Bg5",
              "h6",
              "Bh4",
              "Bg4",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Nd5",
              "Nd4",
              "Nxd4",
              "Bxd4",
              "c3",
              "Bb6",
              "Qf3",
              "c6",
              "Nxf6+",
              "gxf6",
              "Bxf6",
              "Qd7",
              "h4",
              "Bd8",
              "h5",
              "Bxf6",
              "hxg6",
              "Kg7",
              "gxf7",
              "Bg5",
              "Qf5",
              "Qxf5",
              "gxf5",
              "Kf6",
              "Kg2",
              "Rad8",
              "Be6",
              "d5",
              "Rad1",
              "Rd6",
              "Kf3",
              "Rfd8",
              "exd5",
              "cxd5",
              "Kg4",
              "d4",
              "c4",
              "Rb6",
              "b3",
              "Ra6",
              "a4",
              "Rb6",
              "Rb1",
              "Bd2",
              "Rh1",
              "Rh8",
              "Rh5",
              "Bg5",
              "Kf3",
              "Rd6",
              "Ke4",
              "Rdd8",
              "Rbh1",
              "Rh7",
              "Rg1",
              "Rg7",
              "Rg4",
              "Rh7",
              "f4",
              "exf4",
              "Rxf4",
              "Bxf4",
              "Kxf4",
              "Rg7",
              "Rxh6+",
              "Ke7",
              "f6+",
              "Kxe6",
              "fxg7+",
              "Kxf7",
              "Rh8",
              "Rg8",
              "Rxg8",
              "Kxg8",
              "Ke5",
              "Kxg7",
              "Kxd4",
              "Kf7",
              "Kd5",
              "Ke7",
              "d4",
              "Kd7",
              "b4",
              "Kc7",
              "c5",
              "Kd7",
              "b5",
              "Kc7",
              "a5",
              "Kd7",
              "c6+",
              "bxc6+",
              "bxc6+",
              "Kc7",
              "Kc5",
              "a6",
              "d5",
              "Kc8",
              "d6",
              "Kd8",
              "Kb6",
              "Kc8",
              "Kxa6",
              "Kb8",
              "Kb6",
              "Kc8",
              "a6",
              "Kb8",
              "d7",
              "Ka8",
              "d8=Q#",
              "Kxd8",
              "a7",
              "Kc8",
              "a8=Q#",
              "Kd7",
              "c7",
              "Kd6",
              "c8=Q",
              "Ke5",
              "Qac6",
              "Kd4",
              "Q8d7+",
              "Ke5",
              "Qce6+",
              "Kf4",
              "Qdf7+",
              "Kg5",
              "Qeg6+",
              "Kh4",
              "Qfh7#",
              "Kxg6",
              "Qf7+",
              "Kg5",
              "Kc5",
              "Kg4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "d3",
              "Bc5",
              "O-O",
              "O-O",
              "Nc3",
              "d6",
              "Bg5",
              "Be6",
              "Nd5",
              "Bxd5",
              "Bxd5",
              "h6",
              "Bh4",
              "g5",
              "Bg3",
              "Nxd5",
              "exd5",
              "Ne7",
              "c4",
              "f5",
              "h3",
              "f4",
              "Bh2",
              "Nf5",
              "a3",
              "a5",
              "Qd2",
              "Qf6",
              "b4",
              "axb4",
              "axb4",
              "Rxa1",
              "Rxa1",
              "e4",
              "dxe4",
              "Qxa1+",
              "Ne1",
              "Ng3",
              "bxc5",
              "Nxe4",
              "Qe2",
              "Re8",
              "cxd6",
              "Nxd6",
              "Qxe8+",
              "Nxe8",
              "Kf1",
              "Nd6",
              "c5",
              "Ne4",
              "Ke2",
              "Qb2+",
              "Kf3",
              "Nd2+",
              "Kg4",
              "Qf6",
              "h4",
              "Kg7",
              "hxg5",
              "hxg5",
              "Nf3",
              "Nxf3",
              "Kxf3",
              "Qc3+",
              "Kg4",
              "Kg6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "d3",
              "Bc5",
              "O-O",
              "O-O",
              "Nc3",
              "d6",
              "Bg5",
              "h6",
              "Bh4",
              "g5",
              "Nxg5",
              "hxg5",
              "Bxg5",
              "Be6",
              "Qf3",
              "Kg7",
              "Qg3",
              "Nh5",
              "Qh4",
              "f6",
              "Be3",
              "Bxc4",
              "dxc4",
              "Rh8",
              "Qg4+",
              "Kf7",
              "Nd5",
              "Qc8",
              "Qf3",
              "Nd4",
              "Bxd4",
              "Bxd4",
              "c3",
              "Bb6",
              "g4",
              "Qg8",
              "Kh1",
              "Nf4",
              "Nxf4",
              "exf4",
              "Qxf4",
              "Qg5",
              "Qg3",
              "Rag8",
              "f3",
              "Rh4",
              "Rad1",
              "Rgh8",
              "Rd5",
              "Qe3",
              "Rh5",
              "R4xh5",
              "gxh5",
              "Rxh5",
              "Qg4",
              "Qg5",
              "Qd7+",
              "Kg6",
              "Qe8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 130,
            "completion_tokens": 640,
            "total_tokens": 770,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 392,
          "total_completion_tokens": 1920,
          "total_tokens": 2312
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 130,
            "completion_tokens": 640,
            "total_tokens": 770,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bqkbnr/pppp1ppp/2n5/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R b KQkq - 3 3"
    },
    {
      "turn": 8,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d5",
          "aggressive_uci": "d7d5",
          "aggressive_response": "4... d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. Qf3+ Ke6 8. Nc3 Ne7 9. d4 c6 10. O-O b5 11. Bb3 a5 12. Re1 a4 13. Rxe5+ Kd6 14. Bxd5 cxd5 15. Nxd5 Bb7 16. Bf4 Kd7 17. Qg4+ Ke8 18. Nxe7 Bxe7 19. Qxg7 Rf8 20. Rae1 Rf7 21. Qg8+ Rf8 22. Qxh7 Rf7 23. Qg8+ Rf8 24. Qg6+ Rf7 25. Bg5 Ra6 26. Rxe7+ Rxe7 27. Rxe7+ Qxe7 28. Qg8+ Qf8 29. Qh7 Qf7 30. Qh8+ Qf8 31. Qe5+ Kd7 32. Qxb5+ Kc7 33. Qe5+ Qd6 34. Qg7+ Qd7 35. Qe5+ Qd6 36. Qg7+ Qd7 37. Qe5+ Qd6 38. Qg7+ Qd7 39. Qe5+ Qd6 40. Qg7+ Qd7 41. Qe5+ Qd6 42. Qg7+ Qd7 43. Qe5+ Qd6 44. Qg7+ Qd7 45. Qe5+ Qd6 46. Qg7+ Qd7 47. Qe5+ Qd6 48. Qg7+ Qd7 49. Qe5+ Qd6 50. Qg7+ Qd7 51. Qe5+ Qd6 52. Qg7+ Qd7 53. Qe5+ Qd6 54. Qg7+ Qd7 55. Qe5+ Qd6 56. Qg7+ Qd7 57. Qe5+ Qd6 58. Qg7+ Qd7 59. Qe5+ Qd6 60. Qg7+ Qd7 61. Qe5+ Qd6 62. Qg7+ Qd7 63. Qe5+ Qd6 64. Qg7+ Qd7 65. Qe5+ Qd6 66. Qg7+ Qd7 67. Qe5+ Qd6 68. Qg7+ Qd7 69. Qe5+ Qd6 70. Qg7+ Qd7",
          "aggressive_tokens": {
            "prompt_tokens": 139,
            "completion_tokens": 640,
            "total_tokens": 779,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "exd5",
            "Nxd5",
            "Nxf7",
            "Kxf7",
            "Qf3+",
            "Ke6",
            "Nc3",
            "Ne7",
            "d4",
            "c6",
            "O-O",
            "b5",
            "Bb3",
            "a5",
            "Re1",
            "a4",
            "Rxe5+",
            "Kd6",
            "Bxd5",
            "cxd5",
            "Nxd5",
            "Bb7",
            "Bf4",
            "Kd7",
            "Qg4+",
            "Ke8",
            "Nxe7",
            "Bxe7",
            "Qxg7",
            "Rf8",
            "Rae1",
            "Rf7",
            "Qg8+",
            "Rf8",
            "Qxh7",
            "Rf7",
            "Qg8+",
            "Rf8",
            "Qg6+",
            "Rf7",
            "Bg5",
            "Ra6",
            "Rxe7+",
            "Rxe7",
            "Rxe7+",
            "Qxe7",
            "Qg8+",
            "Qf8",
            "Qh7",
            "Qf7",
            "Qh8+",
            "Qf8",
            "Qe5+",
            "Kd7",
            "Qxb5+",
            "Kc7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d5",
          "positional_uci": "d7d5",
          "positional_response": "4... d5 5. exd5 Na5 6. Bb5+ c6 7. dxc6 bxc6 8. Qf3 Be7 9. Bxc6+ Nxc6 10. Qxc6+ Bd7 11. Qf3 O-O 12. O-O Rc8 13. Nc3 Bc6 14. Qe2 Nd5 15. Nxd5 Qxd5 16. Nf3 e4 17. Ne1 Bb5 18. d3 exd3 19. Nxd3 Rfe8 20. Qd1 Bf6 21. a4 Ba6 22. Ra3 Qc4 23. c3 Qxd3 24. Qxd3 Bxd3 25. Rd1 Bc2 26. Rf1 Bd3 27. Rd1 Bc2 28. Rf1 Bd3 29. Rd1 Bc2 30. Rf1 Bd3 31. Rd1 Bc2 32. Rf1 Bd3 33. Rd1 Bc2 34. Rf1 Bd3 35. Rd1 Bc2 36. Rf1 Bd3 37. Rd1 Bc2 38. Rf1 Bd3 39. Rd1 Bc2 40. Rf1 Bd3 41. Rd1 Bc2 42. Rf1 Bd3 43. Rd1 Bc2 44. Rf1 Bd3 45. Rd1 Bc2 46. Rf1 Bd3 47. Rd1 Bc2 48. Rf1 Bd3 49. Rd1 Bc2 50. Rf1 Bd3 51. Rd1 Bc2 52. Rf1 Bd3 53. Rd1 Bc2 54. Rf1 Bd3 55. Rd1 Bc2 56. Rf1 Bd3 57. Rd1 Bc2 58. Rf1 Bd3 59. Rd1 Bc2 60. Rf1 Bd3 61. Rd1 Bc2 62. Rf1 Bd3 63. Rd1 Bc2 64. Rf1 Bd3 65. Rd1 Bc2 66. Rf1 Bd3 67. Rd1 Bc2 68. Rf1 Bd3 69. Rd1 Bc2 70. Rf1 Bd3 71. Rd1 Bc2 72. Rf1 Bd3 73. Rd1 Bc2 74. Rf1 Bd3 75. Rd1 Bc2 76. Rf1 Bd3 77. Rd1 Bc2 78. Rf1 Bd3 79. Rd1 Bc2 80. Rf1 Bd3 81. Rd1 Bc2 82. Rf1 Bd3 83.",
          "positional_tokens": {
            "prompt_tokens": 139,
            "completion_tokens": 640,
            "total_tokens": 779,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "exd5",
            "Na5",
            "Bb5+",
            "c6",
            "dxc6",
            "bxc6",
            "Qf3",
            "Be7",
            "Bxc6+",
            "Nxc6",
            "Qxc6+",
            "Bd7",
            "Qf3",
            "O-O",
            "O-O",
            "Rc8",
            "Nc3",
            "Bc6",
            "Qe2",
            "Nd5",
            "Nxd5",
            "Qxd5",
            "Nf3",
            "e4",
            "Ne1",
            "Bb5",
            "d3",
            "exd3",
            "Nxd3",
            "Rfe8",
            "Qd1",
            "Bf6",
            "a4",
            "Ba6",
            "Ra3",
            "Qc4",
            "c3",
            "Qxd3",
            "Qxd3",
            "Bxd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d5",
          "neutral_uci": "d7d5",
          "neutral_response": "4... d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. Qf3+ Ke6 8. Nc3 Ne7 9. d4 c6 10. O-O b5 11. Bb3 a5 12. Re1 a4 13. Rxe5+ Kd6 14. Bxd5 cxd5 15. Nxd5 Bb7 16. Bf4 Ng6 17. Re8+ Nxf4 18. Rxd8+ Rxd8 19. Qxf4+ Kxd5 20. Qe5+ Kc6 21. c4 bxc4 22. Rc1 Ba6 23. Qa5 Ra8 24. Rxc4+ Bxc4 25. Qxa8+ Kb5 26. Qb7+ Ka5 27. Qc7+ Kb5 28. Qe5+ Kb4 29. Qb8+ Ka5 30. Qa8+ Kb4 31. Qb7+ Ka5 32. Qc7+ Kb4 33. Qb6+ Bb5 34. a3+ Kc4 35. Qe6+ Kxd4 36. Qb6+ Kc4 37. Qc7+ Kb3 38. Qc3+ Ka2 39. b4 axb3 40. Qd2+ b2 41. Qd5+ Kxa3 42. Qa8+ Kb3 43. Qd5+ Bc4 44. Qb7+ Kc2 45. Qe4+ Bd3 46. Qa4+ Kc1 47. Qf4+ Kd1 48. Qa4+ Bc2 49. Qd4+ Kc1 50. Qe3+ Kb1 51. Qe1+ Ka2 52. Qa5+ Ba3 53. Qd5+ Bb3 54. Qd2 Ka1 55. Qc3 Ka2 56. Qd2 Ka1 57. Qc3 Ka2 58. Qd2 Ka1 59. Qc3 Ka2 60. Qd2 Ka1 61. Qc3 Ka2 62. Qd2 Ka1 63. Qc3 Ka2 64. Qd2 Ka1 65. Qc3 Ka2 66. Qd2 Ka1 67. Qc3 Ka2 68. Qd2 Ka1 69. Qc3 Ka2 70. Qd2 Ka1 71. Qc3 Ka2 72. Qd2 Ka1 73. Qc3 Ka2 74. Qd2 Ka1 75. Qc3 Ka2 76. Qd2 Ka1 77. Qc3 Ka2",
          "neutral_tokens": {
            "prompt_tokens": 138,
            "completion_tokens": 640,
            "total_tokens": 778,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "exd5",
            "Nxd5",
            "Nxf7",
            "Kxf7",
            "Qf3+",
            "Ke6",
            "Nc3",
            "Ne7",
            "d4",
            "c6",
            "O-O",
            "b5",
            "Bb3",
            "a5",
            "Re1",
            "a4",
            "Rxe5+",
            "Kd6",
            "Bxd5",
            "cxd5",
            "Nxd5",
            "Bb7",
            "Bf4",
            "Ng6",
            "Re8+",
            "Nxf4",
            "Rxd8+",
            "Rxd8",
            "Qxf4+",
            "Kxd5",
            "Qe5+",
            "Kc6",
            "c4",
            "bxc4",
            "Rc1",
            "Ba6",
            "Qa5",
            "Ra8",
            "Rxc4+",
            "Bxc4",
            "Qxa8+",
            "Kb5",
            "Qb7+",
            "Ka5",
            "Qc7+",
            "Kb5",
            "Qe5+",
            "Kb4",
            "Qb8+",
            "Ka5",
            "Qa8+",
            "Kb4",
            "Qb7+",
            "Ka5",
            "Qc7+",
            "Kb4",
            "Qb6+",
            "Bb5",
            "a3+",
            "Kc4",
            "Qe6+",
            "Kxd4",
            "Qb6+",
            "Kc4",
            "Qc7+",
            "Kb3",
            "Qc3+",
            "Ka2",
            "b4",
            "axb3",
            "Qd2+",
            "b2",
            "Qd5+",
            "Kxa3",
            "Qa8+",
            "Kb3",
            "Qd5+",
            "Bc4",
            "Qb7+",
            "Kc2",
            "Qe4+",
            "Bd3",
            "Qa4+",
            "Kc1",
            "Qf4+",
            "Kd1",
            "Qa4+",
            "Bc2",
            "Qd4+",
            "Kc1",
            "Qe3+",
            "Kb1",
            "Qe1+",
            "Ka2",
            "Qa5+",
            "Ba3",
            "Qd5+",
            "Bb3",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d7d5",
          "positional_uci": "d7d5",
          "neutral_uci": "d7d5",
          "consensus_move": "d7d5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "exd5",
              "Nxd5",
              "Nxf7",
              "Kxf7",
              "Qf3+",
              "Ke6",
              "Nc3",
              "Ne7",
              "d4",
              "c6",
              "O-O",
              "b5",
              "Bb3",
              "a5",
              "Re1",
              "a4",
              "Rxe5+",
              "Kd6",
              "Bxd5",
              "cxd5",
              "Nxd5",
              "Bb7",
              "Bf4",
              "Kd7",
              "Qg4+",
              "Ke8",
              "Nxe7",
              "Bxe7",
              "Qxg7",
              "Rf8",
              "Rae1",
              "Rf7",
              "Qg8+",
              "Rf8",
              "Qxh7",
              "Rf7",
              "Qg8+",
              "Rf8",
              "Qg6+",
              "Rf7",
              "Bg5",
              "Ra6",
              "Rxe7+",
              "Rxe7",
              "Rxe7+",
              "Qxe7",
              "Qg8+",
              "Qf8",
              "Qh7",
              "Qf7",
              "Qh8+",
              "Qf8",
              "Qe5+",
              "Kd7",
              "Qxb5+",
              "Kc7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "exd5",
              "Na5",
              "Bb5+",
              "c6",
              "dxc6",
              "bxc6",
              "Qf3",
              "Be7",
              "Bxc6+",
              "Nxc6",
              "Qxc6+",
              "Bd7",
              "Qf3",
              "O-O",
              "O-O",
              "Rc8",
              "Nc3",
              "Bc6",
              "Qe2",
              "Nd5",
              "Nxd5",
              "Qxd5",
              "Nf3",
              "e4",
              "Ne1",
              "Bb5",
              "d3",
              "exd3",
              "Nxd3",
              "Rfe8",
              "Qd1",
              "Bf6",
              "a4",
              "Ba6",
              "Ra3",
              "Qc4",
              "c3",
              "Qxd3",
              "Qxd3",
              "Bxd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "exd5",
              "Nxd5",
              "Nxf7",
              "Kxf7",
              "Qf3+",
              "Ke6",
              "Nc3",
              "Ne7",
              "d4",
              "c6",
              "O-O",
              "b5",
              "Bb3",
              "a5",
              "Re1",
              "a4",
              "Rxe5+",
              "Kd6",
              "Bxd5",
              "cxd5",
              "Nxd5",
              "Bb7",
              "Bf4",
              "Ng6",
              "Re8+",
              "Nxf4",
              "Rxd8+",
              "Rxd8",
              "Qxf4+",
              "Kxd5",
              "Qe5+",
              "Kc6",
              "c4",
              "bxc4",
              "Rc1",
              "Ba6",
              "Qa5",
              "Ra8",
              "Rxc4+",
              "Bxc4",
              "Qxa8+",
              "Kb5",
              "Qb7+",
              "Ka5",
              "Qc7+",
              "Kb5",
              "Qe5+",
              "Kb4",
              "Qb8+",
              "Ka5",
              "Qa8+",
              "Kb4",
              "Qb7+",
              "Ka5",
              "Qc7+",
              "Kb4",
              "Qb6+",
              "Bb5",
              "a3+",
              "Kc4",
              "Qe6+",
              "Kxd4",
              "Qb6+",
              "Kc4",
              "Qc7+",
              "Kb3",
              "Qc3+",
              "Ka2",
              "b4",
              "axb3",
              "Qd2+",
              "b2",
              "Qd5+",
              "Kxa3",
              "Qa8+",
              "Kb3",
              "Qd5+",
              "Bc4",
              "Qb7+",
              "Kc2",
              "Qe4+",
              "Bd3",
              "Qa4+",
              "Kc1",
              "Qf4+",
              "Kd1",
              "Qa4+",
              "Bc2",
              "Qd4+",
              "Kc1",
              "Qe3+",
              "Kb1",
              "Qe1+",
              "Ka2",
              "Qa5+",
              "Ba3",
              "Qd5+",
              "Bb3",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 139,
            "completion_tokens": 640,
            "total_tokens": 779,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 139,
            "completion_tokens": 640,
            "total_tokens": 779,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 138,
            "completion_tokens": 640,
            "total_tokens": 778,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 416,
          "total_completion_tokens": 1920,
          "total_tokens": 2336
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 139,
            "completion_tokens": 640,
            "total_tokens": 779,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 139,
            "completion_tokens": 640,
            "total_tokens": 779,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 138,
            "completion_tokens": 640,
            "total_tokens": 778,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bqkb1r/pppp1ppp/2n2n2/4p1N1/2B1P3/8/PPPP1PPP/RNBQK2R b KQkq - 5 4"
    },
    {
      "turn": 10,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nxd5",
          "aggressive_uci": "f6d5",
          "aggressive_response": "5... Nxd5 6. Nxf7 Kxf7 7. Qf3+ Ke6 8. Nc3 Ne7 9. d4 c6 10. O-O b5 11. Bb3 a5 12. Re1 a4 13. Rxe5+ Kd6 14. Bxd5 Nxd5 15. Nxd5 cxd5 16. Qxd5+ Kc7 17. Qxa8 Qxd4 18. Qa5+ Kd6 19. Re1 Bd7 20. Be3 Qxb2 21. Rad1+ Kc6 22. Qb6# 1-0\n22... Bb4 23. Bf4 Bxe1 24. Rd6#",
          "aggressive_tokens": {
            "prompt_tokens": 147,
            "completion_tokens": 174,
            "total_tokens": 321,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "cxd5",
            "Qxd5+",
            "Kc7",
            "Qxa8",
            "Qxd4",
            "Qa5+",
            "Kd6",
            "Re1",
            "Bd7",
            "Be3",
            "Qxb2",
            "Rad1+",
            "Kc6",
            "Qb6#",
            "Bb4",
            "Bf4",
            "Bxe1",
            "Rd6#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d6",
          "positional_uci": null,
          "positional_response": "d6 Nxd5 O-O Be6 d3 Bd6 Nc3 Nxc3 bxc3 O-O Bxe6 fxe6 Ng5 Qe7 Qg4 Rf6 Ne4 Rg6 Qh3 Nd7 Be3 Nf6 Nxf6+ Qxf6 Rab1 b6 d4 exd4 cxd4 Rf8 c4 c5 dxc5 Bxc5 Bxc5 bxc5 Rb7 Qd4 Rxa7 Qxc4 a4 Qd5 Qxd5 exd5 a5 Rc6 a6 c4 Rd7 c3 Rxd5 c2 Rc5 Rd1 a7 Ra8 Rxc2 Rxa7 Rxa7 Rxa7 g3 Kf8 Kg2 Ke8 h4 Kd8 g4 Kc8 Kg3 Kb8 Rxg7 Kc8 Rxh7 Kd8 g5 Ke8 g6 Kf8 h5 Kg8 h6 Kf8 Ra7 Kg8 Ra8#",
          "positional_tokens": {
            "prompt_tokens": 147,
            "completion_tokens": 215,
            "total_tokens": 362,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxd5",
            "O-O",
            "Be6",
            "d3",
            "Bd6",
            "Nc3",
            "Nxc3",
            "bxc3",
            "O-O",
            "Bxe6",
            "fxe6",
            "Ng5",
            "Qe7",
            "Qg4",
            "Rf6",
            "Ne4",
            "Rg6",
            "Qh3",
            "Nd7",
            "Be3",
            "Nf6",
            "Nxf6+",
            "Qxf6",
            "Rab1",
            "b6",
            "d4",
            "exd4",
            "cxd4",
            "Rf8",
            "c4",
            "c5",
            "dxc5",
            "Bxc5",
            "Bxc5",
            "bxc5",
            "Rb7",
            "Qd4",
            "Rxa7",
            "Qxc4",
            "a4",
            "Qd5",
            "Qxd5",
            "exd5",
            "a5",
            "Rc6",
            "a6",
            "c4",
            "Rd7",
            "c3",
            "Rxd5",
            "c2",
            "Rc5",
            "Rd1",
            "a7",
            "Ra8",
            "Rxc2",
            "Rxa7",
            "Rxa7",
            "Rxa7",
            "g3",
            "Kf8",
            "Kg2",
            "Ke8",
            "h4",
            "Kd8",
            "g4",
            "Kc8",
            "Kg3",
            "Kb8",
            "Rxg7",
            "Kc8",
            "Rxh7",
            "Kd8",
            "g5",
            "Ke8",
            "g6",
            "Kf8",
            "h5",
            "Kg8",
            "h6",
            "Kf8",
            "Ra7",
            "Kg8",
            "Ra8#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nxd5",
          "neutral_uci": "f6d5",
          "neutral_response": "5... Nxd5 6. Nxf7 Kxf7 7. Qf3+ Ke6 8. Nc3 Ne7 9. d4 c6 10. O-O b5 11. Bb3 a5 12. Re1 Kd7 13. Nxd5 cxd5 14. Bxd5 Ra6 15. Rxe5 Rf6 16. Qe2 Nxd5 17. Rxd5+ Rd6 18. Qxb5+ Ke6 19. Re5+ Kf7 20. Bg5 Qd7 21. Qxa5 Rxd4 22. Rae1 Rd1 23. Qc3 Rxe1+ 24. Rxe1 Qd5 25. Qc7+ Bd7 26. Qf4+ Kg8 27. c4 Qa5 28. Rd1 Qa4 29. b3 Qc6 30. Qd4 Be6 31. Qd8 h6 32. Be7 Kf7 33. Bxf8 Rxf8 34. Qd4 Kg8 35. h3 Qa6 36. Rd2 Qa5 37. Re2 Rd8 38. Qe5 Qxe5 39. Rxe5 Kf7 40. Re2 Rd1+ 41. Kh2 Rc1 42. Kg3 Rc3+ 43. Kf4 Kf6 44. Ke4 Bf5+ 45. Kd4 Rd3+ 46. Kc5 Rd8 47. Kb4 Rb8+ 48. Kc3 Ra8 49. a4 Bd7 50. c5 Bc6 51. f3 Bd5 52. Rd2 Ke6 53. Rd4 Rb8 54. b4 Ra8 55. a5 Rb8 56. Rg4 g5 57. h4 Kf5 58. hxg5 hxg5 59. Rd4 Ke5 60. Rg4 Kf5 61. Rd4 Ke5 62. Rd2 Bc6 63. Rd6 Rc8 64. Kc4 g4 65. fxg4 Bxg2 66. b5 Bf1+ 67. Kb4 Rb8 68. b6 Ba6 69. Rd7 Rg8 70. c6 Rxg4+ 71. Kc5 Rc4+ 72. Kd6 Rd4+ 73. Kc7 Rc4 74. b7 Bxb7 75. Kxb7 Rb4+ 76. Kc8 Ra4 77. Ra7 Kd6 78. c7 Rc4 79. Ra6+ Ke7 80. Kb7 Kd7 81. c8=Q+ Rxc8 82. Rd6",
          "neutral_tokens": {
            "prompt_tokens": 146,
            "completion_tokens": 640,
            "total_tokens": 786,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Rxd5+",
            "Rd6",
            "Qxb5+",
            "Ke6",
            "Re5+",
            "Kf7",
            "Bg5",
            "Qd7",
            "Qxa5",
            "Rxd4",
            "Rae1",
            "Rd1",
            "Qc3",
            "Rxe1+",
            "Rxe1",
            "Qd5",
            "Qc7+",
            "Bd7",
            "Qf4+",
            "Kg8",
            "c4",
            "Qa5",
            "Rd1",
            "Qa4",
            "b3",
            "Qc6",
            "Qd4",
            "Be6",
            "Qd8",
            "h6",
            "Be7",
            "Kf7",
            "Bxf8",
            "Rxf8",
            "Qd4",
            "Kg8",
            "h3",
            "Qa6",
            "Rd2",
            "Qa5",
            "Re2",
            "Rd8",
            "Qe5",
            "Qxe5",
            "Rxe5",
            "Kf7",
            "Re2",
            "Rd1+",
            "Kh2",
            "Rc1",
            "Kg3",
            "Rc3+",
            "Kf4",
            "Kf6",
            "Ke4",
            "Bf5+",
            "Kd4",
            "Rd3+",
            "Kc5",
            "Rd8",
            "Kb4",
            "Rb8+",
            "Kc3",
            "Ra8",
            "a4",
            "Bd7",
            "c5",
            "Bc6",
            "f3",
            "Bd5",
            "Rd2",
            "Ke6",
            "Rd4",
            "Rb8",
            "b4",
            "Ra8",
            "a5",
            "Rb8",
            "Rg4",
            "g5",
            "h4",
            "Kf5",
            "hxg5",
            "hxg5",
            "Rd4",
            "Ke5",
            "Rg4",
            "Kf5",
            "Rd4",
            "Ke5",
            "Rd2",
            "Bc6",
            "Rd6",
            "Rc8",
            "Kc4",
            "g4",
            "fxg4",
            "Bxg2",
            "b5",
            "Bf1+",
            "Kb4",
            "Rb8",
            "b6",
            "Ba6",
            "Rd7",
            "Rg8",
            "c6",
            "Rxg4+",
            "Kc5",
            "Rc4+",
            "Kd6",
            "Rd4+",
            "Kc7",
            "Rc4",
            "b7",
            "Bxb7",
            "Kxb7",
            "Rb4+",
            "Kc8",
            "Ra4",
            "Ra7",
            "Kd6",
            "c7",
            "Rc4",
            "Ra6+",
            "Ke7",
            "Kb7",
            "Kd7",
            "c8=Q+",
            "Rxc8",
            "Rd6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f6d5",
          "positional_uci": null,
          "neutral_uci": "f6d5",
          "consensus_move": "f6d5",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "cxd5",
              "Qxd5+",
              "Kc7",
              "Qxa8",
              "Qxd4",
              "Qa5+",
              "Kd6",
              "Re1",
              "Bd7",
              "Be3",
              "Qxb2",
              "Rad1+",
              "Kc6",
              "Qb6#",
              "Bb4",
              "Bf4",
              "Bxe1",
              "Rd6#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nxd5",
              "O-O",
              "Be6",
              "d3",
              "Bd6",
              "Nc3",
              "Nxc3",
              "bxc3",
              "O-O",
              "Bxe6",
              "fxe6",
              "Ng5",
              "Qe7",
              "Qg4",
              "Rf6",
              "Ne4",
              "Rg6",
              "Qh3",
              "Nd7",
              "Be3",
              "Nf6",
              "Nxf6+",
              "Qxf6",
              "Rab1",
              "b6",
              "d4",
              "exd4",
              "cxd4",
              "Rf8",
              "c4",
              "c5",
              "dxc5",
              "Bxc5",
              "Bxc5",
              "bxc5",
              "Rb7",
              "Qd4",
              "Rxa7",
              "Qxc4",
              "a4",
              "Qd5",
              "Qxd5",
              "exd5",
              "a5",
              "Rc6",
              "a6",
              "c4",
              "Rd7",
              "c3",
              "Rxd5",
              "c2",
              "Rc5",
              "Rd1",
              "a7",
              "Ra8",
              "Rxc2",
              "Rxa7",
              "Rxa7",
              "Rxa7",
              "g3",
              "Kf8",
              "Kg2",
              "Ke8",
              "h4",
              "Kd8",
              "g4",
              "Kc8",
              "Kg3",
              "Kb8",
              "Rxg7",
              "Kc8",
              "Rxh7",
              "Kd8",
              "g5",
              "Ke8",
              "g6",
              "Kf8",
              "h5",
              "Kg8",
              "h6",
              "Kf8",
              "Ra7",
              "Kg8",
              "Ra8#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Rxd5+",
              "Rd6",
              "Qxb5+",
              "Ke6",
              "Re5+",
              "Kf7",
              "Bg5",
              "Qd7",
              "Qxa5",
              "Rxd4",
              "Rae1",
              "Rd1",
              "Qc3",
              "Rxe1+",
              "Rxe1",
              "Qd5",
              "Qc7+",
              "Bd7",
              "Qf4+",
              "Kg8",
              "c4",
              "Qa5",
              "Rd1",
              "Qa4",
              "b3",
              "Qc6",
              "Qd4",
              "Be6",
              "Qd8",
              "h6",
              "Be7",
              "Kf7",
              "Bxf8",
              "Rxf8",
              "Qd4",
              "Kg8",
              "h3",
              "Qa6",
              "Rd2",
              "Qa5",
              "Re2",
              "Rd8",
              "Qe5",
              "Qxe5",
              "Rxe5",
              "Kf7",
              "Re2",
              "Rd1+",
              "Kh2",
              "Rc1",
              "Kg3",
              "Rc3+",
              "Kf4",
              "Kf6",
              "Ke4",
              "Bf5+",
              "Kd4",
              "Rd3+",
              "Kc5",
              "Rd8",
              "Kb4",
              "Rb8+",
              "Kc3",
              "Ra8",
              "a4",
              "Bd7",
              "c5",
              "Bc6",
              "f3",
              "Bd5",
              "Rd2",
              "Ke6",
              "Rd4",
              "Rb8",
              "b4",
              "Ra8",
              "a5",
              "Rb8",
              "Rg4",
              "g5",
              "h4",
              "Kf5",
              "hxg5",
              "hxg5",
              "Rd4",
              "Ke5",
              "Rg4",
              "Kf5",
              "Rd4",
              "Ke5",
              "Rd2",
              "Bc6",
              "Rd6",
              "Rc8",
              "Kc4",
              "g4",
              "fxg4",
              "Bxg2",
              "b5",
              "Bf1+",
              "Kb4",
              "Rb8",
              "b6",
              "Ba6",
              "Rd7",
              "Rg8",
              "c6",
              "Rxg4+",
              "Kc5",
              "Rc4+",
              "Kd6",
              "Rd4+",
              "Kc7",
              "Rc4",
              "b7",
              "Bxb7",
              "Kxb7",
              "Rb4+",
              "Kc8",
              "Ra4",
              "Ra7",
              "Kd6",
              "c7",
              "Rc4",
              "Ra6+",
              "Ke7",
              "Kb7",
              "Kd7",
              "c8=Q+",
              "Rxc8",
              "Rd6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 147,
            "completion_tokens": 174,
            "total_tokens": 321,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 147,
            "completion_tokens": 215,
            "total_tokens": 362,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 146,
            "completion_tokens": 640,
            "total_tokens": 786,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 440,
          "total_completion_tokens": 1029,
          "total_tokens": 1469
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 147,
            "completion_tokens": 174,
            "total_tokens": 321,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 147,
            "completion_tokens": 215,
            "total_tokens": 362,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 146,
            "completion_tokens": 640,
            "total_tokens": 786,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n2n2/3Pp1N1/2B5/8/PPPP1PPP/RNBQK2R b KQkq - 0 5"
    },
    {
      "turn": 12,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Kxf7",
          "aggressive_uci": "e8f7",
          "aggressive_response": "6... Kxf7 7. Qf3+ Ke6 8. Nc3 Ne7 9. d4 c6 10. O-O b5 11. Bb3 a5 12. Re1 Kd7 13. Nxd5 cxd5 14. Bxd5 Ra6 15. dxe5 Kc7 16. Bf4 Nxd5 17. Rad1 Be6 18. b3 Bb4 19. Re4 Rf8 20. c4 bxc4 21. bxc4 g5 22. cxd5 Bxd5 23. Rc4+ Rc6 24. Rxc6+ Kxc6 25. Rc1+ Bc5 26. Qc3 Qb6 27. Be3 *",
          "aggressive_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 178,
            "total_tokens": 334,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qf3+",
            "Ke6",
            "Nc3",
            "Ne7",
            "d4",
            "c6",
            "O-O",
            "b5",
            "Bb3",
            "a5",
            "Re1",
            "Kd7",
            "Nxd5",
            "cxd5",
            "Bxd5",
            "Ra6",
            "dxe5",
            "Kc7",
            "Bf4",
            "Nxd5",
            "Rad1",
            "Be6",
            "b3",
            "Bb4",
            "Re4",
            "Rf8",
            "c4",
            "bxc4",
            "bxc4",
            "g5",
            "cxd5",
            "Bxd5",
            "Rc4+",
            "Rc6",
            "Rxc6+",
            "Kxc6",
            "Rc1+",
            "Bc5",
            "Qc3",
            "Qb6",
            "Be3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Kxf7",
          "positional_uci": "e8f7",
          "positional_response": "6... Kxf7 7. Qf3+ Ke6 8. Nc3 Ne7 9. d4 c6 10. O-O b5 11. Bb3 a5 12. Re1 Kd7 13. Nxd5 cxd5 14. Bxd5 Ra6 15. Rxe5 Rf6 16. Qh3+ Kc7 17. Qc3+ Kb8 18. Bf3 b4 19. Qc5 Nf5 20. Qb5+ Qb6 21. Qxa5 Qxa5 22. Rxa5 Nxd4 23. Be4 Bb7 24. Bxb7 Kxb7 25. c3 Ne2+ 26. Kf1 Nxc1 27. Rxc1 bxc3 28. Rxc3 Bb4 29. Rb5+ Rb6 30. Rxb6+ Kxb6 31. Rb3 Kc5 32. a3 Ba5 33. Rb7 Re8 34. b4+ Kc6 35. Rxg7 Bc7 36. Rxh7 Ra8 37. Rh3 Kb5 38. Rc3 Bd6 39. Rd3 Bc7 40. h4 Kc4 41. Rf3 Be5 42. g4 Bb2 43. h5 Bxa3 44. g5 Bxb4 45. Rf4+ Kc5 46. g6 Bc3 47. h6 Bd2 48. Rf5+ Kd6 49. h7 Bc3 50. Rf7 Ke6 51. f4 Ra1+ 52. Kg2 Ra2+ 53. Kf3 Ra3 54. Kg4 Ra1 55. f5+ Kd6 56. f6 Rg1+ 57. Kf5 Rf1+ 58. Kg5 Rg1+ 59. Kh6 Bd2+ 60. Kg7 Rh1 61. h8=Q Rxh8 62. Kxh8 Ke6 63. Kg7 Bc3 64. Rf8 Bd4 65. Re8+ Kf5 66. Rf8 Ke6 67. Rf7 Bc3 68. Re7+ Kf5 69. Kf7 Bxf6 70. g7 Bxg7 71. Kxg7 Kf4 72. Kf6 Kf3 73. Kf5 Kg3 74. Re3+ Kf2 75. Kf4 Kg2 76. Re2+ Kf1 77. Kf3 Kg1 78. Kg3 Kf1 79. Re3 Kg1 80. Re1# Kf1 81. Re2 Kg",
          "positional_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Qf3+",
            "Ke6",
            "Nc3",
            "Ne7",
            "d4",
            "c6",
            "O-O",
            "b5",
            "Bb3",
            "a5",
            "Re1",
            "Kd7",
            "Nxd5",
            "cxd5",
            "Bxd5",
            "Ra6",
            "Rxe5",
            "Rf6",
            "Qh3+",
            "Kc7",
            "Qc3+",
            "Kb8",
            "Bf3",
            "b4",
            "Qc5",
            "Nf5",
            "Qb5+",
            "Qb6",
            "Qxa5",
            "Qxa5",
            "Rxa5",
            "Nxd4",
            "Be4",
            "Bb7",
            "Bxb7",
            "Kxb7",
            "c3",
            "Ne2+",
            "Kf1",
            "Nxc1",
            "Rxc1",
            "bxc3",
            "Rxc3",
            "Bb4",
            "Rb5+",
            "Rb6",
            "Rxb6+",
            "Kxb6",
            "Rb3",
            "Kc5",
            "a3",
            "Ba5",
            "Rb7",
            "Re8",
            "b4+",
            "Kc6",
            "Rxg7",
            "Bc7",
            "Rxh7",
            "Ra8",
            "Rh3",
            "Kb5",
            "Rc3",
            "Bd6",
            "Rd3",
            "Bc7",
            "h4",
            "Kc4",
            "Rf3",
            "Be5",
            "g4",
            "Bb2",
            "h5",
            "Bxa3",
            "g5",
            "Bxb4",
            "Rf4+",
            "Kc5",
            "g6",
            "Bc3",
            "h6",
            "Bd2",
            "Rf5+",
            "Kd6",
            "h7",
            "Bc3",
            "Rf7",
            "Ke6",
            "f4",
            "Ra1+",
            "Kg2",
            "Ra2+",
            "Kf3",
            "Ra3",
            "Kg4",
            "Ra1",
            "f5+",
            "Kd6",
            "f6",
            "Rg1+",
            "Kf5",
            "Rf1+",
            "Kg5",
            "Rg1+",
            "Kh6",
            "Bd2+",
            "Kg7",
            "Rh1",
            "h8=Q",
            "Rxh8",
            "Kxh8",
            "Ke6",
            "Kg7",
            "Bc3",
            "Rf8",
            "Bd4",
            "Re8+",
            "Kf5",
            "Rf8",
            "Ke6",
            "Rf7",
            "Bc3",
            "Re7+",
            "Kf5",
            "Kf7",
            "Bxf6",
            "g7",
            "Bxg7",
            "Kxg7",
            "Kf4",
            "Kf6",
            "Kf3",
            "Kf5",
            "Kg3",
            "Re3+",
            "Kf2",
            "Kf4",
            "Kg2",
            "Re2+",
            "Kf1",
            "Kf3",
            "Kg1",
            "Kg3",
            "Kf1",
            "Re3",
            "Kg1",
            "Re1#",
            "Kf1",
            "Re2"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Kxf7",
          "neutral_uci": "e8f7",
          "neutral_response": "6... Kxf7 7. Qf3+ Ke6 8. Nc3 Ne7 9. d4 c6 10. O-O b5 11. Bb3 a5 12. Re1 Kd7 13. Nxd5 cxd5 14. Bxd5 Ra6 15. dxe5 Nxd5 16. Qxd5+ Ke8 17. Qxb5+ Qd7 18. Qe2 Bb7 19. Bf4 Rg6 20. Bg3 h5 21. Rad1 Qc6 22. f3 h4 23. Bf4 h3 24. g3 Bc5+ 25. Be3 Bxe3+ 26. Qxe3 Qxf3 27. Qxf3 Bxf3 28. Rd3 Bg4 29. Rd5 Ra6 30. Re4 Be6 31. Rb5 Kf7 32. Rf4+ Kg6 33. c4 Rd8 34. Kf2 Rd2+ 35. Ke3 Rxh2 36. c5 Rc2 37. Rb6 Rxb6 38. cxb6 Rxb2 39. Rh4 Rxb6 40. g4 Rb4 41. Rxh3 Bxg4 42. Rg3 Kf5 43. Rg1 Re4+ 44. Kd3 Rxe5 45. Rf1+ Ke6 46. Rg1 Bf5+ 47. Kd4 g6 48. Rc1 Rd5+ 49. Ke3 Ke5 50. Rc7 Rd3+ 51. Ke2 Ra3 52. Rc5+ Kd4 53. Rc7 Rxa2+ 54. Kf3 a4 55. Ra7 a3 56. Ra4+ Kc3 57. Ke3 Kb3 58. Ra7 Rc2 59. Rb7+ Kc3 60. Rc7+ Kb2 61. Rb7+ Kc1 62. Ra7 a2 63. Kd4 Kb1 64. Rb7+ Rb2 65. Ra7 a1=Q 66. Rxa1+ Kxa1 67. Ke5 Rb4 68. Kf6 Re4 69. Kg5 Kb2 70. Kf6 Kc3 71. Kg5 Kd4 72. Kf6 Kd5 73. Kg5 Ke6 74. Kh6 Kf6 75. Kh7 Re8 76. Kh6 Rh8# e4 e5 Nf3 Nc6 Bc4 Nf6 Ng5 d5 exd5 Nxd5 Nxf7 Kxf7 Qf3+ Ke6 Nc3 Ne7 d4 c6 O-O b5",
          "neutral_tokens": {
            "prompt_tokens": 155,
            "completion_tokens": 640,
            "total_tokens": 795,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Qf3+",
            "Ke6",
            "Nc3",
            "Ne7",
            "d4",
            "c6",
            "O-O",
            "b5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e8f7",
          "positional_uci": "e8f7",
          "neutral_uci": "e8f7",
          "consensus_move": "e8f7",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qf3+",
              "Ke6",
              "Nc3",
              "Ne7",
              "d4",
              "c6",
              "O-O",
              "b5",
              "Bb3",
              "a5",
              "Re1",
              "Kd7",
              "Nxd5",
              "cxd5",
              "Bxd5",
              "Ra6",
              "dxe5",
              "Kc7",
              "Bf4",
              "Nxd5",
              "Rad1",
              "Be6",
              "b3",
              "Bb4",
              "Re4",
              "Rf8",
              "c4",
              "bxc4",
              "bxc4",
              "g5",
              "cxd5",
              "Bxd5",
              "Rc4+",
              "Rc6",
              "Rxc6+",
              "Kxc6",
              "Rc1+",
              "Bc5",
              "Qc3",
              "Qb6",
              "Be3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qf3+",
              "Ke6",
              "Nc3",
              "Ne7",
              "d4",
              "c6",
              "O-O",
              "b5",
              "Bb3",
              "a5",
              "Re1",
              "Kd7",
              "Nxd5",
              "cxd5",
              "Bxd5",
              "Ra6",
              "Rxe5",
              "Rf6",
              "Qh3+",
              "Kc7",
              "Qc3+",
              "Kb8",
              "Bf3",
              "b4",
              "Qc5",
              "Nf5",
              "Qb5+",
              "Qb6",
              "Qxa5",
              "Qxa5",
              "Rxa5",
              "Nxd4",
              "Be4",
              "Bb7",
              "Bxb7",
              "Kxb7",
              "c3",
              "Ne2+",
              "Kf1",
              "Nxc1",
              "Rxc1",
              "bxc3",
              "Rxc3",
              "Bb4",
              "Rb5+",
              "Rb6",
              "Rxb6+",
              "Kxb6",
              "Rb3",
              "Kc5",
              "a3",
              "Ba5",
              "Rb7",
              "Re8",
              "b4+",
              "Kc6",
              "Rxg7",
              "Bc7",
              "Rxh7",
              "Ra8",
              "Rh3",
              "Kb5",
              "Rc3",
              "Bd6",
              "Rd3",
              "Bc7",
              "h4",
              "Kc4",
              "Rf3",
              "Be5",
              "g4",
              "Bb2",
              "h5",
              "Bxa3",
              "g5",
              "Bxb4",
              "Rf4+",
              "Kc5",
              "g6",
              "Bc3",
              "h6",
              "Bd2",
              "Rf5+",
              "Kd6",
              "h7",
              "Bc3",
              "Rf7",
              "Ke6",
              "f4",
              "Ra1+",
              "Kg2",
              "Ra2+",
              "Kf3",
              "Ra3",
              "Kg4",
              "Ra1",
              "f5+",
              "Kd6",
              "f6",
              "Rg1+",
              "Kf5",
              "Rf1+",
              "Kg5",
              "Rg1+",
              "Kh6",
              "Bd2+",
              "Kg7",
              "Rh1",
              "h8=Q",
              "Rxh8",
              "Kxh8",
              "Ke6",
              "Kg7",
              "Bc3",
              "Rf8",
              "Bd4",
              "Re8+",
              "Kf5",
              "Rf8",
              "Ke6",
              "Rf7",
              "Bc3",
              "Re7+",
              "Kf5",
              "Kf7",
              "Bxf6",
              "g7",
              "Bxg7",
              "Kxg7",
              "Kf4",
              "Kf6",
              "Kf3",
              "Kf5",
              "Kg3",
              "Re3+",
              "Kf2",
              "Kf4",
              "Kg2",
              "Re2+",
              "Kf1",
              "Kf3",
              "Kg1",
              "Kg3",
              "Kf1",
              "Re3",
              "Kg1",
              "Re1#",
              "Kf1",
              "Re2"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qf3+",
              "Ke6",
              "Nc3",
              "Ne7",
              "d4",
              "c6",
              "O-O",
              "b5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 156,
            "completion_tokens": 178,
            "total_tokens": 334,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 155,
            "completion_tokens": 640,
            "total_tokens": 795,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 467,
          "total_completion_tokens": 1458,
          "total_tokens": 1925
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 156,
            "completion_tokens": 178,
            "total_tokens": 334,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 155,
            "completion_tokens": 640,
            "total_tokens": 795,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2Npp/2n5/3np3/2B5/8/PPPP1PPP/RNBQK2R b KQkq - 0 6"
    },
    {
      "turn": 14,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nxd4",
          "aggressive_uci": "c6d4",
          "aggressive_response": "7... Nxd4 8. c3 b5 9. Bxd5+ Qxd5 10. cxd4 Qxg2 11. Rf1 Bb4+ 12. Nc3 exd4 13. Qxd4 Re8+ 14. Be3 Bxc3+ 15. Qxc3 Bh3 16. O-O-O Qe4 17. Rg1 g6 18. Bh6 Qc4 19. Qxc4+ bxc4 20. Rd4 Be6 21. Rf4+ Kg8 22. Rd1 Rad8 23. Rxd8 Rxd8 24. Kc2 Bf5+ 25. Kc3 Rd3+ 26. Kxc4 Rd2 27. Kc3 Rc2+ 28. Kb3 Rc6 29. a3 Rb6+ 30. Ka2 Be6+ 31. Ka1 Bf5 32. Rd4 Rd6 33. Rxd6 cxd6 34. b4 Kf7 35. Kb2 Ke6 36. Kc3 Kd5 37. Be3 a6 38. a4 Kc6 39. Kc4 d5+ 40. Kd4 Bc2 41. a5 Bb3 42. Ke5 Kb5 43. Bc5 Kc4 44. Kf6 d4 45. Kg7 d3 46. Be3 Kxb4 47. Kxh7 Kxa5 48. Kxg6 Kb4 49. h4 a5 50. h5 a4 51. h6 a3 52. h7 a2 53. h8=Q Ka3 54. Qc3 Bc2 55. Bc1+ Ka4 56. Qc4+ Ka5 57. Qxa2+ Kb5 58. Qb2+ Kc5 59. Qc3+ Kd5 60. f4 Ke4 61. f5 Kf3 62. f6 Ke2 63. f7 Kd1 64. f8=Q Kxc1 65. Qf1# Nxf1 gxf1=Q h8=Q#",
          "aggressive_tokens": {
            "prompt_tokens": 164,
            "completion_tokens": 505,
            "total_tokens": 669,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "c3",
            "b5",
            "Bxd5+",
            "Qxd5",
            "cxd4",
            "Qxg2",
            "Rf1",
            "Bb4+",
            "Nc3",
            "exd4",
            "Qxd4",
            "Re8+",
            "Be3",
            "Bxc3+",
            "Qxc3",
            "Bh3",
            "O-O-O",
            "Qe4",
            "Rg1",
            "g6",
            "Bh6",
            "Qc4",
            "Qxc4+",
            "bxc4",
            "Rd4",
            "Be6",
            "Rf4+",
            "Kg8",
            "Rd1",
            "Rad8",
            "Rxd8",
            "Rxd8",
            "Kc2",
            "Bf5+",
            "Kc3",
            "Rd3+",
            "Kxc4",
            "Rd2",
            "Kc3",
            "Rc2+",
            "Kb3",
            "Rc6",
            "a3",
            "Rb6+",
            "Ka2",
            "Be6+",
            "Ka1",
            "Bf5",
            "Rd4",
            "Rd6",
            "Rxd6",
            "cxd6",
            "b4",
            "Kf7",
            "Kb2",
            "Ke6",
            "Kc3",
            "Kd5",
            "Be3",
            "a6",
            "a4",
            "Kc6",
            "Kc4",
            "d5+",
            "Kd4",
            "Bc2",
            "a5",
            "Bb3",
            "Ke5",
            "Kb5",
            "Bc5",
            "Kc4",
            "Kf6",
            "d4",
            "Kg7",
            "d3",
            "Be3",
            "Kxb4",
            "Kxh7",
            "Kxa5",
            "Kxg6",
            "Kb4",
            "h4",
            "a5",
            "h5",
            "a4",
            "h6",
            "a3",
            "h7",
            "a2",
            "h8=Q",
            "Ka3",
            "Qc3",
            "Bc2",
            "Bc1+",
            "Ka4",
            "Qc4+",
            "Ka5",
            "Qxa2+",
            "Kb5",
            "Qb2+",
            "Kc5",
            "Qc3+",
            "Kd5",
            "f4",
            "Ke4",
            "f5",
            "Kf3",
            "f6",
            "Ke2",
            "f7",
            "Kd1",
            "f8=Q",
            "Kxc1",
            "Qf1#",
            "Nxf1",
            "gxf1=Q",
            "h8=Q#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nxd4",
          "positional_uci": "c6d4",
          "positional_response": "7... Nxd4 8. c3 b5 9. Bxd5+ Qxd5 10. cxd4 Qxg2 11. Rf1 Bb4+ 12. Nc3 exd4 13. Qxd4 Re8+ 14. Be3 Bxc3+ 15. Qxc3 Bg4 16. Qxc7+ Kg8 17. Qg3 Rxe3+ 18. Qxe3 Rd8 19. f3 Qxb2 20. Rd1 Qb4+ 21. Kf2 Qb2+ 22. Kg1 Rxd1 23. Qe8# e4 d3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd",
          "positional_tokens": {
            "prompt_tokens": 164,
            "completion_tokens": 640,
            "total_tokens": 804,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "c3",
            "b5",
            "Bxd5+",
            "Qxd5",
            "cxd4",
            "Qxg2",
            "Rf1",
            "Bb4+",
            "Nc3",
            "exd4",
            "Qxd4",
            "Re8+",
            "Be3",
            "Bxc3+",
            "Qxc3",
            "Bg4",
            "Qxc7+",
            "Kg8",
            "Qg3",
            "Rxe3+",
            "Qxe3",
            "Rd8",
            "f3",
            "Qxb2",
            "Rd1",
            "Qb4+",
            "Kf2",
            "Qb2+",
            "Kg1",
            "Rxd1",
            "Qe8#",
            "e4",
            "d3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nxd4",
          "neutral_uci": "c6d4",
          "neutral_response": "7... Nxd4 8. c3 b5 9. Bxd5+ Qxd5 10. cxd4 Qxg2 11. Rf1 Bb4+ 12. Nc3 exd4 13. Qxd4 Re8+ 14. Be3 Bxc3+ 15. Qxc3 Bg4 16. Qxc7+ Kg8 17. Qg3 Rxe3+ 18. Qxe3 Rd8 19. f3 Qxb2 20. Rd1 Qb4+ 21. Kf2 Rf8 22. Kg3 Bf5 23. Qb3+ Qxb3+ 24. axb3 Bc2 25. Rd7 Bxb3 26. Rxa7 Bc4 27. Re1 h6 28. Ree7 Rf7 29. Rxf7 Bxf7 30. Rb7 Bc4 31. Kf4 Kh7 32. Ke5 Kg6 33. f4 Kh5 34. Rxg7 Kh4 35. f5 Kh3 36. f6 Kxh2 37. f7 Bxf7 38. Rxf7 h5 39. Rh7 Kg3 40. Rxh5 b4 41. Kd4 b3 42. Kc3 b2 43. Kxb2 Kg4 44. Re5 Kf4 45. Re8 Kf5 46. Kc3 Kf6 47. Kd4 Kf7 48. Re5 Kf6 49. Kd5 Kf7 50. Re6 Kf8 51. Kd6 Kf7 52. Kd7 Kf8 53. Re7 Kg8 54. Ke6 Kf8 55. Kf6 Kg8 56. Rf7 Kh8 57. Kg6 Kg8 58. Rf6 Kh8 59. Rf8# Kxf8 60. Kf6 Ke8 61. Ke6 Kd8 62. Kd6 Kc8 63. Kc6 Kb8 64. Kb6 Ka8 65. Ka6 Kb8 66. Kb6 Ka8 67. Ka6 Kb8 68. Kb6 Ka8 69. Ka6 Kb8 70. Kb6 Ka8 71. Ka6 Kb8 72. Kb6 Ka8 73. Ka6 Kb8 74. Kb6 Ka8 75. Ka6 Kb8 76. Kb6 Ka8 77. Ka6 Kb8 78. Kb6 Ka8 79. Ka6 Kb8 80. Kb6 Ka8 81. Ka6 Kb8 82. Kb6 Ka8 83. Ka6 Kb8 84. Kb6 Ka8 85. Ka6 Kb8 86. Kb6 Ka8 87",
          "neutral_tokens": {
            "prompt_tokens": 163,
            "completion_tokens": 640,
            "total_tokens": 803,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "c3",
            "b5",
            "Bxd5+",
            "Qxd5",
            "cxd4",
            "Qxg2",
            "Rf1",
            "Bb4+",
            "Nc3",
            "exd4",
            "Qxd4",
            "Re8+",
            "Be3",
            "Bxc3+",
            "Qxc3",
            "Bg4",
            "Qxc7+",
            "Kg8",
            "Qg3",
            "Rxe3+",
            "Qxe3",
            "Rd8",
            "f3",
            "Qxb2",
            "Rd1",
            "Qb4+",
            "Kf2",
            "Rf8",
            "Kg3",
            "Bf5",
            "Qb3+",
            "Qxb3+",
            "axb3",
            "Bc2",
            "Rd7",
            "Bxb3",
            "Rxa7",
            "Bc4",
            "Re1",
            "h6",
            "Ree7",
            "Rf7",
            "Rxf7",
            "Bxf7",
            "Rb7",
            "Bc4",
            "Kf4",
            "Kh7",
            "Ke5",
            "Kg6",
            "f4",
            "Kh5",
            "Rxg7",
            "Kh4",
            "f5",
            "Kh3",
            "f6",
            "Kxh2",
            "f7",
            "Bxf7",
            "Rxf7",
            "h5",
            "Rh7",
            "Kg3",
            "Rxh5",
            "b4",
            "Kd4",
            "b3",
            "Kc3",
            "b2",
            "Kxb2",
            "Kg4",
            "Re5",
            "Kf4",
            "Re8",
            "Kf5",
            "Kc3",
            "Kf6",
            "Kd4",
            "Kf7",
            "Re5",
            "Kf6",
            "Kd5",
            "Kf7",
            "Re6",
            "Kf8",
            "Kd6",
            "Kf7",
            "Kd7",
            "Kf8",
            "Re7",
            "Kg8",
            "Ke6",
            "Kf8",
            "Kf6",
            "Kg8",
            "Rf7",
            "Kh8",
            "Kg6",
            "Kg8",
            "Rf6",
            "Kh8",
            "Rf8#",
            "Kxf8",
            "Kf6",
            "Ke8",
            "Ke6",
            "Kd8",
            "Kd6",
            "Kc8",
            "Kc6",
            "Kb8",
            "Kb6",
            "Ka8",
            "Ka6",
            "Kb8",
            "Kb6",
            "Ka8",
            "Ka6",
            "Kb8",
            "Kb6",
            "Ka8",
            "Ka6",
            "Kb8",
            "Kb6",
            "Ka8",
            "Ka6",
            "Kb8",
            "Kb6",
            "Ka8",
            "Ka6",
            "Kb8",
            "Kb6",
            "Ka8",
            "Ka6",
            "Kb8",
            "Kb6",
            "Ka8",
            "Ka6",
            "Kb8",
            "Kb6",
            "Ka8",
            "Ka6",
            "Kb8",
            "Kb6",
            "Ka8",
            "Ka6",
            "Kb8",
            "Kb6",
            "Ka8",
            "Ka6",
            "Kb8",
            "Kb6",
            "Ka8",
            "Ka6",
            "Kb8",
            "Kb6",
            "Ka8"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "c6d4",
          "positional_uci": "c6d4",
          "neutral_uci": "c6d4",
          "consensus_move": "c6d4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "c3",
              "b5",
              "Bxd5+",
              "Qxd5",
              "cxd4",
              "Qxg2",
              "Rf1",
              "Bb4+",
              "Nc3",
              "exd4",
              "Qxd4",
              "Re8+",
              "Be3",
              "Bxc3+",
              "Qxc3",
              "Bh3",
              "O-O-O",
              "Qe4",
              "Rg1",
              "g6",
              "Bh6",
              "Qc4",
              "Qxc4+",
              "bxc4",
              "Rd4",
              "Be6",
              "Rf4+",
              "Kg8",
              "Rd1",
              "Rad8",
              "Rxd8",
              "Rxd8",
              "Kc2",
              "Bf5+",
              "Kc3",
              "Rd3+",
              "Kxc4",
              "Rd2",
              "Kc3",
              "Rc2+",
              "Kb3",
              "Rc6",
              "a3",
              "Rb6+",
              "Ka2",
              "Be6+",
              "Ka1",
              "Bf5",
              "Rd4",
              "Rd6",
              "Rxd6",
              "cxd6",
              "b4",
              "Kf7",
              "Kb2",
              "Ke6",
              "Kc3",
              "Kd5",
              "Be3",
              "a6",
              "a4",
              "Kc6",
              "Kc4",
              "d5+",
              "Kd4",
              "Bc2",
              "a5",
              "Bb3",
              "Ke5",
              "Kb5",
              "Bc5",
              "Kc4",
              "Kf6",
              "d4",
              "Kg7",
              "d3",
              "Be3",
              "Kxb4",
              "Kxh7",
              "Kxa5",
              "Kxg6",
              "Kb4",
              "h4",
              "a5",
              "h5",
              "a4",
              "h6",
              "a3",
              "h7",
              "a2",
              "h8=Q",
              "Ka3",
              "Qc3",
              "Bc2",
              "Bc1+",
              "Ka4",
              "Qc4+",
              "Ka5",
              "Qxa2+",
              "Kb5",
              "Qb2+",
              "Kc5",
              "Qc3+",
              "Kd5",
              "f4",
              "Ke4",
              "f5",
              "Kf3",
              "f6",
              "Ke2",
              "f7",
              "Kd1",
              "f8=Q",
              "Kxc1",
              "Qf1#",
              "Nxf1",
              "gxf1=Q",
              "h8=Q#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "c3",
              "b5",
              "Bxd5+",
              "Qxd5",
              "cxd4",
              "Qxg2",
              "Rf1",
              "Bb4+",
              "Nc3",
              "exd4",
              "Qxd4",
              "Re8+",
              "Be3",
              "Bxc3+",
              "Qxc3",
              "Bg4",
              "Qxc7+",
              "Kg8",
              "Qg3",
              "Rxe3+",
              "Qxe3",
              "Rd8",
              "f3",
              "Qxb2",
              "Rd1",
              "Qb4+",
              "Kf2",
              "Qb2+",
              "Kg1",
              "Rxd1",
              "Qe8#",
              "e4",
              "d3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "c3",
              "b5",
              "Bxd5+",
              "Qxd5",
              "cxd4",
              "Qxg2",
              "Rf1",
              "Bb4+",
              "Nc3",
              "exd4",
              "Qxd4",
              "Re8+",
              "Be3",
              "Bxc3+",
              "Qxc3",
              "Bg4",
              "Qxc7+",
              "Kg8",
              "Qg3",
              "Rxe3+",
              "Qxe3",
              "Rd8",
              "f3",
              "Qxb2",
              "Rd1",
              "Qb4+",
              "Kf2",
              "Rf8",
              "Kg3",
              "Bf5",
              "Qb3+",
              "Qxb3+",
              "axb3",
              "Bc2",
              "Rd7",
              "Bxb3",
              "Rxa7",
              "Bc4",
              "Re1",
              "h6",
              "Ree7",
              "Rf7",
              "Rxf7",
              "Bxf7",
              "Rb7",
              "Bc4",
              "Kf4",
              "Kh7",
              "Ke5",
              "Kg6",
              "f4",
              "Kh5",
              "Rxg7",
              "Kh4",
              "f5",
              "Kh3",
              "f6",
              "Kxh2",
              "f7",
              "Bxf7",
              "Rxf7",
              "h5",
              "Rh7",
              "Kg3",
              "Rxh5",
              "b4",
              "Kd4",
              "b3",
              "Kc3",
              "b2",
              "Kxb2",
              "Kg4",
              "Re5",
              "Kf4",
              "Re8",
              "Kf5",
              "Kc3",
              "Kf6",
              "Kd4",
              "Kf7",
              "Re5",
              "Kf6",
              "Kd5",
              "Kf7",
              "Re6",
              "Kf8",
              "Kd6",
              "Kf7",
              "Kd7",
              "Kf8",
              "Re7",
              "Kg8",
              "Ke6",
              "Kf8",
              "Kf6",
              "Kg8",
              "Rf7",
              "Kh8",
              "Kg6",
              "Kg8",
              "Rf6",
              "Kh8",
              "Rf8#",
              "Kxf8",
              "Kf6",
              "Ke8",
              "Ke6",
              "Kd8",
              "Kd6",
              "Kc8",
              "Kc6",
              "Kb8",
              "Kb6",
              "Ka8",
              "Ka6",
              "Kb8",
              "Kb6",
              "Ka8",
              "Ka6",
              "Kb8",
              "Kb6",
              "Ka8",
              "Ka6",
              "Kb8",
              "Kb6",
              "Ka8",
              "Ka6",
              "Kb8",
              "Kb6",
              "Ka8",
              "Ka6",
              "Kb8",
              "Kb6",
              "Ka8",
              "Ka6",
              "Kb8",
              "Kb6",
              "Ka8",
              "Ka6",
              "Kb8",
              "Kb6",
              "Ka8",
              "Ka6",
              "Kb8",
              "Kb6",
              "Ka8",
              "Ka6",
              "Kb8",
              "Kb6",
              "Ka8",
              "Ka6",
              "Kb8",
              "Kb6",
              "Ka8",
              "Ka6",
              "Kb8",
              "Kb6",
              "Ka8"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 164,
            "completion_tokens": 505,
            "total_tokens": 669,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 164,
            "completion_tokens": 640,
            "total_tokens": 804,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 163,
            "completion_tokens": 640,
            "total_tokens": 803,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 491,
          "total_completion_tokens": 1785,
          "total_tokens": 2276
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 164,
            "completion_tokens": 505,
            "total_tokens": 669,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 164,
            "completion_tokens": 640,
            "total_tokens": 804,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 163,
            "completion_tokens": 640,
            "total_tokens": 803,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bq1b1r/ppp2kpp/2n5/3np3/2BP4/8/PPP2PPP/RNBQK2R b KQ - 0 7"
    },
    {
      "turn": 16,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Ke6",
          "aggressive_uci": "f7e6",
          "aggressive_response": "8... Ke6 9. Nc3 c6 10. O-O Bb4 11. Re1 Bxc3 12. bxc3 Nxc2 13. Bf4 Nxe1 14. Rxe1 Kd7 15. Bxd5 cxd5 16. Qxd5+ Ke8 17. Rxe5+ Kf8 18. Qxd8+ Kf7 19. Qe7+ Kg6 20. Rg5+ Kh6 21. Qxg7# Kxg5 22. Qg7+ Kf5 23. Qxh8 Kxf4 24. Qxh7 Bf5 25. Qxb7 Re8 26. Qf3+ Kg5 27. h4+ Kg6 28. h5+ Kg5 29. Qg3+ Kxh5 30. Qf4 Kg6 31. g4 Re1+ 32. Kh2 Be4 33. f3 Re2+ 34. Kg3 Bb1 35. Qd6+ Kg7 36. Qd4+ Kg6 37. Qxa7 Rxa2 38. Qb6+ Kg7 39. Qxb1 Ra7 40. c4 Kf7 41. c5 Ke7 42. c6 Kd6 43. Qb6 Rc7 44. Qxc7+ Kxc7 45. g5 Kxc6 46. g6 Kd6 47. g7 Ke7 48. g8=Q Kf6 49. Qg4 Ke5 50. Qe4+ Kf6 51. f4 Kf7 52. f5 Kf6 53. Qe6+ Kg5 54. Qg6# Kf4 55. f6 Ke5 56. f7 Kd4 57. f8=Q Kc3 58. Qc5+ Kd2 59. Qgc2+ Ke1 60. Qg1# Kd2 61. Qf2+ Kd1 62. Qcc2# Kc1 63. Qe1# Kxc2 64. Qe3 Kb2 65. Qd3 Ka2 66. Qc3 Kb1 67. Qd2 Ka1 68. Kf3 Kb1 69. Ke3 Ka1 70. Kd3 Kb1 71. Kc3 Ka1 72. Qb2# Ka1 73. Qb4 Ka2 74. Qb3+ Ka1 75. Qb2# Ka1 76. Qb4 Ka2 77. Qb3+ Ka1 78. Qb2# Ka1 79. Qb4 Ka2 80. Qb3+ Ka1 81. Qb2#",
          "aggressive_tokens": {
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nc3",
            "c6",
            "O-O",
            "Bb4",
            "Re1",
            "Bxc3",
            "bxc3",
            "Nxc2",
            "Bf4",
            "Nxe1",
            "Rxe1",
            "Kd7",
            "Bxd5",
            "cxd5",
            "Qxd5+",
            "Ke8",
            "Rxe5+",
            "Kf8",
            "Qxd8+",
            "Kf7",
            "Qe7+",
            "Kg6",
            "Rg5+",
            "Kh6",
            "Qxg7#",
            "Kxg5",
            "Qg7+",
            "Kf5",
            "Qxh8",
            "Kxf4",
            "Qxh7",
            "Bf5",
            "Qxb7",
            "Re8",
            "Qf3+",
            "Kg5",
            "h4+",
            "Kg6",
            "h5+",
            "Kg5",
            "Qg3+",
            "Kxh5",
            "Qf4",
            "Kg6",
            "g4",
            "Re1+",
            "Kh2",
            "Be4",
            "f3",
            "Re2+",
            "Kg3",
            "Bb1",
            "Qd6+",
            "Kg7",
            "Qd4+",
            "Kg6",
            "Qxa7",
            "Rxa2",
            "Qb6+",
            "Kg7",
            "Qxb1",
            "Ra7",
            "c4",
            "Kf7",
            "c5",
            "Ke7",
            "c6",
            "Kd6",
            "Qb6",
            "Rc7",
            "Qxc7+",
            "Kxc7",
            "g5",
            "Kxc6",
            "g6",
            "Kd6",
            "g7",
            "Ke7",
            "g8=Q",
            "Kf6",
            "Qg4",
            "Ke5",
            "Qe4+",
            "Kf6",
            "f4",
            "Kf7",
            "f5",
            "Kf6",
            "Qe6+",
            "Kg5",
            "Qg6#",
            "Kf4",
            "f6",
            "Ke5",
            "f7",
            "Kd4",
            "f8=Q",
            "Kc3",
            "Qc5+",
            "Kd2",
            "Qgc2+",
            "Ke1",
            "Qg1#",
            "Kd2",
            "Qf2+",
            "Kd1",
            "Qcc2#",
            "Kc1",
            "Qe1#",
            "Kxc2",
            "Qe3",
            "Kb2",
            "Qd3",
            "Ka2",
            "Qc3",
            "Kb1",
            "Qd2",
            "Ka1",
            "Kf3",
            "Kb1",
            "Ke3",
            "Ka1",
            "Kd3",
            "Kb1",
            "Kc3",
            "Ka1",
            "Qb2#",
            "Ka1",
            "Qb4",
            "Ka2",
            "Qb3+",
            "Ka1",
            "Qb2#",
            "Ka1",
            "Qb4",
            "Ka2",
            "Qb3+",
            "Ka1",
            "Qb2#",
            "Ka1",
            "Qb4",
            "Ka2",
            "Qb3+",
            "Ka1",
            "Qb2#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Ke6",
          "positional_uci": "f7e6",
          "positional_response": "8... Ke6 9. Nc3 c6 10. O-O Bb4 11. Qg4+ Kd6 12. Ne4+ Kc7 13. Qxg7+ Kb8 14. Qxe5+ Nc7\n15. Bf4 Re8 16. Qxc7+ Qxc7 17. Bxc7+ Kxc7 18. Nf6 Rf8 19. Nxh7 Rh8 20. Ng5 Bd6 21. h3 Bf5\n22. c3 Nc2 23. Rad1 Bf4 24. Ne6+ Bxe6 25. Bxe6 Rae8 26. Bf5 Re2 27. Bd3 Rd2 28. Rxd2 Bxd2\n29. Bxc2 Re8 30. Bd3 Rd8 31. Be2 Re8 32. Bf3 Re1 33. Rxe1 Bxe1 34. Kf1 Bd2 35. Ke2 Bc1\n36. b3 Bb2 37. c4 b5 38. cxb5 cxb5 39. g4 a5 40. h4 a4 41. bxa4 bxa4 42. g5 a3\n43. h5 Kd6 44. h6 Ke7 45. g6 Kf8 46. g7+ Kg8 47. Bd5+ Kh7 48. g8=Q+ Kxh6 49. Be4 Kh5\n50. Qg6+ Kh4 51. Bf5 Bf6 52. Qg4# Bg5 53. f4 Bxf4 54. Qg6 Bg5 55. Kf3 Bf4 56. Qg4# Bg5\n57. Qh3# Bf4 58. Qg4# Bg5 59. Qh3# Bf4 60. Qg4# Bg5 61. Qh3# Bf4 62. Qg4# Bg5 63. Qh3# Bf4 64. Qg4# Bg5 65. Qh3# Bf4 66. Qg4# Bg5 67. Qh3# Bf4 68. Qg4# Bg5 69. Qh3# Bf4 70. Qg4# Bg5 71. Qh3# Bf4 72. Qg4# Bg5 73. Qh3# Bf4 74. Qg4# Bg5 75. Qh3# Bf4 76. Qg4# Bg5 77. Qh3# Bf4 78. Qg4# Bg5 79. Qh3# Bf4 80",
          "positional_tokens": {
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nc3",
            "c6",
            "O-O",
            "Bb4",
            "Qg4+",
            "Kd6",
            "Ne4+",
            "Kc7",
            "Qxg7+",
            "Kb8",
            "Qxe5+",
            "Nc7",
            "Bf4",
            "Re8",
            "Qxc7+",
            "Qxc7",
            "Bxc7+",
            "Kxc7",
            "Nf6",
            "Rf8",
            "Nxh7",
            "Rh8",
            "Ng5",
            "Bd6",
            "h3",
            "Bf5",
            "c3",
            "Nc2",
            "Rad1",
            "Bf4",
            "Ne6+",
            "Bxe6",
            "Bxe6",
            "Rae8",
            "Bf5",
            "Re2",
            "Bd3",
            "Rd2",
            "Rxd2",
            "Bxd2",
            "Bxc2",
            "Re8",
            "Bd3",
            "Rd8",
            "Be2",
            "Re8",
            "Bf3",
            "Re1",
            "Rxe1",
            "Bxe1",
            "Kf1",
            "Bd2",
            "Ke2",
            "Bc1",
            "b3",
            "Bb2",
            "c4",
            "b5",
            "cxb5",
            "cxb5",
            "g4",
            "a5",
            "h4",
            "a4",
            "bxa4",
            "bxa4",
            "g5",
            "a3",
            "h5",
            "Kd6",
            "h6",
            "Ke7",
            "g6",
            "Kf8",
            "g7+",
            "Kg8",
            "Bd5+",
            "Kh7",
            "g8=Q+",
            "Kxh6",
            "Be4",
            "Kh5",
            "Qg6+",
            "Kh4",
            "Bf5",
            "Bf6",
            "Qg4#",
            "Bg5",
            "f4",
            "Bxf4",
            "Qg6",
            "Bg5",
            "Kf3",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Ke6",
          "neutral_uci": "f7e6",
          "neutral_response": "8... Ke6 9. Nc3 c6 10. O-O Bb4 11. Qg4+ Kd6 12. Ne4+ Kc7 13. Qxg7+ Kb8 14. Qxe5+ Nc7 15. Bf4 Re8 16. Qxc7+ Qxc7 17. Bxc7+ Kxc7 18. Nf6 Re7 19. c3 Bc5 20. b4 Bb6 21. a4 a5 22. b5 Bf5 23. Rfe1 Rxe1+ 24. Rxe1 Rf8 25. Ne8+ Kd7 26. Ng7 Bg6 27. Rd1+ Ke7 28. Rd2 Rf4 29. Bb3 Re4 30. Kf1 cxb5 31. axb5 a4 32. Bd5 Re5 33. Bxb7 Rxb5 34. Bc6 Rb1+ 35. Ke2 a3 36. Rd7+ Kf6 37. Bd5 Rb2+ 38. Kf3 Rxf2+ 39. Kg3 a2 40. Bxa2 Rxa2 41. Rd6+ Kxg7 42. Rxb6 Be4 43. Kf4 Bxg2 44. c4 Ra4 45. Kg3 Be4 46. c5 Rc4 47. Rb5 Bc6 48. Ra5 Kf6 49. h4 Ke5 50. Ra7 h5 51. Rh7 Rc3+ 52. Kf2 Bf3 53. Re7+ Kd5 54. Rf7 Bg4 55. Rc7 Rxc5 56. Rxc5+ Kxc5 57. Kg3 Kd4 58. Kf4 Kd3 59. Kg3 Ke3 60. Kg2 Kf4 61. Kh2 Kf3 62. Kh1 Kg3 63. Kg1 Kxh4 64. Kh2 Kg5 65. Kg3 h4+ 66. Kh2 Kf4 67. Kg2 h3+ 68. Kh2 Kf3 69. Kh1 Kg3 70. Kg1 h2+ 71. Kh1 Bf3# 0-1\n8... Ke6 9. Nc3 c6 10. O-O Bb4 11. Qg4+ Kd6 12. Ne4+ Kc7 13. Qxg7+ Kb8 14. Qxe5+ Nc7 15. Bf4 Re8 16. Qxc7+ Qxc7 17. Bxc7+ Kxc7 18. Nf6 Re7 19. c3 Bc",
          "neutral_tokens": {
            "prompt_tokens": 173,
            "completion_tokens": 640,
            "total_tokens": 813,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nc3",
            "c6",
            "O-O",
            "Bb4",
            "Qg4+",
            "Kd6",
            "Ne4+",
            "Kc7",
            "Qxg7+",
            "Kb8",
            "Qxe5+",
            "Nc7",
            "Bf4",
            "Re8",
            "Qxc7+",
            "Qxc7",
            "Bxc7+",
            "Kxc7",
            "Nf6",
            "Re7",
            "c3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f7e6",
          "positional_uci": "f7e6",
          "neutral_uci": "f7e6",
          "consensus_move": "f7e6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nc3",
              "c6",
              "O-O",
              "Bb4",
              "Re1",
              "Bxc3",
              "bxc3",
              "Nxc2",
              "Bf4",
              "Nxe1",
              "Rxe1",
              "Kd7",
              "Bxd5",
              "cxd5",
              "Qxd5+",
              "Ke8",
              "Rxe5+",
              "Kf8",
              "Qxd8+",
              "Kf7",
              "Qe7+",
              "Kg6",
              "Rg5+",
              "Kh6",
              "Qxg7#",
              "Kxg5",
              "Qg7+",
              "Kf5",
              "Qxh8",
              "Kxf4",
              "Qxh7",
              "Bf5",
              "Qxb7",
              "Re8",
              "Qf3+",
              "Kg5",
              "h4+",
              "Kg6",
              "h5+",
              "Kg5",
              "Qg3+",
              "Kxh5",
              "Qf4",
              "Kg6",
              "g4",
              "Re1+",
              "Kh2",
              "Be4",
              "f3",
              "Re2+",
              "Kg3",
              "Bb1",
              "Qd6+",
              "Kg7",
              "Qd4+",
              "Kg6",
              "Qxa7",
              "Rxa2",
              "Qb6+",
              "Kg7",
              "Qxb1",
              "Ra7",
              "c4",
              "Kf7",
              "c5",
              "Ke7",
              "c6",
              "Kd6",
              "Qb6",
              "Rc7",
              "Qxc7+",
              "Kxc7",
              "g5",
              "Kxc6",
              "g6",
              "Kd6",
              "g7",
              "Ke7",
              "g8=Q",
              "Kf6",
              "Qg4",
              "Ke5",
              "Qe4+",
              "Kf6",
              "f4",
              "Kf7",
              "f5",
              "Kf6",
              "Qe6+",
              "Kg5",
              "Qg6#",
              "Kf4",
              "f6",
              "Ke5",
              "f7",
              "Kd4",
              "f8=Q",
              "Kc3",
              "Qc5+",
              "Kd2",
              "Qgc2+",
              "Ke1",
              "Qg1#",
              "Kd2",
              "Qf2+",
              "Kd1",
              "Qcc2#",
              "Kc1",
              "Qe1#",
              "Kxc2",
              "Qe3",
              "Kb2",
              "Qd3",
              "Ka2",
              "Qc3",
              "Kb1",
              "Qd2",
              "Ka1",
              "Kf3",
              "Kb1",
              "Ke3",
              "Ka1",
              "Kd3",
              "Kb1",
              "Kc3",
              "Ka1",
              "Qb2#",
              "Ka1",
              "Qb4",
              "Ka2",
              "Qb3+",
              "Ka1",
              "Qb2#",
              "Ka1",
              "Qb4",
              "Ka2",
              "Qb3+",
              "Ka1",
              "Qb2#",
              "Ka1",
              "Qb4",
              "Ka2",
              "Qb3+",
              "Ka1",
              "Qb2#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nc3",
              "c6",
              "O-O",
              "Bb4",
              "Qg4+",
              "Kd6",
              "Ne4+",
              "Kc7",
              "Qxg7+",
              "Kb8",
              "Qxe5+",
              "Nc7",
              "Bf4",
              "Re8",
              "Qxc7+",
              "Qxc7",
              "Bxc7+",
              "Kxc7",
              "Nf6",
              "Rf8",
              "Nxh7",
              "Rh8",
              "Ng5",
              "Bd6",
              "h3",
              "Bf5",
              "c3",
              "Nc2",
              "Rad1",
              "Bf4",
              "Ne6+",
              "Bxe6",
              "Bxe6",
              "Rae8",
              "Bf5",
              "Re2",
              "Bd3",
              "Rd2",
              "Rxd2",
              "Bxd2",
              "Bxc2",
              "Re8",
              "Bd3",
              "Rd8",
              "Be2",
              "Re8",
              "Bf3",
              "Re1",
              "Rxe1",
              "Bxe1",
              "Kf1",
              "Bd2",
              "Ke2",
              "Bc1",
              "b3",
              "Bb2",
              "c4",
              "b5",
              "cxb5",
              "cxb5",
              "g4",
              "a5",
              "h4",
              "a4",
              "bxa4",
              "bxa4",
              "g5",
              "a3",
              "h5",
              "Kd6",
              "h6",
              "Ke7",
              "g6",
              "Kf8",
              "g7+",
              "Kg8",
              "Bd5+",
              "Kh7",
              "g8=Q+",
              "Kxh6",
              "Be4",
              "Kh5",
              "Qg6+",
              "Kh4",
              "Bf5",
              "Bf6",
              "Qg4#",
              "Bg5",
              "f4",
              "Bxf4",
              "Qg6",
              "Bg5",
              "Kf3",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc3",
              "c6",
              "O-O",
              "Bb4",
              "Qg4+",
              "Kd6",
              "Ne4+",
              "Kc7",
              "Qxg7+",
              "Kb8",
              "Qxe5+",
              "Nc7",
              "Bf4",
              "Re8",
              "Qxc7+",
              "Qxc7",
              "Bxc7+",
              "Kxc7",
              "Nf6",
              "Re7",
              "c3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 173,
            "completion_tokens": 640,
            "total_tokens": 813,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 521,
          "total_completion_tokens": 1920,
          "total_tokens": 2441
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 173,
            "completion_tokens": 640,
            "total_tokens": 813,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bq1b1r/ppp2kpp/8/3np3/2Bn4/5Q2/PPP2PPP/RNB1K2R b KQ - 1 8"
    },
    {
      "turn": 18,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nxc2+",
          "aggressive_uci": "d4c2",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4\n8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd1 Nxe3+ 11. fxe3 *",
          "aggressive_tokens": {
            "prompt_tokens": 182,
            "completion_tokens": 94,
            "total_tokens": 276,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kd1",
            "Nxe3+",
            "fxe3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nxc2+",
          "positional_uci": "d4c2",
          "positional_response": "9... Nxc2+ 10. Kd2 Nxe3 11. fxe3 Bb4+ 12. Nc3 Bxc3+ 13. bxc3 c6 14. e4 Qg5+ 15. Kc2 Qxg2+ 16. Kb3 Qxh1 17. exd5+ cxd5 18. Bb2 dxc4+ 19. Kxc4 Qd5+ 20. Kb4 a5+ 21. Ka3 Qc5+ 22. Kb3 a4+ 23. Kc2 Qf2+ 24. Kd3 Rd8+ 25. Kc4 b5+ 26. Kxb5 Ba6+ 27. Kc6 Rac8# Nf6 Nxe4 Nxe4",
          "positional_tokens": {
            "prompt_tokens": 182,
            "completion_tokens": 183,
            "total_tokens": 365,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kd2",
            "Nxe3",
            "fxe3",
            "Bb4+",
            "Nc3",
            "Bxc3+",
            "bxc3",
            "c6",
            "e4",
            "Qg5+",
            "Kc2",
            "Qxg2+",
            "Kb3",
            "Qxh1",
            "exd5+",
            "cxd5",
            "Bb2",
            "dxc4+",
            "Kxc4",
            "Qd5+",
            "Kb4",
            "a5+",
            "Ka3",
            "Qc5+",
            "Kb3",
            "a4+",
            "Kc2",
            "Qf2+",
            "Kd3",
            "Rd8+",
            "Kc4",
            "b5+",
            "Kxb5",
            "Ba6+",
            "Kc6",
            "Rac8#",
            "Nf6",
            "Nxe4",
            "Nxe4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nxc2+",
          "neutral_uci": "d4c2",
          "neutral_response": "9... Nxc2+ 10. Kd2 Nxe3 11. fxe3 Bb4+ 12. Nc3 Bxc3+ 13. bxc3 c6 14. e4 Qg5+ 15. Kc2 Qxg2+ 16. Bd2 Qxe4+ 17. Kb3 b5 18. Bxd5+ Qxd5+ 19. Kc2 Rd8 20. Rad1 Qxa2+ 21. Kc1 Rd3 22. Rhg1 Kd5 23. Rxg7 Bf5 24. Rg5 Rxc3+ 25. Bxc3+ Ke6 26. Rd2 Qb1# 0-1\n9... Nxc2+ 10. Kd2 Nxe3 11. fxe3 Bb4+ 12. Nc3 Bxc3+ 13. bxc3 c6 14. e4 Qg5+ 15. Kc2 Qxg2+ 16. Bd2 Qxe4+ 17. Kb3 b5 18. Bxd5+ Qxd5+ 19. Kc2 Rd8 20. Rad1 Qxa2+ 21. Kc1 Rd3 22. Rhg1 Kd5 23. Rxg7 Bf5 24. Rg5 Rxc3+ 25. Bxc3+ Ke6 26. Rd2 Qb1# 0-1 9... Nxc2+ 10. Kd2 Nxe3 11. fxe3 Bb4+ 12. Nc3 Bxc3+ 13. bxc3 c6 14. e4 Qg5+ 15. Kc2 Qxg2+ 16. Bd2 Qxe4+ 17. Kb3 b5 18. Bxd5+ Qxd5+ 19. Kc2 Rd8 20. Rad1 Qxa2+ 21. Kc1 Rd3 22. Rhg1 Kd5 23. Rxg7 Bf5 24. Rg5 Rxc3+ 25. Bxc3+ Ke6 26. Rd2 Qb1# 0-1 9... Nxc2+ 10. Kd2 Nxe3 11. fxe3 Bb4+ 12. Nc3 Bxc3+ 13. bxc3 c6 14. e4 Qg5+ 15. Kc2 Qxg2+ 16. Bd2 Qxe4+ 17. Kb3 b5 18. Bxd5+ Qxd5+ 19. Kc2 Rd8 20. Rad1 Qxa2+ 21. Kc1 Rd3 22. Rhg1 Kd5 23. Rxg7 Bf5 24. Rg5 Rxc",
          "neutral_tokens": {
            "prompt_tokens": 181,
            "completion_tokens": 640,
            "total_tokens": 821,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Kd2",
            "Nxe3",
            "fxe3",
            "Bb4+",
            "Nc3",
            "Bxc3+",
            "bxc3",
            "c6",
            "e4",
            "Qg5+",
            "Kc2",
            "Qxg2+",
            "Bd2",
            "Qxe4+",
            "Kb3",
            "b5",
            "Bxd5+",
            "Qxd5+",
            "Kc2",
            "Rd8",
            "Rad1",
            "Qxa2+",
            "Kc1",
            "Rd3",
            "Rhg1",
            "Kd5",
            "Rxg7",
            "Bf5",
            "Rg5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d4c2",
          "positional_uci": "d4c2",
          "neutral_uci": "d4c2",
          "consensus_move": "d4c2",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Kd1",
              "Nxe3+",
              "fxe3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Kd2",
              "Nxe3",
              "fxe3",
              "Bb4+",
              "Nc3",
              "Bxc3+",
              "bxc3",
              "c6",
              "e4",
              "Qg5+",
              "Kc2",
              "Qxg2+",
              "Kb3",
              "Qxh1",
              "exd5+",
              "cxd5",
              "Bb2",
              "dxc4+",
              "Kxc4",
              "Qd5+",
              "Kb4",
              "a5+",
              "Ka3",
              "Qc5+",
              "Kb3",
              "a4+",
              "Kc2",
              "Qf2+",
              "Kd3",
              "Rd8+",
              "Kc4",
              "b5+",
              "Kxb5",
              "Ba6+",
              "Kc6",
              "Rac8#",
              "Nf6",
              "Nxe4",
              "Nxe4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Kd2",
              "Nxe3",
              "fxe3",
              "Bb4+",
              "Nc3",
              "Bxc3+",
              "bxc3",
              "c6",
              "e4",
              "Qg5+",
              "Kc2",
              "Qxg2+",
              "Bd2",
              "Qxe4+",
              "Kb3",
              "b5",
              "Bxd5+",
              "Qxd5+",
              "Kc2",
              "Rd8",
              "Rad1",
              "Qxa2+",
              "Kc1",
              "Rd3",
              "Rhg1",
              "Kd5",
              "Rxg7",
              "Bf5",
              "Rg5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 182,
            "completion_tokens": 94,
            "total_tokens": 276,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 182,
            "completion_tokens": 183,
            "total_tokens": 365,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 181,
            "completion_tokens": 640,
            "total_tokens": 821,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 545,
          "total_completion_tokens": 917,
          "total_tokens": 1462
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 182,
            "completion_tokens": 94,
            "total_tokens": 276,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 182,
            "completion_tokens": 183,
            "total_tokens": 365,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 181,
            "completion_tokens": 640,
            "total_tokens": 821,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bq1b1r/ppp3pp/4k3/3np3/2Bn4/4Q3/PPP2PPP/RNB1K2R b KQ - 3 9"
    },
    {
      "turn": 20,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nxe3",
          "aggressive_uci": "c2e3",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4\n8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxe3 11. fxe3 Bb4+ 12. Nc3 Bxc3+ 13. bxc3 c6\n14. e4 b5 15. exd5+ cxd5 16. Bxb5 Qg5+ 17. Kc2 Qxg2+ 18. Bd2 Rf8 19. Rhg1 Qe4+ 20. Bd3 Qa4+ 21. Kc1 Ba6 22. Bxa6 Qxa6 23. Rxg7 Rf1+ 24. Kb2 Rb8+ 25. Kc2 Qa4+ 26. Kd3 Qe4# 1-0\n1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4\n8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxe3 11. fxe3 Bb4+ 12. Nc3 Bxc3+ 13. bxc3 c6\n14. e4 b5 15. exd5+ cxd5 16. Bxb5 Qg5+ 17. Kc2 Qxg2+ 18. Bd2 Rf8 19. Rhg1 Qe4+ 20. Bd3 Qa4+ 21. Kc1 Ba6 22. Bxa6 Qxa6 23. Rxg7 Rf1+ 24. Kb2 Rb8+ 25. Kc2 Qa4+ 26. Kd3 Qe4# 1-0 1-0 1-0 1-0",
          "aggressive_tokens": {
            "prompt_tokens": 192,
            "completion_tokens": 490,
            "total_tokens": 682,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "fxe3",
            "Bb4+",
            "Nc3",
            "Bxc3+",
            "bxc3",
            "c6",
            "e4",
            "b5",
            "exd5+",
            "cxd5",
            "Bxb5",
            "Qg5+",
            "Kc2",
            "Qxg2+",
            "Bd2",
            "Rf8",
            "Rhg1",
            "Qe4+",
            "Bd3",
            "Qa4+",
            "Kc1",
            "Ba6",
            "Bxa6",
            "Qxa6",
            "Rxg7",
            "Rf1+",
            "Kb2",
            "Rb8+",
            "Kc2",
            "Qa4+",
            "Kd3",
            "Qe4#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nxe3",
          "positional_uci": "c2e3",
          "positional_response": "10... Nxe3 11. Kxe3 Bc5+ 12. Ke4 Qh4+ 13. Kd3 Qd4+ 14. Ke2 Qxc4+ 15. Kf3 Rf8+ 16. Kg3 Bxf2+ 17. Kh3 Qh4#",
          "positional_tokens": {
            "prompt_tokens": 192,
            "completion_tokens": 72,
            "total_tokens": 264,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kxe3",
            "Bc5+",
            "Ke4",
            "Qh4+",
            "Kd3",
            "Qd4+",
            "Ke2",
            "Qxc4+",
            "Kf3",
            "Rf8+",
            "Kg3",
            "Bxf2+",
            "Kh3",
            "Qh4#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nxe3",
          "neutral_uci": "c2e3",
          "neutral_response": "10... Nxe3 11. Kxe3 Bc5+ 12. Ke4 Qh4+ 13. Kd3 Qd4+ 14. Ke2 Qxc4+ 15. Kf3 Rf8+ 16. Kg3 Bxf2+ 17. Kh3 Qh4# Bxf2 Qg3#",
          "neutral_tokens": {
            "prompt_tokens": 191,
            "completion_tokens": 79,
            "total_tokens": 270,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kxe3",
            "Bc5+",
            "Ke4",
            "Qh4+",
            "Kd3",
            "Qd4+",
            "Ke2",
            "Qxc4+",
            "Kf3",
            "Rf8+",
            "Kg3",
            "Bxf2+",
            "Kh3",
            "Qh4#",
            "Bxf2",
            "Qg3#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "c2e3",
          "positional_uci": "c2e3",
          "neutral_uci": "c2e3",
          "consensus_move": "c2e3",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "fxe3",
              "Bb4+",
              "Nc3",
              "Bxc3+",
              "bxc3",
              "c6",
              "e4",
              "b5",
              "exd5+",
              "cxd5",
              "Bxb5",
              "Qg5+",
              "Kc2",
              "Qxg2+",
              "Bd2",
              "Rf8",
              "Rhg1",
              "Qe4+",
              "Bd3",
              "Qa4+",
              "Kc1",
              "Ba6",
              "Bxa6",
              "Qxa6",
              "Rxg7",
              "Rf1+",
              "Kb2",
              "Rb8+",
              "Kc2",
              "Qa4+",
              "Kd3",
              "Qe4#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Kxe3",
              "Bc5+",
              "Ke4",
              "Qh4+",
              "Kd3",
              "Qd4+",
              "Ke2",
              "Qxc4+",
              "Kf3",
              "Rf8+",
              "Kg3",
              "Bxf2+",
              "Kh3",
              "Qh4#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Kxe3",
              "Bc5+",
              "Ke4",
              "Qh4+",
              "Kd3",
              "Qd4+",
              "Ke2",
              "Qxc4+",
              "Kf3",
              "Rf8+",
              "Kg3",
              "Bxf2+",
              "Kh3",
              "Qh4#",
              "Bxf2",
              "Qg3#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 192,
            "completion_tokens": 490,
            "total_tokens": 682,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 192,
            "completion_tokens": 72,
            "total_tokens": 264,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 191,
            "completion_tokens": 79,
            "total_tokens": 270,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 575,
          "total_completion_tokens": 641,
          "total_tokens": 1216
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 19,
            "turn_number": 10,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 192,
            "completion_tokens": 490,
            "total_tokens": 682,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 19,
            "turn_number": 10,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 192,
            "completion_tokens": 72,
            "total_tokens": 264,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 19,
            "turn_number": 10,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 191,
            "completion_tokens": 79,
            "total_tokens": 270,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bq1b1r/ppp3pp/4k3/3np3/2B5/4Q3/PPnK1PPP/RNB4R b - - 1 10"
    }
  ],
  "white_errors": [],
  "black_errors": [],
  "white_plan_log": [],
  "black_plan_log": [],
  "timestamp": "2025-11-30T00:11:51.123181",
  "game_id": "mistralai_mistral-small-24b-instruct-2501_vs_gpt-3.5-turbo-instruct_SC_4",
  "configuration": "SC"
}