{
  "result": "gpt-3.5-turbo-instruct wins by disqualification (mistralai/mistral-small-24b-instruct-2501 failed to produce a legal move)",
  "white_player": "mistralai/mistral-small-24b-instruct-2501",
  "black_player": "gpt-3.5-turbo-instruct",
  "white_score": 0.0,
  "black_score": 1.0,
  "total_moves": 16,
  "moves": [
    "e2e4",
    "e7e5",
    "g1f3",
    "b8c6",
    "f1c4",
    "g8f6",
    "b1c3",
    "f6e4",
    "c3e4",
    "d7d5",
    "c4d3",
    "d5e4",
    "d3e4",
    "f8d6",
    "e4d3",
    "e8g8"
  ],
  "game_history": [
    {
      "move_number": 1,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "e2e4",
      "move_san": "e4",
      "fen_after": "rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "move_number": 2,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "e7e5",
      "move_san": "e5",
      "fen_after": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "move_number": 3,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "g1f3",
      "move_san": "Nf3",
      "fen_after": "rnbqkbnr/pppp1ppp/8/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R b KQkq - 1 2"
    },
    {
      "move_number": 4,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "b8c6",
      "move_san": "Nc6",
      "fen_after": "r1bqkbnr/pppp1ppp/2n5/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3"
    },
    {
      "move_number": 5,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "f1c4",
      "move_san": "Bc4",
      "fen_after": "r1bqkbnr/pppp1ppp/2n5/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R b KQkq - 3 3"
    },
    {
      "move_number": 6,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "g8f6",
      "move_san": "Nf6",
      "fen_after": "r1bqkb1r/pppp1ppp/2n2n2/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R w KQkq - 4 4"
    },
    {
      "move_number": 7,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "b1c3",
      "move_san": "Nc3",
      "fen_after": "r1bqkb1r/pppp1ppp/2n2n2/4p3/2B1P3/2N2N2/PPPP1PPP/R1BQK2R b KQkq - 5 4"
    },
    {
      "move_number": 8,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "f6e4",
      "move_san": "Nxe4",
      "fen_after": "r1bqkb1r/pppp1ppp/2n5/4p3/2B1n3/2N2N2/PPPP1PPP/R1BQK2R w KQkq - 0 5"
    },
    {
      "move_number": 9,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "c3e4",
      "move_san": "Nxe4",
      "fen_after": "r1bqkb1r/pppp1ppp/2n5/4p3/2B1N3/5N2/PPPP1PPP/R1BQK2R b KQkq - 0 5"
    },
    {
      "move_number": 10,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "d7d5",
      "move_san": "d5",
      "fen_after": "r1bqkb1r/ppp2ppp/2n5/3pp3/2B1N3/5N2/PPPP1PPP/R1BQK2R w KQkq - 0 6"
    },
    {
      "move_number": 11,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "c4d3",
      "move_san": "Bd3",
      "fen_after": "r1bqkb1r/ppp2ppp/2n5/3pp3/4N3/3B1N2/PPPP1PPP/R1BQK2R b KQkq - 1 6"
    },
    {
      "move_number": 12,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "d5e4",
      "move_san": "dxe4",
      "fen_after": "r1bqkb1r/ppp2ppp/2n5/4p3/4p3/3B1N2/PPPP1PPP/R1BQK2R w KQkq - 0 7"
    },
    {
      "move_number": 13,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "d3e4",
      "move_san": "Bxe4",
      "fen_after": "r1bqkb1r/ppp2ppp/2n5/4p3/4B3/5N2/PPPP1PPP/R1BQK2R b KQkq - 0 7"
    },
    {
      "move_number": 14,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "f8d6",
      "move_san": "Bd6",
      "fen_after": "r1bqk2r/ppp2ppp/2nb4/4p3/4B3/5N2/PPPP1PPP/R1BQK2R w KQkq - 1 8"
    },
    {
      "move_number": 15,
      "player": "mistralai/mistral-small-24b-instruct-2501",
      "color": "white",
      "move_uci": "e4d3",
      "move_san": "Bd3",
      "fen_after": "r1bqk2r/ppp2ppp/2nb4/4p3/8/3B1N2/PPPP1PPP/R1BQK2R b KQkq - 2 8"
    },
    {
      "move_number": 16,
      "player": "gpt-3.5-turbo-instruct",
      "color": "black",
      "move_uci": "e8g8",
      "move_san": "O-O",
      "fen_after": "r1bq1rk1/ppp2ppp/2nb4/4p3/8/3B1N2/PPPP1PPP/R1BQK2R w KQ - 3 9"
    }
  ],
  "final_fen": "r1bq1rk1/ppp2ppp/2nb4/4p3/8/3B1N2/PPPP1PPP/R1BQK2R w KQ - 3 9",
  "white_tokens": {
    "prompt_tokens": 10748,
    "completion_tokens": 15998,
    "total_tokens": 26746
  },
  "black_tokens": {
    "prompt_tokens": 11900,
    "completion_tokens": 41736,
    "total_tokens": 53636
  },
  "white_token_log": [
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 113,
          "total_tokens": 227,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 215,
          "total_tokens": 329,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 166,
          "total_tokens": 279,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 341,
        "total_completion_tokens": 494,
        "total_tokens": 835
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 113,
          "total_tokens": 227,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 215,
          "total_tokens": 329,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 166,
          "total_tokens": 279,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 3,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 120,
          "completion_tokens": 640,
          "total_tokens": 760,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 120,
          "completion_tokens": 640,
          "total_tokens": 760,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 119,
          "completion_tokens": 640,
          "total_tokens": 759,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 359,
        "total_completion_tokens": 1920,
        "total_tokens": 2279
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 120,
          "completion_tokens": 640,
          "total_tokens": 760,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 120,
          "completion_tokens": 640,
          "total_tokens": 760,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 119,
          "completion_tokens": 640,
          "total_tokens": 759,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 5,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 129,
          "completion_tokens": 95,
          "total_tokens": 224,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 129,
          "completion_tokens": 41,
          "total_tokens": 170,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 128,
          "completion_tokens": 640,
          "total_tokens": 768,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 386,
        "total_completion_tokens": 776,
        "total_tokens": 1162
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 129,
          "completion_tokens": 95,
          "total_tokens": 224,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 129,
          "completion_tokens": 41,
          "total_tokens": 170,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 128,
          "completion_tokens": 640,
          "total_tokens": 768,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 7,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 137,
          "completion_tokens": 27,
          "total_tokens": 164,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 137,
          "completion_tokens": 132,
          "total_tokens": 269,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 136,
          "completion_tokens": 27,
          "total_tokens": 163,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 410,
        "total_completion_tokens": 186,
        "total_tokens": 596
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 137,
          "completion_tokens": 27,
          "total_tokens": 164,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 137,
          "completion_tokens": 132,
          "total_tokens": 269,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 136,
          "completion_tokens": 27,
          "total_tokens": 163,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 9,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 146,
          "completion_tokens": 36,
          "total_tokens": 182,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 146,
          "completion_tokens": 110,
          "total_tokens": 256,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 145,
          "completion_tokens": 39,
          "total_tokens": 184,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 437,
        "total_completion_tokens": 185,
        "total_tokens": 622
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 146,
          "completion_tokens": 36,
          "total_tokens": 182,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 146,
          "completion_tokens": 110,
          "total_tokens": 256,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 145,
          "completion_tokens": 39,
          "total_tokens": 184,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 11,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 154,
          "completion_tokens": 81,
          "total_tokens": 235,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 154,
          "completion_tokens": 68,
          "total_tokens": 222,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 153,
          "completion_tokens": 81,
          "total_tokens": 234,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 461,
        "total_completion_tokens": 230,
        "total_tokens": 691
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 154,
          "completion_tokens": 81,
          "total_tokens": 235,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 154,
          "completion_tokens": 68,
          "total_tokens": 222,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 153,
          "completion_tokens": 81,
          "total_tokens": 234,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 13,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 162,
          "completion_tokens": 55,
          "total_tokens": 217,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 162,
          "completion_tokens": 640,
          "total_tokens": 802,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 161,
          "completion_tokens": 108,
          "total_tokens": 269,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 485,
        "total_completion_tokens": 803,
        "total_tokens": 1288
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 162,
          "completion_tokens": 55,
          "total_tokens": 217,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 162,
          "completion_tokens": 640,
          "total_tokens": 802,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 161,
          "completion_tokens": 108,
          "total_tokens": 269,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 223,
          "total_tokens": 337,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 640,
          "total_tokens": 753,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 341,
        "total_completion_tokens": 1503,
        "total_tokens": 1844
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 223,
          "total_tokens": 337,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 640,
          "total_tokens": 753,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 3,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 120,
          "completion_tokens": 174,
          "total_tokens": 294,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 120,
          "completion_tokens": 163,
          "total_tokens": 283,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 119,
          "completion_tokens": 640,
          "total_tokens": 759,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 359,
        "total_completion_tokens": 977,
        "total_tokens": 1336
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 120,
          "completion_tokens": 174,
          "total_tokens": 294,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 120,
          "completion_tokens": 163,
          "total_tokens": 283,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 119,
          "completion_tokens": 640,
          "total_tokens": 759,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 5,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 129,
          "completion_tokens": 105,
          "total_tokens": 234,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 129,
          "completion_tokens": 174,
          "total_tokens": 303,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 128,
          "completion_tokens": 95,
          "total_tokens": 223,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 386,
        "total_completion_tokens": 374,
        "total_tokens": 760
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 129,
          "completion_tokens": 105,
          "total_tokens": 234,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 129,
          "completion_tokens": 174,
          "total_tokens": 303,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 128,
          "completion_tokens": 95,
          "total_tokens": 223,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 7,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 137,
          "completion_tokens": 27,
          "total_tokens": 164,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 137,
          "completion_tokens": 75,
          "total_tokens": 212,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 136,
          "completion_tokens": 115,
          "total_tokens": 251,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 410,
        "total_completion_tokens": 217,
        "total_tokens": 627
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 137,
          "completion_tokens": 27,
          "total_tokens": 164,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 137,
          "completion_tokens": 75,
          "total_tokens": 212,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 136,
          "completion_tokens": 115,
          "total_tokens": 251,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 9,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 144,
          "completion_tokens": 34,
          "total_tokens": 178,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 144,
          "completion_tokens": 59,
          "total_tokens": 203,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 143,
          "completion_tokens": 640,
          "total_tokens": 783,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 431,
        "total_completion_tokens": 733,
        "total_tokens": 1164
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 144,
          "completion_tokens": 34,
          "total_tokens": 178,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 144,
          "completion_tokens": 59,
          "total_tokens": 203,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 143,
          "completion_tokens": 640,
          "total_tokens": 783,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 11,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 153,
          "completion_tokens": 70,
          "total_tokens": 223,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 153,
          "completion_tokens": 70,
          "total_tokens": 223,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 152,
          "completion_tokens": 70,
          "total_tokens": 222,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 458,
        "total_completion_tokens": 210,
        "total_tokens": 668
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 153,
          "completion_tokens": 70,
          "total_tokens": 223,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 153,
          "completion_tokens": 70,
          "total_tokens": 223,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 152,
          "completion_tokens": 70,
          "total_tokens": 222,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 13,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 162,
          "completion_tokens": 90,
          "total_tokens": 252,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 162,
          "completion_tokens": 70,
          "total_tokens": 232,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 161,
          "completion_tokens": 70,
          "total_tokens": 231,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 485,
        "total_completion_tokens": 230,
        "total_tokens": 715
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 162,
          "completion_tokens": 90,
          "total_tokens": 252,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 162,
          "completion_tokens": 70,
          "total_tokens": 232,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 161,
          "completion_tokens": 70,
          "total_tokens": 231,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 15,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 170,
          "completion_tokens": 71,
          "total_tokens": 241,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 170,
          "completion_tokens": 71,
          "total_tokens": 241,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 169,
          "completion_tokens": 61,
          "total_tokens": 230,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 509,
        "total_completion_tokens": 203,
        "total_tokens": 712
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 170,
          "completion_tokens": 71,
          "total_tokens": 241,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 170,
          "completion_tokens": 71,
          "total_tokens": 241,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 169,
          "completion_tokens": 61,
          "total_tokens": 230,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 17,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 179,
          "completion_tokens": 120,
          "total_tokens": 299,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 179,
          "completion_tokens": 101,
          "total_tokens": 280,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 178,
          "completion_tokens": 95,
          "total_tokens": 273,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 536,
        "total_completion_tokens": 316,
        "total_tokens": 852
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 179,
          "completion_tokens": 120,
          "total_tokens": 299,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 179,
          "completion_tokens": 101,
          "total_tokens": 280,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 178,
          "completion_tokens": 95,
          "total_tokens": 273,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 19,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 189,
          "completion_tokens": 640,
          "total_tokens": 829,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 189,
          "completion_tokens": 167,
          "total_tokens": 356,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 188,
          "completion_tokens": 166,
          "total_tokens": 354,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 566,
        "total_completion_tokens": 973,
        "total_tokens": 1539
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 189,
          "completion_tokens": 640,
          "total_tokens": 829,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 189,
          "completion_tokens": 167,
          "total_tokens": 356,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 188,
          "completion_tokens": 166,
          "total_tokens": 354,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 172,
          "total_tokens": 286,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 640,
          "total_tokens": 753,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 341,
        "total_completion_tokens": 1452,
        "total_tokens": 1793
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 172,
          "total_tokens": 286,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 640,
          "total_tokens": 753,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 3,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 120,
          "completion_tokens": 640,
          "total_tokens": 760,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 120,
          "completion_tokens": 640,
          "total_tokens": 760,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 119,
          "completion_tokens": 640,
          "total_tokens": 759,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 359,
        "total_completion_tokens": 1920,
        "total_tokens": 2279
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 120,
          "completion_tokens": 640,
          "total_tokens": 760,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 120,
          "completion_tokens": 640,
          "total_tokens": 760,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 119,
          "completion_tokens": 640,
          "total_tokens": 759,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 5,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 129,
          "completion_tokens": 640,
          "total_tokens": 769,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 129,
          "completion_tokens": 41,
          "total_tokens": 170,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 128,
          "completion_tokens": 95,
          "total_tokens": 223,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 386,
        "total_completion_tokens": 776,
        "total_tokens": 1162
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 129,
          "completion_tokens": 640,
          "total_tokens": 769,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 129,
          "completion_tokens": 41,
          "total_tokens": 170,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 128,
          "completion_tokens": 95,
          "total_tokens": 223,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 7,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 137,
          "completion_tokens": 27,
          "total_tokens": 164,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 137,
          "completion_tokens": 124,
          "total_tokens": 261,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 136,
          "completion_tokens": 27,
          "total_tokens": 163,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 410,
        "total_completion_tokens": 178,
        "total_tokens": 588
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 137,
          "completion_tokens": 27,
          "total_tokens": 164,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 137,
          "completion_tokens": 124,
          "total_tokens": 261,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 136,
          "completion_tokens": 27,
          "total_tokens": 163,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 9,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 146,
          "completion_tokens": 39,
          "total_tokens": 185,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 146,
          "completion_tokens": 39,
          "total_tokens": 185,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 145,
          "completion_tokens": 79,
          "total_tokens": 224,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 437,
        "total_completion_tokens": 157,
        "total_tokens": 594
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 146,
          "completion_tokens": 39,
          "total_tokens": 185,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 146,
          "completion_tokens": 39,
          "total_tokens": 185,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 145,
          "completion_tokens": 79,
          "total_tokens": 224,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 11,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 154,
          "completion_tokens": 640,
          "total_tokens": 794,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 154,
          "completion_tokens": 69,
          "total_tokens": 223,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 153,
          "completion_tokens": 76,
          "total_tokens": 229,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 461,
        "total_completion_tokens": 785,
        "total_tokens": 1246
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 154,
          "completion_tokens": 640,
          "total_tokens": 794,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 154,
          "completion_tokens": 69,
          "total_tokens": 223,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 153,
          "completion_tokens": 76,
          "total_tokens": 229,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 13,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 162,
          "completion_tokens": 55,
          "total_tokens": 217,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 162,
          "completion_tokens": 84,
          "total_tokens": 246,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 161,
          "completion_tokens": 62,
          "total_tokens": 223,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 485,
        "total_completion_tokens": 201,
        "total_tokens": 686
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 162,
          "completion_tokens": 55,
          "total_tokens": 217,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 162,
          "completion_tokens": 84,
          "total_tokens": 246,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 161,
          "completion_tokens": 62,
          "total_tokens": 223,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 15,
      "attempt": 2,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 170,
          "completion_tokens": 76,
          "total_tokens": 246,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 170,
          "completion_tokens": 60,
          "total_tokens": 230,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 169,
          "completion_tokens": 63,
          "total_tokens": 232,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 509,
        "total_completion_tokens": 199,
        "total_tokens": 708
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 170,
          "completion_tokens": 76,
          "total_tokens": 246,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 170,
          "completion_tokens": 60,
          "total_tokens": 230,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 169,
          "completion_tokens": 63,
          "total_tokens": 232,
          "model": "mistralai/mistral-small-24b-instruct-2501",
          "finish_reason": "stop"
        }
      }
    }
  ],
  "black_token_log": [
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 640,
          "total_tokens": 749,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 329,
        "total_completion_tokens": 1920,
        "total_tokens": 2249
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 640,
          "total_tokens": 749,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 2,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 429,
          "total_tokens": 543,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 498,
          "total_tokens": 612,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 640,
          "total_tokens": 753,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 341,
        "total_completion_tokens": 1567,
        "total_tokens": 1908
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 429,
          "total_tokens": 543,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 498,
          "total_tokens": 612,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 640,
          "total_tokens": 753,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 4,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 122,
          "completion_tokens": 258,
          "total_tokens": 380,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 122,
          "completion_tokens": 640,
          "total_tokens": 762,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 121,
          "completion_tokens": 640,
          "total_tokens": 761,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 365,
        "total_completion_tokens": 1538,
        "total_tokens": 1903
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 122,
          "completion_tokens": 258,
          "total_tokens": 380,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 122,
          "completion_tokens": 640,
          "total_tokens": 762,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 121,
          "completion_tokens": 640,
          "total_tokens": 761,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 6,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 131,
          "completion_tokens": 640,
          "total_tokens": 771,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 131,
          "completion_tokens": 640,
          "total_tokens": 771,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 130,
          "completion_tokens": 174,
          "total_tokens": 304,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 392,
        "total_completion_tokens": 1454,
        "total_tokens": 1846
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 131,
          "completion_tokens": 640,
          "total_tokens": 771,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 131,
          "completion_tokens": 640,
          "total_tokens": 771,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 130,
          "completion_tokens": 174,
          "total_tokens": 304,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 8,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 140,
          "completion_tokens": 640,
          "total_tokens": 780,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 140,
          "completion_tokens": 640,
          "total_tokens": 780,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 139,
          "completion_tokens": 292,
          "total_tokens": 431,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 419,
        "total_completion_tokens": 1572,
        "total_tokens": 1991
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 140,
          "completion_tokens": 640,
          "total_tokens": 780,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 140,
          "completion_tokens": 640,
          "total_tokens": 780,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 139,
          "completion_tokens": 292,
          "total_tokens": 431,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 10,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 640,
          "total_tokens": 789,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 388,
          "total_tokens": 537,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 640,
          "total_tokens": 788,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 446,
        "total_completion_tokens": 1668,
        "total_tokens": 2114
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 640,
          "total_tokens": 789,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 388,
          "total_tokens": 537,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 640,
          "total_tokens": 788,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 12,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 640,
          "total_tokens": 796,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 365,
          "total_tokens": 521,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 640,
          "total_tokens": 795,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 467,
        "total_completion_tokens": 1645,
        "total_tokens": 2112
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 640,
          "total_tokens": 796,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 365,
          "total_tokens": 521,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 640,
          "total_tokens": 795,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 14,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 164,
          "completion_tokens": 156,
          "total_tokens": 320,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 164,
          "completion_tokens": 182,
          "total_tokens": 346,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 163,
          "completion_tokens": 332,
          "total_tokens": 495,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 491,
        "total_completion_tokens": 670,
        "total_tokens": 1161
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 164,
          "completion_tokens": 156,
          "total_tokens": 320,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 164,
          "completion_tokens": 182,
          "total_tokens": 346,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 163,
          "completion_tokens": 332,
          "total_tokens": 495,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 640,
          "total_tokens": 749,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 329,
        "total_completion_tokens": 1920,
        "total_tokens": 2249
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 640,
          "total_tokens": 750,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 640,
          "total_tokens": 749,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 2,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 345,
          "total_tokens": 458,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 341,
        "total_completion_tokens": 1625,
        "total_tokens": 1966
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 345,
          "total_tokens": 458,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 4,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 122,
          "completion_tokens": 362,
          "total_tokens": 484,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 122,
          "completion_tokens": 640,
          "total_tokens": 762,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 121,
          "completion_tokens": 640,
          "total_tokens": 761,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 365,
        "total_completion_tokens": 1642,
        "total_tokens": 2007
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 122,
          "completion_tokens": 362,
          "total_tokens": 484,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 122,
          "completion_tokens": 640,
          "total_tokens": 762,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 121,
          "completion_tokens": 640,
          "total_tokens": 761,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 6,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 131,
          "completion_tokens": 640,
          "total_tokens": 771,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 131,
          "completion_tokens": 640,
          "total_tokens": 771,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 130,
          "completion_tokens": 640,
          "total_tokens": 770,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 392,
        "total_completion_tokens": 1920,
        "total_tokens": 2312
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 131,
          "completion_tokens": 640,
          "total_tokens": 771,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 131,
          "completion_tokens": 640,
          "total_tokens": 771,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 130,
          "completion_tokens": 640,
          "total_tokens": 770,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 8,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 139,
          "completion_tokens": 640,
          "total_tokens": 779,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 139,
          "completion_tokens": 640,
          "total_tokens": 779,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 138,
          "completion_tokens": 640,
          "total_tokens": 778,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 416,
        "total_completion_tokens": 1920,
        "total_tokens": 2336
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 139,
          "completion_tokens": 640,
          "total_tokens": 779,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 139,
          "completion_tokens": 640,
          "total_tokens": 779,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 138,
          "completion_tokens": 640,
          "total_tokens": 778,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 10,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 147,
          "completion_tokens": 174,
          "total_tokens": 321,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 147,
          "completion_tokens": 215,
          "total_tokens": 362,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 146,
          "completion_tokens": 640,
          "total_tokens": 786,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 440,
        "total_completion_tokens": 1029,
        "total_tokens": 1469
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 147,
          "completion_tokens": 174,
          "total_tokens": 321,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 147,
          "completion_tokens": 215,
          "total_tokens": 362,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 146,
          "completion_tokens": 640,
          "total_tokens": 786,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 12,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 178,
          "total_tokens": 334,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 640,
          "total_tokens": 796,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 640,
          "total_tokens": 795,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 467,
        "total_completion_tokens": 1458,
        "total_tokens": 1925
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 178,
          "total_tokens": 334,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 640,
          "total_tokens": 796,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 640,
          "total_tokens": 795,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 14,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 164,
          "completion_tokens": 505,
          "total_tokens": 669,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 164,
          "completion_tokens": 640,
          "total_tokens": 804,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 163,
          "completion_tokens": 640,
          "total_tokens": 803,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 491,
        "total_completion_tokens": 1785,
        "total_tokens": 2276
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 164,
          "completion_tokens": 505,
          "total_tokens": 669,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 164,
          "completion_tokens": 640,
          "total_tokens": 804,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 163,
          "completion_tokens": 640,
          "total_tokens": 803,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 16,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 174,
          "completion_tokens": 640,
          "total_tokens": 814,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 174,
          "completion_tokens": 640,
          "total_tokens": 814,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 173,
          "completion_tokens": 640,
          "total_tokens": 813,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 521,
        "total_completion_tokens": 1920,
        "total_tokens": 2441
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 174,
          "completion_tokens": 640,
          "total_tokens": 814,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 174,
          "completion_tokens": 640,
          "total_tokens": 814,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 173,
          "completion_tokens": 640,
          "total_tokens": 813,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 18,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 182,
          "completion_tokens": 94,
          "total_tokens": 276,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 182,
          "completion_tokens": 183,
          "total_tokens": 365,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 181,
          "completion_tokens": 640,
          "total_tokens": 821,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 545,
        "total_completion_tokens": 917,
        "total_tokens": 1462
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 182,
          "completion_tokens": 94,
          "total_tokens": 276,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 182,
          "completion_tokens": 183,
          "total_tokens": 365,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 181,
          "completion_tokens": 640,
          "total_tokens": 821,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 20,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 192,
          "completion_tokens": 490,
          "total_tokens": 682,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 192,
          "completion_tokens": 72,
          "total_tokens": 264,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 191,
          "completion_tokens": 79,
          "total_tokens": 270,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 575,
        "total_completion_tokens": 641,
        "total_tokens": 1216
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 192,
          "completion_tokens": 490,
          "total_tokens": 682,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 192,
          "completion_tokens": 72,
          "total_tokens": 264,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 191,
          "completion_tokens": 79,
          "total_tokens": 270,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 1,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 138,
          "total_tokens": 248,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 430,
          "total_tokens": 540,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 640,
          "total_tokens": 749,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 329,
        "total_completion_tokens": 1208,
        "total_tokens": 1537
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 110,
          "completion_tokens": 138,
          "total_tokens": 248,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 110,
          "completion_tokens": 430,
          "total_tokens": 540,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 109,
          "completion_tokens": 640,
          "total_tokens": 749,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 2,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 230,
          "total_tokens": 343,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 341,
        "total_completion_tokens": 1510,
        "total_tokens": 1851
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 114,
          "completion_tokens": 640,
          "total_tokens": 754,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 113,
          "completion_tokens": 230,
          "total_tokens": 343,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 4,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 122,
          "completion_tokens": 640,
          "total_tokens": 762,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 122,
          "completion_tokens": 640,
          "total_tokens": 762,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 121,
          "completion_tokens": 401,
          "total_tokens": 522,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 365,
        "total_completion_tokens": 1681,
        "total_tokens": 2046
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 122,
          "completion_tokens": 640,
          "total_tokens": 762,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 122,
          "completion_tokens": 640,
          "total_tokens": 762,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 121,
          "completion_tokens": 401,
          "total_tokens": 522,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 6,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 131,
          "completion_tokens": 441,
          "total_tokens": 572,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 131,
          "completion_tokens": 419,
          "total_tokens": 550,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 130,
          "completion_tokens": 640,
          "total_tokens": 770,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "total_prompt_tokens": 392,
        "total_completion_tokens": 1500,
        "total_tokens": 1892
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 131,
          "completion_tokens": 441,
          "total_tokens": 572,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 131,
          "completion_tokens": 419,
          "total_tokens": 550,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 130,
          "completion_tokens": 640,
          "total_tokens": 770,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        }
      }
    },
    {
      "turn": 8,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 140,
          "completion_tokens": 640,
          "total_tokens": 780,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 140,
          "completion_tokens": 640,
          "total_tokens": 780,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 139,
          "completion_tokens": 285,
          "total_tokens": 424,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 419,
        "total_completion_tokens": 1565,
        "total_tokens": 1984
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 140,
          "completion_tokens": 640,
          "total_tokens": 780,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 140,
          "completion_tokens": 640,
          "total_tokens": 780,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 139,
          "completion_tokens": 285,
          "total_tokens": 424,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 10,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 418,
          "total_tokens": 567,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 640,
          "total_tokens": 789,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 424,
          "total_tokens": 572,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 446,
        "total_completion_tokens": 1482,
        "total_tokens": 1928
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 149,
          "completion_tokens": 418,
          "total_tokens": 567,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 149,
          "completion_tokens": 640,
          "total_tokens": 789,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 148,
          "completion_tokens": 424,
          "total_tokens": 572,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 12,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 199,
          "total_tokens": 355,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 255,
          "total_tokens": 411,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 148,
          "total_tokens": 303,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 467,
        "total_completion_tokens": 602,
        "total_tokens": 1069
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 156,
          "completion_tokens": 199,
          "total_tokens": 355,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "positional": {
          "prompt_tokens": 156,
          "completion_tokens": 255,
          "total_tokens": 411,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 155,
          "completion_tokens": 148,
          "total_tokens": 303,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 14,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 165,
          "completion_tokens": 640,
          "total_tokens": 805,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 165,
          "completion_tokens": 393,
          "total_tokens": 558,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 164,
          "completion_tokens": 462,
          "total_tokens": 626,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 494,
        "total_completion_tokens": 1495,
        "total_tokens": 1989
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 165,
          "completion_tokens": 640,
          "total_tokens": 805,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 165,
          "completion_tokens": 393,
          "total_tokens": 558,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "neutral": {
          "prompt_tokens": 164,
          "completion_tokens": 462,
          "total_tokens": 626,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    },
    {
      "turn": 16,
      "attempt": 1,
      "tokens": {
        "aggressive": {
          "prompt_tokens": 172,
          "completion_tokens": 640,
          "total_tokens": 812,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 172,
          "completion_tokens": 640,
          "total_tokens": 812,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 171,
          "completion_tokens": 602,
          "total_tokens": 773,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        },
        "total_prompt_tokens": 515,
        "total_completion_tokens": 1882,
        "total_tokens": 2397
      },
      "individual_agent_tokens": {
        "aggressive": {
          "prompt_tokens": 172,
          "completion_tokens": 640,
          "total_tokens": 812,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "positional": {
          "prompt_tokens": 172,
          "completion_tokens": 640,
          "total_tokens": 812,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "length"
        },
        "neutral": {
          "prompt_tokens": 171,
          "completion_tokens": 602,
          "total_tokens": 773,
          "model": "gpt-3.5-turbo-instruct",
          "finish_reason": "stop"
        }
      }
    }
  ],
  "white_responses": [
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4\n\n1. d4",
          "aggressive_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4\n\n1. d4",
          "positional_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": null,
          "neutral_response": "1. d4\n\n1. d4",
          "neutral_tokens": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 353,
          "total_completion_tokens": 27,
          "total_tokens": 380
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 2,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4\n\n1. d4",
          "aggressive_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4\n\n1. d4",
          "positional_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": null,
          "neutral_response": "1. d4\n\n1. d4",
          "neutral_tokens": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 353,
          "total_completion_tokens": 27,
          "total_tokens": 380
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 2,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4\n\n1. d4",
          "aggressive_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4\n\n1. d4",
          "positional_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": null,
          "neutral_response": "1. d4\n\n1. d4",
          "neutral_tokens": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 353,
          "total_completion_tokens": 27,
          "total_tokens": 380
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": "e2e4",
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Bc5\n4. c3 Nf6\n5. d4 exd4\n6. cxd4 Bb4+\n7. Nc3 Nxe4\n8. O-O Bxc3\n9. d5 Bxa1\n10. Qxa1 Nc5\n11. Qd4 Ncd7\n12. Qe5 Nc6\n13. Qf5\n\nQd7",
          "aggressive_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 113,
            "total_tokens": 227,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bc4",
            "Bc5",
            "c3",
            "Nf6",
            "d4",
            "exd4",
            "cxd4",
            "Bb4+",
            "Nc3",
            "Nxe4",
            "O-O",
            "Bxc3",
            "d5",
            "Bxa1",
            "Qxa1",
            "Nc5",
            "Qd4",
            "Ncd7",
            "Qe5",
            "Nc6",
            "Qf5",
            "Qd7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bb5 a6\n4. Ba4 Nf6\n5. O-O Be7\n6. Re1 b5\n7. Bb3 d6\n8. c3 O-O\n9. h3 Na5\n10. Bc2 c5\n11. d4 Nc6\n12. Nbd2 cxd4\n13. cxd4 Nxd4\n14. Nxd4 Qxd4\n15. Nf3 Qd7\n16. Bg5 Qd8\n17. Qd3 Qd7\n18. Qxd7 Bxd7\n19. Bxe7 Nxe7\n20. Rc1 Nc6\n21. Rfd1 Rfc8\n22. Rxc8 Rxc8\n23. Rc1 Rxc1\n24. Bxc1\n\nNf5",
          "positional_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 215,
            "total_tokens": 329,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "Nxd4",
            "Nxd4",
            "Qxd4",
            "Nf3",
            "Qd7",
            "Bg5",
            "Qd8",
            "Qd3",
            "Qd7",
            "Qxd7",
            "Bxd7",
            "Bxe7",
            "Nxe7",
            "Rc1",
            "Nc6",
            "Rfd1",
            "Rfc8",
            "Rxc8",
            "Rxc8",
            "Rc1",
            "Rxc1",
            "Bxc1",
            "Nf5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": "e2e4",
          "neutral_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Bc5\n4. c3 Nf6\n5. d4 exd4\n6. cxd4 Bb4+\n7. Nc3 Nxe4\n8. O-O Bxc3\n9. d5 Bxa1\n10. Qxa1 Nc5\n11. Qd4 Ncd7\n12. Nxe5 Nxe5\n13. Qxe5 O-O\n14. Qh5 Qf6\n15. Qxf7+ Kh8\n16. Qf5+ Qg6\n17. Qxg6+ hxg6\n18. Bg5\n\n18...",
          "neutral_tokens": {
            "prompt_tokens": 113,
            "completion_tokens": 166,
            "total_tokens": 279,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bc4",
            "Bc5",
            "c3",
            "Nf6",
            "d4",
            "exd4",
            "cxd4",
            "Bb4+",
            "Nc3",
            "Nxe4",
            "O-O",
            "Bxc3",
            "d5",
            "Bxa1",
            "Qxa1",
            "Nc5",
            "Qd4",
            "Ncd7",
            "Nxe5",
            "Nxe5",
            "Qxe5",
            "O-O",
            "Qh5",
            "Qf6",
            "Qxf7+",
            "Kh8",
            "Qf5+",
            "Qg6",
            "Qxg6+",
            "hxg6",
            "Bg5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e2e4",
          "positional_uci": "e2e4",
          "neutral_uci": "e2e4",
          "consensus_move": "e2e4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bc4",
              "Bc5",
              "c3",
              "Nf6",
              "d4",
              "exd4",
              "cxd4",
              "Bb4+",
              "Nc3",
              "Nxe4",
              "O-O",
              "Bxc3",
              "d5",
              "Bxa1",
              "Qxa1",
              "Nc5",
              "Qd4",
              "Ncd7",
              "Qe5",
              "Nc6",
              "Qf5",
              "Qd7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "Nxd4",
              "Nxd4",
              "Qxd4",
              "Nf3",
              "Qd7",
              "Bg5",
              "Qd8",
              "Qd3",
              "Qd7",
              "Qxd7",
              "Bxd7",
              "Bxe7",
              "Nxe7",
              "Rc1",
              "Nc6",
              "Rfd1",
              "Rfc8",
              "Rxc8",
              "Rxc8",
              "Rc1",
              "Rxc1",
              "Bxc1",
              "Nf5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bc4",
              "Bc5",
              "c3",
              "Nf6",
              "d4",
              "exd4",
              "cxd4",
              "Bb4+",
              "Nc3",
              "Nxe4",
              "O-O",
              "Bxc3",
              "d5",
              "Bxa1",
              "Qxa1",
              "Nc5",
              "Qd4",
              "Ncd7",
              "Nxe5",
              "Nxe5",
              "Qxe5",
              "O-O",
              "Qh5",
              "Qf6",
              "Qxf7+",
              "Kh8",
              "Qf5+",
              "Qg6",
              "Qxg6+",
              "hxg6",
              "Bg5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 114,
            "completion_tokens": 113,
            "total_tokens": 227,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 114,
            "completion_tokens": 215,
            "total_tokens": 329,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 113,
            "completion_tokens": 166,
            "total_tokens": 279,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 341,
          "total_completion_tokens": 494,
          "total_tokens": 835
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 114,
            "completion_tokens": 113,
            "total_tokens": 227,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 114,
            "completion_tokens": 215,
            "total_tokens": 329,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 113,
            "completion_tokens": 166,
            "total_tokens": 279,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 3,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf3",
          "aggressive_uci": "g1f3",
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Bc5\n4. c3 Nf6\n5. d4 exd4\n6. cxd4 Bb4+\n7. Nc3 Nxe4\n8. O-O Bxc3\n9. d5 Bxa1\n10. Qxa1 Nc5\n11. Qd4 Qf6\n12. Qxc5 Qxc5\n13. Nxe5 Qd6\n14. Nc4 O-O\n15. Be3 Qd7\n16. Bxc5 Qxc5\n17. Nxb7 Qxb7\n18. Rc1 Qd7\n19. Rc7 Qd8\n20. Rc8+ Rxc8\n21. Bxc8 Qxc8\n22. Rc1 Qd8\n23. Rc7 Qd6\n24. Rc8+ Rxc8\n25. Bxc8 Qxc8\n26. Rc1 Qd8\n27. Rc7 Qd6\n28. Rc8+ Rxc8\n29. Bxc8 Qxc8\n30. Rc1 Qd8\n31. Rc7 Qd6\n32. Rc8+ Rxc8\n33. Bxc8 Qxc8\n34. Rc1 Qd8\n35. Rc7 Qd6\n36. Rc8+ Rxc8\n37. Bxc8 Qxc8\n38. Rc1 Qd8\n39. Rc7 Qd6\n40. Rc8+ Rxc8\n41. Bxc8 Qxc8\n42. Rc1 Qd8\n43. Rc7 Qd6\n44. Rc8+ Rxc8\n45. Bxc8 Qxc8\n46. Rc1 Qd8\n47. Rc7 Qd6\n48. Rc8+ Rxc8\n49. Bxc8 Qxc8\n50. Rc1 Qd8\n51. Rc7 Qd6\n52. Rc8+ Rxc8\n53. Bxc8 Qxc8\n54. Rc1 Qd8\n55. Rc7 Qd6\n56. Rc8+ Rxc8\n57. Bxc8 Qxc8\n58. Rc1 Qd8\n59. Rc7 Qd6\n60. Rc8+ Rxc8\n61. Bxc8 Qxc8\n62. Rc1 Qd8\n63. Rc7 Qd6\n64. Rc8+ Rxc8\n65. Bxc8 Q",
          "aggressive_tokens": {
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nc6",
            "Bc4",
            "Bc5",
            "c3",
            "Nf6",
            "d4",
            "exd4",
            "cxd4",
            "Bb4+",
            "Nc3",
            "Nxe4",
            "O-O",
            "Bxc3",
            "d5",
            "Bxa1",
            "Qxa1",
            "Nc5",
            "Qd4",
            "Qf6",
            "Qxc5",
            "Qxc5",
            "Nxe5",
            "Qd6",
            "Nc4",
            "O-O",
            "Be3",
            "Qd7",
            "Bxc5",
            "Qxc5",
            "Nxb7",
            "Qxb7",
            "Rc1",
            "Qd7",
            "Rc7",
            "Qd8",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8",
            "Qxc8",
            "Rc1",
            "Qd8",
            "Rc7",
            "Qd6",
            "Rc8+",
            "Rxc8",
            "Bxc8"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf3",
          "positional_uci": "g1f3",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5 11. d4 Nc6 12. Nbd2 cxd4 13. cxd4 exd4 14. Nxd4 Nxd4 15. Qxd4 Bc5 16. Qd2 Bxd4 17. Bxd4 Qxd4 18. Qxd4 Nxd4 19. Rxd4 Rfd8 20. Rd2 Rxd2 21. Nxd2 Rd8 22. Nf3 Rd4 23. Kf1 Kf8 24. Ke2 Ke7 25. Kd3 Kd6 26. Nd2 Rc4 27. Nc4+ Kc5 28. Nc3+ Kd5 29. Nc4+ Kc5 30. Nc3+ Kd5 31. Nc4+ Kc5 32. Nc3+ Kd5 33. Nc4+ Kc5 34. Nc3+ Kd5 35. Nc4+ Kc5 36. Nc3+ Kd5 37. Nc4+ Kc5 38. Nc3+ Kd5 39. Nc4+ Kc5 40. Nc3+ Kd5 41. Nc4+ Kc5 42. Nc3+ Kd5 43. Nc4+ Kc5 44. Nc3+ Kd5 45. Nc4+ Kc5 46. Nc3+ Kd5 47. Nc4+ Kc5 48. Nc3+ Kd5 49. Nc4+ Kc5 50. Nc3+ Kd5 51. Nc4+ Kc5 52. Nc3+ Kd5 53. Nc4+ Kc5 54. Nc3+ Kd5 55. Nc4+ Kc5 56. Nc3+ Kd5 57. Nc4+ Kc5 58. Nc3+ Kd5 59. Nc4+ Kc5 60. Nc3+ Kd5 61. Nc4+ Kc5 62. Nc3+ Kd5 63. Nc4+ Kc5 64. N",
          "positional_tokens": {
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "Rd4",
            "Kf1",
            "Kf8",
            "Ke2",
            "Ke7",
            "Kd3",
            "Kd6",
            "Nd2",
            "Rc4",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5",
            "Nc3+",
            "Kd5",
            "Nc4+",
            "Kc5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf3",
          "neutral_uci": "g1f3",
          "neutral_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bb5 a6\n4. Ba4 Nf6\n5. O-O Be7\n6. Re1 b5\n7. Bb3 d6\n8. c3 O-O\n9. h3 Na5\n10. Bc2 c5\n11. d4 Nc6\n12. Nbd2 cxd4\n13. cxd4 exd4\n14. Nxd4 Bg4\n15. f3 Bxf3\n16. Qxf3 Nxd4\n17. Bxd4 Qxd4\n18. Qxd4 Nxd4\n19. Bxd4 Bc5\n20. Bxc5 dxc5\n21. Rf2 Rfd8\n22. Rc2 Rxd1\n23. Rxd1 Rd8\n24. Rxd8+ Kxd8\n25. Kf2 Ke7\n26. Ke3 Kd6\n27. Kd4 Kc6\n28. Kc4 Kb6\n29. Kb4 Kc6\n30. Kc4 Kb6\n31. Kb4 Kc6\n32. Kc4 Kb6\n33. Kb4 Kc6\n34. Kc4 Kb6\n35. Kb4 Kc6\n36. Kc4 Kb6\n37. Kb4 Kc6\n38. Kc4 Kb6\n39. Kb4 Kc6\n40. Kc4 Kb6\n41. Kb4 Kc6\n42. Kc4 Kb6\n43. Kb4 Kc6\n44. Kc4 Kb6\n45. Kb4 Kc6\n46. Kc4 Kb6\n47. Kb4 Kc6\n48. Kc4 Kb6\n49. Kb4 Kc6\n50. Kc4 Kb6\n51. Kb4 Kc6\n52. Kc4 Kb6\n53. Kb4 Kc6\n54. Kc4 Kb6\n55. Kb4 Kc6\n56. Kc4 Kb6\n57. Kb4 Kc6\n58. Kc4 Kb6\n59. Kb4 Kc6\n60. Kc4 Kb6\n61. Kb4 Kc6\n62. Kc4 Kb6\n63. Kb4 Kc6\n64. Kc4 Kb6\n65. Kb4 Kc6\n66. Kc4 Kb6\n67. Kb4 Kc6",
          "neutral_tokens": {
            "prompt_tokens": 119,
            "completion_tokens": 640,
            "total_tokens": 759,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "exd4",
            "Nxd4",
            "Bg4",
            "f3",
            "Bxf3",
            "Qxf3",
            "Nxd4",
            "Bxd4",
            "Qxd4",
            "Qxd4",
            "Nxd4",
            "Bxd4",
            "Bc5",
            "Bxc5",
            "dxc5",
            "Rf2",
            "Rfd8",
            "Rc2",
            "Rxd1",
            "Rxd1",
            "Rd8",
            "Rxd8+",
            "Kxd8",
            "Kf2",
            "Ke7",
            "Ke3",
            "Kd6",
            "Kd4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kc6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g1f3",
          "positional_uci": "g1f3",
          "neutral_uci": "g1f3",
          "consensus_move": "g1f3",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nc6",
              "Bc4",
              "Bc5",
              "c3",
              "Nf6",
              "d4",
              "exd4",
              "cxd4",
              "Bb4+",
              "Nc3",
              "Nxe4",
              "O-O",
              "Bxc3",
              "d5",
              "Bxa1",
              "Qxa1",
              "Nc5",
              "Qd4",
              "Qf6",
              "Qxc5",
              "Qxc5",
              "Nxe5",
              "Qd6",
              "Nc4",
              "O-O",
              "Be3",
              "Qd7",
              "Bxc5",
              "Qxc5",
              "Nxb7",
              "Qxb7",
              "Rc1",
              "Qd7",
              "Rc7",
              "Qd8",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8",
              "Qxc8",
              "Rc1",
              "Qd8",
              "Rc7",
              "Qd6",
              "Rc8+",
              "Rxc8",
              "Bxc8"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Rd4",
              "Kf1",
              "Kf8",
              "Ke2",
              "Ke7",
              "Kd3",
              "Kd6",
              "Nd2",
              "Rc4",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5",
              "Nc3+",
              "Kd5",
              "Nc4+",
              "Kc5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "exd4",
              "Nxd4",
              "Bg4",
              "f3",
              "Bxf3",
              "Qxf3",
              "Nxd4",
              "Bxd4",
              "Qxd4",
              "Qxd4",
              "Nxd4",
              "Bxd4",
              "Bc5",
              "Bxc5",
              "dxc5",
              "Rf2",
              "Rfd8",
              "Rc2",
              "Rxd1",
              "Rxd1",
              "Rd8",
              "Rxd8+",
              "Kxd8",
              "Kf2",
              "Ke7",
              "Ke3",
              "Kd6",
              "Kd4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kc6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 119,
            "completion_tokens": 640,
            "total_tokens": 759,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 359,
          "total_completion_tokens": 1920,
          "total_tokens": 2279
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 119,
            "completion_tokens": 640,
            "total_tokens": 759,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 5,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bc4",
          "aggressive_uci": "f1c4",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Bc5 4. c3 Nf6 5. d4 exd4 6. cxd4 Bb4+ 7. Nc3 Nxe4 8. O-O Bxc3 9. d5 Bxa1 10. Qxa1+ Nc5 11. Qd4\n\nNxd5",
          "aggressive_tokens": {
            "prompt_tokens": 129,
            "completion_tokens": 95,
            "total_tokens": 224,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "c3",
            "Nf6",
            "d4",
            "exd4",
            "cxd4",
            "Bb4+",
            "Nc3",
            "Nxe4",
            "O-O",
            "Bxc3",
            "d5",
            "Bxa1",
            "Qxa1+",
            "Nc5",
            "Qd4",
            "Nxd5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bc4",
          "positional_uci": "f1c4",
          "positional_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4\n\n1. e4 e5 2. Nf3 Nc6 3. Bc4",
          "positional_tokens": {
            "prompt_tokens": 129,
            "completion_tokens": 41,
            "total_tokens": 170,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bb5",
          "neutral_uci": "f1b5",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5 11. d4 Nc6 12. Nbd2 cxd4 13. cxd4 exd4 14. Nxd4 Bc5 15. Nxc6 Bxc6 16. e5 dxe5 17. Nxe5 Bxe5 18. Rxe5 Qd6 19. Qf3 Qxd4 20. Qxd5+ Kh8 21. Qf5+ Qf6 22. Qxf6+ gxf6 23. Rxe7 Rg8 24. Re8+ Rg8 25. Rxf8+ Rxf8 26. Bxf8 Kg7 27. Bg5+ Kg8 28. Bxf6 Kg7 29. Bg5+ Kg8 30. Bxf8 Kg7 31. Bg5+ Kg8 32. Bxf8 Kg7 33. Bg5+ Kg8 34. Bxf8 Kg7 35. Bg5+ Kg8 36. Bxf8 Kg7 37. Bg5+ Kg8 38. Bxf8 Kg7 39. Bg5+ Kg8 40. Bxf8 Kg7 41. Bg5+ Kg8 42. Bxf8 Kg7 43. Bg5+ Kg8 44. Bxf8 Kg7 45. Bg5+ Kg8 46. Bxf8 Kg7 47. Bg5+ Kg8 48. Bxf8 Kg7 49. Bg5+ Kg8 50. Bxf8 Kg7 51. Bg5+ Kg8 52. Bxf8 Kg7 53. Bg5+ Kg8 54. Bxf8 Kg7 55. Bg5+ Kg8 56. Bxf8 Kg7 57. Bg5+ Kg8 58. Bxf8 Kg7 59. Bg5+ Kg8 60. Bxf8 Kg7 61. Bg5+ Kg8 62. Bxf8 Kg7 63. Bg5+ Kg8 64. Bxf8 Kg7 65. Bg",
          "neutral_tokens": {
            "prompt_tokens": 128,
            "completion_tokens": 640,
            "total_tokens": 768,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "exd4",
            "Nxd4",
            "Bc5",
            "Nxc6",
            "Bxc6",
            "e5",
            "dxe5",
            "Nxe5",
            "Bxe5",
            "Rxe5",
            "Qd6",
            "Qf3",
            "Qxd4",
            "Qxd5+",
            "Kh8",
            "Qf5+",
            "Qf6",
            "Qxf6+",
            "gxf6",
            "Rxe7",
            "Rg8",
            "Re8+",
            "Rg8",
            "Rxf8+",
            "Rxf8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf6",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7",
            "Bg5+",
            "Kg8",
            "Bxf8",
            "Kg7"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f1c4",
          "positional_uci": "f1c4",
          "neutral_uci": "f1b5",
          "consensus_move": "f1c4",
          "source_agents": [
            "aggressive",
            "positional"
          ],
          "selected_agent": "aggressive"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bc5",
              "c3",
              "Nf6",
              "d4",
              "exd4",
              "cxd4",
              "Bb4+",
              "Nc3",
              "Nxe4",
              "O-O",
              "Bxc3",
              "d5",
              "Bxa1",
              "Qxa1+",
              "Nc5",
              "Qd4",
              "Nxd5"
            ],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "exd4",
              "Nxd4",
              "Bc5",
              "Nxc6",
              "Bxc6",
              "e5",
              "dxe5",
              "Nxe5",
              "Bxe5",
              "Rxe5",
              "Qd6",
              "Qf3",
              "Qxd4",
              "Qxd5+",
              "Kh8",
              "Qf5+",
              "Qf6",
              "Qxf6+",
              "gxf6",
              "Rxe7",
              "Rg8",
              "Re8+",
              "Rg8",
              "Rxf8+",
              "Rxf8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf6",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7",
              "Bg5+",
              "Kg8",
              "Bxf8",
              "Kg7"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 129,
            "completion_tokens": 95,
            "total_tokens": 224,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 129,
            "completion_tokens": 41,
            "total_tokens": 170,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 128,
            "completion_tokens": 640,
            "total_tokens": 768,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 386,
          "total_completion_tokens": 776,
          "total_tokens": 1162
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 129,
            "completion_tokens": 95,
            "total_tokens": 224,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 129,
            "completion_tokens": 41,
            "total_tokens": 170,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 128,
            "completion_tokens": 640,
            "total_tokens": 768,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bqkbnr/pppp1ppp/2n5/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3"
    },
    {
      "turn": 7,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc3",
          "aggressive_uci": "b1c3",
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n\nNc3",
          "aggressive_tokens": {
            "prompt_tokens": 137,
            "completion_tokens": 27,
            "total_tokens": 164,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Ng5",
          "positional_uci": "f3g5",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Na5 6. Bb5+ c6 7. dxc6 bxc6 8. Be2 h6 9. Nf3 e4 10. Ne5 Qd4 11. Nc3\n\nQd3+ 12. Kf1 0-0-0 13. d4\n\nNc4 14. Bxc4 Qxc4 15. Nxc4",
          "positional_tokens": {
            "prompt_tokens": 137,
            "completion_tokens": 132,
            "total_tokens": 269,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d5",
            "exd5",
            "Na5",
            "Bb5+",
            "c6",
            "dxc6",
            "bxc6",
            "Be2",
            "h6",
            "Nf3",
            "e4",
            "Ne5",
            "Qd4",
            "Nc3",
            "Qd3+",
            "Kf1",
            "d4",
            "Nc4",
            "Bxc4",
            "Qxc4",
            "Nxc4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc3",
          "neutral_uci": "b1c3",
          "neutral_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n\nNc3",
          "neutral_tokens": {
            "prompt_tokens": 136,
            "completion_tokens": 27,
            "total_tokens": 163,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b1c3",
          "positional_uci": "f3g5",
          "neutral_uci": "b1c3",
          "consensus_move": "b1c3",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [
              "d5",
              "exd5",
              "Na5",
              "Bb5+",
              "c6",
              "dxc6",
              "bxc6",
              "Be2",
              "h6",
              "Nf3",
              "e4",
              "Ne5",
              "Qd4",
              "Nc3",
              "Qd3+",
              "Kf1",
              "d4",
              "Nc4",
              "Bxc4",
              "Qxc4",
              "Nxc4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 137,
            "completion_tokens": 27,
            "total_tokens": 164,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 137,
            "completion_tokens": 132,
            "total_tokens": 269,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 136,
            "completion_tokens": 27,
            "total_tokens": 163,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 410,
          "total_completion_tokens": 186,
          "total_tokens": 596
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 137,
            "completion_tokens": 27,
            "total_tokens": 164,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 137,
            "completion_tokens": 132,
            "total_tokens": 269,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 136,
            "completion_tokens": 27,
            "total_tokens": 163,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/pppp1ppp/2n2n2/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R w KQkq - 4 4"
    },
    {
      "turn": 9,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc3",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Nc3 Nxe4\n\nNc3",
          "aggressive_tokens": {
            "prompt_tokens": 146,
            "completion_tokens": 36,
            "total_tokens": 182,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nxe4",
          "positional_uci": "c3e4",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. Bxe4 Nxe4 8. Qh5 Nd6 9. Qxe5+ Be7 10. 0-0 0-0 11. Re1\n\nQd7 12. Qxd7 13. Nxd7",
          "positional_tokens": {
            "prompt_tokens": 146,
            "completion_tokens": 110,
            "total_tokens": 256,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qh5",
            "Nd6",
            "Qxe5+",
            "Be7",
            "Re1",
            "Qd7",
            "Qxd7",
            "Nxd7"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc3",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Nc3 Nxe4\n\nNc3 Nc6",
          "neutral_tokens": {
            "prompt_tokens": 145,
            "completion_tokens": 39,
            "total_tokens": 184,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": "c3e4",
          "neutral_uci": null,
          "consensus_move": "c3e4",
          "source_agents": [
            "positional"
          ],
          "selected_agent": "positional"
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [
              "Qh5",
              "Nd6",
              "Qxe5+",
              "Be7",
              "Re1",
              "Qd7",
              "Qxd7",
              "Nxd7"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 146,
            "completion_tokens": 36,
            "total_tokens": 182,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 146,
            "completion_tokens": 110,
            "total_tokens": 256,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 145,
            "completion_tokens": 39,
            "total_tokens": 184,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 437,
          "total_completion_tokens": 185,
          "total_tokens": 622
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 146,
            "completion_tokens": 36,
            "total_tokens": 182,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 146,
            "completion_tokens": 110,
            "total_tokens": 256,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 145,
            "completion_tokens": 39,
            "total_tokens": 184,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/pppp1ppp/2n5/4p3/2B1n3/2N2N2/PPPP1PPP/R1BQK2R w KQkq - 0 5"
    },
    {
      "turn": 11,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bd3",
          "aggressive_uci": "c4d3",
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Nc3 Nxe4\n5. Nxe4 d5\n6. Bd3 Bc5\n7. O-O O-O\n8. c3 Nf6\n9. d4 Bb6\n10. Nc3\n\nNf6",
          "aggressive_tokens": {
            "prompt_tokens": 154,
            "completion_tokens": 81,
            "total_tokens": 235,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "O-O",
            "O-O",
            "c3",
            "Nf6",
            "d4",
            "Bb6",
            "Nc3",
            "Nf6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bd3",
          "positional_uci": "c4d3",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 Be7 7. O-O O-O 8. c3\n\nNf6 9. d4",
          "positional_tokens": {
            "prompt_tokens": 154,
            "completion_tokens": 68,
            "total_tokens": 222,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Be7",
            "O-O",
            "O-O",
            "c3",
            "Nf6",
            "d4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bd3",
          "neutral_uci": "c4d3",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 Be7 7. O-O O-O 8. c4\n\nc4 Nxd4 9. Nxd4 dxc4 10. Bxc4",
          "neutral_tokens": {
            "prompt_tokens": 153,
            "completion_tokens": 81,
            "total_tokens": 234,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Be7",
            "O-O",
            "O-O",
            "c4",
            "c4",
            "Nxd4",
            "Nxd4",
            "dxc4",
            "Bxc4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "c4d3",
          "positional_uci": "c4d3",
          "neutral_uci": "c4d3",
          "consensus_move": "c4d3",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bc5",
              "O-O",
              "O-O",
              "c3",
              "Nf6",
              "d4",
              "Bb6",
              "Nc3",
              "Nf6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Be7",
              "O-O",
              "O-O",
              "c3",
              "Nf6",
              "d4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Be7",
              "O-O",
              "O-O",
              "c4",
              "c4",
              "Nxd4",
              "Nxd4",
              "dxc4",
              "Bxc4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 154,
            "completion_tokens": 81,
            "total_tokens": 235,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 154,
            "completion_tokens": 68,
            "total_tokens": 222,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 153,
            "completion_tokens": 81,
            "total_tokens": 234,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 461,
          "total_completion_tokens": 230,
          "total_tokens": 691
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 154,
            "completion_tokens": 81,
            "total_tokens": 235,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 154,
            "completion_tokens": 68,
            "total_tokens": 222,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 153,
            "completion_tokens": 81,
            "total_tokens": 234,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/3pp3/2B1N3/5N2/PPPP1PPP/R1BQK2R w KQkq - 0 6"
    },
    {
      "turn": 13,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bxe4",
          "aggressive_uci": "d3e4",
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Nc3 Nxe4\n5. Nxe4 d5\n6. Bd3 dxe4\n\nBxe4 Nc5",
          "aggressive_tokens": {
            "prompt_tokens": 162,
            "completion_tokens": 55,
            "total_tokens": 217,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "O-O",
          "positional_uci": "e1g1",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7. O-O\n\n7",
          "positional_tokens": {
            "prompt_tokens": 162,
            "completion_tokens": 640,
            "total_tokens": 802,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "O-O",
          "neutral_uci": "e1g1",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O\n\ne4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O\n\nO-O",
          "neutral_tokens": {
            "prompt_tokens": 161,
            "completion_tokens": 108,
            "total_tokens": 269,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d3e4",
          "positional_uci": "e1g1",
          "neutral_uci": "e1g1",
          "consensus_move": "e1g1",
          "source_agents": [
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nc5"
            ],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 162,
            "completion_tokens": 55,
            "total_tokens": 217,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 162,
            "completion_tokens": 640,
            "total_tokens": 802,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 161,
            "completion_tokens": 108,
            "total_tokens": 269,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 485,
          "total_completion_tokens": 803,
          "total_tokens": 1288
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 162,
            "completion_tokens": 55,
            "total_tokens": 217,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 162,
            "completion_tokens": 640,
            "total_tokens": 802,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 161,
            "completion_tokens": 108,
            "total_tokens": 269,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/4p3/4p3/3B1N2/PPPP1PPP/R1BQK2R w KQkq - 0 7"
    },
    {
      "turn": 15,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qxd3",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O exd3 8. Qxd3 Qxd3 9. Rxd3\n\nQxd3 Rxd3",
          "aggressive_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 78,
            "total_tokens": 248,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Rxd3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qxd3",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O exd3 8. Qxd3 Bc5 9. Qe3+ Be7 10. Qxe7+ Kxe7 11. Bg5+ Kf8 12. Bxc6+ bxc6 13. Nxd5\n\nNxd5",
          "positional_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 117,
            "total_tokens": 287,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "Qe3+",
            "Be7",
            "Qxe7+",
            "Kxe7",
            "Bg5+",
            "Kf8",
            "Bxc6+",
            "bxc6",
            "Nxd5",
            "Nxd5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qxd3",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O exd3 8. Qxd3 Bc5 9. Qe3+ Be6 10. Qxe6+ fxe6\n\nQxd3+ Qxd3+",
          "neutral_tokens": {
            "prompt_tokens": 169,
            "completion_tokens": 93,
            "total_tokens": 262,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Rxd3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bc5",
              "Qe3+",
              "Be7",
              "Qxe7+",
              "Kxe7",
              "Bg5+",
              "Kf8",
              "Bxc6+",
              "bxc6",
              "Nxd5",
              "Nxd5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 170,
            "completion_tokens": 78,
            "total_tokens": 248,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 170,
            "completion_tokens": 117,
            "total_tokens": 287,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 169,
            "completion_tokens": 93,
            "total_tokens": 262,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 509,
          "total_completion_tokens": 288,
          "total_tokens": 797
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 170,
            "completion_tokens": 78,
            "total_tokens": 248,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 170,
            "completion_tokens": 117,
            "total_tokens": 287,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 169,
            "completion_tokens": 93,
            "total_tokens": 262,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/4p3/8/3p1N2/PPPP1PPP/R1BQ1RK1 w kq - 0 8"
    },
    {
      "turn": 15,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qxd3",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Nc3 Nxe4\n5. Nxe4 d5\n6. Bd3 dxe4\n7. O-O exd3\n8. Qxd3 Bc5\n9. Qe3+ Kf8\n10. Qe3+ Kf8\n\nQe3+ Kf8 Qe3+",
          "aggressive_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 97,
            "total_tokens": 267,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "Qe3+",
            "Kf8",
            "Qe3+",
            "Kf8",
            "Qe3+",
            "Kf8",
            "Qe3+"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qxd3",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O exd3 8. Qxd3 Nc6 9. Nc3 Be7 10. Be3\n\nQxd3 Nc6 9. Nc3 Be7 10. Be3",
          "positional_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 100,
            "total_tokens": 270,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc6",
            "Nc3",
            "Be7",
            "Be3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qxd3",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O exd3 8. Qxd3 Nxd4 9. Qxd4 Bg4 10. Qxd8+ Kxd8 11. Nxe5\n\nNxe5 12. Bxe5+ Kc8 13. Bxc6+",
          "neutral_tokens": {
            "prompt_tokens": 169,
            "completion_tokens": 115,
            "total_tokens": 284,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxd4",
            "Qxd4",
            "Bg4",
            "Qxd8+",
            "Kxd8",
            "Nxe5",
            "Nxe5",
            "Bxe5+",
            "Kc8",
            "Bxc6+"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bc5",
              "Qe3+",
              "Kf8",
              "Qe3+",
              "Kf8",
              "Qe3+",
              "Kf8",
              "Qe3+"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nc6",
              "Nc3",
              "Be7",
              "Be3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nxd4",
              "Qxd4",
              "Bg4",
              "Qxd8+",
              "Kxd8",
              "Nxe5",
              "Nxe5",
              "Bxe5+",
              "Kc8",
              "Bxc6+"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 170,
            "completion_tokens": 97,
            "total_tokens": 267,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 170,
            "completion_tokens": 100,
            "total_tokens": 270,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 169,
            "completion_tokens": 115,
            "total_tokens": 284,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 509,
          "total_completion_tokens": 312,
          "total_tokens": 821
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 170,
            "completion_tokens": 97,
            "total_tokens": 267,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 170,
            "completion_tokens": 100,
            "total_tokens": 270,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 169,
            "completion_tokens": 115,
            "total_tokens": 284,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/4p3/8/3p1N2/PPPP1PPP/R1BQ1RK1 w kq - 0 8"
    },
    {
      "turn": 15,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qxd3",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Nc3 Nxe4\n5. Nxe4 d5\n6. Bd3 dxe4\n7. O-O exd3\n8. Qxd3 Bc5\n9. Qe3+ Kf8\n10. Qe3+ Kf8\n\nQe3+ Kf8 Qe3+",
          "aggressive_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 97,
            "total_tokens": 267,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "Qe3+",
            "Kf8",
            "Qe3+",
            "Kf8",
            "Qe3+",
            "Kf8",
            "Qe3+"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qxd3",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O exd3 8. Qxd3 Bc5 9. Qe3+ Be6 10. Qxe6+ Qe7\n\nQe3+ Be6 10. Qxe6+ Qe7",
          "positional_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 102,
            "total_tokens": 272,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "Qe3+",
            "Be6",
            "Qxe6+",
            "Qe7",
            "Qe3+",
            "Be6",
            "Qxe6+",
            "Qe7"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Qxd3",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O exd3 8. Qxd3 Bc5 9. Qe3+ Kf8 10. Qxe5\n\nQxe5+ Kg8 11. Qe3+",
          "neutral_tokens": {
            "prompt_tokens": 169,
            "completion_tokens": 97,
            "total_tokens": 266,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "Qe3+",
            "Kf8",
            "Qxe5",
            "Qxe5+",
            "Kg8",
            "Qe3+"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bc5",
              "Qe3+",
              "Kf8",
              "Qe3+",
              "Kf8",
              "Qe3+",
              "Kf8",
              "Qe3+"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bc5",
              "Qe3+",
              "Be6",
              "Qxe6+",
              "Qe7",
              "Qe3+",
              "Be6",
              "Qxe6+",
              "Qe7"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bc5",
              "Qe3+",
              "Kf8",
              "Qxe5",
              "Qxe5+",
              "Kg8",
              "Qe3+"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 170,
            "completion_tokens": 97,
            "total_tokens": 267,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 170,
            "completion_tokens": 102,
            "total_tokens": 272,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 169,
            "completion_tokens": 97,
            "total_tokens": 266,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 509,
          "total_completion_tokens": 296,
          "total_tokens": 805
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 170,
            "completion_tokens": 97,
            "total_tokens": 267,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 170,
            "completion_tokens": 102,
            "total_tokens": 272,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 169,
            "completion_tokens": 97,
            "total_tokens": 266,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/4p3/8/3p1N2/PPPP1PPP/R1BQ1RK1 w kq - 0 8"
    },
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4\n\n1. d4",
          "aggressive_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4\n\n1. d4",
          "positional_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": null,
          "neutral_response": "1. d4\n\n1. d4",
          "neutral_tokens": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 353,
          "total_completion_tokens": 27,
          "total_tokens": 380
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 2,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4\n\n1. d4",
          "aggressive_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4\n\n1. d4",
          "positional_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": null,
          "neutral_response": "1. d4\n\n1. d4",
          "neutral_tokens": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 353,
          "total_completion_tokens": 27,
          "total_tokens": 380
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 2,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4\n\n1. d4",
          "aggressive_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4\n\n1. d4",
          "positional_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": null,
          "neutral_response": "1. d4\n\n1. d4",
          "neutral_tokens": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 353,
          "total_completion_tokens": 27,
          "total_tokens": 380
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": "e2e4",
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bb5 a6\n4. Ba4 Nf6\n5. O-O Be7\n6. Re1 b5\n7. Bb3 d6\n8. c3 O-O\n9. h3 Na5\n10. Bc2 c5\n11. d4 Nc6\n12. Nbd2 cxd4\n13. cxd4 exd4\n14. Nxd4 Bb7\n15. Nxc6 Bxc6\n16. Bg5 h6\n17. Bh4 g5\n18. Bg3 Kh7\n19. Qd3+ f5\n20. exf6+ Kg6\n21. fxe7 Qxe7\n22. Qf5+ Kg7\n23. Qf7+ Kh8\n24. Qf8# 1-0\n\nQf8# 1-0",
          "aggressive_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 223,
            "total_tokens": 337,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "exd4",
            "Nxd4",
            "Bb7",
            "Nxc6",
            "Bxc6",
            "Bg5",
            "h6",
            "Bh4",
            "g5",
            "Bg3",
            "Kh7",
            "Qd3+",
            "f5",
            "exf6+",
            "Kg6",
            "fxe7",
            "Qxe7",
            "Qf5+",
            "Kg7",
            "Qf7+",
            "Kh8",
            "Qf8#",
            "Qf8#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bb5 a6\n4. Ba4 Nf6\n5. O-O Be7\n6. Re1 b5\n7. Bb3 d6\n8. c3 O-O\n9. h3 Na5\n10. Bc2 c5\n11. d4 Nc6\n12. Nbd2 cxd4\n13. cxd4 exd4\n14. Nxd4 Nxd4\n15. Qxd4 Bc5\n16. Qd2 Be6\n17. Nf3 Qd7\n18. Be3 Rad8\n19. Qc2 Rfe8\n20. Rc1 Rc8\n21. Rcd1 Qe7\n22. Qd2 Qd7\n23. Qc2 Qe7\n24. Qd2 Qd7\n25. Qc2 Qe7\n26. Qd2 Qd7\n27. Qc2 Qe7\n28. Qd2 Qd7\n29. Qc2 Qe7\n30. Qd2 Qd7\n31. Qc2 Qe7\n32. Qd2 Qd7\n33. Qc2 Qe7\n34. Qd2 Qd7\n35. Qc2 Qe7\n36. Qd2 Qd7\n37. Qc2 Qe7\n38. Qd2 Qd7\n39. Qc2 Qe7\n40. Qd2 Qd7\n41. Qc2 Qe7\n42. Qd2 Qd7\n43. Qc2 Qe7\n44. Qd2 Qd7\n45. Qc2 Qe7\n46. Qd2 Qd7\n47. Qc2 Qe7\n48. Qd2 Qd7\n49. Qc2 Qe7\n50. Qd2 Qd7\n51. Qc2 Qe7\n52. Qd2 Qd7\n53. Qc2 Qe7\n54. Qd2 Qd7\n55. Qc2 Qe7\n56. Qd2 Qd7\n57. Qc2 Qe7\n58. Qd2 Qd7\n59. Qc2 Qe7\n60. Qd2 Qd7\n61. Qc2 Qe7\n62. Qd2 Qd7\n63. Qc2 Qe7\n64. Qd2 Qd7\n65. Qc2 Qe7\n66. Qd2 Qd7\n67. Qc2 Qe7",
          "positional_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "exd4",
            "Nxd4",
            "Nxd4",
            "Qxd4",
            "Bc5",
            "Qd2",
            "Be6",
            "Nf3",
            "Qd7",
            "Be3",
            "Rad8",
            "Qc2",
            "Rfe8",
            "Rc1",
            "Rc8",
            "Rcd1",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7",
            "Qd2",
            "Qd7",
            "Qc2",
            "Qe7"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": "e2e4",
          "neutral_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bb5 a6\n4. Ba4 Nf6\n5. O-O Be7\n6. Re1 b5\n7. Bb3 d6\n8. c3 O-O\n9. h3 Na5\n10. Bc2 c5\n11. d4 Nc6\n12. Nbd2 cxd4\n13. cxd4 exd4\n14. Nxd4 Nxd4\n15. Qxd4 Bc5\n16. Qd3 Bxd4\n17. Bxd4 Qxd4\n18. Qxd4 Nxd4\n19. Bxd4 Bg4\n20. f3 Be6\n21. Kf2 Kf8\n22. Ke3 Ke7\n23. Kd3 Kd6\n24. Kc3 Kc6\n25. Kb3 Kb6\n26. Ka3 Ka5\n27. Kxa5 Kxa5\n28. Kb3 Kb5\n29. Kc3 Kc5\n30. Kd3 Kd5\n31. Ke3 Ke5\n32. Kf3 Kf5\n33. Kg3 Kg5\n34. Kf3 Kf5\n35. Kg3 Kg5\n36. Kf3 Kf5\n37. Kg3 Kg5\n38. Kf3 Kf5\n39. Kg3 Kg5\n40. Kf3 Kf5\n41. Kg3 Kg5\n42. Kf3 Kf5\n43. Kg3 Kg5\n44. Kf3 Kf5\n45. Kg3 Kg5\n46. Kf3 Kf5\n47. Kg3 Kg5\n48. Kf3 Kf5\n49. Kg3 Kg5\n50. Kf3 Kf5\n51. Kg3 Kg5\n52. Kf3 Kf5\n53. Kg3 Kg5\n54. Kf3 Kf5\n55. Kg3 Kg5\n56. Kf3 Kf5\n57. Kg3 Kg5\n58. Kf3 Kf5\n59. Kg3 Kg5\n60. Kf3 Kf5\n61. Kg3 Kg5\n62. Kf3 Kf5\n63. Kg3 Kg5\n64. Kf3 Kf5\n65. Kg3 Kg5\n66. Kf3 Kf5\n67. Kg3 Kg5\n68. K",
          "neutral_tokens": {
            "prompt_tokens": 113,
            "completion_tokens": 640,
            "total_tokens": 753,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "exd4",
            "Nxd4",
            "Nxd4",
            "Qxd4",
            "Bc5",
            "Qd3",
            "Bxd4",
            "Bxd4",
            "Qxd4",
            "Qxd4",
            "Nxd4",
            "Bxd4",
            "Bg4",
            "f3",
            "Be6",
            "Kf2",
            "Kf8",
            "Ke3",
            "Ke7",
            "Kd3",
            "Kd6",
            "Kc3",
            "Kc6",
            "Kb3",
            "Kb6",
            "Ka3",
            "Ka5",
            "Kxa5",
            "Kxa5",
            "Kb3",
            "Kb5",
            "Kc3",
            "Kc5",
            "Kd3",
            "Kd5",
            "Ke3",
            "Ke5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e2e4",
          "positional_uci": "e2e4",
          "neutral_uci": "e2e4",
          "consensus_move": "e2e4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "exd4",
              "Nxd4",
              "Bb7",
              "Nxc6",
              "Bxc6",
              "Bg5",
              "h6",
              "Bh4",
              "g5",
              "Bg3",
              "Kh7",
              "Qd3+",
              "f5",
              "exf6+",
              "Kg6",
              "fxe7",
              "Qxe7",
              "Qf5+",
              "Kg7",
              "Qf7+",
              "Kh8",
              "Qf8#",
              "Qf8#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "exd4",
              "Nxd4",
              "Nxd4",
              "Qxd4",
              "Bc5",
              "Qd2",
              "Be6",
              "Nf3",
              "Qd7",
              "Be3",
              "Rad8",
              "Qc2",
              "Rfe8",
              "Rc1",
              "Rc8",
              "Rcd1",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7",
              "Qd2",
              "Qd7",
              "Qc2",
              "Qe7"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "exd4",
              "Nxd4",
              "Nxd4",
              "Qxd4",
              "Bc5",
              "Qd3",
              "Bxd4",
              "Bxd4",
              "Qxd4",
              "Qxd4",
              "Nxd4",
              "Bxd4",
              "Bg4",
              "f3",
              "Be6",
              "Kf2",
              "Kf8",
              "Ke3",
              "Ke7",
              "Kd3",
              "Kd6",
              "Kc3",
              "Kc6",
              "Kb3",
              "Kb6",
              "Ka3",
              "Ka5",
              "Kxa5",
              "Kxa5",
              "Kb3",
              "Kb5",
              "Kc3",
              "Kc5",
              "Kd3",
              "Kd5",
              "Ke3",
              "Ke5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 114,
            "completion_tokens": 223,
            "total_tokens": 337,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 113,
            "completion_tokens": 640,
            "total_tokens": 753,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 341,
          "total_completion_tokens": 1503,
          "total_tokens": 1844
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 114,
            "completion_tokens": 223,
            "total_tokens": 337,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 113,
            "completion_tokens": 640,
            "total_tokens": 753,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 3,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf3",
          "aggressive_uci": "g1f3",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5 11. d4 Nc6 12. Nbd2 cxd4 13. cxd4 exd4 14. Nxd4 Bg4 15. f3 Bxf3 16. Qxf3 Nxd4 17. Bxd4 Qxd4 18. Qxd4 Nxd4 19. Nc4 Nc6 20. Bb3\n\nBd7",
          "aggressive_tokens": {
            "prompt_tokens": 120,
            "completion_tokens": 174,
            "total_tokens": 294,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "exd4",
            "Nxd4",
            "Bg4",
            "f3",
            "Bxf3",
            "Qxf3",
            "Nxd4",
            "Bxd4",
            "Qxd4",
            "Qxd4",
            "Nxd4",
            "Nc4",
            "Nc6",
            "Bb3",
            "Bd7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf3",
          "positional_uci": "g1f3",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5 11. d4 Nc6 12. Nbd2 cxd4 13. cxd4 exd4 14. Nxd4 Nxd4 15. Qxd4 Bc5 16. Qd2 Be6 17. Nf3 Rc8 18. Bb3 Qd7 19. Qf4\n\nNf6",
          "positional_tokens": {
            "prompt_tokens": 120,
            "completion_tokens": 163,
            "total_tokens": 283,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Rc8",
            "Bb3",
            "Qd7",
            "Qf4",
            "Nf6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf3",
          "neutral_uci": "g1f3",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5 11. d4 Nc6 12. Nbd2 cxd4 13. cxd4 exd4 14. Nxd4 Nxd4 15. Qxd4 Bc5 16. Qd2 Bxd4 17. Bxd4 Qxd4 18. Qxd4 Nxd4 19. Rxd4 Rfd8 20. Rxd8+ Rxd8 21. Kf1 Kf8 22. Ke2 Ke7 23. Kd3 Kd6 24. Kc4 Kc6 25. Kb4 Kb6 26. Kxa4 Kxa5 27. Kb3 Kb5 28. Kc3 Kc5 29. Kd3 Kd5 30. Ke3 Ke5 31. Kf3 Kf5 32. Kg3 Kg5 33. Kh3 Kh5 34. Kg3 Kg5 35. Kh3 Kh5 36. Kg3 Kg5 37. Kh3 Kh5 38. Kg3 Kg5 39. Kh3 Kh5 40. Kg3 Kg5 41. Kh3 Kh5 42. Kg3 Kg5 43. Kh3 Kh5 44. Kg3 Kg5 45. Kh3 Kh5 46. Kg3 Kg5 47. Kh3 Kh5 48. Kg3 Kg5 49. Kh3 Kh5 50. Kg3 Kg5 51. Kh3 Kh5 52. Kg3 Kg5 53. Kh3 Kh5 54. Kg3 Kg5 55. Kh3 Kh5 56. Kg3 Kg5 57. Kh3 Kh5 58. Kg3 Kg5 59. Kh3 Kh5 60. Kg3 Kg5 61. Kh3 Kh5 62. Kg3 Kg5 63. Kh3 Kh5 64. Kg3 Kg5 65. Kh3 Kh5 66. Kg3 Kg5 67. Kh3 Kh5 68. Kg3 Kg5 69. Kh3 Kh5 70. Kg3 Kg5 71. Kh3 Kh5",
          "neutral_tokens": {
            "prompt_tokens": 119,
            "completion_tokens": 640,
            "total_tokens": 759,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "exd4",
            "Nxd4",
            "Nxd4",
            "Qxd4",
            "Bc5",
            "Qd2",
            "Bxd4",
            "Bxd4",
            "Qxd4",
            "Qxd4",
            "Nxd4",
            "Rxd4",
            "Rfd8",
            "Rxd8+",
            "Rxd8",
            "Kf1",
            "Kf8",
            "Ke2",
            "Ke7",
            "Kd3",
            "Kd6",
            "Kc4",
            "Kc6",
            "Kb4",
            "Kb6",
            "Kxa4",
            "Kxa5",
            "Kb3",
            "Kb5",
            "Kc3",
            "Kc5",
            "Kd3",
            "Kd5",
            "Ke3",
            "Ke5",
            "Kf3",
            "Kf5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5",
            "Kg3",
            "Kg5",
            "Kh3",
            "Kh5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g1f3",
          "positional_uci": "g1f3",
          "neutral_uci": "g1f3",
          "consensus_move": "g1f3",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "exd4",
              "Nxd4",
              "Bg4",
              "f3",
              "Bxf3",
              "Qxf3",
              "Nxd4",
              "Bxd4",
              "Qxd4",
              "Qxd4",
              "Nxd4",
              "Nc4",
              "Nc6",
              "Bb3",
              "Bd7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Rc8",
              "Bb3",
              "Qd7",
              "Qf4",
              "Nf6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "exd4",
              "Nxd4",
              "Nxd4",
              "Qxd4",
              "Bc5",
              "Qd2",
              "Bxd4",
              "Bxd4",
              "Qxd4",
              "Qxd4",
              "Nxd4",
              "Rxd4",
              "Rfd8",
              "Rxd8+",
              "Rxd8",
              "Kf1",
              "Kf8",
              "Ke2",
              "Ke7",
              "Kd3",
              "Kd6",
              "Kc4",
              "Kc6",
              "Kb4",
              "Kb6",
              "Kxa4",
              "Kxa5",
              "Kb3",
              "Kb5",
              "Kc3",
              "Kc5",
              "Kd3",
              "Kd5",
              "Ke3",
              "Ke5",
              "Kf3",
              "Kf5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5",
              "Kg3",
              "Kg5",
              "Kh3",
              "Kh5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 120,
            "completion_tokens": 174,
            "total_tokens": 294,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 120,
            "completion_tokens": 163,
            "total_tokens": 283,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 119,
            "completion_tokens": 640,
            "total_tokens": 759,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 359,
          "total_completion_tokens": 977,
          "total_tokens": 1336
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 120,
            "completion_tokens": 174,
            "total_tokens": 294,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 120,
            "completion_tokens": 163,
            "total_tokens": 283,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 119,
            "completion_tokens": 640,
            "total_tokens": 759,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 5,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bc4",
          "aggressive_uci": "f1c4",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Bc5 4. c3 Nf6 5. d4 exd4 6. cxd4 Bb4+ 7. Nc3 Nxe4 8. O-O Bxc3 9. d5 Bxa1 10. Qxa1+ Nc5 11. Qd4 Ncd7 12. Nxe5\n\nNf6",
          "aggressive_tokens": {
            "prompt_tokens": 129,
            "completion_tokens": 105,
            "total_tokens": 234,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "c3",
            "Nf6",
            "d4",
            "exd4",
            "cxd4",
            "Bb4+",
            "Nc3",
            "Nxe4",
            "O-O",
            "Bxc3",
            "d5",
            "Bxa1",
            "Qxa1+",
            "Nc5",
            "Qd4",
            "Ncd7",
            "Nxe5",
            "Nf6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bb5",
          "positional_uci": "f1b5",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5 11. d4 Nc6 12. Nbd2 cxd4 13. cxd4 exd4 14. Nxd4 Nxd4 15. Qxd4 Bc5 16. Qd3 Bxd4 17. Bxd4 Qxd4 18. Rxd4 Nxd4 19. Rc1 Nf5 20. Rc7\n\nRc8",
          "positional_tokens": {
            "prompt_tokens": 129,
            "completion_tokens": 174,
            "total_tokens": 303,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "exd4",
            "Nxd4",
            "Nxd4",
            "Qxd4",
            "Bc5",
            "Qd3",
            "Bxd4",
            "Bxd4",
            "Qxd4",
            "Rxd4",
            "Nxd4",
            "Rc1",
            "Nf5",
            "Rc7",
            "Rc8"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bc4",
          "neutral_uci": "f1c4",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Bc5 4. c3 Nf6 5. d4 exd4 6. cxd4 Bb4+ 7. Nc3 Nxe4 8. O-O Bxc3 9. d5 Bxa1 10. Qxa1+ Nc6 11. Qxa7\n\nNxd5",
          "neutral_tokens": {
            "prompt_tokens": 128,
            "completion_tokens": 95,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "c3",
            "Nf6",
            "d4",
            "exd4",
            "cxd4",
            "Bb4+",
            "Nc3",
            "Nxe4",
            "O-O",
            "Bxc3",
            "d5",
            "Bxa1",
            "Qxa1+",
            "Nc6",
            "Qxa7",
            "Nxd5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f1c4",
          "positional_uci": "f1b5",
          "neutral_uci": "f1c4",
          "consensus_move": "f1c4",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bc5",
              "c3",
              "Nf6",
              "d4",
              "exd4",
              "cxd4",
              "Bb4+",
              "Nc3",
              "Nxe4",
              "O-O",
              "Bxc3",
              "d5",
              "Bxa1",
              "Qxa1+",
              "Nc5",
              "Qd4",
              "Ncd7",
              "Nxe5",
              "Nf6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "exd4",
              "Nxd4",
              "Nxd4",
              "Qxd4",
              "Bc5",
              "Qd3",
              "Bxd4",
              "Bxd4",
              "Qxd4",
              "Rxd4",
              "Nxd4",
              "Rc1",
              "Nf5",
              "Rc7",
              "Rc8"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bc5",
              "c3",
              "Nf6",
              "d4",
              "exd4",
              "cxd4",
              "Bb4+",
              "Nc3",
              "Nxe4",
              "O-O",
              "Bxc3",
              "d5",
              "Bxa1",
              "Qxa1+",
              "Nc6",
              "Qxa7",
              "Nxd5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 129,
            "completion_tokens": 105,
            "total_tokens": 234,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 129,
            "completion_tokens": 174,
            "total_tokens": 303,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 128,
            "completion_tokens": 95,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 386,
          "total_completion_tokens": 374,
          "total_tokens": 760
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 129,
            "completion_tokens": 105,
            "total_tokens": 234,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 129,
            "completion_tokens": 174,
            "total_tokens": 303,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 128,
            "completion_tokens": 95,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkbnr/pppp1ppp/2n5/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3"
    },
    {
      "turn": 7,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc3",
          "aggressive_uci": "b1c3",
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n\nNc3",
          "aggressive_tokens": {
            "prompt_tokens": 137,
            "completion_tokens": 27,
            "total_tokens": 164,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Ng5",
          "positional_uci": "f3g5",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Na5 6. Bb5+ c6 7. d4 Bd6 8. Nf3 0-0 9. 0-0\n\nNc5",
          "positional_tokens": {
            "prompt_tokens": 137,
            "completion_tokens": 75,
            "total_tokens": 212,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d5",
            "exd5",
            "Na5",
            "Bb5+",
            "c6",
            "d4",
            "Bd6",
            "Nf3",
            "Nc5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Ng5",
          "neutral_uci": "f3g5",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Na5 6. Bb5+ c6 7. dxc6 bxc6 8. Be2 h6 9. Nf3 e4 10. Ne5 Qd6 11. O-O\n\nQxd5 12. Nc3 13. Nxd5 14. Nxd5\n\nNxd5",
          "neutral_tokens": {
            "prompt_tokens": 136,
            "completion_tokens": 115,
            "total_tokens": 251,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d5",
            "exd5",
            "Na5",
            "Bb5+",
            "c6",
            "dxc6",
            "bxc6",
            "Be2",
            "h6",
            "Nf3",
            "e4",
            "Ne5",
            "Qd6",
            "O-O",
            "Qxd5",
            "Nc3",
            "Nxd5",
            "Nxd5",
            "Nxd5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b1c3",
          "positional_uci": "f3g5",
          "neutral_uci": "f3g5",
          "consensus_move": "f3g5",
          "source_agents": [
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [
              "d5",
              "exd5",
              "Na5",
              "Bb5+",
              "c6",
              "d4",
              "Bd6",
              "Nf3",
              "Nc5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "d5",
              "exd5",
              "Na5",
              "Bb5+",
              "c6",
              "dxc6",
              "bxc6",
              "Be2",
              "h6",
              "Nf3",
              "e4",
              "Ne5",
              "Qd6",
              "O-O",
              "Qxd5",
              "Nc3",
              "Nxd5",
              "Nxd5",
              "Nxd5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 137,
            "completion_tokens": 27,
            "total_tokens": 164,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 137,
            "completion_tokens": 75,
            "total_tokens": 212,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 136,
            "completion_tokens": 115,
            "total_tokens": 251,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 410,
          "total_completion_tokens": 217,
          "total_tokens": 627
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 137,
            "completion_tokens": 27,
            "total_tokens": 164,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 137,
            "completion_tokens": 75,
            "total_tokens": 212,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 136,
            "completion_tokens": 115,
            "total_tokens": 251,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/pppp1ppp/2n2n2/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R w KQkq - 4 4"
    },
    {
      "turn": 9,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nxe4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Ng5 d5\n\nNxe4",
          "aggressive_tokens": {
            "prompt_tokens": 144,
            "completion_tokens": 34,
            "total_tokens": 178,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "exd5",
          "positional_uci": "e4d5",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. Bc5\n\nNc6 Bc5",
          "positional_tokens": {
            "prompt_tokens": 144,
            "completion_tokens": 59,
            "total_tokens": 203,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxd5",
            "Nxf7",
            "Kxf7",
            "Bc5",
            "Nc6",
            "Bc5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "exd5",
          "neutral_uci": "e4d5",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. Bc7+ Ke8 8. Qf3+ Kd7 9. Qf7+ Ke6 10. Qf5+ Kd7 11. Qf7+ Ke6 12. Qf5+ Kd7 13. Qf7+ Ke6 14. Qf5+ Kd7 15. Qf7+ Ke6 16. Qf5+ Kd7 17. Qf7+ Ke6 18. Qf5+ Kd7 19. Qf7+ Ke6 20. Qf5+ Kd7 21. Qf7+ Ke6 22. Qf5+ Kd7 23. Qf7+ Ke6 24. Qf5+ Kd7 25. Qf7+ Ke6 26. Qf5+ Kd7 27. Qf7+ Ke6 28. Qf5+ Kd7 29. Qf7+ Ke6 30. Qf5+ Kd7 31. Qf7+ Ke6 32. Qf5+ Kd7 33. Qf7+ Ke6 34. Qf5+ Kd7 35. Qf7+ Ke6 36. Qf5+ Kd7 37. Qf7+ Ke6 38. Qf5+ Kd7 39. Qf7+ Ke6 40. Qf5+ Kd7 41. Qf7+ Ke6 42. Qf5+ Kd7 43. Qf7+ Ke6 44. Qf5+ Kd7 45. Qf7+ Ke6 46. Qf5+ Kd7 47. Qf7+ Ke6 48. Qf5+ Kd7 49. Qf7+ Ke6 50. Qf5+ Kd7 51. Qf7+ Ke6 52. Qf5+ Kd7 53. Qf7+ Ke6 54. Qf5+ Kd7 55. Qf7+ Ke6 56. Qf5+ Kd7 57. Qf7+ Ke6 58. Qf5+ Kd7 59. Qf7+ Ke6 60. Qf5+ Kd7 61. Qf7+ Ke6 62. Qf5+ Kd7 63. Qf7+",
          "neutral_tokens": {
            "prompt_tokens": 143,
            "completion_tokens": 640,
            "total_tokens": 783,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nxd5",
            "Nxf7",
            "Kxf7",
            "Bc7+",
            "Ke8",
            "Qf3+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+",
            "Ke6",
            "Qf5+",
            "Kd7",
            "Qf7+"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": "e4d5",
          "neutral_uci": "e4d5",
          "consensus_move": "e4d5",
          "source_agents": [
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [
              "Nxd5",
              "Nxf7",
              "Kxf7",
              "Bc5",
              "Nc6",
              "Bc5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nxd5",
              "Nxf7",
              "Kxf7",
              "Bc7+",
              "Ke8",
              "Qf3+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+",
              "Ke6",
              "Qf5+",
              "Kd7",
              "Qf7+"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 144,
            "completion_tokens": 34,
            "total_tokens": 178,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 144,
            "completion_tokens": 59,
            "total_tokens": 203,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 143,
            "completion_tokens": 640,
            "total_tokens": 783,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 431,
          "total_completion_tokens": 733,
          "total_tokens": 1164
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 144,
            "completion_tokens": 34,
            "total_tokens": 178,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 144,
            "completion_tokens": 59,
            "total_tokens": 203,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 143,
            "completion_tokens": 640,
            "total_tokens": 783,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n2n2/3pp1N1/2B1P3/8/PPPP1PPP/RNBQK2R w KQkq - 0 5"
    },
    {
      "turn": 11,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nxf7",
          "aggressive_uci": "g5f7",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. Qf3+ Ke6 8. Nc3\n\nNc3 Qd6",
          "aggressive_tokens": {
            "prompt_tokens": 153,
            "completion_tokens": 70,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kxf7",
            "Qf3+",
            "Ke6",
            "Nc3",
            "Nc3",
            "Qd6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nxf7",
          "positional_uci": "g5f7",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. Qf3+ Ke6 8. Nc3\n\nNc3 Qd6",
          "positional_tokens": {
            "prompt_tokens": 153,
            "completion_tokens": 70,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kxf7",
            "Qf3+",
            "Ke6",
            "Nc3",
            "Nc3",
            "Qd6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nxf7",
          "neutral_uci": "g5f7",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. Qf3+ Ke6 8. Nc3\n\nNc3 Qd6",
          "neutral_tokens": {
            "prompt_tokens": 152,
            "completion_tokens": 70,
            "total_tokens": 222,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kxf7",
            "Qf3+",
            "Ke6",
            "Nc3",
            "Nc3",
            "Qd6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g5f7",
          "positional_uci": "g5f7",
          "neutral_uci": "g5f7",
          "consensus_move": "g5f7",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Kxf7",
              "Qf3+",
              "Ke6",
              "Nc3",
              "Nc3",
              "Qd6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Kxf7",
              "Qf3+",
              "Ke6",
              "Nc3",
              "Nc3",
              "Qd6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Kxf7",
              "Qf3+",
              "Ke6",
              "Nc3",
              "Nc3",
              "Qd6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 153,
            "completion_tokens": 70,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 153,
            "completion_tokens": 70,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 152,
            "completion_tokens": 70,
            "total_tokens": 222,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 458,
          "total_completion_tokens": 210,
          "total_tokens": 668
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 153,
            "completion_tokens": 70,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 153,
            "completion_tokens": 70,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 152,
            "completion_tokens": 70,
            "total_tokens": 222,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/3np1N1/2B5/8/PPPP1PPP/RNBQK2R w KQkq - 0 6"
    },
    {
      "turn": 13,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qf3+",
          "aggressive_uci": "d1f3",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. Qf3+ Ke6 8. Nc3 Qd7 9. O-O\n\nQd7 7. Qf3+ Ke6 8. Nc3",
          "aggressive_tokens": {
            "prompt_tokens": 162,
            "completion_tokens": 90,
            "total_tokens": 252,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Ke6",
            "Nc3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": "d2d4",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7\n\nBc4+ Kg8 7. d4 Nxc3 8. bxc3",
          "positional_tokens": {
            "prompt_tokens": 162,
            "completion_tokens": 70,
            "total_tokens": 232,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxc3",
            "bxc3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": "d2d4",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7\n\nBc7+ Kg8 7. d4 Nf6 8. Qf3",
          "neutral_tokens": {
            "prompt_tokens": 161,
            "completion_tokens": 70,
            "total_tokens": 231,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf6",
            "Qf3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d1f3",
          "positional_uci": "d2d4",
          "neutral_uci": "d2d4",
          "consensus_move": "d2d4",
          "source_agents": [
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Ke6",
              "Nc3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nxc3",
              "bxc3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf6",
              "Qf3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 162,
            "completion_tokens": 90,
            "total_tokens": 252,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 162,
            "completion_tokens": 70,
            "total_tokens": 232,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 161,
            "completion_tokens": 70,
            "total_tokens": 231,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 485,
          "total_completion_tokens": 230,
          "total_tokens": 715
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 162,
            "completion_tokens": 90,
            "total_tokens": 252,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 162,
            "completion_tokens": 70,
            "total_tokens": 232,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 161,
            "completion_tokens": 70,
            "total_tokens": 231,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bq1b1r/ppp2kpp/2n5/3np3/2B5/8/PPPP1PPP/RNBQK2R w KQ - 0 7"
    },
    {
      "turn": 15,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qf3+",
          "aggressive_uci": "d1f3",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4\n\nBc5+ Kg8 8. Qf3+",
          "aggressive_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 71,
            "total_tokens": 241,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qf3+",
          "positional_uci": "d1f3",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4\n\nBc5+ Kg8 8. Qf3+",
          "positional_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 71,
            "total_tokens": 241,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bc5+",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4\n\nBc5+",
          "neutral_tokens": {
            "prompt_tokens": 169,
            "completion_tokens": 61,
            "total_tokens": 230,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d1f3",
          "positional_uci": "d1f3",
          "neutral_uci": null,
          "consensus_move": "d1f3",
          "source_agents": [
            "aggressive",
            "positional"
          ],
          "selected_agent": "aggressive"
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 170,
            "completion_tokens": 71,
            "total_tokens": 241,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 170,
            "completion_tokens": 71,
            "total_tokens": 241,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 169,
            "completion_tokens": 61,
            "total_tokens": 230,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 509,
          "total_completion_tokens": 203,
          "total_tokens": 712
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 170,
            "completion_tokens": 71,
            "total_tokens": 241,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 170,
            "completion_tokens": 71,
            "total_tokens": 241,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 169,
            "completion_tokens": 61,
            "total_tokens": 230,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bq1b1r/ppp2kpp/8/3np3/2Bn4/8/PPP2PPP/RNBQK2R w KQ - 0 8"
    },
    {
      "turn": 17,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Qe3+",
          "aggressive_uci": "f3e3",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3+ Kf7 10. Qf3+ Ke6 11. Qe3+ Kf7 12. Qf3+ Ke6\n\nQe3+ Kf7 Qf3+ Ke6",
          "aggressive_tokens": {
            "prompt_tokens": 179,
            "completion_tokens": 120,
            "total_tokens": 299,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kf7",
            "Qf3+",
            "Ke6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Qf3+",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6\n\nQe3+ Kf5 9. Qf3+ Kg6 10. Qf5+ Kh6 11. Qf7+",
          "positional_tokens": {
            "prompt_tokens": 179,
            "completion_tokens": 101,
            "total_tokens": 280,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kg6",
            "Qf5+",
            "Kh6",
            "Qf7+"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Ke7",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6\n\nKe7 Qxd5+ Kf7 Qf5+ Kg6 Qg5+ Kh6 Qf5+ Kg6",
          "neutral_tokens": {
            "prompt_tokens": 178,
            "completion_tokens": 95,
            "total_tokens": 273,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qxd5+",
            "Kf7",
            "Qf5+",
            "Kg6",
            "Qg5+",
            "Kh6",
            "Qf5+",
            "Kg6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f3e3",
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": "f3e3",
          "source_agents": [
            "aggressive"
          ],
          "selected_agent": "aggressive"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Kf7",
              "Qf3+",
              "Ke6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Kg6",
              "Qf5+",
              "Kh6",
              "Qf7+"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qxd5+",
              "Kf7",
              "Qf5+",
              "Kg6",
              "Qg5+",
              "Kh6",
              "Qf5+",
              "Kg6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 179,
            "completion_tokens": 120,
            "total_tokens": 299,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 179,
            "completion_tokens": 101,
            "total_tokens": 280,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 178,
            "completion_tokens": 95,
            "total_tokens": 273,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 536,
          "total_completion_tokens": 316,
          "total_tokens": 852
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 179,
            "completion_tokens": 120,
            "total_tokens": 299,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 179,
            "completion_tokens": 101,
            "total_tokens": 280,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 178,
            "completion_tokens": 95,
            "total_tokens": 273,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bq1b1r/ppp3pp/4k3/3np3/2Bn4/5Q2/PPP2PPP/RNB1K2R w KQ - 2 9"
    },
    {
      "turn": 19,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Kd2",
          "aggressive_uci": "e1d2",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxa1 11. Qe4+ Kf7 12. Qf4+ Kg6 13. Qg4+ Kf7 14. Qf4+ Kg6 15. Qg4+ Kf7 16. Qf4+ Kg6 17. Qg4+ Kf7 18. Qf4+ Kg6 19. Qg4+ Kf7 20. Qf4+ Kg6 21. Qg4+ Kf7 22. Qf4+ Kg6 23. Qg4+ Kf7 24. Qf4+ Kg6 25. Qg4+ Kf7 26. Qf4+ Kg6 27. Qg4+ Kf7 28. Qf4+ Kg6 29. Qg4+ Kf7 30. Qf4+ Kg6 31. Qg4+ Kf7 32. Qf4+ Kg6 33. Qg4+ Kf7 34. Qf4+ Kg6 35. Qg4+ Kf7 36. Qf4+ Kg6 37. Qg4+ Kf7 38. Qf4+ Kg6 39. Qg4+ Kf7 40. Qf4+ Kg6 41. Qg4+ Kf7 42. Qf4+ Kg6 43. Qg4+ Kf7 44. Qf4+ Kg6 45. Qg4+ Kf7 46. Qf4+ Kg6 47. Qg4+ Kf7 48. Qf4+ Kg6 49. Qg4+ Kf7 50. Qf4+ Kg6 51. Qg4+ Kf7 52. Qf4+ Kg6 53. Qg4+ Kf7 54. Qf4+ Kg6 55. Qg4+ Kf7 56. Qf4+ Kg6 57. Qg4+ Kf7 58. Qf4+ Kg6 59. Qg4+ Kf7 60. Qf4+ Kg6 61. Q",
          "aggressive_tokens": {
            "prompt_tokens": 189,
            "completion_tokens": 640,
            "total_tokens": 829,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nxa1",
            "Qe4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6",
            "Qg4+",
            "Kf7",
            "Qf4+",
            "Kg6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Kd1",
          "positional_uci": "e1d1",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd1 Nd4 11. Qe2+ Kf7 12. Qe5+ Kg6 13. Qf5+ Kh6 14. Qf4+ Kg6 15. Qf5+ Kh6 16. Qf4+ Kg6\n\nQf5+ Kg6 Qf4+ Kg6 Qf5+",
          "positional_tokens": {
            "prompt_tokens": 189,
            "completion_tokens": 167,
            "total_tokens": 356,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nd4",
            "Qe2+",
            "Kf7",
            "Qe5+",
            "Kg6",
            "Qf5+",
            "Kh6",
            "Qf4+",
            "Kg6",
            "Qf5+",
            "Kh6",
            "Qf4+",
            "Kg6",
            "Qf5+",
            "Kg6",
            "Qf4+",
            "Kg6",
            "Qf5+"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Kd2",
          "neutral_uci": "e1d2",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxa1 11. Qe4+ Kf7 12. Qf5+ Ke8 13. Qe6+ Kd8 14. Qd6+ Kc8 15. Qc6+ Kb8 16. Qb6+ Ka8 17. Qa6# 1-0\n\nQa6#",
          "neutral_tokens": {
            "prompt_tokens": 188,
            "completion_tokens": 166,
            "total_tokens": 354,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxa1",
            "Qe4+",
            "Kf7",
            "Qf5+",
            "Ke8",
            "Qe6+",
            "Kd8",
            "Qd6+",
            "Kc8",
            "Qc6+",
            "Kb8",
            "Qb6+",
            "Ka8",
            "Qa6#",
            "Qa6#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e1d2",
          "positional_uci": "e1d1",
          "neutral_uci": "e1d2",
          "consensus_move": "e1d2",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nxa1",
              "Qe4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6",
              "Qg4+",
              "Kf7",
              "Qf4+",
              "Kg6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nd4",
              "Qe2+",
              "Kf7",
              "Qe5+",
              "Kg6",
              "Qf5+",
              "Kh6",
              "Qf4+",
              "Kg6",
              "Qf5+",
              "Kh6",
              "Qf4+",
              "Kg6",
              "Qf5+",
              "Kg6",
              "Qf4+",
              "Kg6",
              "Qf5+"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nxa1",
              "Qe4+",
              "Kf7",
              "Qf5+",
              "Ke8",
              "Qe6+",
              "Kd8",
              "Qd6+",
              "Kc8",
              "Qc6+",
              "Kb8",
              "Qb6+",
              "Ka8",
              "Qa6#",
              "Qa6#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 189,
            "completion_tokens": 640,
            "total_tokens": 829,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 189,
            "completion_tokens": 167,
            "total_tokens": 356,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 188,
            "completion_tokens": 166,
            "total_tokens": 354,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 566,
          "total_completion_tokens": 973,
          "total_tokens": 1539
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 18,
            "turn_number": 10,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 189,
            "completion_tokens": 640,
            "total_tokens": 829,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 18,
            "turn_number": 10,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 189,
            "completion_tokens": 167,
            "total_tokens": 356,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 18,
            "turn_number": 10,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 188,
            "completion_tokens": 166,
            "total_tokens": 354,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bq1b1r/ppp3pp/4k3/3np3/2B5/4Q3/PPn2PPP/RNB1K2R w KQ - 0 10"
    },
    {
      "turn": 21,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bxc2+",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxe3\n\nBxc2+ Kxc2 Qd5+",
          "aggressive_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 97,
            "total_tokens": 296,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kxc2",
            "Qd5+"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Kxd1",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxe3\n\nKxd1 Qd3+",
          "positional_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 93,
            "total_tokens": 292,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qd3+"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Kxd1",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxe3\n\nKxd1 Qf7+",
          "neutral_tokens": {
            "prompt_tokens": 198,
            "completion_tokens": 93,
            "total_tokens": 291,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qf7+"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Kxc2",
              "Qd5+"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qd3+"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qf7+"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 199,
            "completion_tokens": 97,
            "total_tokens": 296,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 199,
            "completion_tokens": 93,
            "total_tokens": 292,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 198,
            "completion_tokens": 93,
            "total_tokens": 291,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 596,
          "total_completion_tokens": 283,
          "total_tokens": 879
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 20,
            "turn_number": 11,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 199,
            "completion_tokens": 97,
            "total_tokens": 296,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 20,
            "turn_number": 11,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 199,
            "completion_tokens": 93,
            "total_tokens": 292,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 20,
            "turn_number": 11,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 198,
            "completion_tokens": 93,
            "total_tokens": 291,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bq1b1r/ppp3pp/4k3/3np3/2B5/4n3/PP1K1PPP/RNB4R w - - 0 11"
    },
    {
      "turn": 21,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "Kd2xc3",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxe3\n\nKd2xc3",
          "aggressive_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 91,
            "total_tokens": 290,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Kxd1",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxe3\n\nKxd1 Qd3+",
          "positional_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 93,
            "total_tokens": 292,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qd3+"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Kxd1",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxe3\n\nKxd1 Qd8+",
          "neutral_tokens": {
            "prompt_tokens": 198,
            "completion_tokens": 93,
            "total_tokens": 291,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qd8+"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [
              "Qd3+"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qd8+"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 199,
            "completion_tokens": 91,
            "total_tokens": 290,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 199,
            "completion_tokens": 93,
            "total_tokens": 292,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 198,
            "completion_tokens": 93,
            "total_tokens": 291,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 596,
          "total_completion_tokens": 277,
          "total_tokens": 873
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 20,
            "turn_number": 11,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 199,
            "completion_tokens": 91,
            "total_tokens": 290,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 20,
            "turn_number": 11,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 199,
            "completion_tokens": 93,
            "total_tokens": 292,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 20,
            "turn_number": 11,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 198,
            "completion_tokens": 93,
            "total_tokens": 291,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bq1b1r/ppp3pp/4k3/3np3/2B5/4n3/PP1K1PPP/RNB4R w - - 0 11"
    },
    {
      "turn": 21,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "Kd2xc3",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxe3\n\nKd2xc3 Qd8-f6",
          "aggressive_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 96,
            "total_tokens": 295,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qd8",
            "f6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Kd2xc3",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxe3\n\nKd2xc3 Qd8-f6",
          "positional_tokens": {
            "prompt_tokens": 199,
            "completion_tokens": 96,
            "total_tokens": 295,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qd8",
            "f6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Ke6",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4 8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxe3\n\nKe6 Qxc2+ Kd6",
          "neutral_tokens": {
            "prompt_tokens": 198,
            "completion_tokens": 95,
            "total_tokens": 293,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qxc2+",
            "Kd6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qd8",
              "f6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qd8",
              "f6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qxc2+",
              "Kd6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 199,
            "completion_tokens": 96,
            "total_tokens": 295,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 199,
            "completion_tokens": 96,
            "total_tokens": 295,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 198,
            "completion_tokens": 95,
            "total_tokens": 293,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 596,
          "total_completion_tokens": 287,
          "total_tokens": 883
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 20,
            "turn_number": 11,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 199,
            "completion_tokens": 96,
            "total_tokens": 295,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 20,
            "turn_number": 11,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 199,
            "completion_tokens": 96,
            "total_tokens": 295,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 20,
            "turn_number": 11,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 198,
            "completion_tokens": 95,
            "total_tokens": 293,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bq1b1r/ppp3pp/4k3/3np3/2B5/4n3/PP1K1PPP/RNB4R w - - 0 11"
    },
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4\n\n1. d4",
          "aggressive_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4\n\n1. d4",
          "positional_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": null,
          "neutral_response": "1. d4\n\n1. d4",
          "neutral_tokens": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 353,
          "total_completion_tokens": 27,
          "total_tokens": 380
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 2,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4\n\n1. d4",
          "aggressive_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4\n\n1. d4",
          "positional_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": null,
          "neutral_response": "1. d4\n\n1. d4",
          "neutral_tokens": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 353,
          "total_completion_tokens": 27,
          "total_tokens": 380
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 2,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": null,
          "aggressive_response": "1. d4\n\n1. d4",
          "aggressive_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d4",
          "positional_uci": null,
          "positional_response": "1. d4\n\n1. d4",
          "positional_tokens": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": null,
          "neutral_response": "1. d4\n\n1. d4",
          "neutral_tokens": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 353,
          "total_completion_tokens": 27,
          "total_tokens": 380
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 118,
            "completion_tokens": 9,
            "total_tokens": 127,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 117,
            "completion_tokens": 9,
            "total_tokens": 126,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e4",
          "aggressive_uci": "e2e4",
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bb5 a6\n4. Ba4 Nf6\n5. O-O Be7\n6. Re1 b5\n7. Bb3 d6\n8. c3 O-O\n9. h3 Na5\n10. Bc2 c5\n11. d4 Nc6\n12. Nbd2 cxd4\n13. cxd4 exd4\n14. Nxd4 Bb7\n15. Nxc6 Bxc6\n16. Bg5 h6\n17. Bh4 g5\n18. Bg3 Kh7\n19. Qd3+ Kg8\n20. Qf5\n\nQf6",
          "aggressive_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 172,
            "total_tokens": 286,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "exd4",
            "Nxd4",
            "Bb7",
            "Nxc6",
            "Bxc6",
            "Bg5",
            "h6",
            "Bh4",
            "g5",
            "Bg3",
            "Kh7",
            "Qd3+",
            "Kg8",
            "Qf5",
            "Qf6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bb5 a6\n4. Ba4 Nf6\n5. O-O Be7\n6. Re1 b5\n7. Bb3 d6\n8. c3 O-O\n9. h3 Na5\n10. Bc2 c5\n11. d4 Nc6\n12. Nbd2 cxd4\n13. cxd4 exd4\n14. Nxd4 Nxd4\n15. Qxd4 Bc5\n16. Qd3 Bxd3\n17. Bxd3 Qd7\n18. Nf3 Rfe8\n19. Be3 Rc8\n20. Rc1 Rxc1\n21. Bxc1 Qc7\n22. Bb2 Qc6\n23. Rc1 Qd7\n24. Rc2 Qd8\n25. Rc1 Qd7\n26. Rc2 Qd8\n27. Rc1 Qd7\n28. Rc2 Qd8\n29. Rc1 Qd7\n30. Rc2 Qd8\n31. Rc1 Qd7\n32. Rc2 Qd8\n33. Rc1 Qd7\n34. Rc2 Qd8\n35. Rc1 Qd7\n36. Rc2 Qd8\n37. Rc1 Qd7\n38. Rc2 Qd8\n39. Rc1 Qd7\n40. Rc2 Qd8\n41. Rc1 Qd7\n42. Rc2 Qd8\n43. Rc1 Qd7\n44. Rc2 Qd8\n45. Rc1 Qd7\n46. Rc2 Qd8\n47. Rc1 Qd7\n48. Rc2 Qd8\n49. Rc1 Qd7\n50. Rc2 Qd8\n51. Rc1 Qd7\n52. Rc2 Qd8\n53. Rc1 Qd7\n54. Rc2 Qd8\n55. Rc1 Qd7\n56. Rc2 Qd8\n57. Rc1 Qd7\n58. Rc2 Qd8\n59. Rc1 Qd7\n60. Rc2 Qd8\n61. Rc1 Qd7\n62. Rc2 Qd8\n63. Rc1 Qd7\n64. Rc2 Qd8\n65. Rc1 Qd7\n66. Rc2 Qd8\n67. Rc1 Q",
          "positional_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "exd4",
            "Nxd4",
            "Nxd4",
            "Qxd4",
            "Bc5",
            "Qd3",
            "Bxd3",
            "Bxd3",
            "Qd7",
            "Nf3",
            "Rfe8",
            "Be3",
            "Rc8",
            "Rc1",
            "Rxc1",
            "Bxc1",
            "Qc7",
            "Bb2",
            "Qc6",
            "Rc1",
            "Qd7",
            "Rc2",
            "Qd8",
            "Rc1",
            "Qd7",
            "Rc2",
            "Qd8",
            "Rc1",
            "Qd7",
            "Rc2",
            "Qd8",
            "Rc1",
            "Qd7",
            "Rc2",
            "Qd8",
            "Rc1",
            "Qd7",
            "Rc2",
            "Qd8",
            "Rc1",
            "Qd7",
            "Rc2",
            "Qd8",
            "Rc1",
            "Qd7",
            "Rc2",
            "Qd8",
            "Rc1",
            "Qd7",
            "Rc2",
            "Qd8",
            "Rc1",
            "Qd7",
            "Rc2",
            "Qd8",
            "Rc1",
            "Qd7",
            "Rc2",
            "Qd8",
            "Rc1",
            "Qd7",
            "Rc2",
            "Qd8",
            "Rc1",
            "Qd7",
            "Rc2",
            "Qd8",
            "Rc1",
            "Qd7",
            "Rc2",
            "Qd8",
            "Rc1",
            "Qd7",
            "Rc2",
            "Qd8",
            "Rc1",
            "Qd7",
            "Rc2",
            "Qd8",
            "Rc1",
            "Qd7",
            "Rc2",
            "Qd8",
            "Rc1",
            "Qd7",
            "Rc2",
            "Qd8",
            "Rc1",
            "Qd7",
            "Rc2",
            "Qd8",
            "Rc1",
            "Qd7",
            "Rc2",
            "Qd8",
            "Rc1",
            "Qd7",
            "Rc2",
            "Qd8",
            "Rc1",
            "Qd7",
            "Rc2",
            "Qd8",
            "Rc1",
            "Qd7",
            "Rc2",
            "Qd8",
            "Rc1"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": "e2e4",
          "neutral_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Bc5\n4. c3 Nf6\n5. d4 exd4\n6. cxd4 Bb4+\n7. Nc3 Nxe4\n8. O-O Bxc3\n9. d5 Bxa1\n10. Qxa1 Nc5\n11. Qd4 Ncd7\n12. Nxe5 O-O\n13. Nc4 Qe7\n14. Qxe4 Qxe4\n15. Nxe4 Nxe4\n16. Bg5 Nc5\n17. Bxe7 Nxe7\n18. Rc1 Nc6\n19. Rc3 Nf5\n20. Rc7 Nf6\n21. Rc6 Nf5\n22. Rc7 Nf6\n23. Rc6 Nf5\n24. Rc7 Nf6\n25. Rc6 Nf5\n26. Rc7 Nf6\n27. Rc6 Nf5\n28. Rc7 Nf6\n29. Rc6 Nf5\n30. Rc7 Nf6\n31. Rc6 Nf5\n32. Rc7 Nf6\n33. Rc6 Nf5\n34. Rc7 Nf6\n35. Rc6 Nf5\n36. Rc7 Nf6\n37. Rc6 Nf5\n38. Rc7 Nf6\n39. Rc6 Nf5\n40. Rc7 Nf6\n41. Rc6 Nf5\n42. Rc7 Nf6\n43. Rc6 Nf5\n44. Rc7 Nf6\n45. Rc6 Nf5\n46. Rc7 Nf6\n47. Rc6 Nf5\n48. Rc7 Nf6\n49. Rc6 Nf5\n50. Rc7 Nf6\n51. Rc6 Nf5\n52. Rc7 Nf6\n53. Rc6 Nf5\n54. Rc7 Nf6\n55. Rc6 Nf5\n56. Rc7 Nf6\n57. Rc6 Nf5\n58. Rc7 Nf6\n59. Rc6 Nf5\n60. Rc7 Nf6\n61. Rc6 Nf5\n62. Rc7 Nf6\n63. Rc6 Nf5\n64. Rc7 Nf6\n65. Rc6 Nf5\n66. Rc7 Nf",
          "neutral_tokens": {
            "prompt_tokens": 113,
            "completion_tokens": 640,
            "total_tokens": 753,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bc4",
            "Bc5",
            "c3",
            "Nf6",
            "d4",
            "exd4",
            "cxd4",
            "Bb4+",
            "Nc3",
            "Nxe4",
            "O-O",
            "Bxc3",
            "d5",
            "Bxa1",
            "Qxa1",
            "Nc5",
            "Qd4",
            "Ncd7",
            "Nxe5",
            "O-O",
            "Nc4",
            "Qe7",
            "Qxe4",
            "Qxe4",
            "Nxe4",
            "Nxe4",
            "Bg5",
            "Nc5",
            "Bxe7",
            "Nxe7",
            "Rc1",
            "Nc6",
            "Rc3",
            "Nf5",
            "Rc7",
            "Nf6",
            "Rc6",
            "Nf5",
            "Rc7",
            "Nf6",
            "Rc6",
            "Nf5",
            "Rc7",
            "Nf6",
            "Rc6",
            "Nf5",
            "Rc7",
            "Nf6",
            "Rc6",
            "Nf5",
            "Rc7",
            "Nf6",
            "Rc6",
            "Nf5",
            "Rc7",
            "Nf6",
            "Rc6",
            "Nf5",
            "Rc7",
            "Nf6",
            "Rc6",
            "Nf5",
            "Rc7",
            "Nf6",
            "Rc6",
            "Nf5",
            "Rc7",
            "Nf6",
            "Rc6",
            "Nf5",
            "Rc7",
            "Nf6",
            "Rc6",
            "Nf5",
            "Rc7",
            "Nf6",
            "Rc6",
            "Nf5",
            "Rc7",
            "Nf6",
            "Rc6",
            "Nf5",
            "Rc7",
            "Nf6",
            "Rc6",
            "Nf5",
            "Rc7",
            "Nf6",
            "Rc6",
            "Nf5",
            "Rc7",
            "Nf6",
            "Rc6",
            "Nf5",
            "Rc7",
            "Nf6",
            "Rc6",
            "Nf5",
            "Rc7",
            "Nf6",
            "Rc6",
            "Nf5",
            "Rc7",
            "Nf6",
            "Rc6",
            "Nf5",
            "Rc7",
            "Nf6",
            "Rc6",
            "Nf5",
            "Rc7",
            "Nf6",
            "Rc6",
            "Nf5",
            "Rc7",
            "Nf6",
            "Rc6",
            "Nf5",
            "Rc7",
            "Nf6",
            "Rc6",
            "Nf5",
            "Rc7",
            "Nf6",
            "Rc6",
            "Nf5",
            "Rc7"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e2e4",
          "positional_uci": "e2e4",
          "neutral_uci": "e2e4",
          "consensus_move": "e2e4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "exd4",
              "Nxd4",
              "Bb7",
              "Nxc6",
              "Bxc6",
              "Bg5",
              "h6",
              "Bh4",
              "g5",
              "Bg3",
              "Kh7",
              "Qd3+",
              "Kg8",
              "Qf5",
              "Qf6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "exd4",
              "Nxd4",
              "Nxd4",
              "Qxd4",
              "Bc5",
              "Qd3",
              "Bxd3",
              "Bxd3",
              "Qd7",
              "Nf3",
              "Rfe8",
              "Be3",
              "Rc8",
              "Rc1",
              "Rxc1",
              "Bxc1",
              "Qc7",
              "Bb2",
              "Qc6",
              "Rc1",
              "Qd7",
              "Rc2",
              "Qd8",
              "Rc1",
              "Qd7",
              "Rc2",
              "Qd8",
              "Rc1",
              "Qd7",
              "Rc2",
              "Qd8",
              "Rc1",
              "Qd7",
              "Rc2",
              "Qd8",
              "Rc1",
              "Qd7",
              "Rc2",
              "Qd8",
              "Rc1",
              "Qd7",
              "Rc2",
              "Qd8",
              "Rc1",
              "Qd7",
              "Rc2",
              "Qd8",
              "Rc1",
              "Qd7",
              "Rc2",
              "Qd8",
              "Rc1",
              "Qd7",
              "Rc2",
              "Qd8",
              "Rc1",
              "Qd7",
              "Rc2",
              "Qd8",
              "Rc1",
              "Qd7",
              "Rc2",
              "Qd8",
              "Rc1",
              "Qd7",
              "Rc2",
              "Qd8",
              "Rc1",
              "Qd7",
              "Rc2",
              "Qd8",
              "Rc1",
              "Qd7",
              "Rc2",
              "Qd8",
              "Rc1",
              "Qd7",
              "Rc2",
              "Qd8",
              "Rc1",
              "Qd7",
              "Rc2",
              "Qd8",
              "Rc1",
              "Qd7",
              "Rc2",
              "Qd8",
              "Rc1",
              "Qd7",
              "Rc2",
              "Qd8",
              "Rc1",
              "Qd7",
              "Rc2",
              "Qd8",
              "Rc1",
              "Qd7",
              "Rc2",
              "Qd8",
              "Rc1",
              "Qd7",
              "Rc2",
              "Qd8",
              "Rc1",
              "Qd7",
              "Rc2",
              "Qd8",
              "Rc1"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bc4",
              "Bc5",
              "c3",
              "Nf6",
              "d4",
              "exd4",
              "cxd4",
              "Bb4+",
              "Nc3",
              "Nxe4",
              "O-O",
              "Bxc3",
              "d5",
              "Bxa1",
              "Qxa1",
              "Nc5",
              "Qd4",
              "Ncd7",
              "Nxe5",
              "O-O",
              "Nc4",
              "Qe7",
              "Qxe4",
              "Qxe4",
              "Nxe4",
              "Nxe4",
              "Bg5",
              "Nc5",
              "Bxe7",
              "Nxe7",
              "Rc1",
              "Nc6",
              "Rc3",
              "Nf5",
              "Rc7",
              "Nf6",
              "Rc6",
              "Nf5",
              "Rc7",
              "Nf6",
              "Rc6",
              "Nf5",
              "Rc7",
              "Nf6",
              "Rc6",
              "Nf5",
              "Rc7",
              "Nf6",
              "Rc6",
              "Nf5",
              "Rc7",
              "Nf6",
              "Rc6",
              "Nf5",
              "Rc7",
              "Nf6",
              "Rc6",
              "Nf5",
              "Rc7",
              "Nf6",
              "Rc6",
              "Nf5",
              "Rc7",
              "Nf6",
              "Rc6",
              "Nf5",
              "Rc7",
              "Nf6",
              "Rc6",
              "Nf5",
              "Rc7",
              "Nf6",
              "Rc6",
              "Nf5",
              "Rc7",
              "Nf6",
              "Rc6",
              "Nf5",
              "Rc7",
              "Nf6",
              "Rc6",
              "Nf5",
              "Rc7",
              "Nf6",
              "Rc6",
              "Nf5",
              "Rc7",
              "Nf6",
              "Rc6",
              "Nf5",
              "Rc7",
              "Nf6",
              "Rc6",
              "Nf5",
              "Rc7",
              "Nf6",
              "Rc6",
              "Nf5",
              "Rc7",
              "Nf6",
              "Rc6",
              "Nf5",
              "Rc7",
              "Nf6",
              "Rc6",
              "Nf5",
              "Rc7",
              "Nf6",
              "Rc6",
              "Nf5",
              "Rc7",
              "Nf6",
              "Rc6",
              "Nf5",
              "Rc7",
              "Nf6",
              "Rc6",
              "Nf5",
              "Rc7",
              "Nf6",
              "Rc6",
              "Nf5",
              "Rc7",
              "Nf6",
              "Rc6",
              "Nf5",
              "Rc7"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 114,
            "completion_tokens": 172,
            "total_tokens": 286,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 113,
            "completion_tokens": 640,
            "total_tokens": 753,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 341,
          "total_completion_tokens": 1452,
          "total_tokens": 1793
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 114,
            "completion_tokens": 172,
            "total_tokens": 286,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 113,
            "completion_tokens": 640,
            "total_tokens": 753,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 3,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf3",
          "aggressive_uci": "g1f3",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5 11. d4 Nc6 12. Nbd2 cxd4 13. cxd4 exd4 14. Nxd4 Bg4 15. f3 Bxf3 16. Qxf3 Nxd4 17. Bxd4 Qxd4 18. Qxd4 Nxd4 19. Bxd4 Bxd4 20. Rxd4 Rfd8 21. Rxd8+ Rxd8 22. Kf2 Kf8 23. Ke3 Ke7 24. Kd3 Kd6 25. Kc3 Kc6 26. Kb3 Kb5 27. Ka3 Kc5 28. Kb3 Kb5 29. Ka3 Kc5 30. Kb3 Kb5 31. Ka3 Kc5 32. Kb3 Kb5 33. Ka3 Kc5 34. Kb3 Kb5 35. Ka3 Kc5 36. Kb3 Kb5 37. Ka3 Kc5 38. Kb3 Kb5 39. Ka3 Kc5 40. Kb3 Kb5 41. Ka3 Kc5 42. Kb3 Kb5 43. Ka3 Kc5 44. Kb3 Kb5 45. Ka3 Kc5 46. Kb3 Kb5 47. Ka3 Kc5 48. Kb3 Kb5 49. Ka3 Kc5 50. Kb3 Kb5 51. Ka3 Kc5 52. Kb3 Kb5 53. Ka3 Kc5 54. Kb3 Kb5 55. Ka3 Kc5 56. Kb3 Kb5 57. Ka3 Kc5 58. Kb3 Kb5 59. Ka3 Kc5 60. Kb3 Kb5 61. Ka3 Kc5 62. Kb3 Kb5 63. Ka3 Kc5 64. Kb3 Kb5 65. Ka3 Kc5 66. Kb3 Kb5 67. Ka3 Kc5 68. Kb3 Kb5 69. Ka3 Kc5",
          "aggressive_tokens": {
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "exd4",
            "Nxd4",
            "Bg4",
            "f3",
            "Bxf3",
            "Qxf3",
            "Nxd4",
            "Bxd4",
            "Qxd4",
            "Qxd4",
            "Nxd4",
            "Bxd4",
            "Bxd4",
            "Rxd4",
            "Rfd8",
            "Rxd8+",
            "Rxd8",
            "Kf2",
            "Kf8",
            "Ke3",
            "Ke7",
            "Kd3",
            "Kd6",
            "Kc3",
            "Kc6",
            "Kb3",
            "Kb5",
            "Ka3",
            "Kc5",
            "Kb3",
            "Kb5",
            "Ka3",
            "Kc5",
            "Kb3",
            "Kb5",
            "Ka3",
            "Kc5",
            "Kb3",
            "Kb5",
            "Ka3",
            "Kc5",
            "Kb3",
            "Kb5",
            "Ka3",
            "Kc5",
            "Kb3",
            "Kb5",
            "Ka3",
            "Kc5",
            "Kb3",
            "Kb5",
            "Ka3",
            "Kc5",
            "Kb3",
            "Kb5",
            "Ka3",
            "Kc5",
            "Kb3",
            "Kb5",
            "Ka3",
            "Kc5",
            "Kb3",
            "Kb5",
            "Ka3",
            "Kc5",
            "Kb3",
            "Kb5",
            "Ka3",
            "Kc5",
            "Kb3",
            "Kb5",
            "Ka3",
            "Kc5",
            "Kb3",
            "Kb5",
            "Ka3",
            "Kc5",
            "Kb3",
            "Kb5",
            "Ka3",
            "Kc5",
            "Kb3",
            "Kb5",
            "Ka3",
            "Kc5",
            "Kb3",
            "Kb5",
            "Ka3",
            "Kc5",
            "Kb3",
            "Kb5",
            "Ka3",
            "Kc5",
            "Kb3",
            "Kb5",
            "Ka3",
            "Kc5",
            "Kb3",
            "Kb5",
            "Ka3",
            "Kc5",
            "Kb3",
            "Kb5",
            "Ka3",
            "Kc5",
            "Kb3",
            "Kb5",
            "Ka3",
            "Kc5",
            "Kb3",
            "Kb5",
            "Ka3",
            "Kc5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf3",
          "positional_uci": "g1f3",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5 11. d4 Nc6 12. Nbd2 cxd4 13. cxd4 exd4 14. Nxd4 Bg4 15. f3 Bxf3 16. Qxf3 Nxd4 17. Qxd4 Qxd4 18. Nxd4 Nxd4 19. Bxd4 Bc5 20. Bxc5 dxc5 21. Rxd8 Rxd8 22. Rc1 Rd2 23. Rc7 Rd4 24. Rxc5 Rd1+ 25. Kh2 Rd2 26. Rc7 Rd4 27. Rc5 Rd2 28. Rc7 Rd4 29. Rc5 Rd2 30. Rc7 Rd4 31. Rc5 Rd2 32. Rc7 Rd4 33. Rc5 Rd2 34. Rc7 Rd4 35. Rc5 Rd2 36. Rc7 Rd4 37. Rc5 Rd2 38. Rc7 Rd4 39. Rc5 Rd2 40. Rc7 Rd4 41. Rc5 Rd2 42. Rc7 Rd4 43. Rc5 Rd2 44. Rc7 Rd4 45. Rc5 Rd2 46. Rc7 Rd4 47. Rc5 Rd2 48. Rc7 Rd4 49. Rc5 Rd2 50. Rc7 Rd4 51. Rc5 Rd2 52. Rc7 Rd4 53. Rc5 Rd2 54. Rc7 Rd4 55. Rc5 Rd2 56. Rc7 Rd4 57. Rc5 Rd2 58. Rc7 Rd4 59. Rc5 Rd2 60. Rc7 Rd4 61. Rc5 Rd2 62. Rc7 Rd4 63. Rc5 Rd2 64. Rc7 Rd4 65. Rc5 Rd2 66. Rc7 Rd4 67. Rc5 Rd2 68. Rc7 Rd4 69. Rc5 Rd2 70. Rc7 Rd4 71. Rc5 Rd2 72. Rc7 Rd",
          "positional_tokens": {
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "exd4",
            "Nxd4",
            "Bg4",
            "f3",
            "Bxf3",
            "Qxf3",
            "Nxd4",
            "Qxd4",
            "Qxd4",
            "Nxd4",
            "Nxd4",
            "Bxd4",
            "Bc5",
            "Bxc5",
            "dxc5",
            "Rxd8",
            "Rxd8",
            "Rc1",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rxc5",
            "Rd1+",
            "Kh2",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rc5",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rc5",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rc5",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rc5",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rc5",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rc5",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rc5",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rc5",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rc5",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rc5",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rc5",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rc5",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rc5",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rc5",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rc5",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rc5",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rc5",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rc5",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rc5",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rc5",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rc5",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rc5",
            "Rd2",
            "Rc7",
            "Rd4",
            "Rc5",
            "Rd2",
            "Rc7"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf3",
          "neutral_uci": "g1f3",
          "neutral_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bb5 a6\n4. Ba4 Nf6\n5. O-O Be7\n6. Re1 b5\n7. Bb3 d6\n8. c3 O-O\n9. h3 Na5\n10. Bc2 c5\n11. d4 Nc6\n12. Nbd2 cxd4\n13. cxd4 exd4\n14. Nxd4 Nxd4\n15. Qxd4 Bc5\n16. Qd3 Bxd4\n17. Bxd4 Qxd4\n18. Qxd4 Nxd4\n19. Bxd4 Rfd8\n20. Rc1 Rxd4\n21. Rxc8+ Rxc8\n22. Rc1+ Rxc1+\n23. Kxc1 Kf8\n24. Kd2 Ke7\n25. Ke3 Kd6\n26. Kd4 Kc6\n27. Kc4 Kb6\n28. Kb4 Kxa6\n29. Kxa5 Kb7\n30. Kb5 Kc7\n31. Kc5 Kd7\n32. Kd5 Ke7\n33. Ke5 Kf7\n34. Kf5 Kg7\n35. Kg5 Kg8\n36. Kf6 Kf8\n37. Kf5 Kg7\n38. Kg5 Kg8\n39. Kf6 Kf8\n40. Kf5 Kg7\n41. Kg5 Kg8\n42. Kf6 Kf8\n43. Kf5 Kg7\n44. Kg5 Kg8\n45. Kf6 Kf8\n46. Kf5 Kg7\n47. Kg5 Kg8\n48. Kf6 Kf8\n49. Kf5 Kg7\n50. Kg5 Kg8\n51. Kf6 Kf8\n52. Kf5 Kg7\n53. Kg5 Kg8\n54. Kf6 Kf8\n55. Kf5 Kg7\n56. Kg5 Kg8\n57. Kf6 Kf8\n58. Kf5 Kg7\n59. Kg5 Kg8\n60. Kf6 Kf8\n61. Kf5 Kg7\n62. Kg5 Kg8\n63. Kf6 Kf8\n64. Kf5 Kg7\n65. Kg5 Kg8\n66. Kf6 Kf8\n67. Kf5 Kg",
          "neutral_tokens": {
            "prompt_tokens": 119,
            "completion_tokens": 640,
            "total_tokens": 759,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Nc6",
            "Nbd2",
            "cxd4",
            "cxd4",
            "exd4",
            "Nxd4",
            "Nxd4",
            "Qxd4",
            "Bc5",
            "Qd3",
            "Bxd4",
            "Bxd4",
            "Qxd4",
            "Qxd4",
            "Nxd4",
            "Bxd4",
            "Rfd8",
            "Rc1",
            "Rxd4",
            "Rxc8+",
            "Rxc8",
            "Rc1+",
            "Rxc1+",
            "Kxc1",
            "Kf8",
            "Kd2",
            "Ke7",
            "Ke3",
            "Kd6",
            "Kd4",
            "Kc6",
            "Kc4",
            "Kb6",
            "Kb4",
            "Kxa6",
            "Kxa5",
            "Kb7",
            "Kb5",
            "Kc7",
            "Kc5",
            "Kd7",
            "Kd5",
            "Ke7",
            "Ke5",
            "Kf7",
            "Kf5",
            "Kg7",
            "Kg5",
            "Kg8",
            "Kf6",
            "Kf8",
            "Kf5",
            "Kg7",
            "Kg5",
            "Kg8",
            "Kf6",
            "Kf8",
            "Kf5",
            "Kg7",
            "Kg5",
            "Kg8",
            "Kf6",
            "Kf8",
            "Kf5",
            "Kg7",
            "Kg5",
            "Kg8",
            "Kf6",
            "Kf8",
            "Kf5",
            "Kg7",
            "Kg5",
            "Kg8",
            "Kf6",
            "Kf8",
            "Kf5",
            "Kg7",
            "Kg5",
            "Kg8",
            "Kf6",
            "Kf8",
            "Kf5",
            "Kg7",
            "Kg5",
            "Kg8",
            "Kf6",
            "Kf8",
            "Kf5",
            "Kg7",
            "Kg5",
            "Kg8",
            "Kf6",
            "Kf8",
            "Kf5",
            "Kg7",
            "Kg5",
            "Kg8",
            "Kf6",
            "Kf8",
            "Kf5",
            "Kg7",
            "Kg5",
            "Kg8",
            "Kf6",
            "Kf8",
            "Kf5",
            "Kg7",
            "Kg5",
            "Kg8",
            "Kf6",
            "Kf8",
            "Kf5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g1f3",
          "positional_uci": "g1f3",
          "neutral_uci": "g1f3",
          "consensus_move": "g1f3",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "exd4",
              "Nxd4",
              "Bg4",
              "f3",
              "Bxf3",
              "Qxf3",
              "Nxd4",
              "Bxd4",
              "Qxd4",
              "Qxd4",
              "Nxd4",
              "Bxd4",
              "Bxd4",
              "Rxd4",
              "Rfd8",
              "Rxd8+",
              "Rxd8",
              "Kf2",
              "Kf8",
              "Ke3",
              "Ke7",
              "Kd3",
              "Kd6",
              "Kc3",
              "Kc6",
              "Kb3",
              "Kb5",
              "Ka3",
              "Kc5",
              "Kb3",
              "Kb5",
              "Ka3",
              "Kc5",
              "Kb3",
              "Kb5",
              "Ka3",
              "Kc5",
              "Kb3",
              "Kb5",
              "Ka3",
              "Kc5",
              "Kb3",
              "Kb5",
              "Ka3",
              "Kc5",
              "Kb3",
              "Kb5",
              "Ka3",
              "Kc5",
              "Kb3",
              "Kb5",
              "Ka3",
              "Kc5",
              "Kb3",
              "Kb5",
              "Ka3",
              "Kc5",
              "Kb3",
              "Kb5",
              "Ka3",
              "Kc5",
              "Kb3",
              "Kb5",
              "Ka3",
              "Kc5",
              "Kb3",
              "Kb5",
              "Ka3",
              "Kc5",
              "Kb3",
              "Kb5",
              "Ka3",
              "Kc5",
              "Kb3",
              "Kb5",
              "Ka3",
              "Kc5",
              "Kb3",
              "Kb5",
              "Ka3",
              "Kc5",
              "Kb3",
              "Kb5",
              "Ka3",
              "Kc5",
              "Kb3",
              "Kb5",
              "Ka3",
              "Kc5",
              "Kb3",
              "Kb5",
              "Ka3",
              "Kc5",
              "Kb3",
              "Kb5",
              "Ka3",
              "Kc5",
              "Kb3",
              "Kb5",
              "Ka3",
              "Kc5",
              "Kb3",
              "Kb5",
              "Ka3",
              "Kc5",
              "Kb3",
              "Kb5",
              "Ka3",
              "Kc5",
              "Kb3",
              "Kb5",
              "Ka3",
              "Kc5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "exd4",
              "Nxd4",
              "Bg4",
              "f3",
              "Bxf3",
              "Qxf3",
              "Nxd4",
              "Qxd4",
              "Qxd4",
              "Nxd4",
              "Nxd4",
              "Bxd4",
              "Bc5",
              "Bxc5",
              "dxc5",
              "Rxd8",
              "Rxd8",
              "Rc1",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rxc5",
              "Rd1+",
              "Kh2",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rc5",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rc5",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rc5",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rc5",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rc5",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rc5",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rc5",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rc5",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rc5",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rc5",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rc5",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rc5",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rc5",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rc5",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rc5",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rc5",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rc5",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rc5",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rc5",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rc5",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rc5",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rc5",
              "Rd2",
              "Rc7",
              "Rd4",
              "Rc5",
              "Rd2",
              "Rc7"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Nc6",
              "Nbd2",
              "cxd4",
              "cxd4",
              "exd4",
              "Nxd4",
              "Nxd4",
              "Qxd4",
              "Bc5",
              "Qd3",
              "Bxd4",
              "Bxd4",
              "Qxd4",
              "Qxd4",
              "Nxd4",
              "Bxd4",
              "Rfd8",
              "Rc1",
              "Rxd4",
              "Rxc8+",
              "Rxc8",
              "Rc1+",
              "Rxc1+",
              "Kxc1",
              "Kf8",
              "Kd2",
              "Ke7",
              "Ke3",
              "Kd6",
              "Kd4",
              "Kc6",
              "Kc4",
              "Kb6",
              "Kb4",
              "Kxa6",
              "Kxa5",
              "Kb7",
              "Kb5",
              "Kc7",
              "Kc5",
              "Kd7",
              "Kd5",
              "Ke7",
              "Ke5",
              "Kf7",
              "Kf5",
              "Kg7",
              "Kg5",
              "Kg8",
              "Kf6",
              "Kf8",
              "Kf5",
              "Kg7",
              "Kg5",
              "Kg8",
              "Kf6",
              "Kf8",
              "Kf5",
              "Kg7",
              "Kg5",
              "Kg8",
              "Kf6",
              "Kf8",
              "Kf5",
              "Kg7",
              "Kg5",
              "Kg8",
              "Kf6",
              "Kf8",
              "Kf5",
              "Kg7",
              "Kg5",
              "Kg8",
              "Kf6",
              "Kf8",
              "Kf5",
              "Kg7",
              "Kg5",
              "Kg8",
              "Kf6",
              "Kf8",
              "Kf5",
              "Kg7",
              "Kg5",
              "Kg8",
              "Kf6",
              "Kf8",
              "Kf5",
              "Kg7",
              "Kg5",
              "Kg8",
              "Kf6",
              "Kf8",
              "Kf5",
              "Kg7",
              "Kg5",
              "Kg8",
              "Kf6",
              "Kf8",
              "Kf5",
              "Kg7",
              "Kg5",
              "Kg8",
              "Kf6",
              "Kf8",
              "Kf5",
              "Kg7",
              "Kg5",
              "Kg8",
              "Kf6",
              "Kf8",
              "Kf5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 119,
            "completion_tokens": 640,
            "total_tokens": 759,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 359,
          "total_completion_tokens": 1920,
          "total_tokens": 2279
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 120,
            "completion_tokens": 640,
            "total_tokens": 760,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 2,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 119,
            "completion_tokens": 640,
            "total_tokens": 759,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2"
    },
    {
      "turn": 5,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bc4",
          "aggressive_uci": "f1c4",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Bc5 4. c3 Nf6 5. d4 exd4 6. cxd4 Bb4+ 7. Nc3 Nxe4 8. O-O Bxc3 9. d5 Bxa1 10. Qxa1+ Nc5 11. Qd4 Qf6 12. Qxc5 Qxc5 13. Nxe5 Qxe5 14. Bxf7+ Kd8 15. Bg5+ Kc7 16. Bxe7 Kb6 17. Bxd7 Qxd7 18. Rc1+ Kb5 19. Rc5+ Kb4 20. Rc4+ Kb3 21. Rc3+ Kb4 22. Rc4+ Kb3 23. Rc3+ Kb4 24. Rc4+ Kb3 25. Rc3+ Kb4 26. Rc4+ Kb3 27. Rc3+ Kb4 28. Rc4+ Kb3 29. Rc3+ Kb4 30. Rc4+ Kb3 31. Rc3+ Kb4 32. Rc4+ Kb3 33. Rc3+ Kb4 34. Rc4+ Kb3 35. Rc3+ Kb4 36. Rc4+ Kb3 37. Rc3+ Kb4 38. Rc4+ Kb3 39. Rc3+ Kb4 40. Rc4+ Kb3 41. Rc3+ Kb4 42. Rc4+ Kb3 43. Rc3+ Kb4 44. Rc4+ Kb3 45. Rc3+ Kb4 46. Rc4+ Kb3 47. Rc3+ Kb4 48. Rc4+ Kb3 49. Rc3+ Kb4 50. Rc4+ Kb3 51. Rc3+ Kb4 52. Rc4+ Kb3 53. Rc3+ Kb4 54. Rc4+ Kb3 55. Rc3+ Kb4 56. Rc4+ Kb3 57. Rc3+ Kb4 58. Rc4+ Kb3 59. Rc3+ Kb4 60. Rc4+ Kb3 61. Rc3+ Kb4",
          "aggressive_tokens": {
            "prompt_tokens": 129,
            "completion_tokens": 640,
            "total_tokens": 769,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bc5",
            "c3",
            "Nf6",
            "d4",
            "exd4",
            "cxd4",
            "Bb4+",
            "Nc3",
            "Nxe4",
            "O-O",
            "Bxc3",
            "d5",
            "Bxa1",
            "Qxa1+",
            "Nc5",
            "Qd4",
            "Qf6",
            "Qxc5",
            "Qxc5",
            "Nxe5",
            "Qxe5",
            "Bxf7+",
            "Kd8",
            "Bg5+",
            "Kc7",
            "Bxe7",
            "Kb6",
            "Bxd7",
            "Qxd7",
            "Rc1+",
            "Kb5",
            "Rc5+",
            "Kb4",
            "Rc4+",
            "Kb3",
            "Rc3+",
            "Kb4",
            "Rc4+",
            "Kb3",
            "Rc3+",
            "Kb4",
            "Rc4+",
            "Kb3",
            "Rc3+",
            "Kb4",
            "Rc4+",
            "Kb3",
            "Rc3+",
            "Kb4",
            "Rc4+",
            "Kb3",
            "Rc3+",
            "Kb4",
            "Rc4+",
            "Kb3",
            "Rc3+",
            "Kb4",
            "Rc4+",
            "Kb3",
            "Rc3+",
            "Kb4",
            "Rc4+",
            "Kb3",
            "Rc3+",
            "Kb4",
            "Rc4+",
            "Kb3",
            "Rc3+",
            "Kb4",
            "Rc4+",
            "Kb3",
            "Rc3+",
            "Kb4",
            "Rc4+",
            "Kb3",
            "Rc3+",
            "Kb4",
            "Rc4+",
            "Kb3",
            "Rc3+",
            "Kb4",
            "Rc4+",
            "Kb3",
            "Rc3+",
            "Kb4",
            "Rc4+",
            "Kb3",
            "Rc3+",
            "Kb4",
            "Rc4+",
            "Kb3",
            "Rc3+",
            "Kb4",
            "Rc4+",
            "Kb3",
            "Rc3+",
            "Kb4",
            "Rc4+",
            "Kb3",
            "Rc3+",
            "Kb4",
            "Rc4+",
            "Kb3",
            "Rc3+",
            "Kb4",
            "Rc4+",
            "Kb3",
            "Rc3+",
            "Kb4",
            "Rc4+",
            "Kb3",
            "Rc3+",
            "Kb4",
            "Rc4+",
            "Kb3",
            "Rc3+",
            "Kb4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bc4",
          "positional_uci": "f1c4",
          "positional_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4\n\n1. e4 e5 2. Nf3 Nc6 3. Bc4",
          "positional_tokens": {
            "prompt_tokens": 129,
            "completion_tokens": 41,
            "total_tokens": 170,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bc4",
          "neutral_uci": "f1c4",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Bc5 4. c3 Nf6 5. d4 exd4 6. cxd4 Bb4+ 7. Nc3 Nxe4 8. O-O Bxc3 9. d5 Bxa1 10. Qxa1+ Nc6 11. Qd4\n\nNf6",
          "neutral_tokens": {
            "prompt_tokens": 128,
            "completion_tokens": 95,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc5",
            "c3",
            "Nf6",
            "d4",
            "exd4",
            "cxd4",
            "Bb4+",
            "Nc3",
            "Nxe4",
            "O-O",
            "Bxc3",
            "d5",
            "Bxa1",
            "Qxa1+",
            "Nc6",
            "Qd4",
            "Nf6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f1c4",
          "positional_uci": "f1c4",
          "neutral_uci": "f1c4",
          "consensus_move": "f1c4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bc5",
              "c3",
              "Nf6",
              "d4",
              "exd4",
              "cxd4",
              "Bb4+",
              "Nc3",
              "Nxe4",
              "O-O",
              "Bxc3",
              "d5",
              "Bxa1",
              "Qxa1+",
              "Nc5",
              "Qd4",
              "Qf6",
              "Qxc5",
              "Qxc5",
              "Nxe5",
              "Qxe5",
              "Bxf7+",
              "Kd8",
              "Bg5+",
              "Kc7",
              "Bxe7",
              "Kb6",
              "Bxd7",
              "Qxd7",
              "Rc1+",
              "Kb5",
              "Rc5+",
              "Kb4",
              "Rc4+",
              "Kb3",
              "Rc3+",
              "Kb4",
              "Rc4+",
              "Kb3",
              "Rc3+",
              "Kb4",
              "Rc4+",
              "Kb3",
              "Rc3+",
              "Kb4",
              "Rc4+",
              "Kb3",
              "Rc3+",
              "Kb4",
              "Rc4+",
              "Kb3",
              "Rc3+",
              "Kb4",
              "Rc4+",
              "Kb3",
              "Rc3+",
              "Kb4",
              "Rc4+",
              "Kb3",
              "Rc3+",
              "Kb4",
              "Rc4+",
              "Kb3",
              "Rc3+",
              "Kb4",
              "Rc4+",
              "Kb3",
              "Rc3+",
              "Kb4",
              "Rc4+",
              "Kb3",
              "Rc3+",
              "Kb4",
              "Rc4+",
              "Kb3",
              "Rc3+",
              "Kb4",
              "Rc4+",
              "Kb3",
              "Rc3+",
              "Kb4",
              "Rc4+",
              "Kb3",
              "Rc3+",
              "Kb4",
              "Rc4+",
              "Kb3",
              "Rc3+",
              "Kb4",
              "Rc4+",
              "Kb3",
              "Rc3+",
              "Kb4",
              "Rc4+",
              "Kb3",
              "Rc3+",
              "Kb4",
              "Rc4+",
              "Kb3",
              "Rc3+",
              "Kb4",
              "Rc4+",
              "Kb3",
              "Rc3+",
              "Kb4",
              "Rc4+",
              "Kb3",
              "Rc3+",
              "Kb4",
              "Rc4+",
              "Kb3",
              "Rc3+",
              "Kb4",
              "Rc4+",
              "Kb3",
              "Rc3+",
              "Kb4"
            ],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bc5",
              "c3",
              "Nf6",
              "d4",
              "exd4",
              "cxd4",
              "Bb4+",
              "Nc3",
              "Nxe4",
              "O-O",
              "Bxc3",
              "d5",
              "Bxa1",
              "Qxa1+",
              "Nc6",
              "Qd4",
              "Nf6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 129,
            "completion_tokens": 640,
            "total_tokens": 769,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 129,
            "completion_tokens": 41,
            "total_tokens": 170,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 128,
            "completion_tokens": 95,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 386,
          "total_completion_tokens": 776,
          "total_tokens": 1162
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 129,
            "completion_tokens": 640,
            "total_tokens": 769,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 129,
            "completion_tokens": 41,
            "total_tokens": 170,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 4,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 128,
            "completion_tokens": 95,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkbnr/pppp1ppp/2n5/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3"
    },
    {
      "turn": 7,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc3",
          "aggressive_uci": "b1c3",
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n\nNc3",
          "aggressive_tokens": {
            "prompt_tokens": 137,
            "completion_tokens": 27,
            "total_tokens": 164,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Ng5",
          "positional_uci": "f3g5",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Na5 6. Bb5+ c6 7. dxc6 bxc6 8. Be2 h6 9. Nf3 e4 10. Ne5 Qd6 11. c3 Nc4 12. O-O Nxe3 13. fxe3 Qxd5 14. Qf3\n\nQc5+ 0-0",
          "positional_tokens": {
            "prompt_tokens": 137,
            "completion_tokens": 124,
            "total_tokens": 261,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d5",
            "exd5",
            "Na5",
            "Bb5+",
            "c6",
            "dxc6",
            "bxc6",
            "Be2",
            "h6",
            "Nf3",
            "e4",
            "Ne5",
            "Qd6",
            "c3",
            "Nc4",
            "O-O",
            "Nxe3",
            "fxe3",
            "Qxd5",
            "Qf3",
            "Qc5+"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc3",
          "neutral_uci": "b1c3",
          "neutral_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n\nNc3",
          "neutral_tokens": {
            "prompt_tokens": 136,
            "completion_tokens": 27,
            "total_tokens": 163,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b1c3",
          "positional_uci": "f3g5",
          "neutral_uci": "b1c3",
          "consensus_move": "b1c3",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [
              "d5",
              "exd5",
              "Na5",
              "Bb5+",
              "c6",
              "dxc6",
              "bxc6",
              "Be2",
              "h6",
              "Nf3",
              "e4",
              "Ne5",
              "Qd6",
              "c3",
              "Nc4",
              "O-O",
              "Nxe3",
              "fxe3",
              "Qxd5",
              "Qf3",
              "Qc5+"
            ],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 137,
            "completion_tokens": 27,
            "total_tokens": 164,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 137,
            "completion_tokens": 124,
            "total_tokens": 261,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 136,
            "completion_tokens": 27,
            "total_tokens": 163,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 410,
          "total_completion_tokens": 178,
          "total_tokens": 588
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 137,
            "completion_tokens": 27,
            "total_tokens": 164,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 137,
            "completion_tokens": 124,
            "total_tokens": 261,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 6,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 136,
            "completion_tokens": 27,
            "total_tokens": 163,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/pppp1ppp/2n2n2/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R w KQkq - 4 4"
    },
    {
      "turn": 9,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc3",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Nc3 Nxe4\n\nNc3 Nc6",
          "aggressive_tokens": {
            "prompt_tokens": 146,
            "completion_tokens": 39,
            "total_tokens": 185,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nc3",
          "positional_uci": null,
          "positional_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Nc3 Nxe4\n\nNc3 Nc6",
          "positional_tokens": {
            "prompt_tokens": 146,
            "completion_tokens": 39,
            "total_tokens": 185,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nxe4",
          "neutral_uci": "c3e4",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. Bxe4 Nxe4 8. Qxd8+ Kxd8 9. Nxe4\n\nKd7 e3",
          "neutral_tokens": {
            "prompt_tokens": 145,
            "completion_tokens": 79,
            "total_tokens": 224,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kd7",
            "e3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": "c3e4",
          "consensus_move": "c3e4",
          "source_agents": [
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nc6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nc6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Kd7",
              "e3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 146,
            "completion_tokens": 39,
            "total_tokens": 185,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 146,
            "completion_tokens": 39,
            "total_tokens": 185,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 145,
            "completion_tokens": 79,
            "total_tokens": 224,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 437,
          "total_completion_tokens": 157,
          "total_tokens": 594
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 146,
            "completion_tokens": 39,
            "total_tokens": 185,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 146,
            "completion_tokens": 39,
            "total_tokens": 185,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 8,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 145,
            "completion_tokens": 79,
            "total_tokens": 224,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/pppp1ppp/2n5/4p3/2B1n3/2N2N2/PPPP1PPP/R1BQK2R w KQkq - 0 5"
    },
    {
      "turn": 11,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bd3",
          "aggressive_uci": "c4d3",
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Nc3 Nxe4\n5. Nxe4 d5\n6. Bd3 Nc6\n7. Nc3 Be7\n8. 0-0 0-0\n9. h3 Re8\n10. d4 Bg4\n11. Be3 Bxf3\n12. Qxf3 Nd7\n13. Rfe1 Nf6\n14. Qg3 Qd6\n15. Rad1 Qe6\n16. Qf3 Qd6\n17. Qg3 Qe6\n18. Qf3 Qd6\n19. Qg3 Qe6\n20. Qf3 Qd6\n21. Qg3 Qe6\n22. Qf3 Qd6\n23. Qg3 Qe6\n24. Qf3 Qd6\n25. Qg3 Qe6\n26. Qf3 Qd6\n27. Qg3 Qe6\n28. Qf3 Qd6\n29. Qg3 Qe6\n30. Qf3 Qd6\n31. Qg3 Qe6\n32. Qf3 Qd6\n33. Qg3 Qe6\n34. Qf3 Qd6\n35. Qg3 Qe6\n36. Qf3 Qd6\n37. Qg3 Qe6\n38. Qf3 Qd6\n39. Qg3 Qe6\n40. Qf3 Qd6\n41. Qg3 Qe6\n42. Qf3 Qd6\n43. Qg3 Qe6\n44. Qf3 Qd6\n45. Qg3 Qe6\n46. Qf3 Qd6\n47. Qg3 Qe6\n48. Qf3 Qd6\n49. Qg3 Qe6\n50. Qf3 Qd6\n51. Qg3 Qe6\n52. Qf3 Qd6\n53. Qg3 Qe6\n54. Qf3 Qd6\n55. Qg3 Qe6\n56. Qf3 Qd6\n57. Qg3 Qe6\n58. Qf3 Qd6\n59. Qg3 Qe6\n60. Qf3 Qd6\n61. Qg3 Qe6\n62. Qf3 Qd6\n63. Qg3 Qe6\n64. Qf3 Qd6\n65. Qg3 Qe6\n66. Qf3 Qd6",
          "aggressive_tokens": {
            "prompt_tokens": 154,
            "completion_tokens": 640,
            "total_tokens": 794,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nc6",
            "Nc3",
            "Be7",
            "h3",
            "Re8",
            "d4",
            "Bg4",
            "Be3",
            "Bxf3",
            "Qxf3",
            "Nd7",
            "Rfe1",
            "Nf6",
            "Qg3",
            "Qd6",
            "Rad1",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6",
            "Qg3",
            "Qe6",
            "Qf3",
            "Qd6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bd3",
          "positional_uci": "c4d3",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 Be7 7. O-O O-O 8. c3\n\nNf6, Bd6, O-O",
          "positional_tokens": {
            "prompt_tokens": 154,
            "completion_tokens": 69,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Be7",
            "O-O",
            "O-O",
            "c3",
            "Nf6",
            "Bd6",
            "O-O"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bd3",
          "neutral_uci": "c4d3",
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 Be7 7. O-O O-O 8. c3 Nc6 9. d4\n\nNc6 d4 Nxd4",
          "neutral_tokens": {
            "prompt_tokens": 153,
            "completion_tokens": 76,
            "total_tokens": 229,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Be7",
            "O-O",
            "O-O",
            "c3",
            "Nc6",
            "d4",
            "Nc6",
            "d4",
            "Nxd4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "c4d3",
          "positional_uci": "c4d3",
          "neutral_uci": "c4d3",
          "consensus_move": "c4d3",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nc6",
              "Nc3",
              "Be7",
              "h3",
              "Re8",
              "d4",
              "Bg4",
              "Be3",
              "Bxf3",
              "Qxf3",
              "Nd7",
              "Rfe1",
              "Nf6",
              "Qg3",
              "Qd6",
              "Rad1",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6",
              "Qg3",
              "Qe6",
              "Qf3",
              "Qd6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Be7",
              "O-O",
              "O-O",
              "c3",
              "Nf6",
              "Bd6",
              "O-O"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Be7",
              "O-O",
              "O-O",
              "c3",
              "Nc6",
              "d4",
              "Nc6",
              "d4",
              "Nxd4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 154,
            "completion_tokens": 640,
            "total_tokens": 794,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 154,
            "completion_tokens": 69,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 153,
            "completion_tokens": 76,
            "total_tokens": 229,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 461,
          "total_completion_tokens": 785,
          "total_tokens": 1246
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 154,
            "completion_tokens": 640,
            "total_tokens": 794,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 154,
            "completion_tokens": 69,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 10,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 153,
            "completion_tokens": 76,
            "total_tokens": 229,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/3pp3/2B1N3/5N2/PPPP1PPP/R1BQK2R w KQkq - 0 6"
    },
    {
      "turn": 13,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bxe4",
          "aggressive_uci": "d3e4",
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Nc3 Nxe4\n5. Nxe4 d5\n6. Bd3 dxe4\n\nBxe4 Nc5",
          "aggressive_tokens": {
            "prompt_tokens": 162,
            "completion_tokens": 55,
            "total_tokens": 217,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nc5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "O-O",
          "positional_uci": "e1g1",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. O-O\n\n7. O-O\n\nBc5 8. c3 O-O 9. d4\n\nBb6 10. d5",
          "positional_tokens": {
            "prompt_tokens": 162,
            "completion_tokens": 84,
            "total_tokens": 246,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d4",
            "Bb6",
            "d5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. 0-0\n\n7. 0-0",
          "neutral_tokens": {
            "prompt_tokens": 161,
            "completion_tokens": 62,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bc4",
            "Nf6",
            "Nc3",
            "Nxe4",
            "Nxe4",
            "d5",
            "Bd3",
            "dxe4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d3e4",
          "positional_uci": "e1g1",
          "neutral_uci": null,
          "consensus_move": "d3e4",
          "source_agents": [
            "aggressive"
          ],
          "selected_agent": "aggressive"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nc5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "d4",
              "Bb6",
              "d5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bc4",
              "Nf6",
              "Nc3",
              "Nxe4",
              "Nxe4",
              "d5",
              "Bd3",
              "dxe4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 162,
            "completion_tokens": 55,
            "total_tokens": 217,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 162,
            "completion_tokens": 84,
            "total_tokens": 246,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 161,
            "completion_tokens": 62,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 485,
          "total_completion_tokens": 201,
          "total_tokens": 686
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 162,
            "completion_tokens": 55,
            "total_tokens": 217,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 162,
            "completion_tokens": 84,
            "total_tokens": 246,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 12,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 161,
            "completion_tokens": 62,
            "total_tokens": 223,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/4p3/4p3/3B1N2/PPPP1PPP/R1BQK2R w KQkq - 0 7"
    },
    {
      "turn": 15,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bc4",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. Bxe4 Bd6\n\nBc4",
          "aggressive_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 60,
            "total_tokens": 230,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bc5",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. Bxe4 Bd6\n\nBc5",
          "positional_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 60,
            "total_tokens": 230,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bc4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. Bxe4 Bd6\n\nBc4",
          "neutral_tokens": {
            "prompt_tokens": 169,
            "completion_tokens": 60,
            "total_tokens": 229,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 170,
            "completion_tokens": 60,
            "total_tokens": 230,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 170,
            "completion_tokens": 60,
            "total_tokens": 230,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 169,
            "completion_tokens": 60,
            "total_tokens": 229,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 509,
          "total_completion_tokens": 180,
          "total_tokens": 689
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 170,
            "completion_tokens": 60,
            "total_tokens": 230,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 170,
            "completion_tokens": 60,
            "total_tokens": 230,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 169,
            "completion_tokens": 60,
            "total_tokens": 229,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqk2r/ppp2ppp/2nb4/4p3/4B3/5N2/PPPP1PPP/R1BQK2R w KQkq - 1 8"
    },
    {
      "turn": 15,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bd6",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5\n2. Nf3 Nc6\n3. Bc4 Nf6\n4. Nc3 Nxe4\n5. Nxe4 d5\n6. Bd3 dxe4\n7. Bxe4 Bd6\n\nBd6 8. 0-0 0-0 9. d4",
          "aggressive_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 76,
            "total_tokens": 246,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bd3",
          "positional_uci": "e4d3",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. Bxe4 Bd6\n\nBd3",
          "positional_tokens": {
            "prompt_tokens": 170,
            "completion_tokens": 60,
            "total_tokens": 230,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bc4",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. Bxe4 Bd6\n\nBc4 Nxe4",
          "neutral_tokens": {
            "prompt_tokens": 169,
            "completion_tokens": 63,
            "total_tokens": 232,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxe4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": "e4d3",
          "neutral_uci": null,
          "consensus_move": "e4d3",
          "source_agents": [
            "positional"
          ],
          "selected_agent": "positional"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d4"
            ],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nxe4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 170,
            "completion_tokens": 76,
            "total_tokens": 246,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 170,
            "completion_tokens": 60,
            "total_tokens": 230,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 169,
            "completion_tokens": 63,
            "total_tokens": 232,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 509,
          "total_completion_tokens": 199,
          "total_tokens": 708
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 170,
            "completion_tokens": 76,
            "total_tokens": 246,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 170,
            "completion_tokens": 60,
            "total_tokens": 230,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 14,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 169,
            "completion_tokens": 63,
            "total_tokens": 232,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqk2r/ppp2ppp/2nb4/4p3/4B3/5N2/PPPP1PPP/R1BQK2R w KQkq - 1 8"
    },
    {
      "turn": 17,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf3",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. Bxe4 Bd6 8. Bd3 O-O\n\nNf3",
          "aggressive_tokens": {
            "prompt_tokens": 177,
            "completion_tokens": 67,
            "total_tokens": 244,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf3",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. Bxe4 Bd6 8. Bd3 O-O\n\nNf3",
          "positional_tokens": {
            "prompt_tokens": 177,
            "completion_tokens": 67,
            "total_tokens": 244,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf3",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. Bxe4 Bd6 8. Bd3 O-O\n\nNf3",
          "neutral_tokens": {
            "prompt_tokens": 176,
            "completion_tokens": 67,
            "total_tokens": 243,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 177,
            "completion_tokens": 67,
            "total_tokens": 244,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 177,
            "completion_tokens": 67,
            "total_tokens": 244,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 176,
            "completion_tokens": 67,
            "total_tokens": 243,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 530,
          "total_completion_tokens": 201,
          "total_tokens": 731
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 177,
            "completion_tokens": 67,
            "total_tokens": 244,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 177,
            "completion_tokens": 67,
            "total_tokens": 244,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 176,
            "completion_tokens": 67,
            "total_tokens": 243,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bq1rk1/ppp2ppp/2nb4/4p3/8/3B1N2/PPPP1PPP/R1BQK2R w KQ - 3 9"
    },
    {
      "turn": 17,
      "attempt": 2,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf3",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. Bxe4 Bd6 8. Bd3 O-O\n\nNf3",
          "aggressive_tokens": {
            "prompt_tokens": 177,
            "completion_tokens": 67,
            "total_tokens": 244,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf3",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. Bxe4 Bd6 8. Bd3 O-O\n\nNf3",
          "positional_tokens": {
            "prompt_tokens": 177,
            "completion_tokens": 67,
            "total_tokens": 244,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf3",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. Bxe4 Bd6 8. Bd3 O-O\n\nNf3",
          "neutral_tokens": {
            "prompt_tokens": 176,
            "completion_tokens": 67,
            "total_tokens": 243,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 177,
            "completion_tokens": 67,
            "total_tokens": 244,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 177,
            "completion_tokens": 67,
            "total_tokens": 244,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 176,
            "completion_tokens": 67,
            "total_tokens": 243,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 530,
          "total_completion_tokens": 201,
          "total_tokens": 731
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 177,
            "completion_tokens": 67,
            "total_tokens": 244,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 177,
            "completion_tokens": 67,
            "total_tokens": 244,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 176,
            "completion_tokens": 67,
            "total_tokens": 243,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bq1rk1/ppp2ppp/2nb4/4p3/8/3B1N2/PPPP1PPP/R1BQK2R w KQ - 3 9"
    },
    {
      "turn": 17,
      "attempt": 3,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf3",
          "aggressive_uci": null,
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. Bxe4 Bd6 8. Bd3 O-O\n\nNf3",
          "aggressive_tokens": {
            "prompt_tokens": 177,
            "completion_tokens": 67,
            "total_tokens": 244,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf3",
          "positional_uci": null,
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. Bxe4 Bd6 8. Bd3 O-O\n\nNf3",
          "positional_tokens": {
            "prompt_tokens": 177,
            "completion_tokens": 67,
            "total_tokens": 244,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf3",
          "neutral_uci": null,
          "neutral_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. Bxe4 Bd6 8. Bd3 O-O\n\nNf3",
          "neutral_tokens": {
            "prompt_tokens": 176,
            "completion_tokens": 67,
            "total_tokens": 243,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "plan_full": [],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": null,
          "positional_uci": null,
          "neutral_uci": null,
          "consensus_move": null,
          "source_agents": [],
          "selected_agent": null
        },
        "plans": {
          "aggressive": {
            "full": [],
            "execute": []
          },
          "positional": {
            "full": [],
            "execute": []
          },
          "neutral": {
            "full": [],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 177,
            "completion_tokens": 67,
            "total_tokens": 244,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 177,
            "completion_tokens": 67,
            "total_tokens": 244,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 176,
            "completion_tokens": 67,
            "total_tokens": 243,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 530,
          "total_completion_tokens": 201,
          "total_tokens": 731
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 177,
            "completion_tokens": 67,
            "total_tokens": 244,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 177,
            "completion_tokens": 67,
            "total_tokens": 244,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 16,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 176,
            "completion_tokens": 67,
            "total_tokens": 243,
            "model": "mistralai/mistral-small-24b-instruct-2501",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bq1rk1/ppp2ppp/2nb4/4p3/8/3B1N2/PPPP1PPP/R1BQK2R w KQ - 3 9"
    }
  ],
  "black_responses": [
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": "d2d4",
          "aggressive_response": "d4 d5 2. Nf3 Nf6 3. Bg5 Bf5 4. e3 e6 5. Bd3 Bxd3 6. Qxd3 Be7 7. O-O O-O 8. Nbd2 Nbd7 9. c4 c6 10. Rac1 Rc8 11. cxd5 exd5 12. a3 Re8 13. b4 a6 14. Nb3 Ne4 15. Bxe7 Qxe7 16. Nc5 Ndxc5 17. bxc5 f5 18. Rb1 Rc7 19. Rb6 Rf8 20. Rfb1 Rf7 21. Ne5 Rf6 22. f3 Ng5 23. h4 Nf7 24. f4 Nxe5 25. fxe5 Rf7 26. g3 g5 27. hxg5 Qxg5 28. Kf2 Rg7 29. Rg1 h5 30. Rbb1 Rcf7 31. Qe2 Qg4 32. Qxg4 fxg4+ 33. Ke2 Rf3 34. Rbf1 Rgf7 35. Rxf3 Rxf3 36. Rg2 Kf7 37. Rf2 Rxf2+ 38. Kxf2 Ke6 39. Ke2 Kf5 40. Kd3 Ke6 41. e4 dxe4+ 42. Kxe4 a5 43. a4 Ke7 44. Kf5 Kf7 45. Kg5 Ke6 46. Kxh5 Kd5 47. Kxg4 Kxd4 48. e6 Kxc5 49. e7 Kb4 50. e8=Q Kxa4 51. Qe4+ Kb5 52. Kf3 a4 53. g4 a3 54. g5 a2 55. Qe5+ Ka4 56. g6 b5 57. g7 b4 58. g8=Q b3 59. Qa8+ Kb4 60. Qaa5+ Kc4 61. Qec3# 1-0 d4 d5 Nf3 Nf6 Bg5 Bf5 e3 e6 Bd3 Bxd3 Qxd3 Be7 O-O O-O Nbd2 c6 Rac1 Rc8 cxd5 exd5 a3 Re8 b4 a6 Nb3 Ne4 Bxe7 Qxe7 Nc5 Ndxc5 bxc5 f5 Rb1 Rc7 Rb6 Rf8 Ne5 Rf6 f3 Ng5 h4 Nf7 f4 Nxe5 fxe5 Rf7 g3 g5 hxg5 Qxg5 Kf2 Rg7 Rg",
          "aggressive_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Nf3",
            "Nf6",
            "Bg5",
            "Bf5",
            "e3",
            "e6",
            "Bd3",
            "Bxd3",
            "Qxd3",
            "Be7",
            "O-O",
            "O-O",
            "Nbd2",
            "c6",
            "Rac1",
            "Rc8",
            "cxd5",
            "exd5",
            "a3",
            "Re8",
            "b4",
            "a6",
            "Nb3",
            "Ne4",
            "Bxe7",
            "Qxe7",
            "Nc5",
            "Ndxc5",
            "bxc5",
            "f5",
            "Rb1",
            "Rc7",
            "Rb6",
            "Rf8",
            "Ne5",
            "Rf6",
            "f3",
            "Ng5",
            "h4",
            "Nf7",
            "f4",
            "Nxe5",
            "fxe5",
            "Rf7",
            "g3",
            "g5",
            "hxg5",
            "Qxg5",
            "Kf2",
            "Rg7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "Game 1\n1. e4 e5\n2. Nf3 Nc6\n3. Bb5 a6\n4. Bxc6 dxc6\n5. O-O Bg4\n6. h3 Bh5\n7. d3 Bc5\n8. Be3 Bxe3\n9. fxe3 Nf6\n10. Nbd2 O-O\n11. Qe1 Re8\n12. Qg3 Qd6\n13. Nc4 Qc5\n14. Nfxe5 b5\n15. d4 Qe7\n16. Nxc6 Qxe4\n17. Rxf6 bxc4\n18. Ne5 Qxc2\n19. Raf1 Re7\n20. R6f2 Qe4\n21. Rf4 Qd5\n22. Qg5 f6\n23. Qxh5 fxe5\n24. Rf5 g6\n25. Qg5 Rae8\n26. Rf6 Re6\n27. Rf7 exd4\n28. Qh6 Qxg2+\n29. Kxg2 Rxe3\n30. Qg7# e4 Nf6 Rxf6#\n\n*Game 1\n1. e4 e5\n2. Nf3 Nc6\n3. Bb5 a6\n4. Bxc6 dxc6\n5. O-O Bg4\n6. h3 Bh5\n7. d3 Bc5\n8. Be3 Bxe3\n9. fxe3 Nf6\n10. Nbd2 O-O\n11. Qe1 Re8\n12. Qg3 Qd6\n13. Nc4 Qc5\n14. Nfxe5 b5\n15. d4 Qe7\n16. Nxc6 Qxe4\n17. Rxf6 bxc4\n18. Ne5 Qxc2\n19. Raf1 Re7\n20. R6f2 Qe4\n21. Rf4 Qd5\n22. Qg5 f6\n23. Qxh5 fxe5\n24. Rf5 g6\n25. Qg5 Rae8\n26. Rf6 Re6\n27. Rf7 exd4\n28. Qh6 Qxg2+\n29. Kxg2 Rxe3\n30. Qg7# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 Rxf6# e4 Nf6 R",
          "positional_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": "d2d4",
          "neutral_response": "d4 d5\nd4 d5 Nf3 Nf6 Bf4 Bf5 e3 e6 c4 c6 Nc3 Nbd7 Be2 Be7 O-O O-O Rc1 Rc8 Qb3 Qb6 c5 Qxb3 axb3 a6 b4 Nh5 Bg3 Nxg3 hxg3 Bf6 Nd2 e5 dxe5 Nxe5 Nb3 Nd3 Bxd3 Bxd3 Rfd1 Rfe8 Nd4 Bg6 Na4 Re7 Nb6 Rce8 b3 h6 Kf1 Bh5 Rd2 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke2 Bh5+ Kf1 Bg6 Ke",
          "neutral_tokens": {
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Nf3",
            "Nf6",
            "Bf4",
            "Bf5",
            "e3",
            "e6",
            "c4",
            "c6",
            "Nc3",
            "Nbd7",
            "Be2",
            "Be7",
            "O-O",
            "O-O",
            "Rc1",
            "Rc8",
            "Qb3",
            "Qb6",
            "c5",
            "Qxb3",
            "axb3",
            "a6",
            "b4",
            "Nh5",
            "Bg3",
            "Nxg3",
            "hxg3",
            "Bf6",
            "Nd2",
            "e5",
            "dxe5",
            "Nxe5",
            "Nb3",
            "Nd3",
            "Bxd3",
            "Bxd3",
            "Rfd1",
            "Rfe8",
            "Nd4",
            "Bg6",
            "Na4",
            "Re7",
            "Nb6",
            "Rce8",
            "b3",
            "h6",
            "Kf1",
            "Bh5",
            "Rd2",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6",
            "Ke2",
            "Bh5+",
            "Kf1",
            "Bg6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d2d4",
          "positional_uci": "e2e4",
          "neutral_uci": "d2d4",
          "consensus_move": "d2d4",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d5",
              "Nf3",
              "Nf6",
              "Bg5",
              "Bf5",
              "e3",
              "e6",
              "Bd3",
              "Bxd3",
              "Qxd3",
              "Be7",
              "O-O",
              "O-O",
              "Nbd2",
              "c6",
              "Rac1",
              "Rc8",
              "cxd5",
              "exd5",
              "a3",
              "Re8",
              "b4",
              "a6",
              "Nb3",
              "Ne4",
              "Bxe7",
              "Qxe7",
              "Nc5",
              "Ndxc5",
              "bxc5",
              "f5",
              "Rb1",
              "Rc7",
              "Rb6",
              "Rf8",
              "Ne5",
              "Rf6",
              "f3",
              "Ng5",
              "h4",
              "Nf7",
              "f4",
              "Nxe5",
              "fxe5",
              "Rf7",
              "g3",
              "g5",
              "hxg5",
              "Qxg5",
              "Kf2",
              "Rg7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "d5",
              "Nf3",
              "Nf6",
              "Bf4",
              "Bf5",
              "e3",
              "e6",
              "c4",
              "c6",
              "Nc3",
              "Nbd7",
              "Be2",
              "Be7",
              "O-O",
              "O-O",
              "Rc1",
              "Rc8",
              "Qb3",
              "Qb6",
              "c5",
              "Qxb3",
              "axb3",
              "a6",
              "b4",
              "Nh5",
              "Bg3",
              "Nxg3",
              "hxg3",
              "Bf6",
              "Nd2",
              "e5",
              "dxe5",
              "Nxe5",
              "Nb3",
              "Nd3",
              "Bxd3",
              "Bxd3",
              "Rfd1",
              "Rfe8",
              "Nd4",
              "Bg6",
              "Na4",
              "Re7",
              "Nb6",
              "Rce8",
              "b3",
              "h6",
              "Kf1",
              "Bh5",
              "Rd2",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6",
              "Ke2",
              "Bh5+",
              "Kf1",
              "Bg6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 329,
          "total_completion_tokens": 1920,
          "total_tokens": 2249
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e5",
          "aggressive_uci": "e7e5",
          "aggressive_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7\n6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5\n11. d4 Qc7 12. Nbd2 cxd4 13. cxd4 Nc6 14. Nb3 a5\n15. Be3 a4 16. Nbd2 Bd7 17. Rc1 Qb7 18. Bb1 Rfc8\n19. Nf1 Bd8 20. Ng3 Bb6 21. Qd2 Ba5 22. Qe2 Bxe1\n23. Rxe1 Re8 24. Qd2 Rad8 25. Bg5 Ne7 26. Bxf6 gxf6\n27. Qh6 Ng6 28. Nh5 Re6 29. Qg7# d5 e4 Nxe4\n30. Bxe4 dxe4 31. Rxe4 Qxe4 32. Ng5 Qe1+ 33. Kh2 fxg5\n34. Nf6+ Rxf6 35. Qxf6 Re8 36. Qxg5 Qxf2 37. h4 Qf4+\n38. Qxf4 Nxf4 39. Kg3 Nd3 40. Kf3 Nxb2 41. g4 Nc4\n42. Kf4 b4 43. g5 b3 44. axb3 axb3 45. h5 b2\n46. g6 b1=Q 47. gxh7+ Kxh7 48. h6 Qf5+ 49. Kg3 Rg8+\n50. Kh4 Qg4# e5 Qg5# e6 Qg6#",
          "aggressive_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 429,
            "total_tokens": 543,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qg5#",
            "e6",
            "Qg6#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e5",
          "positional_uci": "e7e5",
          "positional_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7\n6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5\n11. d4 Qc7 12. Nbd2 cxd4 13. cxd4 Nc6 14. Nb3 a5\n15. Be3 a4 16. Nbd2 Bd7 17. Rc1 Qb7 18. Bb1 Rfc8\n19. Nf1 Bd8 20. Ng3 Bb6 21. Qd2 Ba5 22. Qe2 Bxe1\n23. Rxe1 Re8 24. Qd2 Rac8 25. Bg5 Kh8 26. Bxf6 gxf6\n27. Qh6 Rg8 28. Qxf6+ Rg7 29. Nh5 Rg8 30. Ng5 Be8\n31. Nxg7 Rxg7 32. dxe5 dxe5 33. Rd1 Qe7 34. Qxe7 Nxe7\n35. Rd8 Rg8 36. Nf3 f6 37. Bd3 Kg7 38. Rb8 Bf7\n39. Rxb5 Bxa2 40. Ra5 Bb3 41. Nd2 Nc6 42. Ra6 Nb4\n43. Ra7+ Kg6 44. Bc4 Bxc4 45. Nxc4 Nd3 46. Rxa4 Rc8\n47. b3 Nc5 48. Rb4 Nxe4 49. Nxe5+ Kf5 50. Nc4 Rd8\n51. Ne3+ Ke5 52. Rb5+ Ke6 53. Rb6+ Ke5 54. Ng4+ Kf5\n55. f3 Rd1+ 56. Kh2 Nd6 57. Ne3+ Ke5 58. Nxd1 1-0\n58... Kd5 59. Rxd6+ Kxd6 60. Kg3 Kc5",
          "positional_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 498,
            "total_tokens": 612,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Qc7",
            "Nbd2",
            "cxd4",
            "cxd4",
            "Nc6",
            "Nb3",
            "a5",
            "Be3",
            "a4",
            "Nbd2",
            "Bd7",
            "Rc1",
            "Qb7",
            "Bb1",
            "Rfc8",
            "Nf1",
            "Bd8",
            "Ng3",
            "Bb6",
            "Qd2",
            "Ba5",
            "Qe2",
            "Bxe1",
            "Rxe1",
            "Re8",
            "Qd2",
            "Rac8",
            "Bg5",
            "Kh8",
            "Bxf6",
            "gxf6",
            "Qh6",
            "Rg8",
            "Qxf6+",
            "Rg7",
            "Nh5",
            "Rg8",
            "Ng5",
            "Be8",
            "Nxg7",
            "Rxg7",
            "dxe5",
            "dxe5",
            "Rd1",
            "Qe7",
            "Qxe7",
            "Nxe7",
            "Rd8",
            "Rg8",
            "Nf3",
            "f6",
            "Bd3",
            "Kg7",
            "Rb8",
            "Bf7",
            "Rxb5",
            "Bxa2",
            "Ra5",
            "Bb3",
            "Nd2",
            "Nc6",
            "Ra6",
            "Nb4",
            "Ra7+",
            "Kg6",
            "Bc4",
            "Bxc4",
            "Nxc4",
            "Nd3",
            "Rxa4",
            "Rc8",
            "b3",
            "Nc5",
            "Rb4",
            "Nxe4",
            "Nxe5+",
            "Kf5",
            "Nc4",
            "Rd8",
            "Ne3+",
            "Ke5",
            "Rb5+",
            "Ke6",
            "Rb6+",
            "Ke5",
            "Ng4+",
            "Kf5",
            "f3",
            "Rd1+",
            "Kh2",
            "Nd6",
            "Ne3+",
            "Ke5",
            "Nxd1",
            "Kd5",
            "Rxd6+",
            "Kxd6",
            "Kg3",
            "Kc5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e5",
          "neutral_uci": "e7e5",
          "neutral_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6\n4. Ba4 Nf6 5. O-O Be7 6. Re1 b5\n7. Bb3 d6 8. c3 O-O 9. h3 Bb7\n10. d4 Re8 11. Nbd2 Bf8 12. a4 h6\n13. Bc2 exd4 14. cxd4 Nb4 15. Bb1 c5\n16. d5 Nd7 17. Ra3 f5 18. Rae3 fxe4\n19. Nxe4 Nxd5 20. Ba2 c4 21. Nxd6 Rxe3\n22. Nxb7 Rxe1+ 23. Nxe1 Qe7 24. Qxd5+ Kh8\n25. Nf3 Nf6 26. Qc6 Ra7 27. Na5 Qd8\n28. Bd2 Rd7 29. Bc3 Rd1+ 30. Kh2 Bd6+\n31. g3 Bc7 32. Nb7 Qd3 33. Bxf6 Qf1\n34. Qe8+ Kh7 35. Qe4+ Kh8 36. Qe8+ Kh7\n37. Qe4+ Kh8 38. Qe8+ Kh7 39. Qe4+ Kh8 40. Qe8+ Kh7 41. Qe4+ Kh8 42. Qe8+ Kh7 43. Qe4+ Kh8 44. Qe8+ Kh7 45. Qe4+ Kh8 46. Qe8+ Kh7 47. Qe4+ Kh8 48. Qe8+ Kh7 49. Qe4+ Kh8 50. Qe8+ Kh7 51. Qe4+ Kh8 52. Qe8+ Kh7 53. Qe4+ Kh8 54. Qe8+ Kh7 55. Qe4+ Kh8 56. Qe8+ Kh7 57. Qe4+ Kh8 58. Qe8+ Kh7 59. Qe4+ Kh8 60. Qe8+ Kh7 61. Qe4+ Kh8 62. Qe8+ Kh7 63. Qe4+ Kh8 64. Qe8+ Kh7 65. Qe4+ Kh8 66. Qe8+ Kh7 67. Qe4+ Kh8 68. Qe8+ Kh7 69. Qe4+ Kh8 70. Qe8+ Kh7 71. Qe4+ Kh8 72. Qe8+ Kh7 73. Qe4+ Kh8 74. Qe8+ Kh7 75. Qe4+ Kh",
          "neutral_tokens": {
            "prompt_tokens": 113,
            "completion_tokens": 640,
            "total_tokens": 753,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Bb7",
            "d4",
            "Re8",
            "Nbd2",
            "Bf8",
            "a4",
            "h6",
            "Bc2",
            "exd4",
            "cxd4",
            "Nb4",
            "Bb1",
            "c5",
            "d5",
            "Nd7",
            "Ra3",
            "f5",
            "Rae3",
            "fxe4",
            "Nxe4",
            "Nxd5",
            "Ba2",
            "c4",
            "Nxd6",
            "Rxe3",
            "Nxb7",
            "Rxe1+",
            "Nxe1",
            "Qe7",
            "Qxd5+",
            "Kh8",
            "Nf3",
            "Nf6",
            "Qc6",
            "Ra7",
            "Na5",
            "Qd8",
            "Bd2",
            "Rd7",
            "Bc3",
            "Rd1+",
            "Kh2",
            "Bd6+",
            "g3",
            "Bc7",
            "Nb7",
            "Qd3",
            "Bxf6",
            "Qf1",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Kh8",
            "Qe8+",
            "Kh7",
            "Qe4+"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e7e5",
          "positional_uci": "e7e5",
          "neutral_uci": "e7e5",
          "consensus_move": "e7e5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qg5#",
              "e6",
              "Qg6#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Qc7",
              "Nbd2",
              "cxd4",
              "cxd4",
              "Nc6",
              "Nb3",
              "a5",
              "Be3",
              "a4",
              "Nbd2",
              "Bd7",
              "Rc1",
              "Qb7",
              "Bb1",
              "Rfc8",
              "Nf1",
              "Bd8",
              "Ng3",
              "Bb6",
              "Qd2",
              "Ba5",
              "Qe2",
              "Bxe1",
              "Rxe1",
              "Re8",
              "Qd2",
              "Rac8",
              "Bg5",
              "Kh8",
              "Bxf6",
              "gxf6",
              "Qh6",
              "Rg8",
              "Qxf6+",
              "Rg7",
              "Nh5",
              "Rg8",
              "Ng5",
              "Be8",
              "Nxg7",
              "Rxg7",
              "dxe5",
              "dxe5",
              "Rd1",
              "Qe7",
              "Qxe7",
              "Nxe7",
              "Rd8",
              "Rg8",
              "Nf3",
              "f6",
              "Bd3",
              "Kg7",
              "Rb8",
              "Bf7",
              "Rxb5",
              "Bxa2",
              "Ra5",
              "Bb3",
              "Nd2",
              "Nc6",
              "Ra6",
              "Nb4",
              "Ra7+",
              "Kg6",
              "Bc4",
              "Bxc4",
              "Nxc4",
              "Nd3",
              "Rxa4",
              "Rc8",
              "b3",
              "Nc5",
              "Rb4",
              "Nxe4",
              "Nxe5+",
              "Kf5",
              "Nc4",
              "Rd8",
              "Ne3+",
              "Ke5",
              "Rb5+",
              "Ke6",
              "Rb6+",
              "Ke5",
              "Ng4+",
              "Kf5",
              "f3",
              "Rd1+",
              "Kh2",
              "Nd6",
              "Ne3+",
              "Ke5",
              "Nxd1",
              "Kd5",
              "Rxd6+",
              "Kxd6",
              "Kg3",
              "Kc5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Bb7",
              "d4",
              "Re8",
              "Nbd2",
              "Bf8",
              "a4",
              "h6",
              "Bc2",
              "exd4",
              "cxd4",
              "Nb4",
              "Bb1",
              "c5",
              "d5",
              "Nd7",
              "Ra3",
              "f5",
              "Rae3",
              "fxe4",
              "Nxe4",
              "Nxd5",
              "Ba2",
              "c4",
              "Nxd6",
              "Rxe3",
              "Nxb7",
              "Rxe1+",
              "Nxe1",
              "Qe7",
              "Qxd5+",
              "Kh8",
              "Nf3",
              "Nf6",
              "Qc6",
              "Ra7",
              "Na5",
              "Qd8",
              "Bd2",
              "Rd7",
              "Bc3",
              "Rd1+",
              "Kh2",
              "Bd6+",
              "g3",
              "Bc7",
              "Nb7",
              "Qd3",
              "Bxf6",
              "Qf1",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Kh8",
              "Qe8+",
              "Kh7",
              "Qe4+"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 114,
            "completion_tokens": 429,
            "total_tokens": 543,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 114,
            "completion_tokens": 498,
            "total_tokens": 612,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 113,
            "completion_tokens": 640,
            "total_tokens": 753,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 341,
          "total_completion_tokens": 1567,
          "total_tokens": 1908
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 114,
            "completion_tokens": 429,
            "total_tokens": 543,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 114,
            "completion_tokens": 498,
            "total_tokens": 612,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 113,
            "completion_tokens": 640,
            "total_tokens": 753,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 4,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc6",
          "aggressive_uci": "b8c6",
          "aggressive_response": "2... Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 O-O 8. c3 d5\n9. exd5 Nxd5 10. Nxe5 Nxe5 11. Rxe5 c6 12. d4 Bd6 13. Re1 Qh4 14. g3 Qh3\n15. Be3 Bg4 16. Qd3 Rae8 17. Nd2 Re6 18. Qf1 Qh5 19. a4 Rfe8 20. axb5 axb5\n21. Bxd5 Qxd5 22. Qg2 Qh5 23. Qxc6 Bxg3 24. Qg2 Bf4 25. Ra8 Bb8 26. Kh1\n26... Rg6 27. Bf4 Rxe1+ 28. Nf1 Rxf1+ 29. Qxf1 Bf3+ 30. Qg2 Bxg2+ 31. Kg1\n31... Qd1#",
          "aggressive_tokens": {
            "prompt_tokens": 122,
            "completion_tokens": 258,
            "total_tokens": 380,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "O-O",
            "c3",
            "d5",
            "exd5",
            "Nxd5",
            "Nxe5",
            "Nxe5",
            "Rxe5",
            "c6",
            "d4",
            "Bd6",
            "Re1",
            "Qh4",
            "g3",
            "Qh3",
            "Be3",
            "Bg4",
            "Qd3",
            "Rae8",
            "Nd2",
            "Re6",
            "Qf1",
            "Qh5",
            "a4",
            "Rfe8",
            "axb5",
            "axb5",
            "Bxd5",
            "Qxd5",
            "Qg2",
            "Qh5",
            "Qxc6",
            "Bxg3",
            "Qg2",
            "Bf4",
            "Ra8",
            "Bb8",
            "Kh1",
            "Rg6",
            "Bf4",
            "Rxe1+",
            "Nf1",
            "Rxf1+",
            "Qxf1",
            "Bf3+",
            "Qg2",
            "Bxg2+",
            "Kg1",
            "Qd1#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nc6",
          "positional_uci": "b8c6",
          "positional_response": "2... Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 O-O 8. c3 d5\n9. exd5 Nxd5 10. Nxe5 Nxe5 11. Rxe5 c6 12. d4 Bd6 13. Re1 Qh4 14. g3 Qh3\n15. Be3 Bg4 16. Qd3 Rae8 17. Nd2 Re6 18. a4 Qh5 19. axb5 axb5 20. Ra6 Bb8\n21. Qf1 Rfe8 22. Bxd5 Qxd5 23. Qg2 Qh5 24. Rxc6 Bh3 25. Qf3 Bg4 26. Qg2 Bh3\n27. Qf3 Bg4 28. Qg2 Bh3 29. Qf3 Bg4 30. Qg2 Bh3 31. Qf3 Bg4 32. Qg2 Bh3\n33. Qf3 Bg4 34. Qg2 Bh3 35. Qf3 Bg4 36. Qg2 Bh3 37. Qf3 Bg4 38. Qg2 Bh3\n39. Qf3 Bg4 40. Qg2 Bh3 41. Qf3 Bg4 42. Qg2 Bh3 43. Qf3 Bg4 44. Qg2 Bh3\n45. Qf3 Bg4 46. Qg2 Bh3 47. Qf3 Bg4 48. Qg2 Bh3 49. Qf3 Bg4 50. Qg2 Bh3\n51. Qf3 Bg4 52. Qg2 Bh3 53. Qf3 Bg4 54. Qg2 Bh3 55. Qf3 Bg4 56. Qg2 Bh3\n57. Qf3 Bg4 58. Qg2 Bh3 59. Qf3 Bg4 60. Qg2 Bh3 61. Qf3 Bg4 62. Qg2 Bh3\n63. Qf3 Bg4 64. Qg2 Bh3 65. Qf3 Bg4 66. Qg2 Bh3 67. Qf3 Bg4 68. Qg2 Bh3\n69. Qf3 Bg4 70. Qg2 Bh3 71. Qf3 Bg4 72. Qg2 Bh3 73. Qf3 Bg4 74. Qg2 Bh3\n75. Qf3 Bg4 76. Qg2 Bh3 77. Qf3 Bg4 78. Qg2",
          "positional_tokens": {
            "prompt_tokens": 122,
            "completion_tokens": 640,
            "total_tokens": 762,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "O-O",
            "c3",
            "d5",
            "exd5",
            "Nxd5",
            "Nxe5",
            "Nxe5",
            "Rxe5",
            "c6",
            "d4",
            "Bd6",
            "Re1",
            "Qh4",
            "g3",
            "Qh3",
            "Be3",
            "Bg4",
            "Qd3",
            "Rae8",
            "Nd2",
            "Re6",
            "a4",
            "Qh5",
            "axb5",
            "axb5",
            "Ra6",
            "Bb8",
            "Qf1",
            "Rfe8",
            "Bxd5",
            "Qxd5",
            "Qg2",
            "Qh5",
            "Rxc6",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc6",
          "neutral_uci": "b8c6",
          "neutral_response": "2... Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 O-O 8. c3 d5\n9. exd5 Nxd5 10. Nxe5 Nxe5 11. Rxe5 c6 12. d4 Bd6 13. Re1 Qh4 14. g3 Qh3\n15. Be3 Bg4 16. Qd3 Rae8 17. Nd2 Re6 18. Qf1 Qh5 19. a4 Rfe8 20. axb5 axb5\n21. Bxd5 Qxd5 22. Qg2 Qh5 23. Qxc6 Bxg3 24. Qg2 Bf4 25. Ra8 Bb8 26. Kh1\n26... Bh3 27. Qf3 Bg4 28. Qg2 Bh3 29. Qf3 Bg4 30. Qg2 Bh3 31. Qf3 Bg4\n32. Qg2 Bh3 33. Qf3 Bg4 34. Qg2 Bh3 35. Qf3 Bg4 36. Qg2 Bh3 37. Qf3 Bg4\n38. Qg2 Bh3 39. Qf3 Bg4 40. Qg2 Bh3 41. Qf3 Bg4 42. Qg2 Bh3 43. Qf3 Bg4\n44. Qg2 Bh3 45. Qf3 Bg4 46. Qg2 Bh3 47. Qf3 Bg4 48. Qg2 Bh3 49. Qf3 Bg4\n50. Qg2 Bh3 51. Qf3 Bg4 52. Qg2 Bh3 53. Qf3 Bg4 54. Qg2 Bh3 55. Qf3 Bg4\n56. Qg2 Bh3 57. Qf3 Bg4 58. Qg2 Bh3 59. Qf3 Bg4 60. Qg2 Bh3 61. Qf3 Bg4\n62. Qg2 Bh3 63. Qf3 Bg4 64. Qg2 Bh3 65. Qf3 Bg4 66. Qg2 Bh3 67. Qf3 Bg4\n68. Qg2 Bh3 69. Qf3 Bg4 70. Qg2 Bh3 71. Qf3 Bg4 72. Qg2 Bh3 73. Qf3 Bg4\n74. Qg2 Bh3 75. Qf3 Bg4 76. Qg2 Bh3 77. Qf3 Bg4 78",
          "neutral_tokens": {
            "prompt_tokens": 121,
            "completion_tokens": 640,
            "total_tokens": 761,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "O-O",
            "c3",
            "d5",
            "exd5",
            "Nxd5",
            "Nxe5",
            "Nxe5",
            "Rxe5",
            "c6",
            "d4",
            "Bd6",
            "Re1",
            "Qh4",
            "g3",
            "Qh3",
            "Be3",
            "Bg4",
            "Qd3",
            "Rae8",
            "Nd2",
            "Re6",
            "Qf1",
            "Qh5",
            "a4",
            "Rfe8",
            "axb5",
            "axb5",
            "Bxd5",
            "Qxd5",
            "Qg2",
            "Qh5",
            "Qxc6",
            "Bxg3",
            "Qg2",
            "Bf4",
            "Ra8",
            "Bb8",
            "Kh1",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b8c6",
          "positional_uci": "b8c6",
          "neutral_uci": "b8c6",
          "consensus_move": "b8c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "O-O",
              "c3",
              "d5",
              "exd5",
              "Nxd5",
              "Nxe5",
              "Nxe5",
              "Rxe5",
              "c6",
              "d4",
              "Bd6",
              "Re1",
              "Qh4",
              "g3",
              "Qh3",
              "Be3",
              "Bg4",
              "Qd3",
              "Rae8",
              "Nd2",
              "Re6",
              "Qf1",
              "Qh5",
              "a4",
              "Rfe8",
              "axb5",
              "axb5",
              "Bxd5",
              "Qxd5",
              "Qg2",
              "Qh5",
              "Qxc6",
              "Bxg3",
              "Qg2",
              "Bf4",
              "Ra8",
              "Bb8",
              "Kh1",
              "Rg6",
              "Bf4",
              "Rxe1+",
              "Nf1",
              "Rxf1+",
              "Qxf1",
              "Bf3+",
              "Qg2",
              "Bxg2+",
              "Kg1",
              "Qd1#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "O-O",
              "c3",
              "d5",
              "exd5",
              "Nxd5",
              "Nxe5",
              "Nxe5",
              "Rxe5",
              "c6",
              "d4",
              "Bd6",
              "Re1",
              "Qh4",
              "g3",
              "Qh3",
              "Be3",
              "Bg4",
              "Qd3",
              "Rae8",
              "Nd2",
              "Re6",
              "a4",
              "Qh5",
              "axb5",
              "axb5",
              "Ra6",
              "Bb8",
              "Qf1",
              "Rfe8",
              "Bxd5",
              "Qxd5",
              "Qg2",
              "Qh5",
              "Rxc6",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "O-O",
              "c3",
              "d5",
              "exd5",
              "Nxd5",
              "Nxe5",
              "Nxe5",
              "Rxe5",
              "c6",
              "d4",
              "Bd6",
              "Re1",
              "Qh4",
              "g3",
              "Qh3",
              "Be3",
              "Bg4",
              "Qd3",
              "Rae8",
              "Nd2",
              "Re6",
              "Qf1",
              "Qh5",
              "a4",
              "Rfe8",
              "axb5",
              "axb5",
              "Bxd5",
              "Qxd5",
              "Qg2",
              "Qh5",
              "Qxc6",
              "Bxg3",
              "Qg2",
              "Bf4",
              "Ra8",
              "Bb8",
              "Kh1",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 122,
            "completion_tokens": 258,
            "total_tokens": 380,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 122,
            "completion_tokens": 640,
            "total_tokens": 762,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 121,
            "completion_tokens": 640,
            "total_tokens": 761,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 365,
          "total_completion_tokens": 1538,
          "total_tokens": 1903
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 122,
            "completion_tokens": 258,
            "total_tokens": 380,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 122,
            "completion_tokens": 640,
            "total_tokens": 762,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 121,
            "completion_tokens": 640,
            "total_tokens": 761,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R b KQkq - 1 2"
    },
    {
      "turn": 6,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf6",
          "aggressive_uci": "g8f6",
          "aggressive_response": "3... Nf6 4. d3 Bc5 5. O-O O-O 6. Nc3 d6 7. Bg5 Be6 8. Nd5 Bxd5 9. Bxd5 h6 10. Bh4 g5 11. Bg3 Nxd5 12. exd5 Ne7 13. c4 f5 14. h3 f4 15. Bh2 Nf5 16. a3 a5 17. Qd2 Qf6 18. b4 axb4 19. axb4 Rxa1 20. Rxa1 e4 21. dxe4 Qxa1+ 22. Ne1 Ng3 23. bxc5 Nxe4 24. Qe2 Re8 25. cxd6 Nxd6 26. Qf1 Rxe1 27. Qxe1 Qxe1# 1-0\n3... Nf6 4. d3 Bc5 5. O-O O-O 6. Nc3 d6 7. Bg5 Be6 8. Nd5 Bxd5 9. Bxd5 h6 10. Bh4 g5 11. Bg3 Nxd5 12. exd5 Ne7 13. c4 f5 14. h3 f4 15. Bh2 Nf5 16. a3 a5 17. Qd2 Qf6 18. b4 axb4 19. axb4 Rxa1 20. Rxa1 e4 21. dxe4 Qxa1+ 22. Ne1 Ng3 23. bxc5 Nxe4 24. Qe2 Re8 25. cxd6 Nxd6 26. Qf1 Rxe1 27. Qxe1 Qxe1# 1-0 Nf6 d3 Bc5 O-O O-O Nc3 d6 Bg5 Be6 Nd5 Bxd5 Bh4 g5 Bg3 Nxd5 exd5 Ne7 c4 f5 h3 f4 Bh2 Nf5 a3 a5 Qd2 Qf6 b4 axb4 axb4 Rxa1 Rxa1 e4 dxe4 Qxe4 Re8 Qe2 c6 bxc5 dxc5 Qb2 Nd4 Nxd4 cxd4 Qxb7 Re7 Qc8+ Kg7 Qxc5 Qe5 Ra6 f3 g3 Qe1+ Kh2 Qxf2+ Kh1 Qg2# 1-0 Nf6 d3 Bc5 O-O O-O Nc3 d6 Bg5 Be6 Nd5 Bxd5 Bh4 g5 Bg3 Nxd5 exd5 Ne7 c4 f5 h3 f4 Bh2 Nf5 a3 a5 Qd2 Qf6 b4 axb4 axb4 R",
          "aggressive_tokens": {
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d3",
            "Bc5",
            "O-O",
            "O-O",
            "Nc3",
            "d6",
            "Bg5",
            "Be6",
            "Nd5",
            "Bxd5",
            "Bh4",
            "g5",
            "Bg3",
            "Nxd5",
            "exd5",
            "Ne7",
            "c4",
            "f5",
            "h3",
            "f4",
            "Bh2",
            "Nf5",
            "a3",
            "a5",
            "Qd2",
            "Qf6",
            "b4",
            "axb4",
            "axb4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf6",
          "positional_uci": "g8f6",
          "positional_response": "3... Nf6 4. d3 Bc5 5. O-O O-O 6. Nc3 d6 7. Bg5 Be6 8. Nd5 Bxd5 9. Bxd5 h6 10. Bh4 g5 11. Bg3 Nxd5 12. exd5 Ne7 13. c4 f5 14. h3 f4 15. Bh2 Nf5 16. Nd2 Qe8 17. Ne4 Bd4 18. Rb1 Qg6 19. Kh1 h5 20. g4 hxg4 21. Qxg4 Kf7 22. Nxg5+ Ke7 23. Ne6 Qxg4 24. hxg4 Rh8 25. gxf5 Rh3 26. Kg2 Rah8 27. Rh1 f3+ 28. Kg1 Rg8+ 29. Kf1 Rg2 30. Nxd4 exd4 31. Re1+ Kf7 32. Re4 Rhxh2 33. Rxh2 Rxh2 34. Kg1 Rg2+ 35. Kf1 Rh2 36. Kg1 Rg2+ 37. Kf1 Rh2 38. Kg1 Rg2+ 39. Kf1 Rh2 40. Kg1 Rg2+ 41. Kf1 Rh2 42. Kg1 Rg2+ 43. Kf1 Rh2 44. Kg1 Rg2+ 45. Kf1 Rh2 46. Kg1 Rg2+ 47. Kf1 Rh2 48. Kg1 Rg2+ 49. Kf1 Rh2 50. Kg1 Rg2+ 51. Kf1 Rh2 52. Kg1 Rg2+ 53. Kf1 Rh2 54. Kg1 Rg2+ 55. Kf1 Rh2 56. Kg1 Rg2+ 57. Kf1 Rh2 58. Kg1 Rg2+ 59. Kf1 Rh2 60. Kg1 Rg2+ 61. Kf1 Rh2 62. Kg1 Rg2+ 63. Kf1 Rh2 64. Kg1 Rg2+ 65. Kf1 Rh2 66. Kg1 Rg2+ 67. Kf1 Rh2 68. Kg1 Rg2+ 69. Kf1 Rh2 70. Kg1 Rg2+ 71. Kf1 Rh2 72. Kg1 Rg2+ 73. Kf1 Rh2 74. Kg1 Rg2+ 75. Kf1 Rh2 76. Kg1 Rg2+ 77. Kf1 Rh2 78. Kg1 Rg2+ 79. Kf1",
          "positional_tokens": {
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d3",
            "Bc5",
            "O-O",
            "O-O",
            "Nc3",
            "d6",
            "Bg5",
            "Be6",
            "Nd5",
            "Bxd5",
            "Bxd5",
            "h6",
            "Bh4",
            "g5",
            "Bg3",
            "Nxd5",
            "exd5",
            "Ne7",
            "c4",
            "f5",
            "h3",
            "f4",
            "Bh2",
            "Nf5",
            "Nd2",
            "Qe8",
            "Ne4",
            "Bd4",
            "Rb1",
            "Qg6",
            "Kh1",
            "h5",
            "g4",
            "hxg4",
            "Qxg4",
            "Kf7",
            "Nxg5+",
            "Ke7",
            "Ne6",
            "Qxg4",
            "hxg4",
            "Rh8",
            "gxf5",
            "Rh3",
            "Kg2",
            "Rah8",
            "Rh1",
            "f3+",
            "Kg1",
            "Rg8+",
            "Kf1",
            "Rg2",
            "Nxd4",
            "exd4",
            "Re1+",
            "Kf7",
            "Re4",
            "Rhxh2",
            "Rxh2",
            "Rxh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1",
            "Rh2",
            "Kg1",
            "Rg2+",
            "Kf1"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf6",
          "neutral_uci": "g8f6",
          "neutral_response": "3... Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. Qf3+ Ke6 8. Nc3 Ne7 9. d4 c6 10. O-O b5 11. Bb3 a5 12. Re1 a4 13. Rxe5+ Kd6 14. Bxd5 Nxd5 15. Nxd5 cxd5 16. Qxd5+ Kc7 17. Qxa8 Qxd4 18. Qa5+ Kd6 19. Re1 Bd7 20. Be3 Qxb2 21. Rad1+ Kc6 22. Qb6# e4 Nf6 Qc3#",
          "neutral_tokens": {
            "prompt_tokens": 130,
            "completion_tokens": 174,
            "total_tokens": 304,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qc3#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g8f6",
          "positional_uci": "g8f6",
          "neutral_uci": "g8f6",
          "consensus_move": "g8f6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d3",
              "Bc5",
              "O-O",
              "O-O",
              "Nc3",
              "d6",
              "Bg5",
              "Be6",
              "Nd5",
              "Bxd5",
              "Bh4",
              "g5",
              "Bg3",
              "Nxd5",
              "exd5",
              "Ne7",
              "c4",
              "f5",
              "h3",
              "f4",
              "Bh2",
              "Nf5",
              "a3",
              "a5",
              "Qd2",
              "Qf6",
              "b4",
              "axb4",
              "axb4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "d3",
              "Bc5",
              "O-O",
              "O-O",
              "Nc3",
              "d6",
              "Bg5",
              "Be6",
              "Nd5",
              "Bxd5",
              "Bxd5",
              "h6",
              "Bh4",
              "g5",
              "Bg3",
              "Nxd5",
              "exd5",
              "Ne7",
              "c4",
              "f5",
              "h3",
              "f4",
              "Bh2",
              "Nf5",
              "Nd2",
              "Qe8",
              "Ne4",
              "Bd4",
              "Rb1",
              "Qg6",
              "Kh1",
              "h5",
              "g4",
              "hxg4",
              "Qxg4",
              "Kf7",
              "Nxg5+",
              "Ke7",
              "Ne6",
              "Qxg4",
              "hxg4",
              "Rh8",
              "gxf5",
              "Rh3",
              "Kg2",
              "Rah8",
              "Rh1",
              "f3+",
              "Kg1",
              "Rg8+",
              "Kf1",
              "Rg2",
              "Nxd4",
              "exd4",
              "Re1+",
              "Kf7",
              "Re4",
              "Rhxh2",
              "Rxh2",
              "Rxh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1",
              "Rh2",
              "Kg1",
              "Rg2+",
              "Kf1"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qc3#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 130,
            "completion_tokens": 174,
            "total_tokens": 304,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 392,
          "total_completion_tokens": 1454,
          "total_tokens": 1846
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 130,
            "completion_tokens": 174,
            "total_tokens": 304,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkbnr/pppp1ppp/2n5/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R b KQkq - 3 3"
    },
    {
      "turn": 8,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nxe4",
          "aggressive_uci": "f6e4",
          "aggressive_response": "4... Nxe4 5. Nxe4 d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. Re1 Bd6 11. d4 O-O 12. dxe5 Nxe5 13. g4 Nxf3+ 14. Qxf3 Bg6 15. Qxb7 Qh4 16. Qg2 Bxc2 17. Be3 Rab8 18. b3 Be5 19. Rac1 Bxc3 20. Rxc2 Bxe1 21. Rxc7 Rfd8 22. Qf3 Qf6 23. Qe2 Ba5 24. Rxa7 Bb6 25. Bxb6 Rxb6 26. Re7 Re6 27. Rxe6 Qxe6 28. Qc2 Qe1+ 29. Kg2 Qd2 30. Qb1 Qd5+ 31. Kg1 Qd1+ 32. Qxd1 Rxd1+ 33. Kg2 Ra1 34. a4 Rb1 35. a5 Rxb3 36. a6 Ra3 37. a7 Rxa7 38. f4 Ra3 39. h4 Kf8 40. g5 Ke7 41. f5 Kd6 42. h5 Ke5 43. g6 fxg6 44. fxg6 hxg6 45. hxg6 Kf4 46. Kf2 Ra2+ 47. Ke1 Ke3 48. Kd1 Kd3 49. Kc1 Kc3 50. Kb1 Rh2 51. Ka1 Kb3 52. Kb1 Rh1# e4 d5 Nf3 Nc6 Bc4 Nf6 Nc3 Nxe4 Nxe4 d5 Bxd5 Qxd5 Nc6 Qd8 O-O Bg4 h3 Bh5 Re1 Bd6 d4 O-O dxe5 Nxe5 g4 Nxf3 Qxf3 Bg6 Qxb7 Qh4 Qg2 Bxc2 Be3 Rab8 b3 Be5 Rac1 Bxc3 Rxc2 Bxe1 Rxc7 Rfd8 Qf3 Qf6 Qe2 Ba5 Rxa7 Bb6 Bxb6 Rxb6 Re7 Re6 Rxe6 Qxe6 Qc2 Qe1+ Kg2 Qd2 Qb1 Qd5+ Kg1 Qd1+ Qxd1 Rxd1+ Kg2 Rd2 a4 Rb2 a5 Rxb3 a6 Ra3 a7 Rxa7 f4 Ra3 h4 Kf8 g5 Ke7 f5 Kd6 h5 Ke5 g6 fxg6 fxg6 hxg6 Kg2 Kf4 Kf2",
          "aggressive_tokens": {
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Bxd5",
            "Qxd5",
            "Nc6",
            "Qd8",
            "O-O",
            "Bg4",
            "h3",
            "Bh5",
            "Re1",
            "Bd6",
            "d4",
            "O-O",
            "dxe5",
            "Nxe5",
            "g4",
            "Nxf3",
            "Qxf3",
            "Bg6",
            "Qxb7",
            "Qh4",
            "Qg2",
            "Bxc2",
            "Be3",
            "Rab8",
            "b3",
            "Be5",
            "Rac1",
            "Bxc3",
            "Rxc2",
            "Bxe1",
            "Rxc7",
            "Rfd8",
            "Qf3",
            "Qf6",
            "Qe2",
            "Ba5",
            "Rxa7",
            "Bb6",
            "Bxb6",
            "Rxb6",
            "Re7",
            "Re6",
            "Rxe6",
            "Qxe6",
            "Qc2",
            "Qe1+",
            "Kg2",
            "Qd2",
            "Qb1",
            "Qd5+",
            "Kg1",
            "Qd1+",
            "Qxd1",
            "Rxd1+",
            "Kg2",
            "Rd2",
            "a4",
            "Rb2",
            "a5",
            "Rxb3",
            "a6",
            "Ra3",
            "a7",
            "Rxa7",
            "f4",
            "Ra3",
            "h4",
            "Kf8",
            "g5",
            "Ke7",
            "f5",
            "Kd6",
            "h5",
            "Ke5",
            "g6",
            "fxg6",
            "fxg6",
            "hxg6",
            "Kg2",
            "Kf4",
            "Kf2"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nxe4",
          "positional_uci": "f6e4",
          "positional_response": "4... Nxe4 5. Nxe4 d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. Re1 Bd6 11. d4 O-O 12. dxe5 Nxe5 13. g4 Nxf3+ 14. Qxf3 Bg6 15. Qxb7 Qh4 16. Qg2 Bxc2 17. Be3 Rab8 18. b3 Be5 19. Rac1 Bxc3 20. Rxc2 Bxe1 21. Rxc7 Rbc8 22. Rxa7 Rc2 23. a4 Re8 24. Ra8 Rcc8 25. Rxc8 Rxc8 26. Qb7 Qd8 27. b4 Rb8 28. Qe4 Bxb4 29. Bf4 Bd6 30. Be3 Qe7 31. Qc6 Qe5 32. Kf1 Rb1+ 33. Ke2 Rb2+ 34. Kf1 Qh2 35. Qe8+ Bf8 36. Qe4 Qxh3+ 37. Ke1 Bb4+ 38. Kd1 Qf1# 39. Qe1# Bxe1 40. Kxe1 Qe2# 41. Bxe2 Rxe2+ 42. Kf1 Rxe3 43. fxe3 Kf8 44. a5 Ke8 45. a6 Kd8 46. a7 Kc8 47. a8=Q+ Kc7 48. Qa7+ Kd6 49. Qxf7 Ke5 50. Qxg7+ Ke4 51. Qxh7+ Kxe3 52. g5 Kf4 53. g6 Kg5 54. g7 Kf6 55. g8=Q Ke5 56. Qg5+ Kd4 57. Qhh4+ Kd3 58. Qgg3+ Kd2 59. Qhh2+ Kd1 60. Qd3+ Kc1 61. Qhc2# e5 d5 e6 dxe6 fxe6 Qxe6 Qd8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# Bxe8 Kxe8 Qxe8# B",
          "positional_tokens": {
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Bxd5",
            "Qxd5",
            "Nc3",
            "Qd8",
            "O-O",
            "Bg4",
            "h3",
            "Bh5",
            "Re1",
            "Bd6",
            "d4",
            "O-O",
            "dxe5",
            "Nxe5",
            "g4",
            "Nxf3+",
            "Qxf3",
            "Bg6",
            "Qxb7",
            "Qh4",
            "Qg2",
            "Bxc2",
            "Be3",
            "Rab8",
            "b3",
            "Be5",
            "Rac1",
            "Bxc3",
            "Rxc2",
            "Bxe1",
            "Rxc7",
            "Rbc8",
            "Rxa7",
            "Rc2",
            "a4",
            "Re8",
            "Ra8",
            "Rcc8",
            "Rxc8",
            "Rxc8",
            "Qb7",
            "Qd8",
            "b4",
            "Rb8",
            "Qe4",
            "Bxb4",
            "Bf4",
            "Bd6",
            "Be3",
            "Qe7",
            "Qc6",
            "Qe5",
            "Kf1",
            "Rb1+",
            "Ke2",
            "Rb2+",
            "Kf1",
            "Qh2",
            "Qe8+",
            "Bf8",
            "Qe4",
            "Qxh3+",
            "Ke1",
            "Bb4+",
            "Kd1",
            "Qf1#",
            "Qe1#",
            "Bxe1",
            "Kxe1",
            "Qe2#",
            "Bxe2",
            "Rxe2+",
            "Kf1",
            "Rxe3",
            "fxe3",
            "Kf8",
            "a5",
            "Ke8",
            "a6",
            "Kd8",
            "a7",
            "Kc8",
            "a8=Q+",
            "Kc7",
            "Qa7+",
            "Kd6",
            "Qxf7",
            "Ke5",
            "Qxg7+",
            "Ke4",
            "Qxh7+",
            "Kxe3",
            "g5",
            "Kf4",
            "g6",
            "Kg5",
            "g7",
            "Kf6",
            "g8=Q",
            "Ke5",
            "Qg5+",
            "Kd4",
            "Qhh4+",
            "Kd3",
            "Qgg3+",
            "Kd2",
            "Qhh2+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qhc2#",
            "e5",
            "d5",
            "e6",
            "dxe6",
            "fxe6",
            "Qxe6",
            "Qd8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#",
            "Bxe8",
            "Kxe8",
            "Qxe8#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nxe4",
          "neutral_uci": "f6e4",
          "neutral_response": "4... Nxe4 5. Nxe4 d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. Re1 Bd6 11. d4 O-O 12. dxe5 Nxe5 13. g4 Nxf3+ 14. Qxf3 Bg6 15. Qxb7 Qh4 16. Qg2 Bxc2 17. Be3 Rab8 18. b3 Be5 19. Rac1 Bxc3 20. Rxc2 Bxe1 21. Rxc7 Rbc8 22. Rxa7 Rc2 23. a4 Rfc8 24. a5 Bxf2+ 25. Bxf2 Rxf2 26. Qxf2 Rc1+ 27. Kg2 Qxf2+ 28. Kxf2 Rc2+ 29. Kg3 Rc3+ 30. Kh4 Rxb3 31. Ra8+ Rb8 32. Rxb8# Rxb8 33. a6 Ra8 34. a7 Rxa7 35. Kg3 Ra3+ 36. Kh4 f6 37. Kh5 Rxh3# Rxe1 g6+ Kh7 g5#",
          "neutral_tokens": {
            "prompt_tokens": 139,
            "completion_tokens": 292,
            "total_tokens": 431,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d5",
            "Bxd5",
            "Qxd5",
            "Nc3",
            "Qd8",
            "O-O",
            "Bg4",
            "h3",
            "Bh5",
            "Re1",
            "Bd6",
            "d4",
            "O-O",
            "dxe5",
            "Nxe5",
            "g4",
            "Nxf3+",
            "Qxf3",
            "Bg6",
            "Qxb7",
            "Qh4",
            "Qg2",
            "Bxc2",
            "Be3",
            "Rab8",
            "b3",
            "Be5",
            "Rac1",
            "Bxc3",
            "Rxc2",
            "Bxe1",
            "Rxc7",
            "Rbc8",
            "Rxa7",
            "Rc2",
            "a4",
            "Rfc8",
            "a5",
            "Bxf2+",
            "Bxf2",
            "Rxf2",
            "Qxf2",
            "Rc1+",
            "Kg2",
            "Qxf2+",
            "Kxf2",
            "Rc2+",
            "Kg3",
            "Rc3+",
            "Kh4",
            "Rxb3",
            "Ra8+",
            "Rb8",
            "Rxb8#",
            "Rxb8",
            "a6",
            "Ra8",
            "a7",
            "Rxa7",
            "Kg3",
            "Ra3+",
            "Kh4",
            "f6",
            "Kh5",
            "Rxh3#",
            "Rxe1",
            "g6+",
            "Kh7",
            "g5#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f6e4",
          "positional_uci": "f6e4",
          "neutral_uci": "f6e4",
          "consensus_move": "f6e4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d5",
              "Bxd5",
              "Qxd5",
              "Nc6",
              "Qd8",
              "O-O",
              "Bg4",
              "h3",
              "Bh5",
              "Re1",
              "Bd6",
              "d4",
              "O-O",
              "dxe5",
              "Nxe5",
              "g4",
              "Nxf3",
              "Qxf3",
              "Bg6",
              "Qxb7",
              "Qh4",
              "Qg2",
              "Bxc2",
              "Be3",
              "Rab8",
              "b3",
              "Be5",
              "Rac1",
              "Bxc3",
              "Rxc2",
              "Bxe1",
              "Rxc7",
              "Rfd8",
              "Qf3",
              "Qf6",
              "Qe2",
              "Ba5",
              "Rxa7",
              "Bb6",
              "Bxb6",
              "Rxb6",
              "Re7",
              "Re6",
              "Rxe6",
              "Qxe6",
              "Qc2",
              "Qe1+",
              "Kg2",
              "Qd2",
              "Qb1",
              "Qd5+",
              "Kg1",
              "Qd1+",
              "Qxd1",
              "Rxd1+",
              "Kg2",
              "Rd2",
              "a4",
              "Rb2",
              "a5",
              "Rxb3",
              "a6",
              "Ra3",
              "a7",
              "Rxa7",
              "f4",
              "Ra3",
              "h4",
              "Kf8",
              "g5",
              "Ke7",
              "f5",
              "Kd6",
              "h5",
              "Ke5",
              "g6",
              "fxg6",
              "fxg6",
              "hxg6",
              "Kg2",
              "Kf4",
              "Kf2"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "d5",
              "Bxd5",
              "Qxd5",
              "Nc3",
              "Qd8",
              "O-O",
              "Bg4",
              "h3",
              "Bh5",
              "Re1",
              "Bd6",
              "d4",
              "O-O",
              "dxe5",
              "Nxe5",
              "g4",
              "Nxf3+",
              "Qxf3",
              "Bg6",
              "Qxb7",
              "Qh4",
              "Qg2",
              "Bxc2",
              "Be3",
              "Rab8",
              "b3",
              "Be5",
              "Rac1",
              "Bxc3",
              "Rxc2",
              "Bxe1",
              "Rxc7",
              "Rbc8",
              "Rxa7",
              "Rc2",
              "a4",
              "Re8",
              "Ra8",
              "Rcc8",
              "Rxc8",
              "Rxc8",
              "Qb7",
              "Qd8",
              "b4",
              "Rb8",
              "Qe4",
              "Bxb4",
              "Bf4",
              "Bd6",
              "Be3",
              "Qe7",
              "Qc6",
              "Qe5",
              "Kf1",
              "Rb1+",
              "Ke2",
              "Rb2+",
              "Kf1",
              "Qh2",
              "Qe8+",
              "Bf8",
              "Qe4",
              "Qxh3+",
              "Ke1",
              "Bb4+",
              "Kd1",
              "Qf1#",
              "Qe1#",
              "Bxe1",
              "Kxe1",
              "Qe2#",
              "Bxe2",
              "Rxe2+",
              "Kf1",
              "Rxe3",
              "fxe3",
              "Kf8",
              "a5",
              "Ke8",
              "a6",
              "Kd8",
              "a7",
              "Kc8",
              "a8=Q+",
              "Kc7",
              "Qa7+",
              "Kd6",
              "Qxf7",
              "Ke5",
              "Qxg7+",
              "Ke4",
              "Qxh7+",
              "Kxe3",
              "g5",
              "Kf4",
              "g6",
              "Kg5",
              "g7",
              "Kf6",
              "g8=Q",
              "Ke5",
              "Qg5+",
              "Kd4",
              "Qhh4+",
              "Kd3",
              "Qgg3+",
              "Kd2",
              "Qhh2+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qhc2#",
              "e5",
              "d5",
              "e6",
              "dxe6",
              "fxe6",
              "Qxe6",
              "Qd8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#",
              "Bxe8",
              "Kxe8",
              "Qxe8#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "d5",
              "Bxd5",
              "Qxd5",
              "Nc3",
              "Qd8",
              "O-O",
              "Bg4",
              "h3",
              "Bh5",
              "Re1",
              "Bd6",
              "d4",
              "O-O",
              "dxe5",
              "Nxe5",
              "g4",
              "Nxf3+",
              "Qxf3",
              "Bg6",
              "Qxb7",
              "Qh4",
              "Qg2",
              "Bxc2",
              "Be3",
              "Rab8",
              "b3",
              "Be5",
              "Rac1",
              "Bxc3",
              "Rxc2",
              "Bxe1",
              "Rxc7",
              "Rbc8",
              "Rxa7",
              "Rc2",
              "a4",
              "Rfc8",
              "a5",
              "Bxf2+",
              "Bxf2",
              "Rxf2",
              "Qxf2",
              "Rc1+",
              "Kg2",
              "Qxf2+",
              "Kxf2",
              "Rc2+",
              "Kg3",
              "Rc3+",
              "Kh4",
              "Rxb3",
              "Ra8+",
              "Rb8",
              "Rxb8#",
              "Rxb8",
              "a6",
              "Ra8",
              "a7",
              "Rxa7",
              "Kg3",
              "Ra3+",
              "Kh4",
              "f6",
              "Kh5",
              "Rxh3#",
              "Rxe1",
              "g6+",
              "Kh7",
              "g5#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 139,
            "completion_tokens": 292,
            "total_tokens": 431,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 419,
          "total_completion_tokens": 1572,
          "total_tokens": 1991
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 139,
            "completion_tokens": 292,
            "total_tokens": 431,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/pppp1ppp/2n2n2/4p3/2B1P3/2N2N2/PPPP1PPP/R1BQK2R b KQkq - 5 4"
    },
    {
      "turn": 10,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d5",
          "aggressive_uci": "d7d5",
          "aggressive_response": "5... d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. d3 Nd4 11. g4 Nxf3+ 12. Qxf3 Bg6 13. Qxb7 Bd6 14. Qc6+ Kf8 15. Be3 h5 16. g5 Rb8 17. b3 Qc8 18. Kg2 Bf5 19. Rh1 Rb4 20. Bc5 Bxc5 21. Qxc5+ Kg8 22. Qxb4 Qa8+ 23. f3 Kh7 24. Ne4 Rb8 25. Qe7 Qc6 26. Rac1 Re8 27. Qxf7 Bg6 28. Qc4 Qd7 29. Rhf1 Rf8 30. Rce1 Rf4 31. Qc5 Rh4 32. Rh1 Qf5 33. Qxc7 Rf4 34. Ref1 h4 35. Qxa7 Bh5 36. Qe3 Qg6 37. Rhg1 Qf5 38. Kh2 Bxf3 39. Rxf3 Rxf3 40. g6+ Kg8 41. Qg5 Qxh3# 0-1 1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. d3 Nd4 11. g4 Nxf3+ 12. Qxf3 Bg6 13. Qxb7 Bd6 14. Qc6+ Kf8 15. Be3 h5 16. g5 Rb8 17. b3 Qc8 18. Kg2 Bf5 19. Rh1 Rb4 20. Bc5 Bxc5 21. Qxc5+ Kg8 22. Qxb4 Qa8+ 23. f3 Kh7 24. Ne4 Rb8 25. Qe7 Qc6 26. Rac1 Re8 27. Qxf7 Bg6 28. Qc4 Qd7 29. Rhf1 Rf8 30. Rce1 Rf4 31. Qc5 Rh4 32. Rh1 Qf5 33. Qxc7 Rf4 34. Ref1 h4 35. Qxa7 Bh5 36. Qe3 Qg6 37. Rhg1 Qf5 38. Kh2 Bxf3 39. Rxf3 Rxf3",
          "aggressive_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bxd5",
            "Qxd5",
            "Nc3",
            "Qd8",
            "O-O",
            "Bg4",
            "h3",
            "Bh5",
            "d3",
            "Nd4",
            "g4",
            "Nxf3+",
            "Qxf3",
            "Bg6",
            "Qxb7",
            "Bd6",
            "Qc6+",
            "Kf8",
            "Be3",
            "h5",
            "g5",
            "Rb8",
            "b3",
            "Qc8",
            "Kg2",
            "Bf5",
            "Rh1",
            "Rb4",
            "Bc5",
            "Bxc5",
            "Qxc5+",
            "Kg8",
            "Qxb4",
            "Qa8+",
            "f3",
            "Kh7",
            "Ne4",
            "Rb8",
            "Qe7",
            "Qc6",
            "Rac1",
            "Re8",
            "Qxf7",
            "Bg6",
            "Qc4",
            "Qd7",
            "Rhf1",
            "Rf8",
            "Rce1",
            "Rf4",
            "Qc5",
            "Rh4",
            "Rh1",
            "Qf5",
            "Qxc7",
            "Rf4",
            "Ref1",
            "h4",
            "Qxa7",
            "Bh5",
            "Qe3",
            "Qg6",
            "Rhg1",
            "Qf5",
            "Kh2",
            "Bxf3",
            "Rxf3",
            "Rxf3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d5",
          "positional_uci": "d7d5",
          "positional_response": "5... d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. g4 Bg6 11. d3 Bc5 12. Be3 Bxe3 13. fxe3 O-O 14. e4 Nd4 15. Nxe5 Qh4 16. Kg2 Rae8 17. Nxg6 fxg6 18. Rxf8+ Rxf8 19. Qd2 Rf3 20. Rh1 Qg3# 0-1\n1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. g4 Bg6 11. d3 Bc5 12. Be3 Bxe3 13. fxe3 O-O 14. e4 Nd4 15. Nxe5 Qh4 16. Kg2 Rae8 17. Nxg6 fxg6 18. Rxf8+ Rxf8 19. Qd2 Rf3 20. Rh1 Qg3# 0-1 d5 Qxd5 Nc3 Qd8 O-O Bg4 h3 Bh5 g4 Bg6 d3 Bc5 Be3 Bxe3 fxe3 O-O e4 Nd4 Nxe5 Qh4 Kg2 Rae8 Nxg6 fxg6 Rxf8+ Rxf8 Qd2 Rf3 Rh1 Qg3# 0-1",
          "positional_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 388,
            "total_tokens": 537,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qxd5",
            "Nc3",
            "Qd8",
            "O-O",
            "Bg4",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "d3",
            "Bc5",
            "Be3",
            "Bxe3",
            "fxe3",
            "O-O",
            "e4",
            "Nd4",
            "Nxe5",
            "Qh4",
            "Kg2",
            "Rae8",
            "Nxg6",
            "fxg6",
            "Rxf8+",
            "Rxf8",
            "Qd2",
            "Rf3",
            "Rh1",
            "Qg3#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d5",
          "neutral_uci": "d7d5",
          "neutral_response": "5... d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. Re1 Bd6 11. d3 O-O 12. Ne4 Nd4 13. g4 Nxf3+ 14. Qxf3 Bg6 15. Nxd6 cxd6 16. Qxb7 Qh4 17. Qg2 f5 18. gxf5 Bxf5 19. Re3 Rac8 20. c3 Rf6 21. Rg3 Rg6 22. Rxg6 Bxg6 23. Qg3 Qh5 24. Be3 Bxd3 25. Bxa7 Be4 26. Re1 Ba8 27. Qg4 Qe8 28. Rd1 Rd8 29. Bb6 Rb8 30. Rxd6 Qb5 31. Qe6+ Kh8 32. Rd8+ Rxd8 33. Bxd8 Qb8 34. Qe8# d5 35. Qf8# d4 36. cxd4 exd4 37. Qe7 d3 38. Qf8# d2 39. Qe7 d1=Q+ 40. Kh2 Qh1+ 41. Kg3 Qbf3+ 42. Kh4 Qhxh3+ 43. Kg5 Qhg4# d5 44. Qf8# d4 45. Qe7 d3 46. Qf8# d2 47. Qe7 d1=Q 48. Qf8# d5 49. Qe7 d4 50. Qf8# d3 51. Qe7 d2 52. Qf8# d5 53. Qe7 d4 54. Qf8# d3 55. Qe7 d2 56. Qf8# d5 57. Qe7 d4 58. Qf8# d3 59. Qe7 d2 60. Qf8# d5 61. Qe7 d4 62. Qf8# d3 63. Qe7 d2 64. Qf8# d5 65. Qe7 d4 66. Qf8# d3 67. Qe7 d2 68. Qf8# d5 69. Qe7 d4 70. Qf8# d3 71. Qe7 d2 72. Qf8# d5 73. Qe7 d4 74. Qf8# d3 75. Qe7 d2 76. Qf8# d5 77. Qe7 d4 78. Qf8# d3 79. Qe7 d",
          "neutral_tokens": {
            "prompt_tokens": 148,
            "completion_tokens": 640,
            "total_tokens": 788,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Qe7",
            "d4",
            "Qf8#",
            "d3",
            "Qe7"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d7d5",
          "positional_uci": "d7d5",
          "neutral_uci": "d7d5",
          "consensus_move": "d7d5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bxd5",
              "Qxd5",
              "Nc3",
              "Qd8",
              "O-O",
              "Bg4",
              "h3",
              "Bh5",
              "d3",
              "Nd4",
              "g4",
              "Nxf3+",
              "Qxf3",
              "Bg6",
              "Qxb7",
              "Bd6",
              "Qc6+",
              "Kf8",
              "Be3",
              "h5",
              "g5",
              "Rb8",
              "b3",
              "Qc8",
              "Kg2",
              "Bf5",
              "Rh1",
              "Rb4",
              "Bc5",
              "Bxc5",
              "Qxc5+",
              "Kg8",
              "Qxb4",
              "Qa8+",
              "f3",
              "Kh7",
              "Ne4",
              "Rb8",
              "Qe7",
              "Qc6",
              "Rac1",
              "Re8",
              "Qxf7",
              "Bg6",
              "Qc4",
              "Qd7",
              "Rhf1",
              "Rf8",
              "Rce1",
              "Rf4",
              "Qc5",
              "Rh4",
              "Rh1",
              "Qf5",
              "Qxc7",
              "Rf4",
              "Ref1",
              "h4",
              "Qxa7",
              "Bh5",
              "Qe3",
              "Qg6",
              "Rhg1",
              "Qf5",
              "Kh2",
              "Bxf3",
              "Rxf3",
              "Rxf3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qxd5",
              "Nc3",
              "Qd8",
              "O-O",
              "Bg4",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "d3",
              "Bc5",
              "Be3",
              "Bxe3",
              "fxe3",
              "O-O",
              "e4",
              "Nd4",
              "Nxe5",
              "Qh4",
              "Kg2",
              "Rae8",
              "Nxg6",
              "fxg6",
              "Rxf8+",
              "Rxf8",
              "Qd2",
              "Rf3",
              "Rh1",
              "Qg3#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qe7",
              "d4",
              "Qf8#",
              "d3",
              "Qe7"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 149,
            "completion_tokens": 388,
            "total_tokens": 537,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 148,
            "completion_tokens": 640,
            "total_tokens": 788,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 446,
          "total_completion_tokens": 1668,
          "total_tokens": 2114
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 149,
            "completion_tokens": 388,
            "total_tokens": 537,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 148,
            "completion_tokens": 640,
            "total_tokens": 788,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bqkb1r/pppp1ppp/2n5/4p3/2B1N3/5N2/PPPP1PPP/R1BQK2R b KQkq - 0 5"
    },
    {
      "turn": 12,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "dxe4",
          "aggressive_uci": "d5e4",
          "aggressive_response": "6... dxe4 7. Bxe4 Bd6 8. O-O O-O 9. d3 Bg4 10. h3 Bh5 11. g4 Bg6 12. Bxg6 hxg6 13. Be3 f5 14. gxf5 gxf5 15. Ng5 Qe8 16. Kh1 f4 17. Bd2 Rf5 18. Rg1 Be7 19. Ne6 Bf6 20. Nxc7 Qd7 21. Nxa8 Rh5 22. Qg4 Rxh3+ 23. Kg2 Qxg4+ 24. Kf1 Qf3 25. Nc7 Rh2 26. Be1 Nd4 27. Ne6 Qe2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2# e4 d5 Nc3 Nxc3 bxc3 Bc5 d4 Qxf2",
          "aggressive_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bxe4",
            "Bd6",
            "O-O",
            "O-O",
            "d3",
            "Bg4",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Bxg6",
            "hxg6",
            "Be3",
            "f5",
            "gxf5",
            "gxf5",
            "Ng5",
            "Qe8",
            "Kh1",
            "f4",
            "Bd2",
            "Rf5",
            "Rg1",
            "Be7",
            "Ne6",
            "Bf6",
            "Nxc7",
            "Qd7",
            "Nxa8",
            "Rh5",
            "Qg4",
            "Rxh3+",
            "Kg2",
            "Qxg4+",
            "Kf1",
            "Qf3",
            "Nc7",
            "Rh2",
            "Be1",
            "Nd4",
            "Ne6",
            "Qe2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2#",
            "e4",
            "d5",
            "Nc3",
            "Nxc3",
            "bxc3",
            "Bc5",
            "d4",
            "Qxf2"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "dxe4",
          "positional_uci": "d5e4",
          "positional_response": "6... dxe4 7. Bxe4 Bd6 8. O-O O-O 9. d3 Bg4 10. h3 Bh5 11. g4 Bg6 12. Bxg6 hxg6 13. Be3 f5 14. gxf5 gxf5 15. Ng5 Qe8 16. Kh1 f4 17. Bd2 Nd4 18. c3 Ne6 19. Nxe6 Qxe6 20. Qg4 Qd5+ 21. Kh2 e4 22. Rg1 Rf7 23. dxe4 Qxd2 24. Rg2 f3+ 25. Rg3 Qxf2+ 26. Kh1 Bxg3 27. Rg1 Qh2# 0-1\n28. Qxh2 Bxh2 29. Kxh2 f2 30. Rf1 Rd8 31. Kg2 Rd2 32. b4 Rxa2 33. c4 Re2 34. c5 Re1 35. Rxf2 Rxf2+ 36. Kxf2 Rxe4 37. b5 Rc4 38. c6 bxc6 39. bxc6 Rxc6 40. Ke3 a5 41. Kd4 a4 42. Kd5 Rc1 43. Kd4 a3 44. Kd3 a2 45. Kd2 a1=Q 46. Kd3 Qc3+ 47. Ke4 Re1+ 48. Kd5 Re5# 0-1",
          "positional_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 365,
            "total_tokens": 521,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qxd2",
            "Rg2",
            "f3+",
            "Rg3",
            "Qxf2+",
            "Kh1",
            "Bxg3",
            "Rg1",
            "Qh2#",
            "Qxh2",
            "Bxh2",
            "Kxh2",
            "f2",
            "Rf1",
            "Rd8",
            "Kg2",
            "Rd2",
            "b4",
            "Rxa2",
            "c4",
            "Re2",
            "c5",
            "Re1",
            "Rxf2",
            "Rxf2+",
            "Kxf2",
            "Rxe4",
            "b5",
            "Rc4",
            "c6",
            "bxc6",
            "bxc6",
            "Rxc6",
            "Ke3",
            "a5",
            "Kd4",
            "a4",
            "Kd5",
            "Rc1",
            "Kd4",
            "a3",
            "Kd3",
            "a2",
            "Kd2",
            "a1=Q",
            "Kd3",
            "Qc3+",
            "Ke4",
            "Re1+",
            "Kd5",
            "Re5#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Re8",
          "neutral_uci": null,
          "neutral_response": "Re8\n7. O-O Bg4 8. h3 Bh5 9. Ng3 Bg6 10. Bxg6 hxg6 11. d3 Qd7 12. Re1 O-O-O\n13. c3 f6 14. b4 g5 15. b5 Ne7 16. Qa4 Kb8 17. Be3 Nc8 18. Rab1 Bd6\n19. Rb3 g6 20. Reb1 Rh8 21. c4 d4 22. Bd2 g4 23. hxg4 Qxg4 24. Bb4 Bxb4\n25. Qxb4 Rh7 26. c5 Rdh8 27. Kf1 Rh1+ 28. Nxh1 Rxh1+ 29. Ke2 Qxg2\n30. Rxh1 Qxh1 31. Rb1 Qh5 32. c6 b6 33. Qc4 Nd6 34. Qg8+ Nc8 35. Qe6 Nd6\n36. Qxf6 e4 37. dxe4 Nxe4 38. Qd8# e4 Qxf3+ Kd2 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+ Kc1 Qc3+ Kd1 Qd3+",
          "neutral_tokens": {
            "prompt_tokens": 155,
            "completion_tokens": 640,
            "total_tokens": 795,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "O-O",
            "Bg4",
            "h3",
            "Bh5",
            "Ng3",
            "Bg6",
            "Bxg6",
            "hxg6",
            "d3",
            "Qd7",
            "Re1",
            "O-O-O",
            "c3",
            "f6",
            "b4",
            "g5",
            "b5",
            "Ne7",
            "Qa4",
            "Kb8",
            "Be3",
            "Nc8",
            "Rab1",
            "Bd6",
            "Rb3",
            "g6",
            "Reb1",
            "Rh8",
            "c4",
            "d4",
            "Bd2",
            "g4",
            "hxg4",
            "Qxg4",
            "Bb4",
            "Bxb4",
            "Qxb4",
            "Rh7",
            "c5",
            "Rdh8",
            "Kf1",
            "Rh1+",
            "Nxh1",
            "Rxh1+",
            "Ke2",
            "Qxg2",
            "Rxh1",
            "Qxh1",
            "Rb1",
            "Qh5",
            "c6",
            "b6",
            "Qc4",
            "Nd6",
            "Qg8+",
            "Nc8",
            "Qe6",
            "Nd6",
            "Qxf6",
            "e4",
            "dxe4",
            "Nxe4",
            "Qd8#",
            "e4",
            "Qxf3+",
            "Kd2",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+",
            "Kc1",
            "Qc3+",
            "Kd1",
            "Qd3+"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d5e4",
          "positional_uci": "d5e4",
          "neutral_uci": null,
          "consensus_move": "d5e4",
          "source_agents": [
            "aggressive",
            "positional"
          ],
          "selected_agent": "aggressive"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bxe4",
              "Bd6",
              "O-O",
              "O-O",
              "d3",
              "Bg4",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Bxg6",
              "hxg6",
              "Be3",
              "f5",
              "gxf5",
              "gxf5",
              "Ng5",
              "Qe8",
              "Kh1",
              "f4",
              "Bd2",
              "Rf5",
              "Rg1",
              "Be7",
              "Ne6",
              "Bf6",
              "Nxc7",
              "Qd7",
              "Nxa8",
              "Rh5",
              "Qg4",
              "Rxh3+",
              "Kg2",
              "Qxg4+",
              "Kf1",
              "Qf3",
              "Nc7",
              "Rh2",
              "Be1",
              "Nd4",
              "Ne6",
              "Qe2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2#",
              "e4",
              "d5",
              "Nc3",
              "Nxc3",
              "bxc3",
              "Bc5",
              "d4",
              "Qxf2"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qxd2",
              "Rg2",
              "f3+",
              "Rg3",
              "Qxf2+",
              "Kh1",
              "Bxg3",
              "Rg1",
              "Qh2#",
              "Qxh2",
              "Bxh2",
              "Kxh2",
              "f2",
              "Rf1",
              "Rd8",
              "Kg2",
              "Rd2",
              "b4",
              "Rxa2",
              "c4",
              "Re2",
              "c5",
              "Re1",
              "Rxf2",
              "Rxf2+",
              "Kxf2",
              "Rxe4",
              "b5",
              "Rc4",
              "c6",
              "bxc6",
              "bxc6",
              "Rxc6",
              "Ke3",
              "a5",
              "Kd4",
              "a4",
              "Kd5",
              "Rc1",
              "Kd4",
              "a3",
              "Kd3",
              "a2",
              "Kd2",
              "a1=Q",
              "Kd3",
              "Qc3+",
              "Ke4",
              "Re1+",
              "Kd5",
              "Re5#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "O-O",
              "Bg4",
              "h3",
              "Bh5",
              "Ng3",
              "Bg6",
              "Bxg6",
              "hxg6",
              "d3",
              "Qd7",
              "Re1",
              "O-O-O",
              "c3",
              "f6",
              "b4",
              "g5",
              "b5",
              "Ne7",
              "Qa4",
              "Kb8",
              "Be3",
              "Nc8",
              "Rab1",
              "Bd6",
              "Rb3",
              "g6",
              "Reb1",
              "Rh8",
              "c4",
              "d4",
              "Bd2",
              "g4",
              "hxg4",
              "Qxg4",
              "Bb4",
              "Bxb4",
              "Qxb4",
              "Rh7",
              "c5",
              "Rdh8",
              "Kf1",
              "Rh1+",
              "Nxh1",
              "Rxh1+",
              "Ke2",
              "Qxg2",
              "Rxh1",
              "Qxh1",
              "Rb1",
              "Qh5",
              "c6",
              "b6",
              "Qc4",
              "Nd6",
              "Qg8+",
              "Nc8",
              "Qe6",
              "Nd6",
              "Qxf6",
              "e4",
              "dxe4",
              "Nxe4",
              "Qd8#",
              "e4",
              "Qxf3+",
              "Kd2",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+",
              "Kc1",
              "Qc3+",
              "Kd1",
              "Qd3+"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 156,
            "completion_tokens": 365,
            "total_tokens": 521,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 155,
            "completion_tokens": 640,
            "total_tokens": 795,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 467,
          "total_completion_tokens": 1645,
          "total_tokens": 2112
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 156,
            "completion_tokens": 365,
            "total_tokens": 521,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 155,
            "completion_tokens": 640,
            "total_tokens": 795,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/3pp3/4N3/3B1N2/PPPP1PPP/R1BQK2R b KQkq - 1 6"
    },
    {
      "turn": 14,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "exd3",
          "aggressive_uci": "e4d3",
          "aggressive_response": "7... exd3 8. cxd3 Qxd3 9. Re1 Bg4 10. Re3 Qd5 11. h3 Bxf3 12. Rxf3 O-O-O 13. d3 e4 14. Re3 exd3 15. Bd2 Bc5 16. Rf3 Rhe8 17. Rc1 Re2 18. a3 Nd4 19. Rg3 Nb3 20. Rc3 Rxd2 21. Qxb3 Bxf2+ 22. Kh2 Bxg3+ 23. Kxg3 Rxg2+ 24. Kh4 Qg5# d4 d5 dxe6",
          "aggressive_tokens": {
            "prompt_tokens": 164,
            "completion_tokens": 156,
            "total_tokens": 320,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bd2",
            "Bc5",
            "Rf3",
            "Rhe8",
            "Rc1",
            "Re2",
            "a3",
            "Nd4",
            "Rg3",
            "Nb3",
            "Rc3",
            "Rxd2",
            "Qxb3",
            "Bxf2+",
            "Kh2",
            "Bxg3+",
            "Kxg3",
            "Rxg2+",
            "Kh4",
            "Qg5#",
            "d4",
            "d5",
            "dxe6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "exd3",
          "positional_uci": "e4d3",
          "positional_response": "7... exd3 8. cxd3 Qxd3 9. Re1 Bg4 10. Re3 Qd5 11. h3 Bxf3 12. Rxf3 O-O-O 13. d3 Nd4 14. Re3 Bc5 15. Re4 f5 16. Re1 Rhe8 17. Be3 f4 18. Bxd4 Qxd4 19. Qg4+ Kb8 20. Qxg7 Qxf2+ 21. Kh1 Rg8 22. Rf1 Qxf1+ 23. Rxf1 Rxg7 24. Kh2 Rxd3 25. Re1 Rd2 26. Rxe5 Rgxg2+ 27. Kh1 Rh2# Rg8 Qxg1#",
          "positional_tokens": {
            "prompt_tokens": 164,
            "completion_tokens": 182,
            "total_tokens": 346,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "cxd3",
            "Qxd3",
            "Re1",
            "Bg4",
            "Re3",
            "Qd5",
            "h3",
            "Bxf3",
            "Rxf3",
            "O-O-O",
            "d3",
            "Nd4",
            "Re3",
            "Bc5",
            "Re4",
            "f5",
            "Re1",
            "Rhe8",
            "Be3",
            "f4",
            "Bxd4",
            "Qxd4",
            "Qg4+",
            "Kb8",
            "Qxg7",
            "Qxf2+",
            "Kh1",
            "Rg8",
            "Rf1",
            "Qxf1+",
            "Rxf1",
            "Rxg7",
            "Kh2",
            "Rxd3",
            "Re1",
            "Rd2",
            "Rxe5",
            "Rgxg2+",
            "Kh1",
            "Rh2#",
            "Rg8",
            "Qxg1#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "exd3",
          "neutral_uci": "e4d3",
          "neutral_response": "7... exd3 8. cxd3 Qxd3 9. Re1 Bg4 10. Re3 Qd5 11. Qe1 Bxf3 12. Rxf3 O-O-O 13. d3 Nd4 14. Re3 Nc2 15. Qc3 Nxa1 16. Rxe5 Qxd3 17. Qxd3 Rxd3 18. Re1 Nc2 19. Rf1 Bc5 20. Bf4 Re8 21. h3 Re1 22. Rxe1 Nxe1 23. Kf1 Nc2 24. Ke2 Rd7 25. a3 Nd4+ 26. Kf1 Ne6 27. Bg3 Rd2 28. b4 Bd4 29. Ke1 Ra2 30. f4 Rxa3 31. Bh2 Bc3+ 32. Kf2 Bxb4 33. f5 Nd4 34. Be5 Nxf5 35. g4 Bc5+ 36. Kg2 Ne3+ 37. Kf3 Nc4+ 38. Ke4 Nxe5 39. Kxe5 Rxh3 40. Kd5 Bd6 41. g5 a5 42. Kc4 a4 43. Kb5 a3 44. Kc4 a2 45. Kd5 a1=Q 46. Ke4 Qe5# Qe5#",
          "neutral_tokens": {
            "prompt_tokens": 163,
            "completion_tokens": 332,
            "total_tokens": 495,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "cxd3",
            "Qxd3",
            "Re1",
            "Bg4",
            "Re3",
            "Qd5",
            "Qe1",
            "Bxf3",
            "Rxf3",
            "O-O-O",
            "d3",
            "Nd4",
            "Re3",
            "Nc2",
            "Qc3",
            "Nxa1",
            "Rxe5",
            "Qxd3",
            "Qxd3",
            "Rxd3",
            "Re1",
            "Nc2",
            "Rf1",
            "Bc5",
            "Bf4",
            "Re8",
            "h3",
            "Re1",
            "Rxe1",
            "Nxe1",
            "Kf1",
            "Nc2",
            "Ke2",
            "Rd7",
            "a3",
            "Nd4+",
            "Kf1",
            "Ne6",
            "Bg3",
            "Rd2",
            "b4",
            "Bd4",
            "Ke1",
            "Ra2",
            "f4",
            "Rxa3",
            "Bh2",
            "Bc3+",
            "Kf2",
            "Bxb4",
            "f5",
            "Nd4",
            "Be5",
            "Nxf5",
            "g4",
            "Bc5+",
            "Kg2",
            "Ne3+",
            "Kf3",
            "Nc4+",
            "Ke4",
            "Nxe5",
            "Kxe5",
            "Rxh3",
            "Kd5",
            "Bd6",
            "g5",
            "a5",
            "Kc4",
            "a4",
            "Kb5",
            "a3",
            "Kc4",
            "a2",
            "Kd5",
            "a1=Q",
            "Ke4",
            "Qe5#",
            "Qe5#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e4d3",
          "positional_uci": "e4d3",
          "neutral_uci": "e4d3",
          "consensus_move": "e4d3",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bd2",
              "Bc5",
              "Rf3",
              "Rhe8",
              "Rc1",
              "Re2",
              "a3",
              "Nd4",
              "Rg3",
              "Nb3",
              "Rc3",
              "Rxd2",
              "Qxb3",
              "Bxf2+",
              "Kh2",
              "Bxg3+",
              "Kxg3",
              "Rxg2+",
              "Kh4",
              "Qg5#",
              "d4",
              "d5",
              "dxe6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "cxd3",
              "Qxd3",
              "Re1",
              "Bg4",
              "Re3",
              "Qd5",
              "h3",
              "Bxf3",
              "Rxf3",
              "O-O-O",
              "d3",
              "Nd4",
              "Re3",
              "Bc5",
              "Re4",
              "f5",
              "Re1",
              "Rhe8",
              "Be3",
              "f4",
              "Bxd4",
              "Qxd4",
              "Qg4+",
              "Kb8",
              "Qxg7",
              "Qxf2+",
              "Kh1",
              "Rg8",
              "Rf1",
              "Qxf1+",
              "Rxf1",
              "Rxg7",
              "Kh2",
              "Rxd3",
              "Re1",
              "Rd2",
              "Rxe5",
              "Rgxg2+",
              "Kh1",
              "Rh2#",
              "Rg8",
              "Qxg1#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "cxd3",
              "Qxd3",
              "Re1",
              "Bg4",
              "Re3",
              "Qd5",
              "Qe1",
              "Bxf3",
              "Rxf3",
              "O-O-O",
              "d3",
              "Nd4",
              "Re3",
              "Nc2",
              "Qc3",
              "Nxa1",
              "Rxe5",
              "Qxd3",
              "Qxd3",
              "Rxd3",
              "Re1",
              "Nc2",
              "Rf1",
              "Bc5",
              "Bf4",
              "Re8",
              "h3",
              "Re1",
              "Rxe1",
              "Nxe1",
              "Kf1",
              "Nc2",
              "Ke2",
              "Rd7",
              "a3",
              "Nd4+",
              "Kf1",
              "Ne6",
              "Bg3",
              "Rd2",
              "b4",
              "Bd4",
              "Ke1",
              "Ra2",
              "f4",
              "Rxa3",
              "Bh2",
              "Bc3+",
              "Kf2",
              "Bxb4",
              "f5",
              "Nd4",
              "Be5",
              "Nxf5",
              "g4",
              "Bc5+",
              "Kg2",
              "Ne3+",
              "Kf3",
              "Nc4+",
              "Ke4",
              "Nxe5",
              "Kxe5",
              "Rxh3",
              "Kd5",
              "Bd6",
              "g5",
              "a5",
              "Kc4",
              "a4",
              "Kb5",
              "a3",
              "Kc4",
              "a2",
              "Kd5",
              "a1=Q",
              "Ke4",
              "Qe5#",
              "Qe5#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 164,
            "completion_tokens": 156,
            "total_tokens": 320,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 164,
            "completion_tokens": 182,
            "total_tokens": 346,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 163,
            "completion_tokens": 332,
            "total_tokens": 495,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 491,
          "total_completion_tokens": 670,
          "total_tokens": 1161
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 164,
            "completion_tokens": 156,
            "total_tokens": 320,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 164,
            "completion_tokens": 182,
            "total_tokens": 346,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 163,
            "completion_tokens": 332,
            "total_tokens": 495,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/4p3/4p3/3B1N2/PPPP1PPP/R1BQ1RK1 b kq - 1 7"
    },
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": "d2d4",
          "aggressive_response": "d4 d5\nd4 d5 Nc3 Nf6 Bg5 e6 e4 dxe4 Nxe4 Be7 Nxf6+ Bxf6 Bxf6 Qxf6 Nf3 O-O Bd3 Nc6 O-O Rd8 c3 e5 dxe5 Nxe5 Nxe5 Qxe5 Bxh7+ Kxh7 Qxd8 b6 Rfe1 Qf5 Qh4+ Kg8 Re8#\nd4 d5 Nc3 Nf6 Bg5 e6 e4 dxe4 Nxe4 Be7 Nxf6+ Bxf6 Bxf6 Qxf6 Nf3 O-O Bd3 Nc6 O-O Rd8 c3 e5 dxe5 Nxe5 Nxe5 Qxe5 Bxh7+ Kxh7 Qxd8 b6 Rfe1 Qf5 Qh4+ Kg8 Re8# d4 d5 Nc3 Nf6 Bg5 e6 e4 dxe4 Nxe4 Be7 Nxf6+ Bxf6 Bxf6 Qxf6 Nf3 O-O Bd3 Nc6 O-O Rd8 c3 e5 dxe5 Nxe5 Nxe5 Qxe5 Bxh7+ Kxh7 Qxd8 b6 Rfe1 Qf5 Qh4+ Kg8 Re8# d4 d5 Nc3 Nf6 Bg5 e6 e4 dxe4 Nxe4 Be7 Nxf6+ Bxf6 Bxf6 Qxf6 Nf3 O-O Bd3 Nc6 O-O Rd8 c3 e5 dxe5 Nxe5 Nxe5 Qxe5 Bxh7+ Kxh7 Qxd8 b6 Rfe1 Qf5 Qh4+ Kg8 Re8# d4 d5 Nc3 Nf6 Bg5 e6 e4 dxe4 Nxe4 Be7 Nxf6+ Bxf6 Bxf6 Qxf6 Nf3 O-O Bd3 Nc6 O-O Rd8 c3 e5 dxe5 Nxe5 Nxe5 Qxe5 Bxh7+ Kxh7 Qxd8 b6 Rfe1 Qf5 Qh4+ Kg8 Re8# d4 d5 Nc3 Nf6 Bg5 e6 e4 dxe4 Nxe4 Be7 Nxf6+ Bxf6 Bxf6 Qxf6 Nf3 O-O Bd3 Nc6 O-O Rd8 c3 e5 dxe5 Nxe5 Nxe5 Qxe5 Bxh7+ Kxh7 Qxd8 b6 Rfe1 Qf5 Qh4+ Kg8 Re8# d4 d5 Nc3 Nf6 Bg5 e6 e4 dxe4 Nxe4 Be7 Nxf6+ Bxf6 Bxf6 Qxf6 Nf3 O-O Bd3 Nc6 O-O Rd8",
          "aggressive_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Nc3",
            "Nf6",
            "Bg5",
            "e6",
            "e4",
            "dxe4",
            "Nxe4",
            "Be7",
            "Nxf6+",
            "Bxf6",
            "Bxf6",
            "Qxf6",
            "Nf3",
            "O-O",
            "Bd3",
            "Nc6",
            "O-O",
            "Rd8"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "Game 1\n1. e4 e5\n2. Nf3 Nc6\n3. Bb5 Bc5\n4. O-O Nf6\n5. d3 d6\n6. Bg5 O-O\n7. Nc3 Bg4\n8. Nd5 Nd4\n9. Bc4 Bxf3\n10. gxf3 c6\n11. Nxf6+ gxf6\n12. Bh6 Re8\n13. f4 Kh8\n14. Bxf7 Re7\n15. Qh5 Nxc2\n16. Rac1 Nd4\n17. Kh1 Qd7\n18. Bc4 d5\n19. exd5 cxd5\n20. Bb3 Nxb3\n21. axb3 Bd4\n22. Rc2 Rg8\n23. Rfc1 Bb6\n24. fxe5 Rxe5\n25. Qh4 Qf5\n26. f4 Re3\n27. Rg2 Rxg2\n28. Kxg2 Re2+\n29. Kf1 Rf2+\n30. Ke1 Qe6+\n31. Kd1 Qe2#\n1. e4 e5\n2. Nf3 Nc6\n3. Bb5 Bc5\n4. O-O Nf6\n5. d3 d6\n6. Bg5 O-O\n7. Nc3 Bg4\n8. Nd5 Nd4\n9. Bc4 Bxf3\n10. gxf3 c6\n11. Nxf6+ gxf6\n12. Bh6 Re8\n13. f4 Kh8\n14. Bxf7 Re7\n15. Qh5 Nxc2\n16. Rac1 Nd4\n17. Kh1 Qd7\n18. Bc4 d5\n19. exd5 cxd5\n20. Bb3 Nxb3\n21. axb3 Bd4\n22. Rc2 Rg8\n23. Rfc1 Bb6\n24. fxe5 Rxe5\n25. Qh4 Qf5\n26. f4 Re3\n27. Rg2 Rxg2\n28. Kxg2 Re2+\n29. Kf1 Rf2+\n30. Ke1 Qe6+\n31. Kd1 Qe2# e4 e5 Nf3 Nc6 Bb5 Bc5 O-O Nf6 d3 d6 Bg5 O-O Nc3 Bg4 Nd5 Nd4 Bc4 Bxf3 gxf3 c6 Nxf6+ gxf6 Bh6 Re8 f4 Kh8 Bxf7 Re7 Qh5 Nxc2 Rc1 Nd4 Kh1 Qd7 Bc4 d5 exd5 cxd5 Bb3 Nxb3 axb3 Bd4 Rc2 Rg8 fxe5 Rxe5 Qh",
          "positional_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bb5",
            "Bc5",
            "O-O",
            "Nf6",
            "d3",
            "d6",
            "Bg5",
            "O-O",
            "Nc3",
            "Bg4",
            "Nd5",
            "Nd4",
            "Bc4",
            "Bxf3",
            "gxf3",
            "c6",
            "Nxf6+",
            "gxf6",
            "Bh6",
            "Re8",
            "f4",
            "Kh8",
            "Bxf7",
            "Re7",
            "Qh5",
            "Nxc2",
            "Rc1",
            "Nd4",
            "Kh1",
            "Qd7",
            "Bc4",
            "d5",
            "exd5",
            "cxd5",
            "Bb3",
            "Nxb3",
            "axb3",
            "Bd4",
            "Rc2",
            "Rg8",
            "fxe5",
            "Rxe5"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": "d2d4",
          "neutral_response": "d4 d5 2. Nf3 Nf6 3. Bg5 Bf5 4. e3 e6 5. Bd3 Bxd3 6. Qxd3 Be7\n7. O-O O-O 8. Nbd2 Nbd7 9. c4 c6 10. Rac1 Rc8 11. cxd5 cxd5\n12. Rxc8 Qxc8 13. Nb3 Qc4 14. Qxc4 dxc4 15. Nbd2 b5 16. Rc1 Rc8\n17. Ne5 Nxe5 18. dxe5 Nd5 19. Bxe7 Nxe7 20. Ne4 Nc6 21. Nd6 Rb8\n22. f4 Nb4 23. a3 Nd3 24. Rc2 Kf8 25. Kf1 Ke7 26. Ke2 f6\n27. Ne4 fxe5 28. fxe5 Nxe5 29. Nc5 Rc8 30. Na6 Nd3 31. Nb4 Nxb4\n32. axb4 Kd6 33. Rd2+ Ke5 34. Rd7 a6 35. Rxg7 c3 36. bxc3 Rxc3\n37. Rxh7 Rc4 38. Ra7 Rxb4 39. Rxa6 Rb2+ 40. Kf3 b4 41. Ra5+ Kf6\n42. Rb5 b3 43. h4 Rb1 44. g4 b2 45. Ke4 Rg1 46. g5+ Kg6\n47. Rxb2 Rg4+ 48. Ke5 Rxh4 49. Rg2 Rh5 50. Kxe6 Rxg5 51. Rxg5+ Kxg5\n52. e4 Kg6 53. e5 Kg7 54. Ke7 Kg8 55. e6 Kg7 56. Kd8 Kf8\n57. e7+ Kf7 58. e8=Q+ Kf6 59. Qe4 Kg5 60. Qf3 Kg6 61. Qf4 Kg7\n62. Qf5 Kg8 63. Qf6 Kh7 64. Qg5 Kh8 65. Ke7 Kh7 66. Kf7 Kh8\nd4 d5 2. Nf3 Nf6 3. Bg5 Bf5 4. e3 e6 5. Bd3 Bxd3 6. Qxd3 Be7\n7. O-O O-O 8. Nbd2 Nbd7 9. c4 c6 10. Rac1 Rc8 11. cxd5 cxd5\n12. Rxc8 Qxc8 13. Nb3 Qc4 14. Qxc4",
          "neutral_tokens": {
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Nf3",
            "Nf6",
            "Bg5",
            "Bf5",
            "e3",
            "e6",
            "Bd3",
            "Bxd3",
            "Qxd3",
            "Be7",
            "O-O",
            "O-O",
            "Nbd2",
            "Nbd7",
            "c4",
            "c6",
            "Rac1",
            "Rc8",
            "cxd5",
            "cxd5",
            "Rxc8",
            "Qxc8",
            "Nb3",
            "Qc4",
            "Qxc4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d2d4",
          "positional_uci": "e2e4",
          "neutral_uci": "d2d4",
          "consensus_move": "d2d4",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d5",
              "Nc3",
              "Nf6",
              "Bg5",
              "e6",
              "e4",
              "dxe4",
              "Nxe4",
              "Be7",
              "Nxf6+",
              "Bxf6",
              "Bxf6",
              "Qxf6",
              "Nf3",
              "O-O",
              "Bd3",
              "Nc6",
              "O-O",
              "Rd8"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bb5",
              "Bc5",
              "O-O",
              "Nf6",
              "d3",
              "d6",
              "Bg5",
              "O-O",
              "Nc3",
              "Bg4",
              "Nd5",
              "Nd4",
              "Bc4",
              "Bxf3",
              "gxf3",
              "c6",
              "Nxf6+",
              "gxf6",
              "Bh6",
              "Re8",
              "f4",
              "Kh8",
              "Bxf7",
              "Re7",
              "Qh5",
              "Nxc2",
              "Rc1",
              "Nd4",
              "Kh1",
              "Qd7",
              "Bc4",
              "d5",
              "exd5",
              "cxd5",
              "Bb3",
              "Nxb3",
              "axb3",
              "Bd4",
              "Rc2",
              "Rg8",
              "fxe5",
              "Rxe5"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "d5",
              "Nf3",
              "Nf6",
              "Bg5",
              "Bf5",
              "e3",
              "e6",
              "Bd3",
              "Bxd3",
              "Qxd3",
              "Be7",
              "O-O",
              "O-O",
              "Nbd2",
              "Nbd7",
              "c4",
              "c6",
              "Rac1",
              "Rc8",
              "cxd5",
              "cxd5",
              "Rxc8",
              "Qxc8",
              "Nb3",
              "Qc4",
              "Qxc4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 329,
          "total_completion_tokens": 1920,
          "total_tokens": 2249
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 110,
            "completion_tokens": 640,
            "total_tokens": 750,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e5",
          "aggressive_uci": "e7e5",
          "aggressive_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6\n4. Ba4 Nf6 5. O-O Be7 6. Re1 b5\n7. Bb3 d6 8. c3 O-O 9. h3 Bb7\n10. d4 Re8 11. Nbd2 Bf8 12. a4 h6\n13. Bc2 exd4 14. cxd4 Nb4 15. Bb1 c5\n16. d5 Nd7 17. Ra3 c4 18. Nd4 Qf6\n19. N2f3 Nc5 20. axb5 Nbd3 21. Bxd3 Nxd3\n22. Re3 axb5 23. Nxb5 Rxa3 24. Nxa3 Ba6\n25. Qa4 Ra8 26. Nxc4 Nxc1 27. Qc6 Rc8\n28. Qxa6 Qd8 29. Rc3 Ne2+ 30. Kf1 Nxc3\n31. bxc3 Ra8 32. Qb5 Ra1+ 33. Ke2 Qa8\n34. Nfd2 Qa2 35. Qb3 Qa6 36. Qb6 Qa2\n37. Qb3 Qa6 38. Qb6 Qa2 39. Qb3 Qa6\n40. Qb6 Qa2 41. Qb3 Qa6 42. Qb6 Qa2\n43. Qb3 Qa6 44. Qb6 Qa2 45. Qb3 Qa6\n46. Qb6 Qa2 47. Qb3 Qa6 48. Qb6 Qa2\n49. Qb3 Qa6 50. Qb6 Qa2 51. Qb3 Qa6\n52. Qb6 Qa2 53. Qb3 Qa6 54. Qb6 Qa2\n55. Qb3 Qa6 56. Qb6 Qa2 57. Qb3 Qa6\n58. Qb6 Qa2 59. Qb3 Qa6 60. Qb6 Qa2\n61. Qb3 Qa6 62. Qb6 Qa2 63. Qb3 Qa6\n64. Qb6 Qa2 65. Qb3 Qa6 66. Qb6 Qa2\n67. Qb3 Qa6 68. Qb6 Qa2 69. Qb3 Qa6\n70. Qb6 Qa2 71. Qb3 Qa6 72. Qb6 Qa2\n73. Qb3 Qa6 74. Qb6 Qa2 75. Qb",
          "aggressive_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Bb7",
            "d4",
            "Re8",
            "Nbd2",
            "Bf8",
            "a4",
            "h6",
            "Bc2",
            "exd4",
            "cxd4",
            "Nb4",
            "Bb1",
            "c5",
            "d5",
            "Nd7",
            "Ra3",
            "c4",
            "Nd4",
            "Qf6",
            "N2f3",
            "Nc5",
            "axb5",
            "Nbd3",
            "Bxd3",
            "Nxd3",
            "Re3",
            "axb5",
            "Nxb5",
            "Rxa3",
            "Nxa3",
            "Ba6",
            "Qa4",
            "Ra8",
            "Nxc4",
            "Nxc1",
            "Qc6",
            "Rc8",
            "Qxa6",
            "Qd8",
            "Rc3",
            "Ne2+",
            "Kf1",
            "Nxc3",
            "bxc3",
            "Ra8",
            "Qb5",
            "Ra1+",
            "Ke2",
            "Qa8",
            "Nfd2",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2",
            "Qb3",
            "Qa6",
            "Qb6",
            "Qa2"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e5",
          "positional_uci": "e7e5",
          "positional_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7\n6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5\n11. d4 Qc7 12. Nbd2 cxd4 13. cxd4 Nc6 14. Nb3 a5\n15. Be3 a4 16. Nbd2 Bd7 17. Rc1 Qb7 18. Bb1 Rfc8\n19. Nf1 Bd8 20. Ng3 Bb6 21. Qd2 Ba5 22. Qe2 Bxe1\n23. Rxe1 Re8 24. Qd2 Rac8 25. Bg5 Kh8 26. Bxf6 gxf6\n27. Qh6 Rg8 28. Qxf6+ Rg7 29. Nh5 Rg8 30. Ng5 Be8\n31. Nxg7 Rxg7 32. dxe5 dxe5 33. Rd1 Qe7 34. Qxe7 Nxe7\n35. Rd8 Rg8 36. Nf3 f6 37. Bd3 Kg7 38. Rb8 Bf7\n39. Rxb5 Bxa2 40. Rb7 Kf8 41. Ra7 Bb3 42. Nd2 Bd1\n43. Bc4 Rg5 44. Ra8+ Kg7 45. Re8 Nc6 46. Rg8+ Kh6\n47. Rxg5 Kxg5 48. Bb5 Nd4 49. Bd7 Kf4 50. g3+ Kg5\n51. h4+ Kg6 52. Kg2 Kf7 53. f4 Ke7 54. Bc8 Kd6\n55. Kf2 Kc5 56. Ke3 Kb4 57. Kd3 Bc2+ 58. Ke3 Bd1\n59. Kd3 Bc2+ 60. Ke3 Bd1 61. Kd3 Bc2+ 62. Ke3 Bd1\n63. Kd3 Bc2+ 64. Ke3 Bd1 65. Kd3 Bc2+ 66. Ke3 Bd1\n67. Kd3 Bc2+ 68. Ke3 Bd1 69. Kd3 Bc2+ 70. Ke3 Bd1\n71. Kd3 Bc2+ 72. Ke3 Bd1 73. Kd3 Bc2+ 74. Ke3 Bd1\n75. Kd3 Bc2+ 76. Ke3 Bd1 77. Kd3 Bc2+ 78. Ke3",
          "positional_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Qc7",
            "Nbd2",
            "cxd4",
            "cxd4",
            "Nc6",
            "Nb3",
            "a5",
            "Be3",
            "a4",
            "Nbd2",
            "Bd7",
            "Rc1",
            "Qb7",
            "Bb1",
            "Rfc8",
            "Nf1",
            "Bd8",
            "Ng3",
            "Bb6",
            "Qd2",
            "Ba5",
            "Qe2",
            "Bxe1",
            "Rxe1",
            "Re8",
            "Qd2",
            "Rac8",
            "Bg5",
            "Kh8",
            "Bxf6",
            "gxf6",
            "Qh6",
            "Rg8",
            "Qxf6+",
            "Rg7",
            "Nh5",
            "Rg8",
            "Ng5",
            "Be8",
            "Nxg7",
            "Rxg7",
            "dxe5",
            "dxe5",
            "Rd1",
            "Qe7",
            "Qxe7",
            "Nxe7",
            "Rd8",
            "Rg8",
            "Nf3",
            "f6",
            "Bd3",
            "Kg7",
            "Rb8",
            "Bf7",
            "Rxb5",
            "Bxa2",
            "Rb7",
            "Kf8",
            "Ra7",
            "Bb3",
            "Nd2",
            "Bd1",
            "Bc4",
            "Rg5",
            "Ra8+",
            "Kg7",
            "Re8",
            "Nc6",
            "Rg8+",
            "Kh6",
            "Rxg5",
            "Kxg5",
            "Bb5",
            "Nd4",
            "Bd7",
            "Kf4",
            "g3+",
            "Kg5",
            "h4+",
            "Kg6",
            "Kg2",
            "Kf7",
            "f4",
            "Ke7",
            "Bc8",
            "Kd6",
            "Kf2",
            "Kc5",
            "Ke3",
            "Kb4",
            "Kd3",
            "Bc2+",
            "Ke3",
            "Bd1",
            "Kd3",
            "Bc2+",
            "Ke3",
            "Bd1",
            "Kd3",
            "Bc2+",
            "Ke3",
            "Bd1",
            "Kd3",
            "Bc2+",
            "Ke3",
            "Bd1",
            "Kd3",
            "Bc2+",
            "Ke3",
            "Bd1",
            "Kd3",
            "Bc2+",
            "Ke3",
            "Bd1",
            "Kd3",
            "Bc2+",
            "Ke3",
            "Bd1",
            "Kd3",
            "Bc2+",
            "Ke3",
            "Bd1",
            "Kd3",
            "Bc2+",
            "Ke3",
            "Bd1",
            "Kd3",
            "Bc2+",
            "Ke3",
            "Bd1",
            "Kd3",
            "Bc2+",
            "Ke3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e5",
          "neutral_uci": "e7e5",
          "neutral_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6\n4. Ba4 Nf6 5. O-O Be7 6. Re1 b5\n7. Bb3 d6 8. c3 O-O 9. h3 Bb7\n10. d4 Re8 11. Nbd2 Bf8 12. a4 h6\n13. Bc2 exd4 14. cxd4 Nb4 15. Bb1 c5\n16. d5 Nd7 17. Ra3 f5 18. Rae3 fxe4\n19. Nxe4 Nxd5 20. Ba2 c4 21. Nxd6 Rxe3\n22. Nxb7 Rxe1+ 23. Nxe1 Qe7 24. Qxd5+ Kh8\n25. Nf3 Nf6 26. Qc6 Ra7 27. Na5 Qd8\n28. Bd2 Rd7 29. Bc3 Rd1+ 30. Kh2 Bd6+\n31. g3 Bc7 32. Nb7 Qd7 33. Qxa6 Qd5\n34. Kg2 Rd3 35. Qa8+ Kh7 36. Bb1 Qxf3+\n37. Kh2 Qxf2+ 38. Kh1 Qf1+ 39. Kh2 Bxg3#\n40. Kh1 Qxh3+ 41. Kg1 Qh2+ 42. Kf1 Qf2#",
          "neutral_tokens": {
            "prompt_tokens": 113,
            "completion_tokens": 345,
            "total_tokens": 458,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Bb7",
            "d4",
            "Re8",
            "Nbd2",
            "Bf8",
            "a4",
            "h6",
            "Bc2",
            "exd4",
            "cxd4",
            "Nb4",
            "Bb1",
            "c5",
            "d5",
            "Nd7",
            "Ra3",
            "f5",
            "Rae3",
            "fxe4",
            "Nxe4",
            "Nxd5",
            "Ba2",
            "c4",
            "Nxd6",
            "Rxe3",
            "Nxb7",
            "Rxe1+",
            "Nxe1",
            "Qe7",
            "Qxd5+",
            "Kh8",
            "Nf3",
            "Nf6",
            "Qc6",
            "Ra7",
            "Na5",
            "Qd8",
            "Bd2",
            "Rd7",
            "Bc3",
            "Rd1+",
            "Kh2",
            "Bd6+",
            "g3",
            "Bc7",
            "Nb7",
            "Qd7",
            "Qxa6",
            "Qd5",
            "Kg2",
            "Rd3",
            "Qa8+",
            "Kh7",
            "Bb1",
            "Qxf3+",
            "Kh2",
            "Qxf2+",
            "Kh1",
            "Qf1+",
            "Kh2",
            "Bxg3#",
            "Kh1",
            "Qxh3+",
            "Kg1",
            "Qh2+",
            "Kf1",
            "Qf2#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e7e5",
          "positional_uci": "e7e5",
          "neutral_uci": "e7e5",
          "consensus_move": "e7e5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Bb7",
              "d4",
              "Re8",
              "Nbd2",
              "Bf8",
              "a4",
              "h6",
              "Bc2",
              "exd4",
              "cxd4",
              "Nb4",
              "Bb1",
              "c5",
              "d5",
              "Nd7",
              "Ra3",
              "c4",
              "Nd4",
              "Qf6",
              "N2f3",
              "Nc5",
              "axb5",
              "Nbd3",
              "Bxd3",
              "Nxd3",
              "Re3",
              "axb5",
              "Nxb5",
              "Rxa3",
              "Nxa3",
              "Ba6",
              "Qa4",
              "Ra8",
              "Nxc4",
              "Nxc1",
              "Qc6",
              "Rc8",
              "Qxa6",
              "Qd8",
              "Rc3",
              "Ne2+",
              "Kf1",
              "Nxc3",
              "bxc3",
              "Ra8",
              "Qb5",
              "Ra1+",
              "Ke2",
              "Qa8",
              "Nfd2",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2",
              "Qb3",
              "Qa6",
              "Qb6",
              "Qa2"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Qc7",
              "Nbd2",
              "cxd4",
              "cxd4",
              "Nc6",
              "Nb3",
              "a5",
              "Be3",
              "a4",
              "Nbd2",
              "Bd7",
              "Rc1",
              "Qb7",
              "Bb1",
              "Rfc8",
              "Nf1",
              "Bd8",
              "Ng3",
              "Bb6",
              "Qd2",
              "Ba5",
              "Qe2",
              "Bxe1",
              "Rxe1",
              "Re8",
              "Qd2",
              "Rac8",
              "Bg5",
              "Kh8",
              "Bxf6",
              "gxf6",
              "Qh6",
              "Rg8",
              "Qxf6+",
              "Rg7",
              "Nh5",
              "Rg8",
              "Ng5",
              "Be8",
              "Nxg7",
              "Rxg7",
              "dxe5",
              "dxe5",
              "Rd1",
              "Qe7",
              "Qxe7",
              "Nxe7",
              "Rd8",
              "Rg8",
              "Nf3",
              "f6",
              "Bd3",
              "Kg7",
              "Rb8",
              "Bf7",
              "Rxb5",
              "Bxa2",
              "Rb7",
              "Kf8",
              "Ra7",
              "Bb3",
              "Nd2",
              "Bd1",
              "Bc4",
              "Rg5",
              "Ra8+",
              "Kg7",
              "Re8",
              "Nc6",
              "Rg8+",
              "Kh6",
              "Rxg5",
              "Kxg5",
              "Bb5",
              "Nd4",
              "Bd7",
              "Kf4",
              "g3+",
              "Kg5",
              "h4+",
              "Kg6",
              "Kg2",
              "Kf7",
              "f4",
              "Ke7",
              "Bc8",
              "Kd6",
              "Kf2",
              "Kc5",
              "Ke3",
              "Kb4",
              "Kd3",
              "Bc2+",
              "Ke3",
              "Bd1",
              "Kd3",
              "Bc2+",
              "Ke3",
              "Bd1",
              "Kd3",
              "Bc2+",
              "Ke3",
              "Bd1",
              "Kd3",
              "Bc2+",
              "Ke3",
              "Bd1",
              "Kd3",
              "Bc2+",
              "Ke3",
              "Bd1",
              "Kd3",
              "Bc2+",
              "Ke3",
              "Bd1",
              "Kd3",
              "Bc2+",
              "Ke3",
              "Bd1",
              "Kd3",
              "Bc2+",
              "Ke3",
              "Bd1",
              "Kd3",
              "Bc2+",
              "Ke3",
              "Bd1",
              "Kd3",
              "Bc2+",
              "Ke3",
              "Bd1",
              "Kd3",
              "Bc2+",
              "Ke3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Bb7",
              "d4",
              "Re8",
              "Nbd2",
              "Bf8",
              "a4",
              "h6",
              "Bc2",
              "exd4",
              "cxd4",
              "Nb4",
              "Bb1",
              "c5",
              "d5",
              "Nd7",
              "Ra3",
              "f5",
              "Rae3",
              "fxe4",
              "Nxe4",
              "Nxd5",
              "Ba2",
              "c4",
              "Nxd6",
              "Rxe3",
              "Nxb7",
              "Rxe1+",
              "Nxe1",
              "Qe7",
              "Qxd5+",
              "Kh8",
              "Nf3",
              "Nf6",
              "Qc6",
              "Ra7",
              "Na5",
              "Qd8",
              "Bd2",
              "Rd7",
              "Bc3",
              "Rd1+",
              "Kh2",
              "Bd6+",
              "g3",
              "Bc7",
              "Nb7",
              "Qd7",
              "Qxa6",
              "Qd5",
              "Kg2",
              "Rd3",
              "Qa8+",
              "Kh7",
              "Bb1",
              "Qxf3+",
              "Kh2",
              "Qxf2+",
              "Kh1",
              "Qf1+",
              "Kh2",
              "Bxg3#",
              "Kh1",
              "Qxh3+",
              "Kg1",
              "Qh2+",
              "Kf1",
              "Qf2#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 113,
            "completion_tokens": 345,
            "total_tokens": 458,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 341,
          "total_completion_tokens": 1625,
          "total_tokens": 1966
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 113,
            "completion_tokens": 345,
            "total_tokens": 458,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 4,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc6",
          "aggressive_uci": "b8c6",
          "aggressive_response": "2... Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 O-O 8. c3 d5\n9. exd5 Nxd5 10. Nxe5 Nxe5 11. Rxe5 c6 12. d4 Bd6 13. Re1 Qh4 14. g3 Qh3\n15. Be3 Bg4 16. Qd3 Rae8 17. Nd2 Re6 18. Qf1 Qh5 19. a4 Rfe8 20. axb5 axb5\n21. Bxd5 Qxd5 22. Qg2 Qh5 23. Qxc6 Bxg3 24. Qg2 Bf4 25. Ra8 Bb8 26. Rea1 Rg6\n27. R1a6 Be6 28. Qxg6 Qxg6+ 29. Kf1 Qd3+ 30. Ke1 Bg4 31. f3 Qxe3+ 32. Kd1 Bxf3+\n33. Nxf3 Qxf3+ 34. Kc2 Qe2+ 35. Kb3 Qc4+ 36. Ka3 b4+ 37. cxb4 Re3+ 38. Ka4 Qb3+ 39. Kb5 Qd5+ 40. Ka4 Qd7+ 41. b5 Qxd4+ 42. b4 Qa1# Bb7 Rxa8# Rxa8#",
          "aggressive_tokens": {
            "prompt_tokens": 122,
            "completion_tokens": 362,
            "total_tokens": 484,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "O-O",
            "c3",
            "d5",
            "exd5",
            "Nxd5",
            "Nxe5",
            "Nxe5",
            "Rxe5",
            "c6",
            "d4",
            "Bd6",
            "Re1",
            "Qh4",
            "g3",
            "Qh3",
            "Be3",
            "Bg4",
            "Qd3",
            "Rae8",
            "Nd2",
            "Re6",
            "Qf1",
            "Qh5",
            "a4",
            "Rfe8",
            "axb5",
            "axb5",
            "Bxd5",
            "Qxd5",
            "Qg2",
            "Qh5",
            "Qxc6",
            "Bxg3",
            "Qg2",
            "Bf4",
            "Ra8",
            "Bb8",
            "Rea1",
            "Rg6",
            "R1a6",
            "Be6",
            "Qxg6",
            "Qxg6+",
            "Kf1",
            "Qd3+",
            "Ke1",
            "Bg4",
            "f3",
            "Qxe3+",
            "Kd1",
            "Bxf3+",
            "Nxf3",
            "Qxf3+",
            "Kc2",
            "Qe2+",
            "Kb3",
            "Qc4+",
            "Ka3",
            "b4+",
            "cxb4",
            "Re3+",
            "Ka4",
            "Qb3+",
            "Kb5",
            "Qd5+",
            "Ka4",
            "Qd7+",
            "b5",
            "Qxd4+",
            "b4",
            "Qa1#",
            "Bb7",
            "Rxa8#",
            "Rxa8#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nc6",
          "positional_uci": "b8c6",
          "positional_response": "2... Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 O-O 8. c3 d5\n9. exd5 Nxd5 10. Nxe5 Nxe5 11. Rxe5 c6 12. d4 Bd6 13. Re1 Qh4 14. g3 Qh3\n15. Be3 Bg4 16. Qd3 Rae8 17. Nd2 Re6 18. Qf1 Qh5 19. a4 Rfe8 20. axb5 axb5\n21. Bxd5 Qxd5 22. Qg2 Qh5 23. Ra6 Bf8 24. Raa1 h6 25. Ra6 Bh3 26. Qf3 Bg4\n27. Qg2 Bh3 28. Qf3 Bg4 29. Qg2 Bh3 30. Qf3 Bg4 31. Qg2 Bh3 32. Qf3 Bg4\n33. Qg2 Bh3 34. Qf3 Bg4 35. Qg2 Bh3 36. Qf3 Bg4 37. Qg2 Bh3 38. Qf3 Bg4\n39. Qg2 Bh3 40. Qf3 Bg4 41. Qg2 Bh3 42. Qf3 Bg4 43. Qg2 Bh3 44. Qf3 Bg4\n45. Qg2 Bh3 46. Qf3 Bg4 47. Qg2 Bh3 48. Qf3 Bg4 49. Qg2 Bh3 50. Qf3 Bg4\n51. Qg2 Bh3 52. Qf3 Bg4 53. Qg2 Bh3 54. Qf3 Bg4 55. Qg2 Bh3 56. Qf3 Bg4\n57. Qg2 Bh3 58. Qf3 Bg4 59. Qg2 Bh3 60. Qf3 Bg4 61. Qg2 Bh3 62. Qf3 Bg4\n63. Qg2 Bh3 64. Qf3 Bg4 65. Qg2 Bh3 66. Qf3 Bg4 67. Qg2 Bh3 68. Qf3 Bg4\n69. Qg2 Bh3 70. Qf3 Bg4 71. Qg2 Bh3 72. Qf3 Bg4 73. Qg2 Bh3 74. Qf3 Bg4\n75. Qg2 Bh3 76. Qf3 Bg4 77. Qg2 Bh3 78. Qf3 Bg",
          "positional_tokens": {
            "prompt_tokens": 122,
            "completion_tokens": 640,
            "total_tokens": 762,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "O-O",
            "c3",
            "d5",
            "exd5",
            "Nxd5",
            "Nxe5",
            "Nxe5",
            "Rxe5",
            "c6",
            "d4",
            "Bd6",
            "Re1",
            "Qh4",
            "g3",
            "Qh3",
            "Be3",
            "Bg4",
            "Qd3",
            "Rae8",
            "Nd2",
            "Re6",
            "Qf1",
            "Qh5",
            "a4",
            "Rfe8",
            "axb5",
            "axb5",
            "Bxd5",
            "Qxd5",
            "Qg2",
            "Qh5",
            "Ra6",
            "Bf8",
            "Raa1",
            "h6",
            "Ra6",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc6",
          "neutral_uci": "b8c6",
          "neutral_response": "2... Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 O-O 8. c3 d5\n9. exd5 Nxd5 10. Nxe5 Nxe5 11. Rxe5 c6 12. d4 Bd6 13. Re1 Qh4 14. g3 Qh3\n15. Be3 Bg4 16. Qd3 Rae8 17. Nd2 Re6 18. Qf1 Qh5 19. a4 Rfe8 20. axb5 axb5\n21. Bxd5 Qxd5 22. Qg2 Qh5 23. Ra6 Bf8 24. Rxc6 Bh3 25. Qh1 Qf5 26. Rxe6 Rxe6\n27. Qf3 Qc2 28. Qd1 Qxb2 29. Qb3 Qxb3 30. Nxb3 Rc6 31. Bd2 Be6 32. Na5 Ra6\n33. Nb7 Ra2 34. Be3 Rc2 35. Rc1 Rxc1+ 36. Bxc1 b4 37. cxb4 Bxb4 38. Nc5 Bd5\n39. Be3 f6 40. Nd3 Bd6 41. Bf4 Ba3 42. Bc1 Bd6 43. Bf4 Ba3 44. Bc1 Bd6\n45. Bf4 Ba3 46. Bc1 Bd6 47. Bf4 Ba3 48. Bc1 Bd6 49. Bf4 Ba3 50. Bc1 Bd6\n51. Bf4 Ba3 52. Bc1 Bd6 53. Bf4 Ba3 54. Bc1 Bd6 55. Bf4 Ba3 56. Bc1 Bd6\n57. Bf4 Ba3 58. Bc1 Bd6 59. Bf4 Ba3 60. Bc1 Bd6 61. Bf4 Ba3 62. Bc1 Bd6\n63. Bf4 Ba3 64. Bc1 Bd6 65. Bf4 Ba3 66. Bc1 Bd6 67. Bf4 Ba3 68. Bc1 Bd6\n69. Bf4 Ba3 70. Bc1 Bd6 71. Bf4 Ba3 72. Bc1 Bd6 73. Bf4 Ba3 74. Bc1 Bd6\n75. Bf4 Ba3 76. Bc1 Bd6 77. Bf4 Ba3 78. Bc1 Bd6 79. Bf4 Ba3 80. Bc1 Bd6\n81. Bf4 Ba3",
          "neutral_tokens": {
            "prompt_tokens": 121,
            "completion_tokens": 640,
            "total_tokens": 761,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "O-O",
            "c3",
            "d5",
            "exd5",
            "Nxd5",
            "Nxe5",
            "Nxe5",
            "Rxe5",
            "c6",
            "d4",
            "Bd6",
            "Re1",
            "Qh4",
            "g3",
            "Qh3",
            "Be3",
            "Bg4",
            "Qd3",
            "Rae8",
            "Nd2",
            "Re6",
            "Qf1",
            "Qh5",
            "a4",
            "Rfe8",
            "axb5",
            "axb5",
            "Bxd5",
            "Qxd5",
            "Qg2",
            "Qh5",
            "Ra6",
            "Bf8",
            "Rxc6",
            "Bh3",
            "Qh1",
            "Qf5",
            "Rxe6",
            "Rxe6",
            "Qf3",
            "Qc2",
            "Qd1",
            "Qxb2",
            "Qb3",
            "Qxb3",
            "Nxb3",
            "Rc6",
            "Bd2",
            "Be6",
            "Na5",
            "Ra6",
            "Nb7",
            "Ra2",
            "Be3",
            "Rc2",
            "Rc1",
            "Rxc1+",
            "Bxc1",
            "b4",
            "cxb4",
            "Bxb4",
            "Nc5",
            "Bd5",
            "Be3",
            "f6",
            "Nd3",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3",
            "Bc1",
            "Bd6",
            "Bf4",
            "Ba3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b8c6",
          "positional_uci": "b8c6",
          "neutral_uci": "b8c6",
          "consensus_move": "b8c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "O-O",
              "c3",
              "d5",
              "exd5",
              "Nxd5",
              "Nxe5",
              "Nxe5",
              "Rxe5",
              "c6",
              "d4",
              "Bd6",
              "Re1",
              "Qh4",
              "g3",
              "Qh3",
              "Be3",
              "Bg4",
              "Qd3",
              "Rae8",
              "Nd2",
              "Re6",
              "Qf1",
              "Qh5",
              "a4",
              "Rfe8",
              "axb5",
              "axb5",
              "Bxd5",
              "Qxd5",
              "Qg2",
              "Qh5",
              "Qxc6",
              "Bxg3",
              "Qg2",
              "Bf4",
              "Ra8",
              "Bb8",
              "Rea1",
              "Rg6",
              "R1a6",
              "Be6",
              "Qxg6",
              "Qxg6+",
              "Kf1",
              "Qd3+",
              "Ke1",
              "Bg4",
              "f3",
              "Qxe3+",
              "Kd1",
              "Bxf3+",
              "Nxf3",
              "Qxf3+",
              "Kc2",
              "Qe2+",
              "Kb3",
              "Qc4+",
              "Ka3",
              "b4+",
              "cxb4",
              "Re3+",
              "Ka4",
              "Qb3+",
              "Kb5",
              "Qd5+",
              "Ka4",
              "Qd7+",
              "b5",
              "Qxd4+",
              "b4",
              "Qa1#",
              "Bb7",
              "Rxa8#",
              "Rxa8#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "O-O",
              "c3",
              "d5",
              "exd5",
              "Nxd5",
              "Nxe5",
              "Nxe5",
              "Rxe5",
              "c6",
              "d4",
              "Bd6",
              "Re1",
              "Qh4",
              "g3",
              "Qh3",
              "Be3",
              "Bg4",
              "Qd3",
              "Rae8",
              "Nd2",
              "Re6",
              "Qf1",
              "Qh5",
              "a4",
              "Rfe8",
              "axb5",
              "axb5",
              "Bxd5",
              "Qxd5",
              "Qg2",
              "Qh5",
              "Ra6",
              "Bf8",
              "Raa1",
              "h6",
              "Ra6",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "O-O",
              "c3",
              "d5",
              "exd5",
              "Nxd5",
              "Nxe5",
              "Nxe5",
              "Rxe5",
              "c6",
              "d4",
              "Bd6",
              "Re1",
              "Qh4",
              "g3",
              "Qh3",
              "Be3",
              "Bg4",
              "Qd3",
              "Rae8",
              "Nd2",
              "Re6",
              "Qf1",
              "Qh5",
              "a4",
              "Rfe8",
              "axb5",
              "axb5",
              "Bxd5",
              "Qxd5",
              "Qg2",
              "Qh5",
              "Ra6",
              "Bf8",
              "Rxc6",
              "Bh3",
              "Qh1",
              "Qf5",
              "Rxe6",
              "Rxe6",
              "Qf3",
              "Qc2",
              "Qd1",
              "Qxb2",
              "Qb3",
              "Qxb3",
              "Nxb3",
              "Rc6",
              "Bd2",
              "Be6",
              "Na5",
              "Ra6",
              "Nb7",
              "Ra2",
              "Be3",
              "Rc2",
              "Rc1",
              "Rxc1+",
              "Bxc1",
              "b4",
              "cxb4",
              "Bxb4",
              "Nc5",
              "Bd5",
              "Be3",
              "f6",
              "Nd3",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3",
              "Bc1",
              "Bd6",
              "Bf4",
              "Ba3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 122,
            "completion_tokens": 362,
            "total_tokens": 484,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 122,
            "completion_tokens": 640,
            "total_tokens": 762,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 121,
            "completion_tokens": 640,
            "total_tokens": 761,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 365,
          "total_completion_tokens": 1642,
          "total_tokens": 2007
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 122,
            "completion_tokens": 362,
            "total_tokens": 484,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 122,
            "completion_tokens": 640,
            "total_tokens": 762,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 121,
            "completion_tokens": 640,
            "total_tokens": 761,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R b KQkq - 1 2"
    },
    {
      "turn": 6,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf6",
          "aggressive_uci": "g8f6",
          "aggressive_response": "3... Nf6 4. d3 Bc5 5. O-O O-O 6. Nc3 d6 7. Bg5 h6 8. Bh4 Bg4 9. h3 Bh5 10. g4 Bg6 11. Nd5 Nd4 12. Nxd4 Bxd4 13. c3 Bb6 14. Qf3 c6 15. Nxf6+ gxf6 16. Bxf6 Qd7 17. h4 Bd8 18. h5 Bxf6 19. hxg6 Kg7 20. gxf7 Bg5 21. Qf5 Qxf5 22. gxf5 Kf6 23. Kg2 Rad8 24. Be6 d5 25. Rad1 Rd6 26. Kf3 Rfd8 27. exd5 cxd5 28. Kg4 d4 29. c4 Rb6 30. b3 Ra6 31. a4 Rb6 32. Rb1 Bd2 33. Rh1 Rh8 34. Rh5 Bg5 35. Kf3 Rd6 36. Ke4 Rdd8 37. Rbh1 Rh7 38. Rg1 Rg7 39. Rg4 Rh7 40. f4 exf4 41. Rxf4 Bxf4 42. Kxf4 Rg7 43. Rxh6+ Ke7 44. f6+ Kxe6 45. fxg7+ Kxf7 46. Rh8 Rg8 47. Rxg8 Kxg8 48. Ke5 Kxg7 49. Kxd4 Kf7 50. Kd5 Ke7 51. d4 Kd7 52. b4 Kc7 53. c5 Kd7 54. b5 Kc7 55. a5 Kd7 56. c6+ bxc6+ 57. bxc6+ Kc7 58. Kc5 a6 59. d5 Kc8 60. d6 Kd8 61. Kb6 Kc8 62. Kxa6 Kb8 63. Kb6 Kc8 64. a6 Kb8 65. d7 Ka8 66. d8=Q# Kxd8 67. a7 Kc8 68. a8=Q# Kd7 69. c7 Kd6 70. c8=Q Ke5 71. Qac6 Kd4 72. Q8d7+ Ke5 73. Qce6+ Kf4 74. Qdf7+ Kg5 75. Qeg6+ Kh4 76. Qfh7# Kxg6 77. Qf7+ Kg5 78. Kc5 Kg4 79. K",
          "aggressive_tokens": {
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d3",
            "Bc5",
            "O-O",
            "O-O",
            "Nc3",
            "d6",
            "Bg5",
            "h6",
            "Bh4",
            "Bg4",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Nd5",
            "Nd4",
            "Nxd4",
            "Bxd4",
            "c3",
            "Bb6",
            "Qf3",
            "c6",
            "Nxf6+",
            "gxf6",
            "Bxf6",
            "Qd7",
            "h4",
            "Bd8",
            "h5",
            "Bxf6",
            "hxg6",
            "Kg7",
            "gxf7",
            "Bg5",
            "Qf5",
            "Qxf5",
            "gxf5",
            "Kf6",
            "Kg2",
            "Rad8",
            "Be6",
            "d5",
            "Rad1",
            "Rd6",
            "Kf3",
            "Rfd8",
            "exd5",
            "cxd5",
            "Kg4",
            "d4",
            "c4",
            "Rb6",
            "b3",
            "Ra6",
            "a4",
            "Rb6",
            "Rb1",
            "Bd2",
            "Rh1",
            "Rh8",
            "Rh5",
            "Bg5",
            "Kf3",
            "Rd6",
            "Ke4",
            "Rdd8",
            "Rbh1",
            "Rh7",
            "Rg1",
            "Rg7",
            "Rg4",
            "Rh7",
            "f4",
            "exf4",
            "Rxf4",
            "Bxf4",
            "Kxf4",
            "Rg7",
            "Rxh6+",
            "Ke7",
            "f6+",
            "Kxe6",
            "fxg7+",
            "Kxf7",
            "Rh8",
            "Rg8",
            "Rxg8",
            "Kxg8",
            "Ke5",
            "Kxg7",
            "Kxd4",
            "Kf7",
            "Kd5",
            "Ke7",
            "d4",
            "Kd7",
            "b4",
            "Kc7",
            "c5",
            "Kd7",
            "b5",
            "Kc7",
            "a5",
            "Kd7",
            "c6+",
            "bxc6+",
            "bxc6+",
            "Kc7",
            "Kc5",
            "a6",
            "d5",
            "Kc8",
            "d6",
            "Kd8",
            "Kb6",
            "Kc8",
            "Kxa6",
            "Kb8",
            "Kb6",
            "Kc8",
            "a6",
            "Kb8",
            "d7",
            "Ka8",
            "d8=Q#",
            "Kxd8",
            "a7",
            "Kc8",
            "a8=Q#",
            "Kd7",
            "c7",
            "Kd6",
            "c8=Q",
            "Ke5",
            "Qac6",
            "Kd4",
            "Q8d7+",
            "Ke5",
            "Qce6+",
            "Kf4",
            "Qdf7+",
            "Kg5",
            "Qeg6+",
            "Kh4",
            "Qfh7#",
            "Kxg6",
            "Qf7+",
            "Kg5",
            "Kc5",
            "Kg4"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf6",
          "positional_uci": "g8f6",
          "positional_response": "3... Nf6 4. d3 Bc5 5. O-O O-O 6. Nc3 d6 7. Bg5 Be6 8. Nd5 Bxd5 9. Bxd5 h6 10. Bh4 g5 11. Bg3 Nxd5 12. exd5 Ne7 13. c4 f5 14. h3 f4 15. Bh2 Nf5 16. a3 a5 17. Qd2 Qf6 18. b4 axb4 19. axb4 Rxa1 20. Rxa1 e4 21. dxe4 Qxa1+ 22. Ne1 Ng3 23. bxc5 Nxe4 24. Qe2 Re8 25. cxd6 Nxd6 26. Qxe8+ Nxe8 27. Kf1 Nd6 28. c5 Ne4 29. Ke2 Qb2+ 30. Kf3 Nd2+ 31. Kg4 Qf6 32. h4 Kg7 33. hxg5 hxg5 34. Nf3 Nxf3 35. Kxf3 Qc3+ 36. Kg4 Kg6 37. g3 Qxc5 38. gxf4 Qxf2 39. Bg3 Qe2+ 40. Kh3 g4+ 41. Kh4 Kf5 42. Kh5 Qe8+ 43. Kh6 Qg6# 0-1\n1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. d3 Bc5 5. O-O O-O 6. Nc3 d6 7. Bg5 Be6 8. Nd5 Bxd5 9. Bxd5 h6 10. Bh4 g5 11. Bg3 Nxd5 12. exd5 Ne7 13. c4 f5 14. h3 f4 15. Bh2 Nf5 16. a3 a5 17. Qd2 Qf6 18. b4 axb4 19. axb4 Rxa1 20. Rxa1 e4 21. dxe4 Qxa1+ 22. Ne1 Ng3 23. bxc5 Nxe4 24. Qe2 Re8 25. cxd6 Nxd6 26. Qxe8+ Nxe8 27. Kf1 Nd6 28. c5 Ne4 29. Ke2 Qb2+ 30. Kf3 Nd2+ 31. Kg4 Qf6 32. h4 Kg7 33. hxg5 hxg5 34. Nf3 Nxf3 35. Kxf3 Qc3+ 36. Kg4 Kg6 37. g",
          "positional_tokens": {
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d3",
            "Bc5",
            "O-O",
            "O-O",
            "Nc3",
            "d6",
            "Bg5",
            "Be6",
            "Nd5",
            "Bxd5",
            "Bxd5",
            "h6",
            "Bh4",
            "g5",
            "Bg3",
            "Nxd5",
            "exd5",
            "Ne7",
            "c4",
            "f5",
            "h3",
            "f4",
            "Bh2",
            "Nf5",
            "a3",
            "a5",
            "Qd2",
            "Qf6",
            "b4",
            "axb4",
            "axb4",
            "Rxa1",
            "Rxa1",
            "e4",
            "dxe4",
            "Qxa1+",
            "Ne1",
            "Ng3",
            "bxc5",
            "Nxe4",
            "Qe2",
            "Re8",
            "cxd6",
            "Nxd6",
            "Qxe8+",
            "Nxe8",
            "Kf1",
            "Nd6",
            "c5",
            "Ne4",
            "Ke2",
            "Qb2+",
            "Kf3",
            "Nd2+",
            "Kg4",
            "Qf6",
            "h4",
            "Kg7",
            "hxg5",
            "hxg5",
            "Nf3",
            "Nxf3",
            "Kxf3",
            "Qc3+",
            "Kg4",
            "Kg6"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf6",
          "neutral_uci": "g8f6",
          "neutral_response": "3... Nf6 4. d3 Bc5 5. O-O O-O 6. Nc3 d6 7. Bg5 h6 8. Bh4 g5 9. Nxg5 hxg5 10. Bxg5 Be6 11. Qf3 Kg7 12. Qg3 Nh5 13. Qh4 f6 14. Be3 Bxc4 15. dxc4 Rh8 16. Qg4+ Kf7 17. Nd5 Qc8 18. Qf3 Nd4 19. Bxd4 Bxd4 20. c3 Bb6 21. g4 Qg8 22. Kh1 Nf4 23. Nxf4 exf4 24. Qxf4 Qg5 25. Qg3 Rag8 26. f3 Rh4 27. Rad1 Rgh8 28. Rd5 Qe3 29. Rh5 R4xh5 30. gxh5 Rxh5 31. Qg4 Qg5 32. Qd7+ Kg6 33. Qe8+ Kh6 34. Qh8+ Kg6 35. Qg8+ Kh6 36. Qh8+ Kg6 37. Qg8+ Kh6 38. Qh8+ Kg6 39. Qg8+ Kh6 40. Qh8+ Kg6 41. Qg8+ Kh6 42. Qh8+ Kg6 43. Qg8+ Kh6 44. Qh8+ Kg6 45. Qg8+ Kh6 46. Qh8+ Kg6 47. Qg8+ Kh6 48. Qh8+ Kg6 49. Qg8+ Kh6 50. Qh8+ Kg6 51. Qg8+ Kh6 52. Qh8+ Kg6 53. Qg8+ Kh6 54. Qh8+ Kg6 55. Qg8+ Kh6 56. Qh8+ Kg6 57. Qg8+ Kh6 58. Qh8+ Kg6 59. Qg8+ Kh6 60. Qh8+ Kg6 61. Qg8+ Kh6 62. Qh8+ Kg6 63. Qg8+ Kh6 64. Qh8+ Kg6 65. Qg8+ Kh6 66. Qh8+ Kg6 67. Qg8+ Kh6 68. Qh8+ Kg6 69. Qg8+ Kh6 70. Qh8+ Kg6 71. Qg8+ Kh6 72. Qh8+ Kg6 73. Qg8+ Kh6 74. Qh8+ Kg6 75. Qg8+ Kh6 76. Qh8",
          "neutral_tokens": {
            "prompt_tokens": 130,
            "completion_tokens": 640,
            "total_tokens": 770,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d3",
            "Bc5",
            "O-O",
            "O-O",
            "Nc3",
            "d6",
            "Bg5",
            "h6",
            "Bh4",
            "g5",
            "Nxg5",
            "hxg5",
            "Bxg5",
            "Be6",
            "Qf3",
            "Kg7",
            "Qg3",
            "Nh5",
            "Qh4",
            "f6",
            "Be3",
            "Bxc4",
            "dxc4",
            "Rh8",
            "Qg4+",
            "Kf7",
            "Nd5",
            "Qc8",
            "Qf3",
            "Nd4",
            "Bxd4",
            "Bxd4",
            "c3",
            "Bb6",
            "g4",
            "Qg8",
            "Kh1",
            "Nf4",
            "Nxf4",
            "exf4",
            "Qxf4",
            "Qg5",
            "Qg3",
            "Rag8",
            "f3",
            "Rh4",
            "Rad1",
            "Rgh8",
            "Rd5",
            "Qe3",
            "Rh5",
            "R4xh5",
            "gxh5",
            "Rxh5",
            "Qg4",
            "Qg5",
            "Qd7+",
            "Kg6",
            "Qe8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8+",
            "Kg6",
            "Qg8+",
            "Kh6",
            "Qh8"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g8f6",
          "positional_uci": "g8f6",
          "neutral_uci": "g8f6",
          "consensus_move": "g8f6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d3",
              "Bc5",
              "O-O",
              "O-O",
              "Nc3",
              "d6",
              "Bg5",
              "h6",
              "Bh4",
              "Bg4",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Nd5",
              "Nd4",
              "Nxd4",
              "Bxd4",
              "c3",
              "Bb6",
              "Qf3",
              "c6",
              "Nxf6+",
              "gxf6",
              "Bxf6",
              "Qd7",
              "h4",
              "Bd8",
              "h5",
              "Bxf6",
              "hxg6",
              "Kg7",
              "gxf7",
              "Bg5",
              "Qf5",
              "Qxf5",
              "gxf5",
              "Kf6",
              "Kg2",
              "Rad8",
              "Be6",
              "d5",
              "Rad1",
              "Rd6",
              "Kf3",
              "Rfd8",
              "exd5",
              "cxd5",
              "Kg4",
              "d4",
              "c4",
              "Rb6",
              "b3",
              "Ra6",
              "a4",
              "Rb6",
              "Rb1",
              "Bd2",
              "Rh1",
              "Rh8",
              "Rh5",
              "Bg5",
              "Kf3",
              "Rd6",
              "Ke4",
              "Rdd8",
              "Rbh1",
              "Rh7",
              "Rg1",
              "Rg7",
              "Rg4",
              "Rh7",
              "f4",
              "exf4",
              "Rxf4",
              "Bxf4",
              "Kxf4",
              "Rg7",
              "Rxh6+",
              "Ke7",
              "f6+",
              "Kxe6",
              "fxg7+",
              "Kxf7",
              "Rh8",
              "Rg8",
              "Rxg8",
              "Kxg8",
              "Ke5",
              "Kxg7",
              "Kxd4",
              "Kf7",
              "Kd5",
              "Ke7",
              "d4",
              "Kd7",
              "b4",
              "Kc7",
              "c5",
              "Kd7",
              "b5",
              "Kc7",
              "a5",
              "Kd7",
              "c6+",
              "bxc6+",
              "bxc6+",
              "Kc7",
              "Kc5",
              "a6",
              "d5",
              "Kc8",
              "d6",
              "Kd8",
              "Kb6",
              "Kc8",
              "Kxa6",
              "Kb8",
              "Kb6",
              "Kc8",
              "a6",
              "Kb8",
              "d7",
              "Ka8",
              "d8=Q#",
              "Kxd8",
              "a7",
              "Kc8",
              "a8=Q#",
              "Kd7",
              "c7",
              "Kd6",
              "c8=Q",
              "Ke5",
              "Qac6",
              "Kd4",
              "Q8d7+",
              "Ke5",
              "Qce6+",
              "Kf4",
              "Qdf7+",
              "Kg5",
              "Qeg6+",
              "Kh4",
              "Qfh7#",
              "Kxg6",
              "Qf7+",
              "Kg5",
              "Kc5",
              "Kg4"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "d3",
              "Bc5",
              "O-O",
              "O-O",
              "Nc3",
              "d6",
              "Bg5",
              "Be6",
              "Nd5",
              "Bxd5",
              "Bxd5",
              "h6",
              "Bh4",
              "g5",
              "Bg3",
              "Nxd5",
              "exd5",
              "Ne7",
              "c4",
              "f5",
              "h3",
              "f4",
              "Bh2",
              "Nf5",
              "a3",
              "a5",
              "Qd2",
              "Qf6",
              "b4",
              "axb4",
              "axb4",
              "Rxa1",
              "Rxa1",
              "e4",
              "dxe4",
              "Qxa1+",
              "Ne1",
              "Ng3",
              "bxc5",
              "Nxe4",
              "Qe2",
              "Re8",
              "cxd6",
              "Nxd6",
              "Qxe8+",
              "Nxe8",
              "Kf1",
              "Nd6",
              "c5",
              "Ne4",
              "Ke2",
              "Qb2+",
              "Kf3",
              "Nd2+",
              "Kg4",
              "Qf6",
              "h4",
              "Kg7",
              "hxg5",
              "hxg5",
              "Nf3",
              "Nxf3",
              "Kxf3",
              "Qc3+",
              "Kg4",
              "Kg6"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "d3",
              "Bc5",
              "O-O",
              "O-O",
              "Nc3",
              "d6",
              "Bg5",
              "h6",
              "Bh4",
              "g5",
              "Nxg5",
              "hxg5",
              "Bxg5",
              "Be6",
              "Qf3",
              "Kg7",
              "Qg3",
              "Nh5",
              "Qh4",
              "f6",
              "Be3",
              "Bxc4",
              "dxc4",
              "Rh8",
              "Qg4+",
              "Kf7",
              "Nd5",
              "Qc8",
              "Qf3",
              "Nd4",
              "Bxd4",
              "Bxd4",
              "c3",
              "Bb6",
              "g4",
              "Qg8",
              "Kh1",
              "Nf4",
              "Nxf4",
              "exf4",
              "Qxf4",
              "Qg5",
              "Qg3",
              "Rag8",
              "f3",
              "Rh4",
              "Rad1",
              "Rgh8",
              "Rd5",
              "Qe3",
              "Rh5",
              "R4xh5",
              "gxh5",
              "Rxh5",
              "Qg4",
              "Qg5",
              "Qd7+",
              "Kg6",
              "Qe8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8+",
              "Kg6",
              "Qg8+",
              "Kh6",
              "Qh8"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 130,
            "completion_tokens": 640,
            "total_tokens": 770,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 392,
          "total_completion_tokens": 1920,
          "total_tokens": 2312
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 131,
            "completion_tokens": 640,
            "total_tokens": 771,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 130,
            "completion_tokens": 640,
            "total_tokens": 770,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bqkbnr/pppp1ppp/2n5/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R b KQkq - 3 3"
    },
    {
      "turn": 8,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d5",
          "aggressive_uci": "d7d5",
          "aggressive_response": "4... d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. Qf3+ Ke6 8. Nc3 Ne7 9. d4 c6 10. O-O b5 11. Bb3 a5 12. Re1 a4 13. Rxe5+ Kd6 14. Bxd5 cxd5 15. Nxd5 Bb7 16. Bf4 Kd7 17. Qg4+ Ke8 18. Nxe7 Bxe7 19. Qxg7 Rf8 20. Rae1 Rf7 21. Qg8+ Rf8 22. Qxh7 Rf7 23. Qg8+ Rf8 24. Qg6+ Rf7 25. Bg5 Ra6 26. Rxe7+ Rxe7 27. Rxe7+ Qxe7 28. Qg8+ Qf8 29. Qh7 Qf7 30. Qh8+ Qf8 31. Qe5+ Kd7 32. Qxb5+ Kc7 33. Qe5+ Qd6 34. Qg7+ Qd7 35. Qe5+ Qd6 36. Qg7+ Qd7 37. Qe5+ Qd6 38. Qg7+ Qd7 39. Qe5+ Qd6 40. Qg7+ Qd7 41. Qe5+ Qd6 42. Qg7+ Qd7 43. Qe5+ Qd6 44. Qg7+ Qd7 45. Qe5+ Qd6 46. Qg7+ Qd7 47. Qe5+ Qd6 48. Qg7+ Qd7 49. Qe5+ Qd6 50. Qg7+ Qd7 51. Qe5+ Qd6 52. Qg7+ Qd7 53. Qe5+ Qd6 54. Qg7+ Qd7 55. Qe5+ Qd6 56. Qg7+ Qd7 57. Qe5+ Qd6 58. Qg7+ Qd7 59. Qe5+ Qd6 60. Qg7+ Qd7 61. Qe5+ Qd6 62. Qg7+ Qd7 63. Qe5+ Qd6 64. Qg7+ Qd7 65. Qe5+ Qd6 66. Qg7+ Qd7 67. Qe5+ Qd6 68. Qg7+ Qd7 69. Qe5+ Qd6 70. Qg7+ Qd7",
          "aggressive_tokens": {
            "prompt_tokens": 139,
            "completion_tokens": 640,
            "total_tokens": 779,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "exd5",
            "Nxd5",
            "Nxf7",
            "Kxf7",
            "Qf3+",
            "Ke6",
            "Nc3",
            "Ne7",
            "d4",
            "c6",
            "O-O",
            "b5",
            "Bb3",
            "a5",
            "Re1",
            "a4",
            "Rxe5+",
            "Kd6",
            "Bxd5",
            "cxd5",
            "Nxd5",
            "Bb7",
            "Bf4",
            "Kd7",
            "Qg4+",
            "Ke8",
            "Nxe7",
            "Bxe7",
            "Qxg7",
            "Rf8",
            "Rae1",
            "Rf7",
            "Qg8+",
            "Rf8",
            "Qxh7",
            "Rf7",
            "Qg8+",
            "Rf8",
            "Qg6+",
            "Rf7",
            "Bg5",
            "Ra6",
            "Rxe7+",
            "Rxe7",
            "Rxe7+",
            "Qxe7",
            "Qg8+",
            "Qf8",
            "Qh7",
            "Qf7",
            "Qh8+",
            "Qf8",
            "Qe5+",
            "Kd7",
            "Qxb5+",
            "Kc7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7",
            "Qe5+",
            "Qd6",
            "Qg7+",
            "Qd7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d5",
          "positional_uci": "d7d5",
          "positional_response": "4... d5 5. exd5 Na5 6. Bb5+ c6 7. dxc6 bxc6 8. Qf3 Be7 9. Bxc6+ Nxc6 10. Qxc6+ Bd7 11. Qf3 O-O 12. O-O Rc8 13. Nc3 Bc6 14. Qe2 Nd5 15. Nxd5 Qxd5 16. Nf3 e4 17. Ne1 Bb5 18. d3 exd3 19. Nxd3 Rfe8 20. Qd1 Bf6 21. a4 Ba6 22. Ra3 Qc4 23. c3 Qxd3 24. Qxd3 Bxd3 25. Rd1 Bc2 26. Rf1 Bd3 27. Rd1 Bc2 28. Rf1 Bd3 29. Rd1 Bc2 30. Rf1 Bd3 31. Rd1 Bc2 32. Rf1 Bd3 33. Rd1 Bc2 34. Rf1 Bd3 35. Rd1 Bc2 36. Rf1 Bd3 37. Rd1 Bc2 38. Rf1 Bd3 39. Rd1 Bc2 40. Rf1 Bd3 41. Rd1 Bc2 42. Rf1 Bd3 43. Rd1 Bc2 44. Rf1 Bd3 45. Rd1 Bc2 46. Rf1 Bd3 47. Rd1 Bc2 48. Rf1 Bd3 49. Rd1 Bc2 50. Rf1 Bd3 51. Rd1 Bc2 52. Rf1 Bd3 53. Rd1 Bc2 54. Rf1 Bd3 55. Rd1 Bc2 56. Rf1 Bd3 57. Rd1 Bc2 58. Rf1 Bd3 59. Rd1 Bc2 60. Rf1 Bd3 61. Rd1 Bc2 62. Rf1 Bd3 63. Rd1 Bc2 64. Rf1 Bd3 65. Rd1 Bc2 66. Rf1 Bd3 67. Rd1 Bc2 68. Rf1 Bd3 69. Rd1 Bc2 70. Rf1 Bd3 71. Rd1 Bc2 72. Rf1 Bd3 73. Rd1 Bc2 74. Rf1 Bd3 75. Rd1 Bc2 76. Rf1 Bd3 77. Rd1 Bc2 78. Rf1 Bd3 79. Rd1 Bc2 80. Rf1 Bd3 81. Rd1 Bc2 82. Rf1 Bd3 83.",
          "positional_tokens": {
            "prompt_tokens": 139,
            "completion_tokens": 640,
            "total_tokens": 779,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "exd5",
            "Na5",
            "Bb5+",
            "c6",
            "dxc6",
            "bxc6",
            "Qf3",
            "Be7",
            "Bxc6+",
            "Nxc6",
            "Qxc6+",
            "Bd7",
            "Qf3",
            "O-O",
            "O-O",
            "Rc8",
            "Nc3",
            "Bc6",
            "Qe2",
            "Nd5",
            "Nxd5",
            "Qxd5",
            "Nf3",
            "e4",
            "Ne1",
            "Bb5",
            "d3",
            "exd3",
            "Nxd3",
            "Rfe8",
            "Qd1",
            "Bf6",
            "a4",
            "Ba6",
            "Ra3",
            "Qc4",
            "c3",
            "Qxd3",
            "Qxd3",
            "Bxd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3",
            "Rd1",
            "Bc2",
            "Rf1",
            "Bd3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d5",
          "neutral_uci": "d7d5",
          "neutral_response": "4... d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. Qf3+ Ke6 8. Nc3 Ne7 9. d4 c6 10. O-O b5 11. Bb3 a5 12. Re1 a4 13. Rxe5+ Kd6 14. Bxd5 cxd5 15. Nxd5 Bb7 16. Bf4 Ng6 17. Re8+ Nxf4 18. Rxd8+ Rxd8 19. Qxf4+ Kxd5 20. Qe5+ Kc6 21. c4 bxc4 22. Rc1 Ba6 23. Qa5 Ra8 24. Rxc4+ Bxc4 25. Qxa8+ Kb5 26. Qb7+ Ka5 27. Qc7+ Kb5 28. Qe5+ Kb4 29. Qb8+ Ka5 30. Qa8+ Kb4 31. Qb7+ Ka5 32. Qc7+ Kb4 33. Qb6+ Bb5 34. a3+ Kc4 35. Qe6+ Kxd4 36. Qb6+ Kc4 37. Qc7+ Kb3 38. Qc3+ Ka2 39. b4 axb3 40. Qd2+ b2 41. Qd5+ Kxa3 42. Qa8+ Kb3 43. Qd5+ Bc4 44. Qb7+ Kc2 45. Qe4+ Bd3 46. Qa4+ Kc1 47. Qf4+ Kd1 48. Qa4+ Bc2 49. Qd4+ Kc1 50. Qe3+ Kb1 51. Qe1+ Ka2 52. Qa5+ Ba3 53. Qd5+ Bb3 54. Qd2 Ka1 55. Qc3 Ka2 56. Qd2 Ka1 57. Qc3 Ka2 58. Qd2 Ka1 59. Qc3 Ka2 60. Qd2 Ka1 61. Qc3 Ka2 62. Qd2 Ka1 63. Qc3 Ka2 64. Qd2 Ka1 65. Qc3 Ka2 66. Qd2 Ka1 67. Qc3 Ka2 68. Qd2 Ka1 69. Qc3 Ka2 70. Qd2 Ka1 71. Qc3 Ka2 72. Qd2 Ka1 73. Qc3 Ka2 74. Qd2 Ka1 75. Qc3 Ka2 76. Qd2 Ka1 77. Qc3 Ka2",
          "neutral_tokens": {
            "prompt_tokens": 138,
            "completion_tokens": 640,
            "total_tokens": 778,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "exd5",
            "Nxd5",
            "Nxf7",
            "Kxf7",
            "Qf3+",
            "Ke6",
            "Nc3",
            "Ne7",
            "d4",
            "c6",
            "O-O",
            "b5",
            "Bb3",
            "a5",
            "Re1",
            "a4",
            "Rxe5+",
            "Kd6",
            "Bxd5",
            "cxd5",
            "Nxd5",
            "Bb7",
            "Bf4",
            "Ng6",
            "Re8+",
            "Nxf4",
            "Rxd8+",
            "Rxd8",
            "Qxf4+",
            "Kxd5",
            "Qe5+",
            "Kc6",
            "c4",
            "bxc4",
            "Rc1",
            "Ba6",
            "Qa5",
            "Ra8",
            "Rxc4+",
            "Bxc4",
            "Qxa8+",
            "Kb5",
            "Qb7+",
            "Ka5",
            "Qc7+",
            "Kb5",
            "Qe5+",
            "Kb4",
            "Qb8+",
            "Ka5",
            "Qa8+",
            "Kb4",
            "Qb7+",
            "Ka5",
            "Qc7+",
            "Kb4",
            "Qb6+",
            "Bb5",
            "a3+",
            "Kc4",
            "Qe6+",
            "Kxd4",
            "Qb6+",
            "Kc4",
            "Qc7+",
            "Kb3",
            "Qc3+",
            "Ka2",
            "b4",
            "axb3",
            "Qd2+",
            "b2",
            "Qd5+",
            "Kxa3",
            "Qa8+",
            "Kb3",
            "Qd5+",
            "Bc4",
            "Qb7+",
            "Kc2",
            "Qe4+",
            "Bd3",
            "Qa4+",
            "Kc1",
            "Qf4+",
            "Kd1",
            "Qa4+",
            "Bc2",
            "Qd4+",
            "Kc1",
            "Qe3+",
            "Kb1",
            "Qe1+",
            "Ka2",
            "Qa5+",
            "Ba3",
            "Qd5+",
            "Bb3",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2",
            "Qd2",
            "Ka1",
            "Qc3",
            "Ka2"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d7d5",
          "positional_uci": "d7d5",
          "neutral_uci": "d7d5",
          "consensus_move": "d7d5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "exd5",
              "Nxd5",
              "Nxf7",
              "Kxf7",
              "Qf3+",
              "Ke6",
              "Nc3",
              "Ne7",
              "d4",
              "c6",
              "O-O",
              "b5",
              "Bb3",
              "a5",
              "Re1",
              "a4",
              "Rxe5+",
              "Kd6",
              "Bxd5",
              "cxd5",
              "Nxd5",
              "Bb7",
              "Bf4",
              "Kd7",
              "Qg4+",
              "Ke8",
              "Nxe7",
              "Bxe7",
              "Qxg7",
              "Rf8",
              "Rae1",
              "Rf7",
              "Qg8+",
              "Rf8",
              "Qxh7",
              "Rf7",
              "Qg8+",
              "Rf8",
              "Qg6+",
              "Rf7",
              "Bg5",
              "Ra6",
              "Rxe7+",
              "Rxe7",
              "Rxe7+",
              "Qxe7",
              "Qg8+",
              "Qf8",
              "Qh7",
              "Qf7",
              "Qh8+",
              "Qf8",
              "Qe5+",
              "Kd7",
              "Qxb5+",
              "Kc7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7",
              "Qe5+",
              "Qd6",
              "Qg7+",
              "Qd7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "exd5",
              "Na5",
              "Bb5+",
              "c6",
              "dxc6",
              "bxc6",
              "Qf3",
              "Be7",
              "Bxc6+",
              "Nxc6",
              "Qxc6+",
              "Bd7",
              "Qf3",
              "O-O",
              "O-O",
              "Rc8",
              "Nc3",
              "Bc6",
              "Qe2",
              "Nd5",
              "Nxd5",
              "Qxd5",
              "Nf3",
              "e4",
              "Ne1",
              "Bb5",
              "d3",
              "exd3",
              "Nxd3",
              "Rfe8",
              "Qd1",
              "Bf6",
              "a4",
              "Ba6",
              "Ra3",
              "Qc4",
              "c3",
              "Qxd3",
              "Qxd3",
              "Bxd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3",
              "Rd1",
              "Bc2",
              "Rf1",
              "Bd3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "exd5",
              "Nxd5",
              "Nxf7",
              "Kxf7",
              "Qf3+",
              "Ke6",
              "Nc3",
              "Ne7",
              "d4",
              "c6",
              "O-O",
              "b5",
              "Bb3",
              "a5",
              "Re1",
              "a4",
              "Rxe5+",
              "Kd6",
              "Bxd5",
              "cxd5",
              "Nxd5",
              "Bb7",
              "Bf4",
              "Ng6",
              "Re8+",
              "Nxf4",
              "Rxd8+",
              "Rxd8",
              "Qxf4+",
              "Kxd5",
              "Qe5+",
              "Kc6",
              "c4",
              "bxc4",
              "Rc1",
              "Ba6",
              "Qa5",
              "Ra8",
              "Rxc4+",
              "Bxc4",
              "Qxa8+",
              "Kb5",
              "Qb7+",
              "Ka5",
              "Qc7+",
              "Kb5",
              "Qe5+",
              "Kb4",
              "Qb8+",
              "Ka5",
              "Qa8+",
              "Kb4",
              "Qb7+",
              "Ka5",
              "Qc7+",
              "Kb4",
              "Qb6+",
              "Bb5",
              "a3+",
              "Kc4",
              "Qe6+",
              "Kxd4",
              "Qb6+",
              "Kc4",
              "Qc7+",
              "Kb3",
              "Qc3+",
              "Ka2",
              "b4",
              "axb3",
              "Qd2+",
              "b2",
              "Qd5+",
              "Kxa3",
              "Qa8+",
              "Kb3",
              "Qd5+",
              "Bc4",
              "Qb7+",
              "Kc2",
              "Qe4+",
              "Bd3",
              "Qa4+",
              "Kc1",
              "Qf4+",
              "Kd1",
              "Qa4+",
              "Bc2",
              "Qd4+",
              "Kc1",
              "Qe3+",
              "Kb1",
              "Qe1+",
              "Ka2",
              "Qa5+",
              "Ba3",
              "Qd5+",
              "Bb3",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2",
              "Qd2",
              "Ka1",
              "Qc3",
              "Ka2"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 139,
            "completion_tokens": 640,
            "total_tokens": 779,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 139,
            "completion_tokens": 640,
            "total_tokens": 779,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 138,
            "completion_tokens": 640,
            "total_tokens": 778,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 416,
          "total_completion_tokens": 1920,
          "total_tokens": 2336
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 139,
            "completion_tokens": 640,
            "total_tokens": 779,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 139,
            "completion_tokens": 640,
            "total_tokens": 779,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 138,
            "completion_tokens": 640,
            "total_tokens": 778,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bqkb1r/pppp1ppp/2n2n2/4p1N1/2B1P3/8/PPPP1PPP/RNBQK2R b KQkq - 5 4"
    },
    {
      "turn": 10,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nxd5",
          "aggressive_uci": "f6d5",
          "aggressive_response": "5... Nxd5 6. Nxf7 Kxf7 7. Qf3+ Ke6 8. Nc3 Ne7 9. d4 c6 10. O-O b5 11. Bb3 a5 12. Re1 a4 13. Rxe5+ Kd6 14. Bxd5 Nxd5 15. Nxd5 cxd5 16. Qxd5+ Kc7 17. Qxa8 Qxd4 18. Qa5+ Kd6 19. Re1 Bd7 20. Be3 Qxb2 21. Rad1+ Kc6 22. Qb6# 1-0\n22... Bb4 23. Bf4 Bxe1 24. Rd6#",
          "aggressive_tokens": {
            "prompt_tokens": 147,
            "completion_tokens": 174,
            "total_tokens": 321,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "cxd5",
            "Qxd5+",
            "Kc7",
            "Qxa8",
            "Qxd4",
            "Qa5+",
            "Kd6",
            "Re1",
            "Bd7",
            "Be3",
            "Qxb2",
            "Rad1+",
            "Kc6",
            "Qb6#",
            "Bb4",
            "Bf4",
            "Bxe1",
            "Rd6#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d6",
          "positional_uci": null,
          "positional_response": "d6 Nxd5 O-O Be6 d3 Bd6 Nc3 Nxc3 bxc3 O-O Bxe6 fxe6 Ng5 Qe7 Qg4 Rf6 Ne4 Rg6 Qh3 Nd7 Be3 Nf6 Nxf6+ Qxf6 Rab1 b6 d4 exd4 cxd4 Rf8 c4 c5 dxc5 Bxc5 Bxc5 bxc5 Rb7 Qd4 Rxa7 Qxc4 a4 Qd5 Qxd5 exd5 a5 Rc6 a6 c4 Rd7 c3 Rxd5 c2 Rc5 Rd1 a7 Ra8 Rxc2 Rxa7 Rxa7 Rxa7 g3 Kf8 Kg2 Ke8 h4 Kd8 g4 Kc8 Kg3 Kb8 Rxg7 Kc8 Rxh7 Kd8 g5 Ke8 g6 Kf8 h5 Kg8 h6 Kf8 Ra7 Kg8 Ra8#",
          "positional_tokens": {
            "prompt_tokens": 147,
            "completion_tokens": 215,
            "total_tokens": 362,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nxd5",
            "O-O",
            "Be6",
            "d3",
            "Bd6",
            "Nc3",
            "Nxc3",
            "bxc3",
            "O-O",
            "Bxe6",
            "fxe6",
            "Ng5",
            "Qe7",
            "Qg4",
            "Rf6",
            "Ne4",
            "Rg6",
            "Qh3",
            "Nd7",
            "Be3",
            "Nf6",
            "Nxf6+",
            "Qxf6",
            "Rab1",
            "b6",
            "d4",
            "exd4",
            "cxd4",
            "Rf8",
            "c4",
            "c5",
            "dxc5",
            "Bxc5",
            "Bxc5",
            "bxc5",
            "Rb7",
            "Qd4",
            "Rxa7",
            "Qxc4",
            "a4",
            "Qd5",
            "Qxd5",
            "exd5",
            "a5",
            "Rc6",
            "a6",
            "c4",
            "Rd7",
            "c3",
            "Rxd5",
            "c2",
            "Rc5",
            "Rd1",
            "a7",
            "Ra8",
            "Rxc2",
            "Rxa7",
            "Rxa7",
            "Rxa7",
            "g3",
            "Kf8",
            "Kg2",
            "Ke8",
            "h4",
            "Kd8",
            "g4",
            "Kc8",
            "Kg3",
            "Kb8",
            "Rxg7",
            "Kc8",
            "Rxh7",
            "Kd8",
            "g5",
            "Ke8",
            "g6",
            "Kf8",
            "h5",
            "Kg8",
            "h6",
            "Kf8",
            "Ra7",
            "Kg8",
            "Ra8#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nxd5",
          "neutral_uci": "f6d5",
          "neutral_response": "5... Nxd5 6. Nxf7 Kxf7 7. Qf3+ Ke6 8. Nc3 Ne7 9. d4 c6 10. O-O b5 11. Bb3 a5 12. Re1 Kd7 13. Nxd5 cxd5 14. Bxd5 Ra6 15. Rxe5 Rf6 16. Qe2 Nxd5 17. Rxd5+ Rd6 18. Qxb5+ Ke6 19. Re5+ Kf7 20. Bg5 Qd7 21. Qxa5 Rxd4 22. Rae1 Rd1 23. Qc3 Rxe1+ 24. Rxe1 Qd5 25. Qc7+ Bd7 26. Qf4+ Kg8 27. c4 Qa5 28. Rd1 Qa4 29. b3 Qc6 30. Qd4 Be6 31. Qd8 h6 32. Be7 Kf7 33. Bxf8 Rxf8 34. Qd4 Kg8 35. h3 Qa6 36. Rd2 Qa5 37. Re2 Rd8 38. Qe5 Qxe5 39. Rxe5 Kf7 40. Re2 Rd1+ 41. Kh2 Rc1 42. Kg3 Rc3+ 43. Kf4 Kf6 44. Ke4 Bf5+ 45. Kd4 Rd3+ 46. Kc5 Rd8 47. Kb4 Rb8+ 48. Kc3 Ra8 49. a4 Bd7 50. c5 Bc6 51. f3 Bd5 52. Rd2 Ke6 53. Rd4 Rb8 54. b4 Ra8 55. a5 Rb8 56. Rg4 g5 57. h4 Kf5 58. hxg5 hxg5 59. Rd4 Ke5 60. Rg4 Kf5 61. Rd4 Ke5 62. Rd2 Bc6 63. Rd6 Rc8 64. Kc4 g4 65. fxg4 Bxg2 66. b5 Bf1+ 67. Kb4 Rb8 68. b6 Ba6 69. Rd7 Rg8 70. c6 Rxg4+ 71. Kc5 Rc4+ 72. Kd6 Rd4+ 73. Kc7 Rc4 74. b7 Bxb7 75. Kxb7 Rb4+ 76. Kc8 Ra4 77. Ra7 Kd6 78. c7 Rc4 79. Ra6+ Ke7 80. Kb7 Kd7 81. c8=Q+ Rxc8 82. Rd6",
          "neutral_tokens": {
            "prompt_tokens": 146,
            "completion_tokens": 640,
            "total_tokens": 786,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Rxd5+",
            "Rd6",
            "Qxb5+",
            "Ke6",
            "Re5+",
            "Kf7",
            "Bg5",
            "Qd7",
            "Qxa5",
            "Rxd4",
            "Rae1",
            "Rd1",
            "Qc3",
            "Rxe1+",
            "Rxe1",
            "Qd5",
            "Qc7+",
            "Bd7",
            "Qf4+",
            "Kg8",
            "c4",
            "Qa5",
            "Rd1",
            "Qa4",
            "b3",
            "Qc6",
            "Qd4",
            "Be6",
            "Qd8",
            "h6",
            "Be7",
            "Kf7",
            "Bxf8",
            "Rxf8",
            "Qd4",
            "Kg8",
            "h3",
            "Qa6",
            "Rd2",
            "Qa5",
            "Re2",
            "Rd8",
            "Qe5",
            "Qxe5",
            "Rxe5",
            "Kf7",
            "Re2",
            "Rd1+",
            "Kh2",
            "Rc1",
            "Kg3",
            "Rc3+",
            "Kf4",
            "Kf6",
            "Ke4",
            "Bf5+",
            "Kd4",
            "Rd3+",
            "Kc5",
            "Rd8",
            "Kb4",
            "Rb8+",
            "Kc3",
            "Ra8",
            "a4",
            "Bd7",
            "c5",
            "Bc6",
            "f3",
            "Bd5",
            "Rd2",
            "Ke6",
            "Rd4",
            "Rb8",
            "b4",
            "Ra8",
            "a5",
            "Rb8",
            "Rg4",
            "g5",
            "h4",
            "Kf5",
            "hxg5",
            "hxg5",
            "Rd4",
            "Ke5",
            "Rg4",
            "Kf5",
            "Rd4",
            "Ke5",
            "Rd2",
            "Bc6",
            "Rd6",
            "Rc8",
            "Kc4",
            "g4",
            "fxg4",
            "Bxg2",
            "b5",
            "Bf1+",
            "Kb4",
            "Rb8",
            "b6",
            "Ba6",
            "Rd7",
            "Rg8",
            "c6",
            "Rxg4+",
            "Kc5",
            "Rc4+",
            "Kd6",
            "Rd4+",
            "Kc7",
            "Rc4",
            "b7",
            "Bxb7",
            "Kxb7",
            "Rb4+",
            "Kc8",
            "Ra4",
            "Ra7",
            "Kd6",
            "c7",
            "Rc4",
            "Ra6+",
            "Ke7",
            "Kb7",
            "Kd7",
            "c8=Q+",
            "Rxc8",
            "Rd6"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f6d5",
          "positional_uci": null,
          "neutral_uci": "f6d5",
          "consensus_move": "f6d5",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "cxd5",
              "Qxd5+",
              "Kc7",
              "Qxa8",
              "Qxd4",
              "Qa5+",
              "Kd6",
              "Re1",
              "Bd7",
              "Be3",
              "Qxb2",
              "Rad1+",
              "Kc6",
              "Qb6#",
              "Bb4",
              "Bf4",
              "Bxe1",
              "Rd6#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nxd5",
              "O-O",
              "Be6",
              "d3",
              "Bd6",
              "Nc3",
              "Nxc3",
              "bxc3",
              "O-O",
              "Bxe6",
              "fxe6",
              "Ng5",
              "Qe7",
              "Qg4",
              "Rf6",
              "Ne4",
              "Rg6",
              "Qh3",
              "Nd7",
              "Be3",
              "Nf6",
              "Nxf6+",
              "Qxf6",
              "Rab1",
              "b6",
              "d4",
              "exd4",
              "cxd4",
              "Rf8",
              "c4",
              "c5",
              "dxc5",
              "Bxc5",
              "Bxc5",
              "bxc5",
              "Rb7",
              "Qd4",
              "Rxa7",
              "Qxc4",
              "a4",
              "Qd5",
              "Qxd5",
              "exd5",
              "a5",
              "Rc6",
              "a6",
              "c4",
              "Rd7",
              "c3",
              "Rxd5",
              "c2",
              "Rc5",
              "Rd1",
              "a7",
              "Ra8",
              "Rxc2",
              "Rxa7",
              "Rxa7",
              "Rxa7",
              "g3",
              "Kf8",
              "Kg2",
              "Ke8",
              "h4",
              "Kd8",
              "g4",
              "Kc8",
              "Kg3",
              "Kb8",
              "Rxg7",
              "Kc8",
              "Rxh7",
              "Kd8",
              "g5",
              "Ke8",
              "g6",
              "Kf8",
              "h5",
              "Kg8",
              "h6",
              "Kf8",
              "Ra7",
              "Kg8",
              "Ra8#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Rxd5+",
              "Rd6",
              "Qxb5+",
              "Ke6",
              "Re5+",
              "Kf7",
              "Bg5",
              "Qd7",
              "Qxa5",
              "Rxd4",
              "Rae1",
              "Rd1",
              "Qc3",
              "Rxe1+",
              "Rxe1",
              "Qd5",
              "Qc7+",
              "Bd7",
              "Qf4+",
              "Kg8",
              "c4",
              "Qa5",
              "Rd1",
              "Qa4",
              "b3",
              "Qc6",
              "Qd4",
              "Be6",
              "Qd8",
              "h6",
              "Be7",
              "Kf7",
              "Bxf8",
              "Rxf8",
              "Qd4",
              "Kg8",
              "h3",
              "Qa6",
              "Rd2",
              "Qa5",
              "Re2",
              "Rd8",
              "Qe5",
              "Qxe5",
              "Rxe5",
              "Kf7",
              "Re2",
              "Rd1+",
              "Kh2",
              "Rc1",
              "Kg3",
              "Rc3+",
              "Kf4",
              "Kf6",
              "Ke4",
              "Bf5+",
              "Kd4",
              "Rd3+",
              "Kc5",
              "Rd8",
              "Kb4",
              "Rb8+",
              "Kc3",
              "Ra8",
              "a4",
              "Bd7",
              "c5",
              "Bc6",
              "f3",
              "Bd5",
              "Rd2",
              "Ke6",
              "Rd4",
              "Rb8",
              "b4",
              "Ra8",
              "a5",
              "Rb8",
              "Rg4",
              "g5",
              "h4",
              "Kf5",
              "hxg5",
              "hxg5",
              "Rd4",
              "Ke5",
              "Rg4",
              "Kf5",
              "Rd4",
              "Ke5",
              "Rd2",
              "Bc6",
              "Rd6",
              "Rc8",
              "Kc4",
              "g4",
              "fxg4",
              "Bxg2",
              "b5",
              "Bf1+",
              "Kb4",
              "Rb8",
              "b6",
              "Ba6",
              "Rd7",
              "Rg8",
              "c6",
              "Rxg4+",
              "Kc5",
              "Rc4+",
              "Kd6",
              "Rd4+",
              "Kc7",
              "Rc4",
              "b7",
              "Bxb7",
              "Kxb7",
              "Rb4+",
              "Kc8",
              "Ra4",
              "Ra7",
              "Kd6",
              "c7",
              "Rc4",
              "Ra6+",
              "Ke7",
              "Kb7",
              "Kd7",
              "c8=Q+",
              "Rxc8",
              "Rd6"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 147,
            "completion_tokens": 174,
            "total_tokens": 321,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 147,
            "completion_tokens": 215,
            "total_tokens": 362,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 146,
            "completion_tokens": 640,
            "total_tokens": 786,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 440,
          "total_completion_tokens": 1029,
          "total_tokens": 1469
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 147,
            "completion_tokens": 174,
            "total_tokens": 321,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 147,
            "completion_tokens": 215,
            "total_tokens": 362,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 146,
            "completion_tokens": 640,
            "total_tokens": 786,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n2n2/3Pp1N1/2B5/8/PPPP1PPP/RNBQK2R b KQkq - 0 5"
    },
    {
      "turn": 12,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Kxf7",
          "aggressive_uci": "e8f7",
          "aggressive_response": "6... Kxf7 7. Qf3+ Ke6 8. Nc3 Ne7 9. d4 c6 10. O-O b5 11. Bb3 a5 12. Re1 Kd7 13. Nxd5 cxd5 14. Bxd5 Ra6 15. dxe5 Kc7 16. Bf4 Nxd5 17. Rad1 Be6 18. b3 Bb4 19. Re4 Rf8 20. c4 bxc4 21. bxc4 g5 22. cxd5 Bxd5 23. Rc4+ Rc6 24. Rxc6+ Kxc6 25. Rc1+ Bc5 26. Qc3 Qb6 27. Be3 *",
          "aggressive_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 178,
            "total_tokens": 334,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Qf3+",
            "Ke6",
            "Nc3",
            "Ne7",
            "d4",
            "c6",
            "O-O",
            "b5",
            "Bb3",
            "a5",
            "Re1",
            "Kd7",
            "Nxd5",
            "cxd5",
            "Bxd5",
            "Ra6",
            "dxe5",
            "Kc7",
            "Bf4",
            "Nxd5",
            "Rad1",
            "Be6",
            "b3",
            "Bb4",
            "Re4",
            "Rf8",
            "c4",
            "bxc4",
            "bxc4",
            "g5",
            "cxd5",
            "Bxd5",
            "Rc4+",
            "Rc6",
            "Rxc6+",
            "Kxc6",
            "Rc1+",
            "Bc5",
            "Qc3",
            "Qb6",
            "Be3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Kxf7",
          "positional_uci": "e8f7",
          "positional_response": "6... Kxf7 7. Qf3+ Ke6 8. Nc3 Ne7 9. d4 c6 10. O-O b5 11. Bb3 a5 12. Re1 Kd7 13. Nxd5 cxd5 14. Bxd5 Ra6 15. Rxe5 Rf6 16. Qh3+ Kc7 17. Qc3+ Kb8 18. Bf3 b4 19. Qc5 Nf5 20. Qb5+ Qb6 21. Qxa5 Qxa5 22. Rxa5 Nxd4 23. Be4 Bb7 24. Bxb7 Kxb7 25. c3 Ne2+ 26. Kf1 Nxc1 27. Rxc1 bxc3 28. Rxc3 Bb4 29. Rb5+ Rb6 30. Rxb6+ Kxb6 31. Rb3 Kc5 32. a3 Ba5 33. Rb7 Re8 34. b4+ Kc6 35. Rxg7 Bc7 36. Rxh7 Ra8 37. Rh3 Kb5 38. Rc3 Bd6 39. Rd3 Bc7 40. h4 Kc4 41. Rf3 Be5 42. g4 Bb2 43. h5 Bxa3 44. g5 Bxb4 45. Rf4+ Kc5 46. g6 Bc3 47. h6 Bd2 48. Rf5+ Kd6 49. h7 Bc3 50. Rf7 Ke6 51. f4 Ra1+ 52. Kg2 Ra2+ 53. Kf3 Ra3 54. Kg4 Ra1 55. f5+ Kd6 56. f6 Rg1+ 57. Kf5 Rf1+ 58. Kg5 Rg1+ 59. Kh6 Bd2+ 60. Kg7 Rh1 61. h8=Q Rxh8 62. Kxh8 Ke6 63. Kg7 Bc3 64. Rf8 Bd4 65. Re8+ Kf5 66. Rf8 Ke6 67. Rf7 Bc3 68. Re7+ Kf5 69. Kf7 Bxf6 70. g7 Bxg7 71. Kxg7 Kf4 72. Kf6 Kf3 73. Kf5 Kg3 74. Re3+ Kf2 75. Kf4 Kg2 76. Re2+ Kf1 77. Kf3 Kg1 78. Kg3 Kf1 79. Re3 Kg1 80. Re1# Kf1 81. Re2 Kg",
          "positional_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Qf3+",
            "Ke6",
            "Nc3",
            "Ne7",
            "d4",
            "c6",
            "O-O",
            "b5",
            "Bb3",
            "a5",
            "Re1",
            "Kd7",
            "Nxd5",
            "cxd5",
            "Bxd5",
            "Ra6",
            "Rxe5",
            "Rf6",
            "Qh3+",
            "Kc7",
            "Qc3+",
            "Kb8",
            "Bf3",
            "b4",
            "Qc5",
            "Nf5",
            "Qb5+",
            "Qb6",
            "Qxa5",
            "Qxa5",
            "Rxa5",
            "Nxd4",
            "Be4",
            "Bb7",
            "Bxb7",
            "Kxb7",
            "c3",
            "Ne2+",
            "Kf1",
            "Nxc1",
            "Rxc1",
            "bxc3",
            "Rxc3",
            "Bb4",
            "Rb5+",
            "Rb6",
            "Rxb6+",
            "Kxb6",
            "Rb3",
            "Kc5",
            "a3",
            "Ba5",
            "Rb7",
            "Re8",
            "b4+",
            "Kc6",
            "Rxg7",
            "Bc7",
            "Rxh7",
            "Ra8",
            "Rh3",
            "Kb5",
            "Rc3",
            "Bd6",
            "Rd3",
            "Bc7",
            "h4",
            "Kc4",
            "Rf3",
            "Be5",
            "g4",
            "Bb2",
            "h5",
            "Bxa3",
            "g5",
            "Bxb4",
            "Rf4+",
            "Kc5",
            "g6",
            "Bc3",
            "h6",
            "Bd2",
            "Rf5+",
            "Kd6",
            "h7",
            "Bc3",
            "Rf7",
            "Ke6",
            "f4",
            "Ra1+",
            "Kg2",
            "Ra2+",
            "Kf3",
            "Ra3",
            "Kg4",
            "Ra1",
            "f5+",
            "Kd6",
            "f6",
            "Rg1+",
            "Kf5",
            "Rf1+",
            "Kg5",
            "Rg1+",
            "Kh6",
            "Bd2+",
            "Kg7",
            "Rh1",
            "h8=Q",
            "Rxh8",
            "Kxh8",
            "Ke6",
            "Kg7",
            "Bc3",
            "Rf8",
            "Bd4",
            "Re8+",
            "Kf5",
            "Rf8",
            "Ke6",
            "Rf7",
            "Bc3",
            "Re7+",
            "Kf5",
            "Kf7",
            "Bxf6",
            "g7",
            "Bxg7",
            "Kxg7",
            "Kf4",
            "Kf6",
            "Kf3",
            "Kf5",
            "Kg3",
            "Re3+",
            "Kf2",
            "Kf4",
            "Kg2",
            "Re2+",
            "Kf1",
            "Kf3",
            "Kg1",
            "Kg3",
            "Kf1",
            "Re3",
            "Kg1",
            "Re1#",
            "Kf1",
            "Re2"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Kxf7",
          "neutral_uci": "e8f7",
          "neutral_response": "6... Kxf7 7. Qf3+ Ke6 8. Nc3 Ne7 9. d4 c6 10. O-O b5 11. Bb3 a5 12. Re1 Kd7 13. Nxd5 cxd5 14. Bxd5 Ra6 15. dxe5 Nxd5 16. Qxd5+ Ke8 17. Qxb5+ Qd7 18. Qe2 Bb7 19. Bf4 Rg6 20. Bg3 h5 21. Rad1 Qc6 22. f3 h4 23. Bf4 h3 24. g3 Bc5+ 25. Be3 Bxe3+ 26. Qxe3 Qxf3 27. Qxf3 Bxf3 28. Rd3 Bg4 29. Rd5 Ra6 30. Re4 Be6 31. Rb5 Kf7 32. Rf4+ Kg6 33. c4 Rd8 34. Kf2 Rd2+ 35. Ke3 Rxh2 36. c5 Rc2 37. Rb6 Rxb6 38. cxb6 Rxb2 39. Rh4 Rxb6 40. g4 Rb4 41. Rxh3 Bxg4 42. Rg3 Kf5 43. Rg1 Re4+ 44. Kd3 Rxe5 45. Rf1+ Ke6 46. Rg1 Bf5+ 47. Kd4 g6 48. Rc1 Rd5+ 49. Ke3 Ke5 50. Rc7 Rd3+ 51. Ke2 Ra3 52. Rc5+ Kd4 53. Rc7 Rxa2+ 54. Kf3 a4 55. Ra7 a3 56. Ra4+ Kc3 57. Ke3 Kb3 58. Ra7 Rc2 59. Rb7+ Kc3 60. Rc7+ Kb2 61. Rb7+ Kc1 62. Ra7 a2 63. Kd4 Kb1 64. Rb7+ Rb2 65. Ra7 a1=Q 66. Rxa1+ Kxa1 67. Ke5 Rb4 68. Kf6 Re4 69. Kg5 Kb2 70. Kf6 Kc3 71. Kg5 Kd4 72. Kf6 Kd5 73. Kg5 Ke6 74. Kh6 Kf6 75. Kh7 Re8 76. Kh6 Rh8# e4 e5 Nf3 Nc6 Bc4 Nf6 Ng5 d5 exd5 Nxd5 Nxf7 Kxf7 Qf3+ Ke6 Nc3 Ne7 d4 c6 O-O b5",
          "neutral_tokens": {
            "prompt_tokens": 155,
            "completion_tokens": 640,
            "total_tokens": 795,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Qf3+",
            "Ke6",
            "Nc3",
            "Ne7",
            "d4",
            "c6",
            "O-O",
            "b5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e8f7",
          "positional_uci": "e8f7",
          "neutral_uci": "e8f7",
          "consensus_move": "e8f7",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Qf3+",
              "Ke6",
              "Nc3",
              "Ne7",
              "d4",
              "c6",
              "O-O",
              "b5",
              "Bb3",
              "a5",
              "Re1",
              "Kd7",
              "Nxd5",
              "cxd5",
              "Bxd5",
              "Ra6",
              "dxe5",
              "Kc7",
              "Bf4",
              "Nxd5",
              "Rad1",
              "Be6",
              "b3",
              "Bb4",
              "Re4",
              "Rf8",
              "c4",
              "bxc4",
              "bxc4",
              "g5",
              "cxd5",
              "Bxd5",
              "Rc4+",
              "Rc6",
              "Rxc6+",
              "Kxc6",
              "Rc1+",
              "Bc5",
              "Qc3",
              "Qb6",
              "Be3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Qf3+",
              "Ke6",
              "Nc3",
              "Ne7",
              "d4",
              "c6",
              "O-O",
              "b5",
              "Bb3",
              "a5",
              "Re1",
              "Kd7",
              "Nxd5",
              "cxd5",
              "Bxd5",
              "Ra6",
              "Rxe5",
              "Rf6",
              "Qh3+",
              "Kc7",
              "Qc3+",
              "Kb8",
              "Bf3",
              "b4",
              "Qc5",
              "Nf5",
              "Qb5+",
              "Qb6",
              "Qxa5",
              "Qxa5",
              "Rxa5",
              "Nxd4",
              "Be4",
              "Bb7",
              "Bxb7",
              "Kxb7",
              "c3",
              "Ne2+",
              "Kf1",
              "Nxc1",
              "Rxc1",
              "bxc3",
              "Rxc3",
              "Bb4",
              "Rb5+",
              "Rb6",
              "Rxb6+",
              "Kxb6",
              "Rb3",
              "Kc5",
              "a3",
              "Ba5",
              "Rb7",
              "Re8",
              "b4+",
              "Kc6",
              "Rxg7",
              "Bc7",
              "Rxh7",
              "Ra8",
              "Rh3",
              "Kb5",
              "Rc3",
              "Bd6",
              "Rd3",
              "Bc7",
              "h4",
              "Kc4",
              "Rf3",
              "Be5",
              "g4",
              "Bb2",
              "h5",
              "Bxa3",
              "g5",
              "Bxb4",
              "Rf4+",
              "Kc5",
              "g6",
              "Bc3",
              "h6",
              "Bd2",
              "Rf5+",
              "Kd6",
              "h7",
              "Bc3",
              "Rf7",
              "Ke6",
              "f4",
              "Ra1+",
              "Kg2",
              "Ra2+",
              "Kf3",
              "Ra3",
              "Kg4",
              "Ra1",
              "f5+",
              "Kd6",
              "f6",
              "Rg1+",
              "Kf5",
              "Rf1+",
              "Kg5",
              "Rg1+",
              "Kh6",
              "Bd2+",
              "Kg7",
              "Rh1",
              "h8=Q",
              "Rxh8",
              "Kxh8",
              "Ke6",
              "Kg7",
              "Bc3",
              "Rf8",
              "Bd4",
              "Re8+",
              "Kf5",
              "Rf8",
              "Ke6",
              "Rf7",
              "Bc3",
              "Re7+",
              "Kf5",
              "Kf7",
              "Bxf6",
              "g7",
              "Bxg7",
              "Kxg7",
              "Kf4",
              "Kf6",
              "Kf3",
              "Kf5",
              "Kg3",
              "Re3+",
              "Kf2",
              "Kf4",
              "Kg2",
              "Re2+",
              "Kf1",
              "Kf3",
              "Kg1",
              "Kg3",
              "Kf1",
              "Re3",
              "Kg1",
              "Re1#",
              "Kf1",
              "Re2"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Qf3+",
              "Ke6",
              "Nc3",
              "Ne7",
              "d4",
              "c6",
              "O-O",
              "b5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 156,
            "completion_tokens": 178,
            "total_tokens": 334,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 155,
            "completion_tokens": 640,
            "total_tokens": 795,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 467,
          "total_completion_tokens": 1458,
          "total_tokens": 1925
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 156,
            "completion_tokens": 178,
            "total_tokens": 334,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 156,
            "completion_tokens": 640,
            "total_tokens": 796,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 155,
            "completion_tokens": 640,
            "total_tokens": 795,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2Npp/2n5/3np3/2B5/8/PPPP1PPP/RNBQK2R b KQkq - 0 6"
    },
    {
      "turn": 14,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nxd4",
          "aggressive_uci": "c6d4",
          "aggressive_response": "7... Nxd4 8. c3 b5 9. Bxd5+ Qxd5 10. cxd4 Qxg2 11. Rf1 Bb4+ 12. Nc3 exd4 13. Qxd4 Re8+ 14. Be3 Bxc3+ 15. Qxc3 Bh3 16. O-O-O Qe4 17. Rg1 g6 18. Bh6 Qc4 19. Qxc4+ bxc4 20. Rd4 Be6 21. Rf4+ Kg8 22. Rd1 Rad8 23. Rxd8 Rxd8 24. Kc2 Bf5+ 25. Kc3 Rd3+ 26. Kxc4 Rd2 27. Kc3 Rc2+ 28. Kb3 Rc6 29. a3 Rb6+ 30. Ka2 Be6+ 31. Ka1 Bf5 32. Rd4 Rd6 33. Rxd6 cxd6 34. b4 Kf7 35. Kb2 Ke6 36. Kc3 Kd5 37. Be3 a6 38. a4 Kc6 39. Kc4 d5+ 40. Kd4 Bc2 41. a5 Bb3 42. Ke5 Kb5 43. Bc5 Kc4 44. Kf6 d4 45. Kg7 d3 46. Be3 Kxb4 47. Kxh7 Kxa5 48. Kxg6 Kb4 49. h4 a5 50. h5 a4 51. h6 a3 52. h7 a2 53. h8=Q Ka3 54. Qc3 Bc2 55. Bc1+ Ka4 56. Qc4+ Ka5 57. Qxa2+ Kb5 58. Qb2+ Kc5 59. Qc3+ Kd5 60. f4 Ke4 61. f5 Kf3 62. f6 Ke2 63. f7 Kd1 64. f8=Q Kxc1 65. Qf1# Nxf1 gxf1=Q h8=Q#",
          "aggressive_tokens": {
            "prompt_tokens": 164,
            "completion_tokens": 505,
            "total_tokens": 669,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "c3",
            "b5",
            "Bxd5+",
            "Qxd5",
            "cxd4",
            "Qxg2",
            "Rf1",
            "Bb4+",
            "Nc3",
            "exd4",
            "Qxd4",
            "Re8+",
            "Be3",
            "Bxc3+",
            "Qxc3",
            "Bh3",
            "O-O-O",
            "Qe4",
            "Rg1",
            "g6",
            "Bh6",
            "Qc4",
            "Qxc4+",
            "bxc4",
            "Rd4",
            "Be6",
            "Rf4+",
            "Kg8",
            "Rd1",
            "Rad8",
            "Rxd8",
            "Rxd8",
            "Kc2",
            "Bf5+",
            "Kc3",
            "Rd3+",
            "Kxc4",
            "Rd2",
            "Kc3",
            "Rc2+",
            "Kb3",
            "Rc6",
            "a3",
            "Rb6+",
            "Ka2",
            "Be6+",
            "Ka1",
            "Bf5",
            "Rd4",
            "Rd6",
            "Rxd6",
            "cxd6",
            "b4",
            "Kf7",
            "Kb2",
            "Ke6",
            "Kc3",
            "Kd5",
            "Be3",
            "a6",
            "a4",
            "Kc6",
            "Kc4",
            "d5+",
            "Kd4",
            "Bc2",
            "a5",
            "Bb3",
            "Ke5",
            "Kb5",
            "Bc5",
            "Kc4",
            "Kf6",
            "d4",
            "Kg7",
            "d3",
            "Be3",
            "Kxb4",
            "Kxh7",
            "Kxa5",
            "Kxg6",
            "Kb4",
            "h4",
            "a5",
            "h5",
            "a4",
            "h6",
            "a3",
            "h7",
            "a2",
            "h8=Q",
            "Ka3",
            "Qc3",
            "Bc2",
            "Bc1+",
            "Ka4",
            "Qc4+",
            "Ka5",
            "Qxa2+",
            "Kb5",
            "Qb2+",
            "Kc5",
            "Qc3+",
            "Kd5",
            "f4",
            "Ke4",
            "f5",
            "Kf3",
            "f6",
            "Ke2",
            "f7",
            "Kd1",
            "f8=Q",
            "Kxc1",
            "Qf1#",
            "Nxf1",
            "gxf1=Q",
            "h8=Q#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nxd4",
          "positional_uci": "c6d4",
          "positional_response": "7... Nxd4 8. c3 b5 9. Bxd5+ Qxd5 10. cxd4 Qxg2 11. Rf1 Bb4+ 12. Nc3 exd4 13. Qxd4 Re8+ 14. Be3 Bxc3+ 15. Qxc3 Bg4 16. Qxc7+ Kg8 17. Qg3 Rxe3+ 18. Qxe3 Rd8 19. f3 Qxb2 20. Rd1 Qb4+ 21. Kf2 Qb2+ 22. Kg1 Rxd1 23. Qe8# e4 d3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd4+ Kh8 Qe3 Qd",
          "positional_tokens": {
            "prompt_tokens": 164,
            "completion_tokens": 640,
            "total_tokens": 804,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "c3",
            "b5",
            "Bxd5+",
            "Qxd5",
            "cxd4",
            "Qxg2",
            "Rf1",
            "Bb4+",
            "Nc3",
            "exd4",
            "Qxd4",
            "Re8+",
            "Be3",
            "Bxc3+",
            "Qxc3",
            "Bg4",
            "Qxc7+",
            "Kg8",
            "Qg3",
            "Rxe3+",
            "Qxe3",
            "Rd8",
            "f3",
            "Qxb2",
            "Rd1",
            "Qb4+",
            "Kf2",
            "Qb2+",
            "Kg1",
            "Rxd1",
            "Qe8#",
            "e4",
            "d3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3",
            "Qd4+",
            "Kh8",
            "Qe3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nxd4",
          "neutral_uci": "c6d4",
          "neutral_response": "7... Nxd4 8. c3 b5 9. Bxd5+ Qxd5 10. cxd4 Qxg2 11. Rf1 Bb4+ 12. Nc3 exd4 13. Qxd4 Re8+ 14. Be3 Bxc3+ 15. Qxc3 Bg4 16. Qxc7+ Kg8 17. Qg3 Rxe3+ 18. Qxe3 Rd8 19. f3 Qxb2 20. Rd1 Qb4+ 21. Kf2 Rf8 22. Kg3 Bf5 23. Qb3+ Qxb3+ 24. axb3 Bc2 25. Rd7 Bxb3 26. Rxa7 Bc4 27. Re1 h6 28. Ree7 Rf7 29. Rxf7 Bxf7 30. Rb7 Bc4 31. Kf4 Kh7 32. Ke5 Kg6 33. f4 Kh5 34. Rxg7 Kh4 35. f5 Kh3 36. f6 Kxh2 37. f7 Bxf7 38. Rxf7 h5 39. Rh7 Kg3 40. Rxh5 b4 41. Kd4 b3 42. Kc3 b2 43. Kxb2 Kg4 44. Re5 Kf4 45. Re8 Kf5 46. Kc3 Kf6 47. Kd4 Kf7 48. Re5 Kf6 49. Kd5 Kf7 50. Re6 Kf8 51. Kd6 Kf7 52. Kd7 Kf8 53. Re7 Kg8 54. Ke6 Kf8 55. Kf6 Kg8 56. Rf7 Kh8 57. Kg6 Kg8 58. Rf6 Kh8 59. Rf8# Kxf8 60. Kf6 Ke8 61. Ke6 Kd8 62. Kd6 Kc8 63. Kc6 Kb8 64. Kb6 Ka8 65. Ka6 Kb8 66. Kb6 Ka8 67. Ka6 Kb8 68. Kb6 Ka8 69. Ka6 Kb8 70. Kb6 Ka8 71. Ka6 Kb8 72. Kb6 Ka8 73. Ka6 Kb8 74. Kb6 Ka8 75. Ka6 Kb8 76. Kb6 Ka8 77. Ka6 Kb8 78. Kb6 Ka8 79. Ka6 Kb8 80. Kb6 Ka8 81. Ka6 Kb8 82. Kb6 Ka8 83. Ka6 Kb8 84. Kb6 Ka8 85. Ka6 Kb8 86. Kb6 Ka8 87",
          "neutral_tokens": {
            "prompt_tokens": 163,
            "completion_tokens": 640,
            "total_tokens": 803,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "c3",
            "b5",
            "Bxd5+",
            "Qxd5",
            "cxd4",
            "Qxg2",
            "Rf1",
            "Bb4+",
            "Nc3",
            "exd4",
            "Qxd4",
            "Re8+",
            "Be3",
            "Bxc3+",
            "Qxc3",
            "Bg4",
            "Qxc7+",
            "Kg8",
            "Qg3",
            "Rxe3+",
            "Qxe3",
            "Rd8",
            "f3",
            "Qxb2",
            "Rd1",
            "Qb4+",
            "Kf2",
            "Rf8",
            "Kg3",
            "Bf5",
            "Qb3+",
            "Qxb3+",
            "axb3",
            "Bc2",
            "Rd7",
            "Bxb3",
            "Rxa7",
            "Bc4",
            "Re1",
            "h6",
            "Ree7",
            "Rf7",
            "Rxf7",
            "Bxf7",
            "Rb7",
            "Bc4",
            "Kf4",
            "Kh7",
            "Ke5",
            "Kg6",
            "f4",
            "Kh5",
            "Rxg7",
            "Kh4",
            "f5",
            "Kh3",
            "f6",
            "Kxh2",
            "f7",
            "Bxf7",
            "Rxf7",
            "h5",
            "Rh7",
            "Kg3",
            "Rxh5",
            "b4",
            "Kd4",
            "b3",
            "Kc3",
            "b2",
            "Kxb2",
            "Kg4",
            "Re5",
            "Kf4",
            "Re8",
            "Kf5",
            "Kc3",
            "Kf6",
            "Kd4",
            "Kf7",
            "Re5",
            "Kf6",
            "Kd5",
            "Kf7",
            "Re6",
            "Kf8",
            "Kd6",
            "Kf7",
            "Kd7",
            "Kf8",
            "Re7",
            "Kg8",
            "Ke6",
            "Kf8",
            "Kf6",
            "Kg8",
            "Rf7",
            "Kh8",
            "Kg6",
            "Kg8",
            "Rf6",
            "Kh8",
            "Rf8#",
            "Kxf8",
            "Kf6",
            "Ke8",
            "Ke6",
            "Kd8",
            "Kd6",
            "Kc8",
            "Kc6",
            "Kb8",
            "Kb6",
            "Ka8",
            "Ka6",
            "Kb8",
            "Kb6",
            "Ka8",
            "Ka6",
            "Kb8",
            "Kb6",
            "Ka8",
            "Ka6",
            "Kb8",
            "Kb6",
            "Ka8",
            "Ka6",
            "Kb8",
            "Kb6",
            "Ka8",
            "Ka6",
            "Kb8",
            "Kb6",
            "Ka8",
            "Ka6",
            "Kb8",
            "Kb6",
            "Ka8",
            "Ka6",
            "Kb8",
            "Kb6",
            "Ka8",
            "Ka6",
            "Kb8",
            "Kb6",
            "Ka8",
            "Ka6",
            "Kb8",
            "Kb6",
            "Ka8",
            "Ka6",
            "Kb8",
            "Kb6",
            "Ka8",
            "Ka6",
            "Kb8",
            "Kb6",
            "Ka8"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "c6d4",
          "positional_uci": "c6d4",
          "neutral_uci": "c6d4",
          "consensus_move": "c6d4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "c3",
              "b5",
              "Bxd5+",
              "Qxd5",
              "cxd4",
              "Qxg2",
              "Rf1",
              "Bb4+",
              "Nc3",
              "exd4",
              "Qxd4",
              "Re8+",
              "Be3",
              "Bxc3+",
              "Qxc3",
              "Bh3",
              "O-O-O",
              "Qe4",
              "Rg1",
              "g6",
              "Bh6",
              "Qc4",
              "Qxc4+",
              "bxc4",
              "Rd4",
              "Be6",
              "Rf4+",
              "Kg8",
              "Rd1",
              "Rad8",
              "Rxd8",
              "Rxd8",
              "Kc2",
              "Bf5+",
              "Kc3",
              "Rd3+",
              "Kxc4",
              "Rd2",
              "Kc3",
              "Rc2+",
              "Kb3",
              "Rc6",
              "a3",
              "Rb6+",
              "Ka2",
              "Be6+",
              "Ka1",
              "Bf5",
              "Rd4",
              "Rd6",
              "Rxd6",
              "cxd6",
              "b4",
              "Kf7",
              "Kb2",
              "Ke6",
              "Kc3",
              "Kd5",
              "Be3",
              "a6",
              "a4",
              "Kc6",
              "Kc4",
              "d5+",
              "Kd4",
              "Bc2",
              "a5",
              "Bb3",
              "Ke5",
              "Kb5",
              "Bc5",
              "Kc4",
              "Kf6",
              "d4",
              "Kg7",
              "d3",
              "Be3",
              "Kxb4",
              "Kxh7",
              "Kxa5",
              "Kxg6",
              "Kb4",
              "h4",
              "a5",
              "h5",
              "a4",
              "h6",
              "a3",
              "h7",
              "a2",
              "h8=Q",
              "Ka3",
              "Qc3",
              "Bc2",
              "Bc1+",
              "Ka4",
              "Qc4+",
              "Ka5",
              "Qxa2+",
              "Kb5",
              "Qb2+",
              "Kc5",
              "Qc3+",
              "Kd5",
              "f4",
              "Ke4",
              "f5",
              "Kf3",
              "f6",
              "Ke2",
              "f7",
              "Kd1",
              "f8=Q",
              "Kxc1",
              "Qf1#",
              "Nxf1",
              "gxf1=Q",
              "h8=Q#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "c3",
              "b5",
              "Bxd5+",
              "Qxd5",
              "cxd4",
              "Qxg2",
              "Rf1",
              "Bb4+",
              "Nc3",
              "exd4",
              "Qxd4",
              "Re8+",
              "Be3",
              "Bxc3+",
              "Qxc3",
              "Bg4",
              "Qxc7+",
              "Kg8",
              "Qg3",
              "Rxe3+",
              "Qxe3",
              "Rd8",
              "f3",
              "Qxb2",
              "Rd1",
              "Qb4+",
              "Kf2",
              "Qb2+",
              "Kg1",
              "Rxd1",
              "Qe8#",
              "e4",
              "d3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3",
              "Qd4+",
              "Kh8",
              "Qe3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "c3",
              "b5",
              "Bxd5+",
              "Qxd5",
              "cxd4",
              "Qxg2",
              "Rf1",
              "Bb4+",
              "Nc3",
              "exd4",
              "Qxd4",
              "Re8+",
              "Be3",
              "Bxc3+",
              "Qxc3",
              "Bg4",
              "Qxc7+",
              "Kg8",
              "Qg3",
              "Rxe3+",
              "Qxe3",
              "Rd8",
              "f3",
              "Qxb2",
              "Rd1",
              "Qb4+",
              "Kf2",
              "Rf8",
              "Kg3",
              "Bf5",
              "Qb3+",
              "Qxb3+",
              "axb3",
              "Bc2",
              "Rd7",
              "Bxb3",
              "Rxa7",
              "Bc4",
              "Re1",
              "h6",
              "Ree7",
              "Rf7",
              "Rxf7",
              "Bxf7",
              "Rb7",
              "Bc4",
              "Kf4",
              "Kh7",
              "Ke5",
              "Kg6",
              "f4",
              "Kh5",
              "Rxg7",
              "Kh4",
              "f5",
              "Kh3",
              "f6",
              "Kxh2",
              "f7",
              "Bxf7",
              "Rxf7",
              "h5",
              "Rh7",
              "Kg3",
              "Rxh5",
              "b4",
              "Kd4",
              "b3",
              "Kc3",
              "b2",
              "Kxb2",
              "Kg4",
              "Re5",
              "Kf4",
              "Re8",
              "Kf5",
              "Kc3",
              "Kf6",
              "Kd4",
              "Kf7",
              "Re5",
              "Kf6",
              "Kd5",
              "Kf7",
              "Re6",
              "Kf8",
              "Kd6",
              "Kf7",
              "Kd7",
              "Kf8",
              "Re7",
              "Kg8",
              "Ke6",
              "Kf8",
              "Kf6",
              "Kg8",
              "Rf7",
              "Kh8",
              "Kg6",
              "Kg8",
              "Rf6",
              "Kh8",
              "Rf8#",
              "Kxf8",
              "Kf6",
              "Ke8",
              "Ke6",
              "Kd8",
              "Kd6",
              "Kc8",
              "Kc6",
              "Kb8",
              "Kb6",
              "Ka8",
              "Ka6",
              "Kb8",
              "Kb6",
              "Ka8",
              "Ka6",
              "Kb8",
              "Kb6",
              "Ka8",
              "Ka6",
              "Kb8",
              "Kb6",
              "Ka8",
              "Ka6",
              "Kb8",
              "Kb6",
              "Ka8",
              "Ka6",
              "Kb8",
              "Kb6",
              "Ka8",
              "Ka6",
              "Kb8",
              "Kb6",
              "Ka8",
              "Ka6",
              "Kb8",
              "Kb6",
              "Ka8",
              "Ka6",
              "Kb8",
              "Kb6",
              "Ka8",
              "Ka6",
              "Kb8",
              "Kb6",
              "Ka8",
              "Ka6",
              "Kb8",
              "Kb6",
              "Ka8",
              "Ka6",
              "Kb8",
              "Kb6",
              "Ka8"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 164,
            "completion_tokens": 505,
            "total_tokens": 669,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 164,
            "completion_tokens": 640,
            "total_tokens": 804,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 163,
            "completion_tokens": 640,
            "total_tokens": 803,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 491,
          "total_completion_tokens": 1785,
          "total_tokens": 2276
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 164,
            "completion_tokens": 505,
            "total_tokens": 669,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 164,
            "completion_tokens": 640,
            "total_tokens": 804,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 163,
            "completion_tokens": 640,
            "total_tokens": 803,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bq1b1r/ppp2kpp/2n5/3np3/2BP4/8/PPP2PPP/RNBQK2R b KQ - 0 7"
    },
    {
      "turn": 16,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Ke6",
          "aggressive_uci": "f7e6",
          "aggressive_response": "8... Ke6 9. Nc3 c6 10. O-O Bb4 11. Re1 Bxc3 12. bxc3 Nxc2 13. Bf4 Nxe1 14. Rxe1 Kd7 15. Bxd5 cxd5 16. Qxd5+ Ke8 17. Rxe5+ Kf8 18. Qxd8+ Kf7 19. Qe7+ Kg6 20. Rg5+ Kh6 21. Qxg7# Kxg5 22. Qg7+ Kf5 23. Qxh8 Kxf4 24. Qxh7 Bf5 25. Qxb7 Re8 26. Qf3+ Kg5 27. h4+ Kg6 28. h5+ Kg5 29. Qg3+ Kxh5 30. Qf4 Kg6 31. g4 Re1+ 32. Kh2 Be4 33. f3 Re2+ 34. Kg3 Bb1 35. Qd6+ Kg7 36. Qd4+ Kg6 37. Qxa7 Rxa2 38. Qb6+ Kg7 39. Qxb1 Ra7 40. c4 Kf7 41. c5 Ke7 42. c6 Kd6 43. Qb6 Rc7 44. Qxc7+ Kxc7 45. g5 Kxc6 46. g6 Kd6 47. g7 Ke7 48. g8=Q Kf6 49. Qg4 Ke5 50. Qe4+ Kf6 51. f4 Kf7 52. f5 Kf6 53. Qe6+ Kg5 54. Qg6# Kf4 55. f6 Ke5 56. f7 Kd4 57. f8=Q Kc3 58. Qc5+ Kd2 59. Qgc2+ Ke1 60. Qg1# Kd2 61. Qf2+ Kd1 62. Qcc2# Kc1 63. Qe1# Kxc2 64. Qe3 Kb2 65. Qd3 Ka2 66. Qc3 Kb1 67. Qd2 Ka1 68. Kf3 Kb1 69. Ke3 Ka1 70. Kd3 Kb1 71. Kc3 Ka1 72. Qb2# Ka1 73. Qb4 Ka2 74. Qb3+ Ka1 75. Qb2# Ka1 76. Qb4 Ka2 77. Qb3+ Ka1 78. Qb2# Ka1 79. Qb4 Ka2 80. Qb3+ Ka1 81. Qb2#",
          "aggressive_tokens": {
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nc3",
            "c6",
            "O-O",
            "Bb4",
            "Re1",
            "Bxc3",
            "bxc3",
            "Nxc2",
            "Bf4",
            "Nxe1",
            "Rxe1",
            "Kd7",
            "Bxd5",
            "cxd5",
            "Qxd5+",
            "Ke8",
            "Rxe5+",
            "Kf8",
            "Qxd8+",
            "Kf7",
            "Qe7+",
            "Kg6",
            "Rg5+",
            "Kh6",
            "Qxg7#",
            "Kxg5",
            "Qg7+",
            "Kf5",
            "Qxh8",
            "Kxf4",
            "Qxh7",
            "Bf5",
            "Qxb7",
            "Re8",
            "Qf3+",
            "Kg5",
            "h4+",
            "Kg6",
            "h5+",
            "Kg5",
            "Qg3+",
            "Kxh5",
            "Qf4",
            "Kg6",
            "g4",
            "Re1+",
            "Kh2",
            "Be4",
            "f3",
            "Re2+",
            "Kg3",
            "Bb1",
            "Qd6+",
            "Kg7",
            "Qd4+",
            "Kg6",
            "Qxa7",
            "Rxa2",
            "Qb6+",
            "Kg7",
            "Qxb1",
            "Ra7",
            "c4",
            "Kf7",
            "c5",
            "Ke7",
            "c6",
            "Kd6",
            "Qb6",
            "Rc7",
            "Qxc7+",
            "Kxc7",
            "g5",
            "Kxc6",
            "g6",
            "Kd6",
            "g7",
            "Ke7",
            "g8=Q",
            "Kf6",
            "Qg4",
            "Ke5",
            "Qe4+",
            "Kf6",
            "f4",
            "Kf7",
            "f5",
            "Kf6",
            "Qe6+",
            "Kg5",
            "Qg6#",
            "Kf4",
            "f6",
            "Ke5",
            "f7",
            "Kd4",
            "f8=Q",
            "Kc3",
            "Qc5+",
            "Kd2",
            "Qgc2+",
            "Ke1",
            "Qg1#",
            "Kd2",
            "Qf2+",
            "Kd1",
            "Qcc2#",
            "Kc1",
            "Qe1#",
            "Kxc2",
            "Qe3",
            "Kb2",
            "Qd3",
            "Ka2",
            "Qc3",
            "Kb1",
            "Qd2",
            "Ka1",
            "Kf3",
            "Kb1",
            "Ke3",
            "Ka1",
            "Kd3",
            "Kb1",
            "Kc3",
            "Ka1",
            "Qb2#",
            "Ka1",
            "Qb4",
            "Ka2",
            "Qb3+",
            "Ka1",
            "Qb2#",
            "Ka1",
            "Qb4",
            "Ka2",
            "Qb3+",
            "Ka1",
            "Qb2#",
            "Ka1",
            "Qb4",
            "Ka2",
            "Qb3+",
            "Ka1",
            "Qb2#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Ke6",
          "positional_uci": "f7e6",
          "positional_response": "8... Ke6 9. Nc3 c6 10. O-O Bb4 11. Qg4+ Kd6 12. Ne4+ Kc7 13. Qxg7+ Kb8 14. Qxe5+ Nc7\n15. Bf4 Re8 16. Qxc7+ Qxc7 17. Bxc7+ Kxc7 18. Nf6 Rf8 19. Nxh7 Rh8 20. Ng5 Bd6 21. h3 Bf5\n22. c3 Nc2 23. Rad1 Bf4 24. Ne6+ Bxe6 25. Bxe6 Rae8 26. Bf5 Re2 27. Bd3 Rd2 28. Rxd2 Bxd2\n29. Bxc2 Re8 30. Bd3 Rd8 31. Be2 Re8 32. Bf3 Re1 33. Rxe1 Bxe1 34. Kf1 Bd2 35. Ke2 Bc1\n36. b3 Bb2 37. c4 b5 38. cxb5 cxb5 39. g4 a5 40. h4 a4 41. bxa4 bxa4 42. g5 a3\n43. h5 Kd6 44. h6 Ke7 45. g6 Kf8 46. g7+ Kg8 47. Bd5+ Kh7 48. g8=Q+ Kxh6 49. Be4 Kh5\n50. Qg6+ Kh4 51. Bf5 Bf6 52. Qg4# Bg5 53. f4 Bxf4 54. Qg6 Bg5 55. Kf3 Bf4 56. Qg4# Bg5\n57. Qh3# Bf4 58. Qg4# Bg5 59. Qh3# Bf4 60. Qg4# Bg5 61. Qh3# Bf4 62. Qg4# Bg5 63. Qh3# Bf4 64. Qg4# Bg5 65. Qh3# Bf4 66. Qg4# Bg5 67. Qh3# Bf4 68. Qg4# Bg5 69. Qh3# Bf4 70. Qg4# Bg5 71. Qh3# Bf4 72. Qg4# Bg5 73. Qh3# Bf4 74. Qg4# Bg5 75. Qh3# Bf4 76. Qg4# Bg5 77. Qh3# Bf4 78. Qg4# Bg5 79. Qh3# Bf4 80",
          "positional_tokens": {
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nc3",
            "c6",
            "O-O",
            "Bb4",
            "Qg4+",
            "Kd6",
            "Ne4+",
            "Kc7",
            "Qxg7+",
            "Kb8",
            "Qxe5+",
            "Nc7",
            "Bf4",
            "Re8",
            "Qxc7+",
            "Qxc7",
            "Bxc7+",
            "Kxc7",
            "Nf6",
            "Rf8",
            "Nxh7",
            "Rh8",
            "Ng5",
            "Bd6",
            "h3",
            "Bf5",
            "c3",
            "Nc2",
            "Rad1",
            "Bf4",
            "Ne6+",
            "Bxe6",
            "Bxe6",
            "Rae8",
            "Bf5",
            "Re2",
            "Bd3",
            "Rd2",
            "Rxd2",
            "Bxd2",
            "Bxc2",
            "Re8",
            "Bd3",
            "Rd8",
            "Be2",
            "Re8",
            "Bf3",
            "Re1",
            "Rxe1",
            "Bxe1",
            "Kf1",
            "Bd2",
            "Ke2",
            "Bc1",
            "b3",
            "Bb2",
            "c4",
            "b5",
            "cxb5",
            "cxb5",
            "g4",
            "a5",
            "h4",
            "a4",
            "bxa4",
            "bxa4",
            "g5",
            "a3",
            "h5",
            "Kd6",
            "h6",
            "Ke7",
            "g6",
            "Kf8",
            "g7+",
            "Kg8",
            "Bd5+",
            "Kh7",
            "g8=Q+",
            "Kxh6",
            "Be4",
            "Kh5",
            "Qg6+",
            "Kh4",
            "Bf5",
            "Bf6",
            "Qg4#",
            "Bg5",
            "f4",
            "Bxf4",
            "Qg6",
            "Bg5",
            "Kf3",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4",
            "Qg4#",
            "Bg5",
            "Qh3#",
            "Bf4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Ke6",
          "neutral_uci": "f7e6",
          "neutral_response": "8... Ke6 9. Nc3 c6 10. O-O Bb4 11. Qg4+ Kd6 12. Ne4+ Kc7 13. Qxg7+ Kb8 14. Qxe5+ Nc7 15. Bf4 Re8 16. Qxc7+ Qxc7 17. Bxc7+ Kxc7 18. Nf6 Re7 19. c3 Bc5 20. b4 Bb6 21. a4 a5 22. b5 Bf5 23. Rfe1 Rxe1+ 24. Rxe1 Rf8 25. Ne8+ Kd7 26. Ng7 Bg6 27. Rd1+ Ke7 28. Rd2 Rf4 29. Bb3 Re4 30. Kf1 cxb5 31. axb5 a4 32. Bd5 Re5 33. Bxb7 Rxb5 34. Bc6 Rb1+ 35. Ke2 a3 36. Rd7+ Kf6 37. Bd5 Rb2+ 38. Kf3 Rxf2+ 39. Kg3 a2 40. Bxa2 Rxa2 41. Rd6+ Kxg7 42. Rxb6 Be4 43. Kf4 Bxg2 44. c4 Ra4 45. Kg3 Be4 46. c5 Rc4 47. Rb5 Bc6 48. Ra5 Kf6 49. h4 Ke5 50. Ra7 h5 51. Rh7 Rc3+ 52. Kf2 Bf3 53. Re7+ Kd5 54. Rf7 Bg4 55. Rc7 Rxc5 56. Rxc5+ Kxc5 57. Kg3 Kd4 58. Kf4 Kd3 59. Kg3 Ke3 60. Kg2 Kf4 61. Kh2 Kf3 62. Kh1 Kg3 63. Kg1 Kxh4 64. Kh2 Kg5 65. Kg3 h4+ 66. Kh2 Kf4 67. Kg2 h3+ 68. Kh2 Kf3 69. Kh1 Kg3 70. Kg1 h2+ 71. Kh1 Bf3# 0-1\n8... Ke6 9. Nc3 c6 10. O-O Bb4 11. Qg4+ Kd6 12. Ne4+ Kc7 13. Qxg7+ Kb8 14. Qxe5+ Nc7 15. Bf4 Re8 16. Qxc7+ Qxc7 17. Bxc7+ Kxc7 18. Nf6 Re7 19. c3 Bc",
          "neutral_tokens": {
            "prompt_tokens": 173,
            "completion_tokens": 640,
            "total_tokens": 813,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nc3",
            "c6",
            "O-O",
            "Bb4",
            "Qg4+",
            "Kd6",
            "Ne4+",
            "Kc7",
            "Qxg7+",
            "Kb8",
            "Qxe5+",
            "Nc7",
            "Bf4",
            "Re8",
            "Qxc7+",
            "Qxc7",
            "Bxc7+",
            "Kxc7",
            "Nf6",
            "Re7",
            "c3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f7e6",
          "positional_uci": "f7e6",
          "neutral_uci": "f7e6",
          "consensus_move": "f7e6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nc3",
              "c6",
              "O-O",
              "Bb4",
              "Re1",
              "Bxc3",
              "bxc3",
              "Nxc2",
              "Bf4",
              "Nxe1",
              "Rxe1",
              "Kd7",
              "Bxd5",
              "cxd5",
              "Qxd5+",
              "Ke8",
              "Rxe5+",
              "Kf8",
              "Qxd8+",
              "Kf7",
              "Qe7+",
              "Kg6",
              "Rg5+",
              "Kh6",
              "Qxg7#",
              "Kxg5",
              "Qg7+",
              "Kf5",
              "Qxh8",
              "Kxf4",
              "Qxh7",
              "Bf5",
              "Qxb7",
              "Re8",
              "Qf3+",
              "Kg5",
              "h4+",
              "Kg6",
              "h5+",
              "Kg5",
              "Qg3+",
              "Kxh5",
              "Qf4",
              "Kg6",
              "g4",
              "Re1+",
              "Kh2",
              "Be4",
              "f3",
              "Re2+",
              "Kg3",
              "Bb1",
              "Qd6+",
              "Kg7",
              "Qd4+",
              "Kg6",
              "Qxa7",
              "Rxa2",
              "Qb6+",
              "Kg7",
              "Qxb1",
              "Ra7",
              "c4",
              "Kf7",
              "c5",
              "Ke7",
              "c6",
              "Kd6",
              "Qb6",
              "Rc7",
              "Qxc7+",
              "Kxc7",
              "g5",
              "Kxc6",
              "g6",
              "Kd6",
              "g7",
              "Ke7",
              "g8=Q",
              "Kf6",
              "Qg4",
              "Ke5",
              "Qe4+",
              "Kf6",
              "f4",
              "Kf7",
              "f5",
              "Kf6",
              "Qe6+",
              "Kg5",
              "Qg6#",
              "Kf4",
              "f6",
              "Ke5",
              "f7",
              "Kd4",
              "f8=Q",
              "Kc3",
              "Qc5+",
              "Kd2",
              "Qgc2+",
              "Ke1",
              "Qg1#",
              "Kd2",
              "Qf2+",
              "Kd1",
              "Qcc2#",
              "Kc1",
              "Qe1#",
              "Kxc2",
              "Qe3",
              "Kb2",
              "Qd3",
              "Ka2",
              "Qc3",
              "Kb1",
              "Qd2",
              "Ka1",
              "Kf3",
              "Kb1",
              "Ke3",
              "Ka1",
              "Kd3",
              "Kb1",
              "Kc3",
              "Ka1",
              "Qb2#",
              "Ka1",
              "Qb4",
              "Ka2",
              "Qb3+",
              "Ka1",
              "Qb2#",
              "Ka1",
              "Qb4",
              "Ka2",
              "Qb3+",
              "Ka1",
              "Qb2#",
              "Ka1",
              "Qb4",
              "Ka2",
              "Qb3+",
              "Ka1",
              "Qb2#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nc3",
              "c6",
              "O-O",
              "Bb4",
              "Qg4+",
              "Kd6",
              "Ne4+",
              "Kc7",
              "Qxg7+",
              "Kb8",
              "Qxe5+",
              "Nc7",
              "Bf4",
              "Re8",
              "Qxc7+",
              "Qxc7",
              "Bxc7+",
              "Kxc7",
              "Nf6",
              "Rf8",
              "Nxh7",
              "Rh8",
              "Ng5",
              "Bd6",
              "h3",
              "Bf5",
              "c3",
              "Nc2",
              "Rad1",
              "Bf4",
              "Ne6+",
              "Bxe6",
              "Bxe6",
              "Rae8",
              "Bf5",
              "Re2",
              "Bd3",
              "Rd2",
              "Rxd2",
              "Bxd2",
              "Bxc2",
              "Re8",
              "Bd3",
              "Rd8",
              "Be2",
              "Re8",
              "Bf3",
              "Re1",
              "Rxe1",
              "Bxe1",
              "Kf1",
              "Bd2",
              "Ke2",
              "Bc1",
              "b3",
              "Bb2",
              "c4",
              "b5",
              "cxb5",
              "cxb5",
              "g4",
              "a5",
              "h4",
              "a4",
              "bxa4",
              "bxa4",
              "g5",
              "a3",
              "h5",
              "Kd6",
              "h6",
              "Ke7",
              "g6",
              "Kf8",
              "g7+",
              "Kg8",
              "Bd5+",
              "Kh7",
              "g8=Q+",
              "Kxh6",
              "Be4",
              "Kh5",
              "Qg6+",
              "Kh4",
              "Bf5",
              "Bf6",
              "Qg4#",
              "Bg5",
              "f4",
              "Bxf4",
              "Qg6",
              "Bg5",
              "Kf3",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4",
              "Qg4#",
              "Bg5",
              "Qh3#",
              "Bf4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nc3",
              "c6",
              "O-O",
              "Bb4",
              "Qg4+",
              "Kd6",
              "Ne4+",
              "Kc7",
              "Qxg7+",
              "Kb8",
              "Qxe5+",
              "Nc7",
              "Bf4",
              "Re8",
              "Qxc7+",
              "Qxc7",
              "Bxc7+",
              "Kxc7",
              "Nf6",
              "Re7",
              "c3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 173,
            "completion_tokens": 640,
            "total_tokens": 813,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 521,
          "total_completion_tokens": 1920,
          "total_tokens": 2441
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 174,
            "completion_tokens": 640,
            "total_tokens": 814,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 173,
            "completion_tokens": 640,
            "total_tokens": 813,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bq1b1r/ppp2kpp/8/3np3/2Bn4/5Q2/PPP2PPP/RNB1K2R b KQ - 1 8"
    },
    {
      "turn": 18,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nxc2+",
          "aggressive_uci": "d4c2",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4\n8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd1 Nxe3+ 11. fxe3 *",
          "aggressive_tokens": {
            "prompt_tokens": 182,
            "completion_tokens": 94,
            "total_tokens": 276,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kd1",
            "Nxe3+",
            "fxe3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nxc2+",
          "positional_uci": "d4c2",
          "positional_response": "9... Nxc2+ 10. Kd2 Nxe3 11. fxe3 Bb4+ 12. Nc3 Bxc3+ 13. bxc3 c6 14. e4 Qg5+ 15. Kc2 Qxg2+ 16. Kb3 Qxh1 17. exd5+ cxd5 18. Bb2 dxc4+ 19. Kxc4 Qd5+ 20. Kb4 a5+ 21. Ka3 Qc5+ 22. Kb3 a4+ 23. Kc2 Qf2+ 24. Kd3 Rd8+ 25. Kc4 b5+ 26. Kxb5 Ba6+ 27. Kc6 Rac8# Nf6 Nxe4 Nxe4",
          "positional_tokens": {
            "prompt_tokens": 182,
            "completion_tokens": 183,
            "total_tokens": 365,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kd2",
            "Nxe3",
            "fxe3",
            "Bb4+",
            "Nc3",
            "Bxc3+",
            "bxc3",
            "c6",
            "e4",
            "Qg5+",
            "Kc2",
            "Qxg2+",
            "Kb3",
            "Qxh1",
            "exd5+",
            "cxd5",
            "Bb2",
            "dxc4+",
            "Kxc4",
            "Qd5+",
            "Kb4",
            "a5+",
            "Ka3",
            "Qc5+",
            "Kb3",
            "a4+",
            "Kc2",
            "Qf2+",
            "Kd3",
            "Rd8+",
            "Kc4",
            "b5+",
            "Kxb5",
            "Ba6+",
            "Kc6",
            "Rac8#",
            "Nf6",
            "Nxe4",
            "Nxe4"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nxc2+",
          "neutral_uci": "d4c2",
          "neutral_response": "9... Nxc2+ 10. Kd2 Nxe3 11. fxe3 Bb4+ 12. Nc3 Bxc3+ 13. bxc3 c6 14. e4 Qg5+ 15. Kc2 Qxg2+ 16. Bd2 Qxe4+ 17. Kb3 b5 18. Bxd5+ Qxd5+ 19. Kc2 Rd8 20. Rad1 Qxa2+ 21. Kc1 Rd3 22. Rhg1 Kd5 23. Rxg7 Bf5 24. Rg5 Rxc3+ 25. Bxc3+ Ke6 26. Rd2 Qb1# 0-1\n9... Nxc2+ 10. Kd2 Nxe3 11. fxe3 Bb4+ 12. Nc3 Bxc3+ 13. bxc3 c6 14. e4 Qg5+ 15. Kc2 Qxg2+ 16. Bd2 Qxe4+ 17. Kb3 b5 18. Bxd5+ Qxd5+ 19. Kc2 Rd8 20. Rad1 Qxa2+ 21. Kc1 Rd3 22. Rhg1 Kd5 23. Rxg7 Bf5 24. Rg5 Rxc3+ 25. Bxc3+ Ke6 26. Rd2 Qb1# 0-1 9... Nxc2+ 10. Kd2 Nxe3 11. fxe3 Bb4+ 12. Nc3 Bxc3+ 13. bxc3 c6 14. e4 Qg5+ 15. Kc2 Qxg2+ 16. Bd2 Qxe4+ 17. Kb3 b5 18. Bxd5+ Qxd5+ 19. Kc2 Rd8 20. Rad1 Qxa2+ 21. Kc1 Rd3 22. Rhg1 Kd5 23. Rxg7 Bf5 24. Rg5 Rxc3+ 25. Bxc3+ Ke6 26. Rd2 Qb1# 0-1 9... Nxc2+ 10. Kd2 Nxe3 11. fxe3 Bb4+ 12. Nc3 Bxc3+ 13. bxc3 c6 14. e4 Qg5+ 15. Kc2 Qxg2+ 16. Bd2 Qxe4+ 17. Kb3 b5 18. Bxd5+ Qxd5+ 19. Kc2 Rd8 20. Rad1 Qxa2+ 21. Kc1 Rd3 22. Rhg1 Kd5 23. Rxg7 Bf5 24. Rg5 Rxc",
          "neutral_tokens": {
            "prompt_tokens": 181,
            "completion_tokens": 640,
            "total_tokens": 821,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Kd2",
            "Nxe3",
            "fxe3",
            "Bb4+",
            "Nc3",
            "Bxc3+",
            "bxc3",
            "c6",
            "e4",
            "Qg5+",
            "Kc2",
            "Qxg2+",
            "Bd2",
            "Qxe4+",
            "Kb3",
            "b5",
            "Bxd5+",
            "Qxd5+",
            "Kc2",
            "Rd8",
            "Rad1",
            "Qxa2+",
            "Kc1",
            "Rd3",
            "Rhg1",
            "Kd5",
            "Rxg7",
            "Bf5",
            "Rg5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d4c2",
          "positional_uci": "d4c2",
          "neutral_uci": "d4c2",
          "consensus_move": "d4c2",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Kd1",
              "Nxe3+",
              "fxe3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Kd2",
              "Nxe3",
              "fxe3",
              "Bb4+",
              "Nc3",
              "Bxc3+",
              "bxc3",
              "c6",
              "e4",
              "Qg5+",
              "Kc2",
              "Qxg2+",
              "Kb3",
              "Qxh1",
              "exd5+",
              "cxd5",
              "Bb2",
              "dxc4+",
              "Kxc4",
              "Qd5+",
              "Kb4",
              "a5+",
              "Ka3",
              "Qc5+",
              "Kb3",
              "a4+",
              "Kc2",
              "Qf2+",
              "Kd3",
              "Rd8+",
              "Kc4",
              "b5+",
              "Kxb5",
              "Ba6+",
              "Kc6",
              "Rac8#",
              "Nf6",
              "Nxe4",
              "Nxe4"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Kd2",
              "Nxe3",
              "fxe3",
              "Bb4+",
              "Nc3",
              "Bxc3+",
              "bxc3",
              "c6",
              "e4",
              "Qg5+",
              "Kc2",
              "Qxg2+",
              "Bd2",
              "Qxe4+",
              "Kb3",
              "b5",
              "Bxd5+",
              "Qxd5+",
              "Kc2",
              "Rd8",
              "Rad1",
              "Qxa2+",
              "Kc1",
              "Rd3",
              "Rhg1",
              "Kd5",
              "Rxg7",
              "Bf5",
              "Rg5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 182,
            "completion_tokens": 94,
            "total_tokens": 276,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 182,
            "completion_tokens": 183,
            "total_tokens": 365,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 181,
            "completion_tokens": 640,
            "total_tokens": 821,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 545,
          "total_completion_tokens": 917,
          "total_tokens": 1462
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 182,
            "completion_tokens": 94,
            "total_tokens": 276,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 182,
            "completion_tokens": 183,
            "total_tokens": 365,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 17,
            "turn_number": 9,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 181,
            "completion_tokens": 640,
            "total_tokens": 821,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bq1b1r/ppp3pp/4k3/3np3/2Bn4/4Q3/PPP2PPP/RNB1K2R b KQ - 3 9"
    },
    {
      "turn": 20,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nxe3",
          "aggressive_uci": "c2e3",
          "aggressive_response": "1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4\n8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxe3 11. fxe3 Bb4+ 12. Nc3 Bxc3+ 13. bxc3 c6\n14. e4 b5 15. exd5+ cxd5 16. Bxb5 Qg5+ 17. Kc2 Qxg2+ 18. Bd2 Rf8 19. Rhg1 Qe4+ 20. Bd3 Qa4+ 21. Kc1 Ba6 22. Bxa6 Qxa6 23. Rxg7 Rf1+ 24. Kb2 Rb8+ 25. Kc2 Qa4+ 26. Kd3 Qe4# 1-0\n1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Ng5 d5 5. exd5 Nxd5 6. Nxf7 Kxf7 7. d4 Nxd4\n8. Qf3+ Ke6 9. Qe3 Nxc2+ 10. Kd2 Nxe3 11. fxe3 Bb4+ 12. Nc3 Bxc3+ 13. bxc3 c6\n14. e4 b5 15. exd5+ cxd5 16. Bxb5 Qg5+ 17. Kc2 Qxg2+ 18. Bd2 Rf8 19. Rhg1 Qe4+ 20. Bd3 Qa4+ 21. Kc1 Ba6 22. Bxa6 Qxa6 23. Rxg7 Rf1+ 24. Kb2 Rb8+ 25. Kc2 Qa4+ 26. Kd3 Qe4# 1-0 1-0 1-0 1-0",
          "aggressive_tokens": {
            "prompt_tokens": 192,
            "completion_tokens": 490,
            "total_tokens": 682,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "fxe3",
            "Bb4+",
            "Nc3",
            "Bxc3+",
            "bxc3",
            "c6",
            "e4",
            "b5",
            "exd5+",
            "cxd5",
            "Bxb5",
            "Qg5+",
            "Kc2",
            "Qxg2+",
            "Bd2",
            "Rf8",
            "Rhg1",
            "Qe4+",
            "Bd3",
            "Qa4+",
            "Kc1",
            "Ba6",
            "Bxa6",
            "Qxa6",
            "Rxg7",
            "Rf1+",
            "Kb2",
            "Rb8+",
            "Kc2",
            "Qa4+",
            "Kd3",
            "Qe4#"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nxe3",
          "positional_uci": "c2e3",
          "positional_response": "10... Nxe3 11. Kxe3 Bc5+ 12. Ke4 Qh4+ 13. Kd3 Qd4+ 14. Ke2 Qxc4+ 15. Kf3 Rf8+ 16. Kg3 Bxf2+ 17. Kh3 Qh4#",
          "positional_tokens": {
            "prompt_tokens": 192,
            "completion_tokens": 72,
            "total_tokens": 264,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kxe3",
            "Bc5+",
            "Ke4",
            "Qh4+",
            "Kd3",
            "Qd4+",
            "Ke2",
            "Qxc4+",
            "Kf3",
            "Rf8+",
            "Kg3",
            "Bxf2+",
            "Kh3",
            "Qh4#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nxe3",
          "neutral_uci": "c2e3",
          "neutral_response": "10... Nxe3 11. Kxe3 Bc5+ 12. Ke4 Qh4+ 13. Kd3 Qd4+ 14. Ke2 Qxc4+ 15. Kf3 Rf8+ 16. Kg3 Bxf2+ 17. Kh3 Qh4# Bxf2 Qg3#",
          "neutral_tokens": {
            "prompt_tokens": 191,
            "completion_tokens": 79,
            "total_tokens": 270,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Kxe3",
            "Bc5+",
            "Ke4",
            "Qh4+",
            "Kd3",
            "Qd4+",
            "Ke2",
            "Qxc4+",
            "Kf3",
            "Rf8+",
            "Kg3",
            "Bxf2+",
            "Kh3",
            "Qh4#",
            "Bxf2",
            "Qg3#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "c2e3",
          "positional_uci": "c2e3",
          "neutral_uci": "c2e3",
          "consensus_move": "c2e3",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "fxe3",
              "Bb4+",
              "Nc3",
              "Bxc3+",
              "bxc3",
              "c6",
              "e4",
              "b5",
              "exd5+",
              "cxd5",
              "Bxb5",
              "Qg5+",
              "Kc2",
              "Qxg2+",
              "Bd2",
              "Rf8",
              "Rhg1",
              "Qe4+",
              "Bd3",
              "Qa4+",
              "Kc1",
              "Ba6",
              "Bxa6",
              "Qxa6",
              "Rxg7",
              "Rf1+",
              "Kb2",
              "Rb8+",
              "Kc2",
              "Qa4+",
              "Kd3",
              "Qe4#"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Kxe3",
              "Bc5+",
              "Ke4",
              "Qh4+",
              "Kd3",
              "Qd4+",
              "Ke2",
              "Qxc4+",
              "Kf3",
              "Rf8+",
              "Kg3",
              "Bxf2+",
              "Kh3",
              "Qh4#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Kxe3",
              "Bc5+",
              "Ke4",
              "Qh4+",
              "Kd3",
              "Qd4+",
              "Ke2",
              "Qxc4+",
              "Kf3",
              "Rf8+",
              "Kg3",
              "Bxf2+",
              "Kh3",
              "Qh4#",
              "Bxf2",
              "Qg3#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 192,
            "completion_tokens": 490,
            "total_tokens": 682,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 192,
            "completion_tokens": 72,
            "total_tokens": 264,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 191,
            "completion_tokens": 79,
            "total_tokens": 270,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 575,
          "total_completion_tokens": 641,
          "total_tokens": 1216
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 19,
            "turn_number": 10,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 192,
            "completion_tokens": 490,
            "total_tokens": 682,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 19,
            "turn_number": 10,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 192,
            "completion_tokens": 72,
            "total_tokens": 264,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 19,
            "turn_number": 10,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 191,
            "completion_tokens": 79,
            "total_tokens": 270,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bq1b1r/ppp3pp/4k3/3np3/2B5/4Q3/PPnK1PPP/RNB4R b - - 1 10"
    },
    {
      "turn": 1,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d4",
          "aggressive_uci": "d2d4",
          "aggressive_response": "d4 d5\nd4 d5 Nc3 Nf6 Bg5 e6 e4 dxe4 Nxe4 Be7 Nxf6+ Bxf6 Bxf6 Qxf6 Nf3 O-O Bd3 Nc6 O-O Rd8 c3 e5 dxe5 Nxe5 Nxe5 Qxe5 Bxh7+ Kxh7 Qxd8 b6 Rfe1 Qf5 Qh4+ Kg6 Rad1 Bb7 Rd4 f6 Rg4+ Kf7 Qg3 g5 Qxc7+ Kg6 Qxb7 Rh8 Re7 Qb1+",
          "aggressive_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 138,
            "total_tokens": 248,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d5",
            "Nc3",
            "Nf6",
            "Bg5",
            "e6",
            "e4",
            "dxe4",
            "Nxe4",
            "Be7",
            "Nxf6+",
            "Bxf6",
            "Bxf6",
            "Qxf6",
            "Nf3",
            "O-O",
            "Bd3",
            "Nc6",
            "O-O",
            "Rd8",
            "c3",
            "e5",
            "dxe5",
            "Nxe5",
            "Nxe5",
            "Qxe5",
            "Bxh7+",
            "Kxh7",
            "Qxd8",
            "b6",
            "Rfe1",
            "Qf5",
            "Qh4+",
            "Kg6",
            "Rad1",
            "Bb7",
            "Rd4",
            "f6",
            "Rg4+",
            "Kf7",
            "Qg3",
            "g5",
            "Qxc7+",
            "Kg6",
            "Qxb7",
            "Rh8",
            "Re7",
            "Qb1+"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e4",
          "positional_uci": "e2e4",
          "positional_response": "1. e4 e5 2. Nf3 Nc6 3. Bb5 Bc5 4. O-O Nge7 5. Nxe5 Nxe5 6. d4 Bxd4 7. Qxd4 N7c6 8. Bxc6 Nxc6 9. Qxg7 Rf8 10. Bh6 Qe7 11. Nc3 d6 12. Nd5 Qe5 13. Qxf8+ Kd7 14. Qxf7+ Kd8 15. Qxc7+ Ke8 16. Qxh7 Be6 17. Nc7+ Kd8 18. Nxa8 Nd4 19. Qc7+ Ke8 20. Qb8+ Kf7 21. Qxb7+ Kg6 22. Be3 Ne2+ 23. Kh1 Qxh2+ 24. Kxh2\n\n1. e4 e5 2. Nf3 Nc6 3. Bb5 Bc5 4. O-O Nge7 5. Nxe5 Nxe5 6. d4 Bxd4 7. Qxd4 N7c6 8. Bxc6 Nxc6 9. Qxg7 Rf8 10. Bh6 Qe7 11. Nc3 d6 12. Nd5 Qe5 13. Qxf8+ Kd7 14. Qxf7+ Kd8 15. Qxc7+ Ke8 16. Qxh7 Be6 17. Nc7+ Kd8 18. Nxa8 Nd4 19. Qc7+ Ke8 20. Qb8+ Kf7 21. Qxb7+ Kg6 22. Be3 Ne2+ 23. Kh1 Qxh2+ 24. Kxh2\n\n24... Qh5#",
          "positional_tokens": {
            "prompt_tokens": 110,
            "completion_tokens": 430,
            "total_tokens": 540,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "Nf3",
            "Nc6",
            "Bb5",
            "Bc5",
            "O-O",
            "Nge7",
            "Nxe5",
            "Nxe5",
            "d4",
            "Bxd4",
            "Qxd4",
            "N7c6",
            "Bxc6",
            "Nxc6",
            "Qxg7",
            "Rf8",
            "Bh6",
            "Qe7",
            "Nc3",
            "d6",
            "Nd5",
            "Qe5",
            "Qxf8+",
            "Kd7",
            "Qxf7+",
            "Kd8",
            "Qxc7+",
            "Ke8",
            "Qxh7",
            "Be6",
            "Nc7+",
            "Kd8",
            "Nxa8",
            "Nd4",
            "Qc7+",
            "Ke8",
            "Qb8+",
            "Kf7",
            "Qxb7+",
            "Kg6",
            "Be3",
            "Ne2+",
            "Kh1",
            "Qxh2+",
            "Kxh2",
            "Qh5#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d4",
          "neutral_uci": "d2d4",
          "neutral_response": "d4 d5\nd4 d5 Nf3 Nf6 Bf4 Bf5 e3 e6 c4 c6 Nc3 Nbd7 Qb3 Qb6 c5 Qxb3 axb3 a6 b4 Rc8 Nd2 Be7 Nb3 Bd8 Na5 Bxa5 bxa5 O-O Ra4 Rfe8 Rb4 e5 dxe5 Nxe5 Bxe5 Rxe5 Rxb7 d4 exd4 Re6 Bc4 Ree8 Bxa6 Rcd8 Ne2 d3 Bxd3 Bxd3 O-O Bxe2 Rfe1 Nd5 Nc3 Rxe1 Rxe1 Nxc3 bxc3 d2 Rd1 Bc2 Rxd2 Rxd2 g3 Be4 h4 Rd1+ Kh2 Rh1# d4 d5 Nf3 Nf6 Bf4 Bf5 e3 e6 c4 c6 Nc3 Nbd7 Qb3 Qb6 c5 Qxb3 axb3 a6 b4 Rc8 Nd2 Be7 Nb3 Bd8 Na5 Bxa5 bxa5 O-O Ra4 Rfe8 Rb4 e5 dxe5 Nxe5 Bxe5 Rxe5 Rxb7 d4 exd4 Re6 Bc4 Ree8 Bxa6 Rcd8 Ne2 d3 Bxd3 Bxd3 O-O Bxe2 Rfe1 Nd5 Nc3 Rxe1 Rxe1 Nxc3 bxc3 d2 Rd1 Bc2 Rxd2 Rxd2 g3 Be4 h4 Rd1+ Kh2 Rh1# d4 d5 Nf3 Nf6 Bf4 Bf5 e3 e6 c4 c6 Nc3 Nbd7 Qb3 Qb6 c5 Qxb3 axb3 a6 b4 Rc8 Nd2 Be7 Nb3 Bd8 Na5 Bxa5 bxa5 O-O Ra4 Rfe8 Rb4 e5 dxe5 Nxe5 Bxe5 Rxe5 Rxb7 d4 exd4 Re6 Bc4 Ree8 Bxa6 Rcd8 Ne2 d3 Bxd3 Bxd3 O-O Bxe2 Rfe1 Nd5 Nc3 Rxe1 Rxe1 Nxc3 bxc3 d2 Rd1 Bc2 Rxd2 Rxd2 g3 Be4 h4 Rd1+ Kh2 Rh1# d4 d5 Nf3 Nf6 Bf4 Bf5 e3 e6 c4 c6 Nc3 Nbd7 Qb3 Qb6 c5 Qxb3 axb3 a6 b4 Rc8 Nd2 Be7 Nb3 Bd8 Na5 Bxa5 bxa5 O-O Ra4 Rfe8 Rb4 e5 dxe5 Nxe5 Bxe5 Rxe5 Rxb7 d4 exd4 Re6 Bc4 Ree8 Bxa6 Rcd8 Ne2 d3 B",
          "neutral_tokens": {
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "exd4",
            "Re6",
            "Bc4",
            "Ree8",
            "Bxa6",
            "Rcd8",
            "Ne2",
            "d3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d2d4",
          "positional_uci": "e2e4",
          "neutral_uci": "d2d4",
          "consensus_move": "d2d4",
          "source_agents": [
            "aggressive",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d5",
              "Nc3",
              "Nf6",
              "Bg5",
              "e6",
              "e4",
              "dxe4",
              "Nxe4",
              "Be7",
              "Nxf6+",
              "Bxf6",
              "Bxf6",
              "Qxf6",
              "Nf3",
              "O-O",
              "Bd3",
              "Nc6",
              "O-O",
              "Rd8",
              "c3",
              "e5",
              "dxe5",
              "Nxe5",
              "Nxe5",
              "Qxe5",
              "Bxh7+",
              "Kxh7",
              "Qxd8",
              "b6",
              "Rfe1",
              "Qf5",
              "Qh4+",
              "Kg6",
              "Rad1",
              "Bb7",
              "Rd4",
              "f6",
              "Rg4+",
              "Kf7",
              "Qg3",
              "g5",
              "Qxc7+",
              "Kg6",
              "Qxb7",
              "Rh8",
              "Re7",
              "Qb1+"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "e5",
              "Nf3",
              "Nc6",
              "Bb5",
              "Bc5",
              "O-O",
              "Nge7",
              "Nxe5",
              "Nxe5",
              "d4",
              "Bxd4",
              "Qxd4",
              "N7c6",
              "Bxc6",
              "Nxc6",
              "Qxg7",
              "Rf8",
              "Bh6",
              "Qe7",
              "Nc3",
              "d6",
              "Nd5",
              "Qe5",
              "Qxf8+",
              "Kd7",
              "Qxf7+",
              "Kd8",
              "Qxc7+",
              "Ke8",
              "Qxh7",
              "Be6",
              "Nc7+",
              "Kd8",
              "Nxa8",
              "Nd4",
              "Qc7+",
              "Ke8",
              "Qb8+",
              "Kf7",
              "Qxb7+",
              "Kg6",
              "Be3",
              "Ne2+",
              "Kh1",
              "Qxh2+",
              "Kxh2",
              "Qh5#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "exd4",
              "Re6",
              "Bc4",
              "Ree8",
              "Bxa6",
              "Rcd8",
              "Ne2",
              "d3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 110,
            "completion_tokens": 138,
            "total_tokens": 248,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 110,
            "completion_tokens": 430,
            "total_tokens": 540,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 329,
          "total_completion_tokens": 1208,
          "total_tokens": 1537
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 110,
            "completion_tokens": 138,
            "total_tokens": 248,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 110,
            "completion_tokens": 430,
            "total_tokens": 540,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 0,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 109,
            "completion_tokens": 640,
            "total_tokens": 749,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    },
    {
      "turn": 2,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "e5",
          "aggressive_uci": "e7e5",
          "aggressive_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7\n6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Na5 10. Bc2 c5\n11. d4 Qc7 12. Nbd2 cxd4 13. cxd4 Nc6 14. Nb3 a5\n15. Be3 a4 16. Nbd2 Bd7 17. Rc1 Qb7 18. Bb1 Rfc8\n19. Nf1 Bd8 20. Ng3 Bb6 21. Qd2 Ba5 22. Qe2 Bxe1\n23. Rxe1 Re8 24. Qd2 Rac8 25. Bg5 Kh8 26. Bxf6 gxf6\n27. Qh6 Rg8 28. Qxf6+ Rg7 29. Nh5 Rg8 30. Ng5 Be8\n31. Nxg7 Rxg7 32. dxe5 dxe5 33. Rd1 Qe7 34. Qxe7 Nxe7\n35. Rd8 Rg8 36. Nf3 f6 37. Bd3 Kg7 38. Rb8 Bc6\n39. Rxg8+ Kxg8 40. Kf1 Kf7 41. Ke2 Ke6 42. Ke3 Kd6\n43. Nd2 Kc5 44. a3 Bd7 45. Nb1 Nc6 46. Nc3 Nd4\n47. Nd5 f5 48. Nf6 f4+ 49. Kd2 Be6 50. Nxh7 b4\n51. Ng5 Bg8 52. h4 bxa3 53. bxa3 Nb3+ 54. Kc3 Nd4\n55. h5 Nb5+ 56. Bxb5 Kxb5 57. h6 Kc5 58. h7 Bxh7\n59. Nxh7 Kd6 60. Kb4 Ke6 61. Kxa4 Kf7 62. Kb5 Kg6\n63. a4 Kxh7 64. a5 Kg6 65. a6 Kg5 66. a7 Kg4\n67. a8=Q f3 68. Qg8+ Kf4 69. Qg3+ Kxe4 70. Qxf3+ Kd4\n71. Qe3+ Kd5 72. g4 Ke6 73. g5 Kf5 74. g6 Kxg6\n75. Qxe5 Kf7 76. f4 Kg6 77. f5+ Kf7 78. f6 Kg6\n79",
          "aggressive_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Qc7",
            "Nbd2",
            "cxd4",
            "cxd4",
            "Nc6",
            "Nb3",
            "a5",
            "Be3",
            "a4",
            "Nbd2",
            "Bd7",
            "Rc1",
            "Qb7",
            "Bb1",
            "Rfc8",
            "Nf1",
            "Bd8",
            "Ng3",
            "Bb6",
            "Qd2",
            "Ba5",
            "Qe2",
            "Bxe1",
            "Rxe1",
            "Re8",
            "Qd2",
            "Rac8",
            "Bg5",
            "Kh8",
            "Bxf6",
            "gxf6",
            "Qh6",
            "Rg8",
            "Qxf6+",
            "Rg7",
            "Nh5",
            "Rg8",
            "Ng5",
            "Be8",
            "Nxg7",
            "Rxg7",
            "dxe5",
            "dxe5",
            "Rd1",
            "Qe7",
            "Qxe7",
            "Nxe7",
            "Rd8",
            "Rg8",
            "Nf3",
            "f6",
            "Bd3",
            "Kg7",
            "Rb8",
            "Bc6",
            "Rxg8+",
            "Kxg8",
            "Kf1",
            "Kf7",
            "Ke2",
            "Ke6",
            "Ke3",
            "Kd6",
            "Nd2",
            "Kc5",
            "a3",
            "Bd7",
            "Nb1",
            "Nc6",
            "Nc3",
            "Nd4",
            "Nd5",
            "f5",
            "Nf6",
            "f4+",
            "Kd2",
            "Be6",
            "Nxh7",
            "b4",
            "Ng5",
            "Bg8",
            "h4",
            "bxa3",
            "bxa3",
            "Nb3+",
            "Kc3",
            "Nd4",
            "h5",
            "Nb5+",
            "Bxb5",
            "Kxb5",
            "h6",
            "Kc5",
            "h7",
            "Bxh7",
            "Nxh7",
            "Kd6",
            "Kb4",
            "Ke6",
            "Kxa4",
            "Kf7",
            "Kb5",
            "Kg6",
            "a4",
            "Kxh7",
            "a5",
            "Kg6",
            "a6",
            "Kg5",
            "a7",
            "Kg4",
            "a8=Q",
            "f3",
            "Qg8+",
            "Kf4",
            "Qg3+",
            "Kxe4",
            "Qxf3+",
            "Kd4",
            "Qe3+",
            "Kd5",
            "g4",
            "Ke6",
            "g5",
            "Kf5",
            "g6",
            "Kxg6",
            "Qxe5",
            "Kf7",
            "f4",
            "Kg6",
            "f5+",
            "Kf7",
            "f6",
            "Kg6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "e5",
          "positional_uci": "e7e5",
          "positional_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6\n4. Ba4 Nf6 5. O-O Be7 6. Re1 b5\n7. Bb3 d6 8. c3 O-O 9. h3 Na5\n10. Bc2 c5 11. d4 Qc7 12. Nbd2 cxd4\n13. cxd4 Nc6 14. Nb3 a5 15. Be3 a4\n16. Nbd2 Bd7 17. Rc1 Qb7 18. Bb1 Rfc8\n19. Nf1 Bd8 20. Ng3 Bb6 21. Qd2 Ba5\n22. Qe2 Bxe1 23. Rxe1 Re8 24. Qd2 Rac8\n25. Bg5 Kh8 26. Bxf6 gxf6 27. Qh6 Rg8\n28. Qxf6+ Rg7 29. Nh5 Rg8 30. Ng5 Be8\n31. Nxg7 Rxg7 32. dxe5 dxe5 33. Rd1 Qe7\n34. Qxe7 Nxe7 35. Rd8 Rg8 36. Nf3 f6\n37. Bd3 Kg7 38. Rb8 Bc6 39. Rxg8+ Kxg8\n40. Kf1 Kf7 41. Ke2 Ke6 42. Ke3 Kd6\n43. Nd2 Kc5 44. a3 Bd7 45. Nb1 Nc6\n46. Nc3 Nd4 47. Nd5 f5 48. Nf6 f4+\n49. Kd2 Be6 50. Nxh7 b4 51. Ng5 Bg8\n52. h4 bxa3 53. bxa3 Nb3+ 54. Kc3 Nd4\n55. h5 Nb5+ 56. Bxb5 Kxb5 57. h6 Kc5\n58. h7 Bxh7 59. Nxh7 Kd6 60. Kb4 Ke6\n61. Kxa4 Kf7 62. Kb5 Kg6 63. a4 Kxh7\n64. a5 Kg6 65. a6 Kg5 66. a7 Kg4\n67. a8=Q f3 68. Qg8+ Kf4 69. Qg3+ Kxe4\n70. Qxf3+ Kd4 71. Qe3+ Kd5 72. g4 e4\n73. g5 Ke5 74. g6 Kf6 75. Qxe4 Kg7\n76. Qf5 Kh6 77. Qf7 Kg5 78. g7 Kh4\n79. g8=",
          "positional_tokens": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Na5",
            "Bc2",
            "c5",
            "d4",
            "Qc7",
            "Nbd2",
            "cxd4",
            "cxd4",
            "Nc6",
            "Nb3",
            "a5",
            "Be3",
            "a4",
            "Nbd2",
            "Bd7",
            "Rc1",
            "Qb7",
            "Bb1",
            "Rfc8",
            "Nf1",
            "Bd8",
            "Ng3",
            "Bb6",
            "Qd2",
            "Ba5",
            "Qe2",
            "Bxe1",
            "Rxe1",
            "Re8",
            "Qd2",
            "Rac8",
            "Bg5",
            "Kh8",
            "Bxf6",
            "gxf6",
            "Qh6",
            "Rg8",
            "Qxf6+",
            "Rg7",
            "Nh5",
            "Rg8",
            "Ng5",
            "Be8",
            "Nxg7",
            "Rxg7",
            "dxe5",
            "dxe5",
            "Rd1",
            "Qe7",
            "Qxe7",
            "Nxe7",
            "Rd8",
            "Rg8",
            "Nf3",
            "f6",
            "Bd3",
            "Kg7",
            "Rb8",
            "Bc6",
            "Rxg8+",
            "Kxg8",
            "Kf1",
            "Kf7",
            "Ke2",
            "Ke6",
            "Ke3",
            "Kd6",
            "Nd2",
            "Kc5",
            "a3",
            "Bd7",
            "Nb1",
            "Nc6",
            "Nc3",
            "Nd4",
            "Nd5",
            "f5",
            "Nf6",
            "f4+",
            "Kd2",
            "Be6",
            "Nxh7",
            "b4",
            "Ng5",
            "Bg8",
            "h4",
            "bxa3",
            "bxa3",
            "Nb3+",
            "Kc3",
            "Nd4",
            "h5",
            "Nb5+",
            "Bxb5",
            "Kxb5",
            "h6",
            "Kc5",
            "h7",
            "Bxh7",
            "Nxh7",
            "Kd6",
            "Kb4",
            "Ke6",
            "Kxa4",
            "Kf7",
            "Kb5",
            "Kg6",
            "a4",
            "Kxh7",
            "a5",
            "Kg6",
            "a6",
            "Kg5",
            "a7",
            "Kg4",
            "a8=Q",
            "f3",
            "Qg8+",
            "Kf4",
            "Qg3+",
            "Kxe4",
            "Qxf3+",
            "Kd4",
            "Qe3+",
            "Kd5",
            "g4",
            "e4",
            "g5",
            "Ke5",
            "g6",
            "Kf6",
            "Qxe4",
            "Kg7",
            "Qf5",
            "Kh6",
            "Qf7",
            "Kg5",
            "g7",
            "Kh4",
            "g8"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "e5",
          "neutral_uci": "e7e5",
          "neutral_response": "1... e5 2. Nf3 Nc6 3. Bb5 a6\n4. Ba4 Nf6 5. O-O Be7 6. Re1 b5\n7. Bb3 d6 8. c3 O-O 9. h3 Bb7\n10. d4 Re8 11. Nbd2 Bf8 12. a4 h6\n13. Bc2 exd4 14. cxd4 Nb4 15. Bb1 c5\n16. d5 Nd7 17. Ra3 c4 18. Nd4 Qf6\n19. N2f3 Nc5 20. axb5 Nbd3 21. Bxd3 Nxd3\n22. Re2 axb5 23. Nxb5 Rxa3 24. Nxa3 Ba6\n25. Qa4 Ra8 26. Be3 Nxb2\n27. Qc6 Rc8 28. Qxa6 c3 29. Qxc8 Nd3",
          "neutral_tokens": {
            "prompt_tokens": 113,
            "completion_tokens": 230,
            "total_tokens": 343,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Nf3",
            "Nc6",
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "d6",
            "c3",
            "O-O",
            "h3",
            "Bb7",
            "d4",
            "Re8",
            "Nbd2",
            "Bf8",
            "a4",
            "h6",
            "Bc2",
            "exd4",
            "cxd4",
            "Nb4",
            "Bb1",
            "c5",
            "d5",
            "Nd7",
            "Ra3",
            "c4",
            "Nd4",
            "Qf6",
            "N2f3",
            "Nc5",
            "axb5",
            "Nbd3",
            "Bxd3",
            "Nxd3",
            "Re2",
            "axb5",
            "Nxb5",
            "Rxa3",
            "Nxa3",
            "Ba6",
            "Qa4",
            "Ra8",
            "Be3",
            "Nxb2",
            "Qc6",
            "Rc8",
            "Qxa6",
            "c3",
            "Qxc8",
            "Nd3"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e7e5",
          "positional_uci": "e7e5",
          "neutral_uci": "e7e5",
          "consensus_move": "e7e5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Qc7",
              "Nbd2",
              "cxd4",
              "cxd4",
              "Nc6",
              "Nb3",
              "a5",
              "Be3",
              "a4",
              "Nbd2",
              "Bd7",
              "Rc1",
              "Qb7",
              "Bb1",
              "Rfc8",
              "Nf1",
              "Bd8",
              "Ng3",
              "Bb6",
              "Qd2",
              "Ba5",
              "Qe2",
              "Bxe1",
              "Rxe1",
              "Re8",
              "Qd2",
              "Rac8",
              "Bg5",
              "Kh8",
              "Bxf6",
              "gxf6",
              "Qh6",
              "Rg8",
              "Qxf6+",
              "Rg7",
              "Nh5",
              "Rg8",
              "Ng5",
              "Be8",
              "Nxg7",
              "Rxg7",
              "dxe5",
              "dxe5",
              "Rd1",
              "Qe7",
              "Qxe7",
              "Nxe7",
              "Rd8",
              "Rg8",
              "Nf3",
              "f6",
              "Bd3",
              "Kg7",
              "Rb8",
              "Bc6",
              "Rxg8+",
              "Kxg8",
              "Kf1",
              "Kf7",
              "Ke2",
              "Ke6",
              "Ke3",
              "Kd6",
              "Nd2",
              "Kc5",
              "a3",
              "Bd7",
              "Nb1",
              "Nc6",
              "Nc3",
              "Nd4",
              "Nd5",
              "f5",
              "Nf6",
              "f4+",
              "Kd2",
              "Be6",
              "Nxh7",
              "b4",
              "Ng5",
              "Bg8",
              "h4",
              "bxa3",
              "bxa3",
              "Nb3+",
              "Kc3",
              "Nd4",
              "h5",
              "Nb5+",
              "Bxb5",
              "Kxb5",
              "h6",
              "Kc5",
              "h7",
              "Bxh7",
              "Nxh7",
              "Kd6",
              "Kb4",
              "Ke6",
              "Kxa4",
              "Kf7",
              "Kb5",
              "Kg6",
              "a4",
              "Kxh7",
              "a5",
              "Kg6",
              "a6",
              "Kg5",
              "a7",
              "Kg4",
              "a8=Q",
              "f3",
              "Qg8+",
              "Kf4",
              "Qg3+",
              "Kxe4",
              "Qxf3+",
              "Kd4",
              "Qe3+",
              "Kd5",
              "g4",
              "Ke6",
              "g5",
              "Kf5",
              "g6",
              "Kxg6",
              "Qxe5",
              "Kf7",
              "f4",
              "Kg6",
              "f5+",
              "Kf7",
              "f6",
              "Kg6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Na5",
              "Bc2",
              "c5",
              "d4",
              "Qc7",
              "Nbd2",
              "cxd4",
              "cxd4",
              "Nc6",
              "Nb3",
              "a5",
              "Be3",
              "a4",
              "Nbd2",
              "Bd7",
              "Rc1",
              "Qb7",
              "Bb1",
              "Rfc8",
              "Nf1",
              "Bd8",
              "Ng3",
              "Bb6",
              "Qd2",
              "Ba5",
              "Qe2",
              "Bxe1",
              "Rxe1",
              "Re8",
              "Qd2",
              "Rac8",
              "Bg5",
              "Kh8",
              "Bxf6",
              "gxf6",
              "Qh6",
              "Rg8",
              "Qxf6+",
              "Rg7",
              "Nh5",
              "Rg8",
              "Ng5",
              "Be8",
              "Nxg7",
              "Rxg7",
              "dxe5",
              "dxe5",
              "Rd1",
              "Qe7",
              "Qxe7",
              "Nxe7",
              "Rd8",
              "Rg8",
              "Nf3",
              "f6",
              "Bd3",
              "Kg7",
              "Rb8",
              "Bc6",
              "Rxg8+",
              "Kxg8",
              "Kf1",
              "Kf7",
              "Ke2",
              "Ke6",
              "Ke3",
              "Kd6",
              "Nd2",
              "Kc5",
              "a3",
              "Bd7",
              "Nb1",
              "Nc6",
              "Nc3",
              "Nd4",
              "Nd5",
              "f5",
              "Nf6",
              "f4+",
              "Kd2",
              "Be6",
              "Nxh7",
              "b4",
              "Ng5",
              "Bg8",
              "h4",
              "bxa3",
              "bxa3",
              "Nb3+",
              "Kc3",
              "Nd4",
              "h5",
              "Nb5+",
              "Bxb5",
              "Kxb5",
              "h6",
              "Kc5",
              "h7",
              "Bxh7",
              "Nxh7",
              "Kd6",
              "Kb4",
              "Ke6",
              "Kxa4",
              "Kf7",
              "Kb5",
              "Kg6",
              "a4",
              "Kxh7",
              "a5",
              "Kg6",
              "a6",
              "Kg5",
              "a7",
              "Kg4",
              "a8=Q",
              "f3",
              "Qg8+",
              "Kf4",
              "Qg3+",
              "Kxe4",
              "Qxf3+",
              "Kd4",
              "Qe3+",
              "Kd5",
              "g4",
              "e4",
              "g5",
              "Ke5",
              "g6",
              "Kf6",
              "Qxe4",
              "Kg7",
              "Qf5",
              "Kh6",
              "Qf7",
              "Kg5",
              "g7",
              "Kh4",
              "g8"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Nf3",
              "Nc6",
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "d6",
              "c3",
              "O-O",
              "h3",
              "Bb7",
              "d4",
              "Re8",
              "Nbd2",
              "Bf8",
              "a4",
              "h6",
              "Bc2",
              "exd4",
              "cxd4",
              "Nb4",
              "Bb1",
              "c5",
              "d5",
              "Nd7",
              "Ra3",
              "c4",
              "Nd4",
              "Qf6",
              "N2f3",
              "Nc5",
              "axb5",
              "Nbd3",
              "Bxd3",
              "Nxd3",
              "Re2",
              "axb5",
              "Nxb5",
              "Rxa3",
              "Nxa3",
              "Ba6",
              "Qa4",
              "Ra8",
              "Be3",
              "Nxb2",
              "Qc6",
              "Rc8",
              "Qxa6",
              "c3",
              "Qxc8",
              "Nd3"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 113,
            "completion_tokens": 230,
            "total_tokens": 343,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 341,
          "total_completion_tokens": 1510,
          "total_tokens": 1851
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 114,
            "completion_tokens": 640,
            "total_tokens": 754,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 1,
            "turn_number": 1,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 113,
            "completion_tokens": 230,
            "total_tokens": 343,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1"
    },
    {
      "turn": 4,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nc6",
          "aggressive_uci": "b8c6",
          "aggressive_response": "2... Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 O-O 8. c3 d5 9. exd5 Nxd5 10. Nxe5 Nxe5 11. Rxe5 c6 12. d4 Bd6 13. Re1 Qh4 14. g3 Qh3 15. Be3 Bg4 16. Qd3 Rae8 17. Nd2 Re6 18. Qf1 Qh5 19. a4 Rfe8 20. axb5 axb5 21. Bxd5 Qxd5 22. Qg2 Qh5 23. Qxc6 Bxg3 24. Qg2 Bf4 25. Ra8 Bb8 26. Rea1 Rg6 27. R1a6 Be6 28. Qxg6 Qxg6+ 29. Kf1 Qd3+ 30. Ke1 Bg4 31. f3 Qxe3+ 32. Kd1 Bf5 33. Ne4 Qd3+ 34. Kc1 Bf4+ 35. Nd2 Qc2# e4 e5 Nf3 Nc6 Bb5 a6 Ba4 Nf6 O-O Be7 Re1 b5 Bb3 O-O c3 d5 exd5 Nxd5 Nxe5 Nxe5 Rxe5 c6 d4 Bd6 Re1 Qh4 g3 Qh3 Be3 Bg4 Qd3 Rae8 Nd2 Re6 Qf1 Qh5 a4 Rfe8 axb5 axb5 Bxd5 Qxd5 Qg2 Qh5 Qxc6 Bxg3 Qg2 Bf4 Bxf4 Rxe1+ Rxe1 Rxe1+ Nf1 Bh3 Qa8+ Bc8 Qxc8# e4 e5 Nf3 Nc6 Bb5 a6 Ba4 Nf6 O-O Be7 Re1 b5 Bb3 O-O c3 d5 exd5 Nxd5 Nxe5 Nxe5 Rxe5 c6 d4 Bd6 Re1 Qh4 g3 Qh3 Be3 Bg4 Qd3 Rae8 Nd2 Re6 Qf1 Qh5 a4 Rfe8 axb5 axb5 Bxd5 Qxd5 Qg2 Qh5 Qxc6 Bxg3 Qg2 Bf4 Bxf4 Rxe1+ Rxe1 Rxe1+ Nf1 Bh3 Qa8+ Bc8 Qxc8# e4 e5 Nf3 Nc6 Bb5 a6 Ba4 Nf6 O-O Be7 Re1 b5 Bb3 O-O c3 d5 exd5 Nxd5 N",
          "aggressive_tokens": {
            "prompt_tokens": 122,
            "completion_tokens": 640,
            "total_tokens": 762,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "O-O",
            "c3",
            "d5",
            "exd5",
            "Nxd5"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nc6",
          "positional_uci": "b8c6",
          "positional_response": "2... Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 O-O 8. c3 d5\n9. exd5 Nxd5 10. Nxe5 Nxe5 11. Rxe5 c6 12. d4 Bd6 13. Re1 Qh4 14. g3 Qh3\n15. Be3 Bg4 16. Qd3 Rae8 17. Nd2 Re6 18. a4 Qh5 19. axb5 axb5 20. Ra6 Bb8\n21. Qf1 Rfe8 22. Bxd5 Qxd5 23. Qg2 Qh5 24. Rxc6 Bh3 25. Qf3 Bg4 26. Qg2 Bh3\n27. Qf3 Bg4 28. Qg2 Bh3 29. Qf3 Bg4 30. Qg2 Bh3 31. Qf3 Bg4 32. Qg2 Bh3\n33. Qf3 Bg4 34. Qg2 Bh3 35. Qf3 Bg4 36. Qg2 Bh3 37. Qf3 Bg4 38. Qg2 Bh3\n39. Qf3 Bg4 40. Qg2 Bh3 41. Qf3 Bg4 42. Qg2 Bh3 43. Qf3 Bg4 44. Qg2 Bh3\n45. Qf3 Bg4 46. Qg2 Bh3 47. Qf3 Bg4 48. Qg2 Bh3 49. Qf3 Bg4 50. Qg2 Bh3\n51. Qf3 Bg4 52. Qg2 Bh3 53. Qf3 Bg4 54. Qg2 Bh3 55. Qf3 Bg4 56. Qg2 Bh3\n57. Qf3 Bg4 58. Qg2 Bh3 59. Qf3 Bg4 60. Qg2 Bh3 61. Qf3 Bg4 62. Qg2 Bh3\n63. Qf3 Bg4 64. Qg2 Bh3 65. Qf3 Bg4 66. Qg2 Bh3 67. Qf3 Bg4 68. Qg2 Bh3\n69. Qf3 Bg4 70. Qg2 Bh3 71. Qf3 Bg4 72. Qg2 Bh3 73. Qf3 Bg4 74. Qg2 Bh3\n75. Qf3 Bg4 76. Qg2 Bh3 77. Qf3 Bg4 78. Qg2",
          "positional_tokens": {
            "prompt_tokens": 122,
            "completion_tokens": 640,
            "total_tokens": 762,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "O-O",
            "c3",
            "d5",
            "exd5",
            "Nxd5",
            "Nxe5",
            "Nxe5",
            "Rxe5",
            "c6",
            "d4",
            "Bd6",
            "Re1",
            "Qh4",
            "g3",
            "Qh3",
            "Be3",
            "Bg4",
            "Qd3",
            "Rae8",
            "Nd2",
            "Re6",
            "a4",
            "Qh5",
            "axb5",
            "axb5",
            "Ra6",
            "Bb8",
            "Qf1",
            "Rfe8",
            "Bxd5",
            "Qxd5",
            "Qg2",
            "Qh5",
            "Rxc6",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2",
            "Bh3",
            "Qf3",
            "Bg4",
            "Qg2"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nc6",
          "neutral_uci": "b8c6",
          "neutral_response": "2... Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 O-O 8. c3 d5\n9. exd5 Nxd5 10. Nxe5 Nxe5 11. Rxe5 c6 12. d4 Bd6 13. Re1 Qh4 14. g3 Qh3\n15. Be3 Bg4 16. Qd3 Rae8 17. Nd2 Re6 18. a4 Qh5 19. axb5 axb5 20. Ra6 Bc7\n21. Bxd5 Qxd5 22. c4 bxc4 23. Qxc4 Qh5 24. Rxc6 Rxc6 25. Qxc6 Ba5 26. Ra1 Bxd2\n27. Bxd2 Bf3 28. Qc4 Bd5 29. Qd3 Re8 30. Re1 Rxe1+ 31. Bxe1 Qh3 32. f3 Qe6\n33. Kf2 Bc4 34. Qe3 Qc8 35. Bc3 h6 36. Qe5 Qh3 37. d5 Qxh2+ 38. Ke3 Qe2+\n39. Kd4 Qd3+ 40. Kc5 f6 41. Qe8+ Kh7 42. Qe4+ Qxe4 43. fxe4 Bb3 44. d6 Ba4\n45. b4 Kg8 46. b5 Kf7 47. b6 1-0\n47... Ke8 48. b7 Kd7 49. b8=Q",
          "neutral_tokens": {
            "prompt_tokens": 121,
            "completion_tokens": 401,
            "total_tokens": 522,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bb5",
            "a6",
            "Ba4",
            "Nf6",
            "O-O",
            "Be7",
            "Re1",
            "b5",
            "Bb3",
            "O-O",
            "c3",
            "d5",
            "exd5",
            "Nxd5",
            "Nxe5",
            "Nxe5",
            "Rxe5",
            "c6",
            "d4",
            "Bd6",
            "Re1",
            "Qh4",
            "g3",
            "Qh3",
            "Be3",
            "Bg4",
            "Qd3",
            "Rae8",
            "Nd2",
            "Re6",
            "a4",
            "Qh5",
            "axb5",
            "axb5",
            "Ra6",
            "Bc7",
            "Bxd5",
            "Qxd5",
            "c4",
            "bxc4",
            "Qxc4",
            "Qh5",
            "Rxc6",
            "Rxc6",
            "Qxc6",
            "Ba5",
            "Ra1",
            "Bxd2",
            "Bxd2",
            "Bf3",
            "Qc4",
            "Bd5",
            "Qd3",
            "Re8",
            "Re1",
            "Rxe1+",
            "Bxe1",
            "Qh3",
            "f3",
            "Qe6",
            "Kf2",
            "Bc4",
            "Qe3",
            "Qc8",
            "Bc3",
            "h6",
            "Qe5",
            "Qh3",
            "d5",
            "Qxh2+",
            "Ke3",
            "Qe2+",
            "Kd4",
            "Qd3+",
            "Kc5",
            "f6",
            "Qe8+",
            "Kh7",
            "Qe4+",
            "Qxe4",
            "fxe4",
            "Bb3",
            "d6",
            "Ba4",
            "b4",
            "Kg8",
            "b5",
            "Kf7",
            "b6",
            "Ke8",
            "b7",
            "Kd7",
            "b8=Q"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "b8c6",
          "positional_uci": "b8c6",
          "neutral_uci": "b8c6",
          "consensus_move": "b8c6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "O-O",
              "c3",
              "d5",
              "exd5",
              "Nxd5"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "O-O",
              "c3",
              "d5",
              "exd5",
              "Nxd5",
              "Nxe5",
              "Nxe5",
              "Rxe5",
              "c6",
              "d4",
              "Bd6",
              "Re1",
              "Qh4",
              "g3",
              "Qh3",
              "Be3",
              "Bg4",
              "Qd3",
              "Rae8",
              "Nd2",
              "Re6",
              "a4",
              "Qh5",
              "axb5",
              "axb5",
              "Ra6",
              "Bb8",
              "Qf1",
              "Rfe8",
              "Bxd5",
              "Qxd5",
              "Qg2",
              "Qh5",
              "Rxc6",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2",
              "Bh3",
              "Qf3",
              "Bg4",
              "Qg2"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bb5",
              "a6",
              "Ba4",
              "Nf6",
              "O-O",
              "Be7",
              "Re1",
              "b5",
              "Bb3",
              "O-O",
              "c3",
              "d5",
              "exd5",
              "Nxd5",
              "Nxe5",
              "Nxe5",
              "Rxe5",
              "c6",
              "d4",
              "Bd6",
              "Re1",
              "Qh4",
              "g3",
              "Qh3",
              "Be3",
              "Bg4",
              "Qd3",
              "Rae8",
              "Nd2",
              "Re6",
              "a4",
              "Qh5",
              "axb5",
              "axb5",
              "Ra6",
              "Bc7",
              "Bxd5",
              "Qxd5",
              "c4",
              "bxc4",
              "Qxc4",
              "Qh5",
              "Rxc6",
              "Rxc6",
              "Qxc6",
              "Ba5",
              "Ra1",
              "Bxd2",
              "Bxd2",
              "Bf3",
              "Qc4",
              "Bd5",
              "Qd3",
              "Re8",
              "Re1",
              "Rxe1+",
              "Bxe1",
              "Qh3",
              "f3",
              "Qe6",
              "Kf2",
              "Bc4",
              "Qe3",
              "Qc8",
              "Bc3",
              "h6",
              "Qe5",
              "Qh3",
              "d5",
              "Qxh2+",
              "Ke3",
              "Qe2+",
              "Kd4",
              "Qd3+",
              "Kc5",
              "f6",
              "Qe8+",
              "Kh7",
              "Qe4+",
              "Qxe4",
              "fxe4",
              "Bb3",
              "d6",
              "Ba4",
              "b4",
              "Kg8",
              "b5",
              "Kf7",
              "b6",
              "Ke8",
              "b7",
              "Kd7",
              "b8=Q"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 122,
            "completion_tokens": 640,
            "total_tokens": 762,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 122,
            "completion_tokens": 640,
            "total_tokens": 762,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 121,
            "completion_tokens": 401,
            "total_tokens": 522,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 365,
          "total_completion_tokens": 1681,
          "total_tokens": 2046
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 122,
            "completion_tokens": 640,
            "total_tokens": 762,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 122,
            "completion_tokens": 640,
            "total_tokens": 762,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 3,
            "turn_number": 2,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 121,
            "completion_tokens": 401,
            "total_tokens": 522,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "rnbqkbnr/pppp1ppp/8/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R b KQkq - 1 2"
    },
    {
      "turn": 6,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nf6",
          "aggressive_uci": "g8f6",
          "aggressive_response": "3... Nf6 4. d3 Bc5 5. O-O O-O 6. Nc3 d6 7. Bg5 h6 8. Bh4 g5 9. Nxg5 hxg5 10. Bxg5 Be6 11. Qf3 Kg7 12. Qg3 Nh5 13. Qh4 f6 14. Bxe6 fxg5 15. Qxh5 Qf6 16. Bf5 Rh8 17. Qg4 Rh4 18. Qg3 Rah8 19. Nd5 Qh6 20. h3 Nd4 21. Rae1 Nxf5 22. exf5 g4 23. f6+ Kf7 24. Re4 gxh3 25. Rxh4 Qxh4 26. Qg7+ Ke6 27. Nxc7+ Kf5 28. f7 hxg2 29. f8=Q+ Rxf8 30. Qxf8+ Kg6 31. Qg8+ Kf6 32. Qxg2 Ke7 33. Nd5+ Kd7 34. Qg7+ Kc6 35. Ne7+ Kb5 36. c4+ Ka6 37. Nd5 Qd4 38. b4 Bb6 39. b5+ Ka5 40. Nxb6 axb6 41. Qxb7 Qg4+ 42. Qg2 Qe2 43. Qe4 Qxa2 44. Qa8+ Kb4 45. Qxa2 Kc5 46. Qa3+ Kd4 47. Qxd6+ Kc3 48. Qxe5+ Kxd3 49. Rd1+ Kxc4 50. Qd5+ Kb4 51. Rb1+ Ka5 52. Qa2# Qe2 Qf3 Qg4+",
          "aggressive_tokens": {
            "prompt_tokens": 131,
            "completion_tokens": 441,
            "total_tokens": 572,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d3",
            "Bc5",
            "O-O",
            "O-O",
            "Nc3",
            "d6",
            "Bg5",
            "h6",
            "Bh4",
            "g5",
            "Nxg5",
            "hxg5",
            "Bxg5",
            "Be6",
            "Qf3",
            "Kg7",
            "Qg3",
            "Nh5",
            "Qh4",
            "f6",
            "Bxe6",
            "fxg5",
            "Qxh5",
            "Qf6",
            "Bf5",
            "Rh8",
            "Qg4",
            "Rh4",
            "Qg3",
            "Rah8",
            "Nd5",
            "Qh6",
            "h3",
            "Nd4",
            "Rae1",
            "Nxf5",
            "exf5",
            "g4",
            "f6+",
            "Kf7",
            "Re4",
            "gxh3",
            "Rxh4",
            "Qxh4",
            "Qg7+",
            "Ke6",
            "Nxc7+",
            "Kf5",
            "f7",
            "hxg2",
            "f8=Q+",
            "Rxf8",
            "Qxf8+",
            "Kg6",
            "Qg8+",
            "Kf6",
            "Qxg2",
            "Ke7",
            "Nd5+",
            "Kd7",
            "Qg7+",
            "Kc6",
            "Ne7+",
            "Kb5",
            "c4+",
            "Ka6",
            "Nd5",
            "Qd4",
            "b4",
            "Bb6",
            "b5+",
            "Ka5",
            "Nxb6",
            "axb6",
            "Qxb7",
            "Qg4+",
            "Qg2",
            "Qe2",
            "Qe4",
            "Qxa2",
            "Qa8+",
            "Kb4",
            "Qxa2",
            "Kc5",
            "Qa3+",
            "Kd4",
            "Qxd6+",
            "Kc3",
            "Qxe5+",
            "Kxd3",
            "Rd1+",
            "Kxc4",
            "Qd5+",
            "Kb4",
            "Rb1+",
            "Ka5",
            "Qa2#",
            "Qe2",
            "Qf3",
            "Qg4+"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nf6",
          "positional_uci": "g8f6",
          "positional_response": "3... Nf6 4. d3 Bc5 5. O-O O-O 6. Nc3 d6 7. Bg5 h6 8. Bh4 Be6 9. Nd5 Bxd5 10. Bxd5 g5 11. Bxc6 bxc6 12. Bg3 Nh5 13. c3 Nxg3 14. hxg3 f5 15. exf5 Rxf5 16. d4 exd4 17. Nxd4 Bxd4 18. Qxd4 Qf6 19. Qc4+ d5 20. Qc5 Qd6 21. Qxd6 cxd6 22. Rfe1 Kf7 23. Re2 Re8 24. Rae1 Rxe2 25. Rxe2 Re5 26. Kf1 Rxe2 27. Kxe2 Ke6 28. Ke3 Ke5 29. f4+ gxf4+ 30. gxf4+ Kf5 31. Kf3 h5 32. g3 c5 33. b3 a5 34. a3 d4 35. cxd4 cxd4 36. b4 axb4 37. axb4 d3 38. b5 d2 39. Ke2 Kg4 40. b6 Kxg3 41. b7 h4 42. b8=Q h3 43. Qxd6 h2 44. f5+ Kg2 45. Qg6+ Kh3 46. Qh5+ Kg2 47. Qg4+ Kh1 48. Qf3+ Kg1 49. Qf2+ Kh1 50. Qf1# 1-0\n50... Kg1 51. Qf2+ Kh1 52. Qf3+ Kg1",
          "positional_tokens": {
            "prompt_tokens": 131,
            "completion_tokens": 419,
            "total_tokens": 550,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d3",
            "Bc5",
            "O-O",
            "O-O",
            "Nc3",
            "d6",
            "Bg5",
            "h6",
            "Bh4",
            "Be6",
            "Nd5",
            "Bxd5",
            "Bxd5",
            "g5",
            "Bxc6",
            "bxc6",
            "Bg3",
            "Nh5",
            "c3",
            "Nxg3",
            "hxg3",
            "f5",
            "exf5",
            "Rxf5",
            "d4",
            "exd4",
            "Nxd4",
            "Bxd4",
            "Qxd4",
            "Qf6",
            "Qc4+",
            "d5",
            "Qc5",
            "Qd6",
            "Qxd6",
            "cxd6",
            "Rfe1",
            "Kf7",
            "Re2",
            "Re8",
            "Rae1",
            "Rxe2",
            "Rxe2",
            "Re5",
            "Kf1",
            "Rxe2",
            "Kxe2",
            "Ke6",
            "Ke3",
            "Ke5",
            "f4+",
            "gxf4+",
            "gxf4+",
            "Kf5",
            "Kf3",
            "h5",
            "g3",
            "c5",
            "b3",
            "a5",
            "a3",
            "d4",
            "cxd4",
            "cxd4",
            "b4",
            "axb4",
            "axb4",
            "d3",
            "b5",
            "d2",
            "Ke2",
            "Kg4",
            "b6",
            "Kxg3",
            "b7",
            "h4",
            "b8=Q",
            "h3",
            "Qxd6",
            "h2",
            "f5+",
            "Kg2",
            "Qg6+",
            "Kh3",
            "Qh5+",
            "Kg2",
            "Qg4+",
            "Kh1",
            "Qf3+",
            "Kg1",
            "Qf2+",
            "Kh1",
            "Qf1#",
            "Kg1",
            "Qf2+",
            "Kh1",
            "Qf3+",
            "Kg1"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nf6",
          "neutral_uci": "g8f6",
          "neutral_response": "3... Nf6 4. d3 Bc5 5. O-O O-O 6. Nc3 d6 7. Bg5 h6 8. Bh4 g5 9. Nxg5 hxg5 10. Bxg5 Be6 11. Qf3 Kg7 12. Qg3 Nh5 13. Qh4 f6 14. Be3 Bxc4 15. dxc4 Rh8 16. Qg4+ Kf7 17. Nd5 Qc8 18. Qf3 Nd4 19. Bxd4 Bxd4 20. c3 Bb6 21. g4 Qg8 22. Kh1 Nf4 23. Nxf4 exf4 24. Qxf4 Qg5 25. Qg3 Rag8 26. f3 Rh4 27. Rad1 Rgh8 28. Rd5 Qe3 29. Rh5 R4xh5 30. gxh5 Rxh5 31. Qg4 Rg5 32. Qd7+ Kg6 33. Qe8+ Kh6 34. Qh8+ Kg6 35. Qe8+ Kh7 36. Qf7+ Rg7 37. Qh5+ Qh6 38. Qf5+ Qg6 39. Qh3+ Kg8 40. Qc8+ Kf7 41. Qd7+ Kf8 42. Qd8+ Qe8 43. Qxf6+ Kg8 44. Qh4 Qg6 45. Qd8+ Kh7 46. Qh4+ Qh6 47. Qe1 Qh3 48. Qe2 Rg3 49. e5 dxe5 50. Qe4+ Kh6 51. Qe2 Rg5 52. b4 Be3 53. Rd1 Bf4 54. Rd8 Kg7 55. Rd1 Kf6 56. c5 Rh5 57. Kg1 Bxh2+ 58. Kf2 Bg3+ 59. Ke3 Bf4+ 60. Kd3 Qf5+ 61. Kc4 Qe6+ 62. Kd3 Rh2 63. Qe4 Qxa2 64. Qxb7 Qe2# 1-0\n3... Nf6 4. d3 Bc5 5. O-O O-O 6. Nc3 d6 7. Bg5 h6 8. Bh4 g5 9. Nxg5 hxg5 10. Bxg5 Be6 11. Qf3 Kg7 12. Qg3 Nh5 13. Qh4 f6 14. Be3 Bxc4 15. dxc",
          "neutral_tokens": {
            "prompt_tokens": 130,
            "completion_tokens": 640,
            "total_tokens": 770,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d3",
            "Bc5",
            "O-O",
            "O-O",
            "Nc3",
            "d6",
            "Bg5",
            "h6",
            "Bh4",
            "g5",
            "Nxg5",
            "hxg5",
            "Bxg5",
            "Be6",
            "Qf3",
            "Kg7",
            "Qg3",
            "Nh5",
            "Qh4",
            "f6",
            "Be3",
            "Bxc4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "g8f6",
          "positional_uci": "g8f6",
          "neutral_uci": "g8f6",
          "consensus_move": "g8f6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d3",
              "Bc5",
              "O-O",
              "O-O",
              "Nc3",
              "d6",
              "Bg5",
              "h6",
              "Bh4",
              "g5",
              "Nxg5",
              "hxg5",
              "Bxg5",
              "Be6",
              "Qf3",
              "Kg7",
              "Qg3",
              "Nh5",
              "Qh4",
              "f6",
              "Bxe6",
              "fxg5",
              "Qxh5",
              "Qf6",
              "Bf5",
              "Rh8",
              "Qg4",
              "Rh4",
              "Qg3",
              "Rah8",
              "Nd5",
              "Qh6",
              "h3",
              "Nd4",
              "Rae1",
              "Nxf5",
              "exf5",
              "g4",
              "f6+",
              "Kf7",
              "Re4",
              "gxh3",
              "Rxh4",
              "Qxh4",
              "Qg7+",
              "Ke6",
              "Nxc7+",
              "Kf5",
              "f7",
              "hxg2",
              "f8=Q+",
              "Rxf8",
              "Qxf8+",
              "Kg6",
              "Qg8+",
              "Kf6",
              "Qxg2",
              "Ke7",
              "Nd5+",
              "Kd7",
              "Qg7+",
              "Kc6",
              "Ne7+",
              "Kb5",
              "c4+",
              "Ka6",
              "Nd5",
              "Qd4",
              "b4",
              "Bb6",
              "b5+",
              "Ka5",
              "Nxb6",
              "axb6",
              "Qxb7",
              "Qg4+",
              "Qg2",
              "Qe2",
              "Qe4",
              "Qxa2",
              "Qa8+",
              "Kb4",
              "Qxa2",
              "Kc5",
              "Qa3+",
              "Kd4",
              "Qxd6+",
              "Kc3",
              "Qxe5+",
              "Kxd3",
              "Rd1+",
              "Kxc4",
              "Qd5+",
              "Kb4",
              "Rb1+",
              "Ka5",
              "Qa2#",
              "Qe2",
              "Qf3",
              "Qg4+"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "d3",
              "Bc5",
              "O-O",
              "O-O",
              "Nc3",
              "d6",
              "Bg5",
              "h6",
              "Bh4",
              "Be6",
              "Nd5",
              "Bxd5",
              "Bxd5",
              "g5",
              "Bxc6",
              "bxc6",
              "Bg3",
              "Nh5",
              "c3",
              "Nxg3",
              "hxg3",
              "f5",
              "exf5",
              "Rxf5",
              "d4",
              "exd4",
              "Nxd4",
              "Bxd4",
              "Qxd4",
              "Qf6",
              "Qc4+",
              "d5",
              "Qc5",
              "Qd6",
              "Qxd6",
              "cxd6",
              "Rfe1",
              "Kf7",
              "Re2",
              "Re8",
              "Rae1",
              "Rxe2",
              "Rxe2",
              "Re5",
              "Kf1",
              "Rxe2",
              "Kxe2",
              "Ke6",
              "Ke3",
              "Ke5",
              "f4+",
              "gxf4+",
              "gxf4+",
              "Kf5",
              "Kf3",
              "h5",
              "g3",
              "c5",
              "b3",
              "a5",
              "a3",
              "d4",
              "cxd4",
              "cxd4",
              "b4",
              "axb4",
              "axb4",
              "d3",
              "b5",
              "d2",
              "Ke2",
              "Kg4",
              "b6",
              "Kxg3",
              "b7",
              "h4",
              "b8=Q",
              "h3",
              "Qxd6",
              "h2",
              "f5+",
              "Kg2",
              "Qg6+",
              "Kh3",
              "Qh5+",
              "Kg2",
              "Qg4+",
              "Kh1",
              "Qf3+",
              "Kg1",
              "Qf2+",
              "Kh1",
              "Qf1#",
              "Kg1",
              "Qf2+",
              "Kh1",
              "Qf3+",
              "Kg1"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "d3",
              "Bc5",
              "O-O",
              "O-O",
              "Nc3",
              "d6",
              "Bg5",
              "h6",
              "Bh4",
              "g5",
              "Nxg5",
              "hxg5",
              "Bxg5",
              "Be6",
              "Qf3",
              "Kg7",
              "Qg3",
              "Nh5",
              "Qh4",
              "f6",
              "Be3",
              "Bxc4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 131,
            "completion_tokens": 441,
            "total_tokens": 572,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 131,
            "completion_tokens": 419,
            "total_tokens": 550,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 130,
            "completion_tokens": 640,
            "total_tokens": 770,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "total_prompt_tokens": 392,
          "total_completion_tokens": 1500,
          "total_tokens": 1892
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 131,
            "completion_tokens": 441,
            "total_tokens": 572,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 131,
            "completion_tokens": 419,
            "total_tokens": 550,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 5,
            "turn_number": 3,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 130,
            "completion_tokens": 640,
            "total_tokens": 770,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          }
        ]
      },
      "board_fen": "r1bqkbnr/pppp1ppp/2n5/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R b KQkq - 3 3"
    },
    {
      "turn": 8,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Nxe4",
          "aggressive_uci": "f6e4",
          "aggressive_response": "4... Nxe4 5. Nxe4 d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. d3 Nd4 11. g4 Bg6 12. Nxe5 Bd6 13. Re1 O-O 14. Nxg6 fxg6 15. Be3 Nf3+ 16. Kg2 Nxe1+ 17. Qxe1 Qf6 18. Ne4 Qf3+ 19. Kg1 Qxh3 20. Nxd6 Qxg4+ 21. Kf1 cxd6 22. Qc3 Rae8 23. Qb3+ Kh8 24. Re1 Qh3+ 25. Ke2 Rxf2+ 26. Kxf2 Qh2+ 27. Kf1 Rf8+ 28. Bf4 Rxf4# 29. Ke1 Qf2+ 30. Kd1 Qf3+ 31. Kc1 Rf8 32. Qe6 Qf4+ 33. Kb1 Qf1 34. a3 Qf6 35. Qe4 b6 36. Qb7 Qf7 37. Re7 Qf1+ 38. Ka2 Qf6 39. Qxa7 h5 40. Re4 d5 41. Rb4 h4 42. Rxb6 Qf7 43. Rb7 Qf6 44. Rb6 Qf7 45. Rb7 Qf6 46. Rb6 Qf7 47. Rb7 Qf6 48. Rb6 Qf7 49. Rb7 Qf6 50. Rb6 Qf7 51. Rb7 Qf6 52. Rb6 Qf7 53. Rb7 Qf6 54. Rb6 Qf7 55. Rb7 Qf6 56. Rb6 Qf7 57. Rb7 Qf6 58. Rb6 Qf7 59. Rb7 Qf6 60. Rb6 Qf7 61. Rb7 Qf6 62. Rb6 Qf7 63. Rb7 Qf6 64. Rb6 Qf7 65. Rb7 Qf6 66. Rb6 Qf7 67. Rb7 Qf6 68. Rb6 Qf7 69. Rb7 Qf6 70. Rb6 Qf7 71. Rb7 Qf6 72. Rb6 Qf7 73. Rb7 Qf6 74. Rb6 Qf7 75. Rb7 Qf6 76. Rb6 Q",
          "aggressive_tokens": {
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Bxd5",
            "Qxd5",
            "Nc3",
            "Qd8",
            "O-O",
            "Bg4",
            "h3",
            "Bh5",
            "d3",
            "Nd4",
            "g4",
            "Bg6",
            "Nxe5",
            "Bd6",
            "Re1",
            "O-O",
            "Nxg6",
            "fxg6",
            "Be3",
            "Nf3+",
            "Kg2",
            "Nxe1+",
            "Qxe1",
            "Qf6",
            "Ne4",
            "Qf3+",
            "Kg1",
            "Qxh3",
            "Nxd6",
            "Qxg4+",
            "Kf1",
            "cxd6",
            "Qc3",
            "Rae8",
            "Qb3+",
            "Kh8",
            "Re1",
            "Qh3+",
            "Ke2",
            "Rxf2+",
            "Kxf2",
            "Qh2+",
            "Kf1",
            "Rf8+",
            "Bf4",
            "Rxf4#",
            "Ke1",
            "Qf2+",
            "Kd1",
            "Qf3+",
            "Kc1",
            "Rf8",
            "Qe6",
            "Qf4+",
            "Kb1",
            "Qf1",
            "a3",
            "Qf6",
            "Qe4",
            "b6",
            "Qb7",
            "Qf7",
            "Re7",
            "Qf1+",
            "Ka2",
            "Qf6",
            "Qxa7",
            "h5",
            "Re4",
            "d5",
            "Rb4",
            "h4",
            "Rxb6",
            "Qf7",
            "Rb7",
            "Qf6",
            "Rb6",
            "Qf7",
            "Rb7",
            "Qf6",
            "Rb6",
            "Qf7",
            "Rb7",
            "Qf6",
            "Rb6",
            "Qf7",
            "Rb7",
            "Qf6",
            "Rb6",
            "Qf7",
            "Rb7",
            "Qf6",
            "Rb6",
            "Qf7",
            "Rb7",
            "Qf6",
            "Rb6",
            "Qf7",
            "Rb7",
            "Qf6",
            "Rb6",
            "Qf7",
            "Rb7",
            "Qf6",
            "Rb6",
            "Qf7",
            "Rb7",
            "Qf6",
            "Rb6",
            "Qf7",
            "Rb7",
            "Qf6",
            "Rb6",
            "Qf7",
            "Rb7",
            "Qf6",
            "Rb6",
            "Qf7",
            "Rb7",
            "Qf6",
            "Rb6",
            "Qf7",
            "Rb7",
            "Qf6",
            "Rb6",
            "Qf7",
            "Rb7",
            "Qf6",
            "Rb6",
            "Qf7",
            "Rb7",
            "Qf6",
            "Rb6",
            "Qf7",
            "Rb7",
            "Qf6",
            "Rb6",
            "Qf7",
            "Rb7",
            "Qf6",
            "Rb6"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Nxe4",
          "positional_uci": "f6e4",
          "positional_response": "4... Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. Bxe4 Bd6 8. O-O O-O 9. d3 Bg4 10. h3 Bh5 11. g4 Bg6 12. Bxg6 hxg6 13. Ng5 Be7 14. Ne4 f5 15. Ng3 Qd7 16. f3 Bh4 17. Kg2 Bxg3 18. Kxg3 Nd4 19. c3 Ne6 20. Qe2 Rad8 21. Rd1 Qd6 22. Kg2 e4 23. fxe4 fxe4 24. Qxe4 Rde8 25. Qc4 b5 26. Qb3 Kh7 27. Be3 Nc5 28. Bxc5 Re2+ 29. Bf2 Rfxf2+ 30. Kg1 Qh2# 0-1\n1. e4 e5 2. Nf3 Nc6 3. Bc4 Nf6 4. Nc3 Nxe4 5. Nxe4 d5 6. Bd3 dxe4 7. Bxe4 Bd6 8. O-O O-O 9. d3 Bg4 10. h3 Bh5 11. g4 Bg6 12. Bxg6 hxg6 13. Ng5 Be7 14. Ne4 f5 15. Ng3 Qd7 16. f3 Bh4 17. Kg2 Bxg3 18. Kxg3 Nd4 19. c3 Ne6 20. Qe2 Rad8 21. Rd1 Qd6 22. Kg2 e4 23. fxe4 fxe4 24. Qxe4 Rde8 25. Qc4 b5 26. Qb3 Kh7 27. Be3 Nc5 28. Bxc5 Re2+ 29. Bf2 Rfxf2+ 30. Kg1 Qh2# 0-1 e4 e5 Nf3 Nc6 Bc4 Nf6 Nc3 Nxe4 Nxe4 d5 Bd3 dxe4 Bxe4 Bd6 O-O O-O d3 Bg4 h3 Bh5 g4 Bg6 Bxg6 hxg6 Ng5 Be7 Ne4 f5 Ng3 Qd7 f3 Bh4 Kg2 Bxg3 Kxg3 Nd4 c3 Ne6 Qe2 Rad8 Rd1 Qd6 Kg2 e4 fxe4 fxe4 Qxe4 Rde8 Qc4 b5 Qb3 Kh7 Be3 Nc5 Bxc5 Re2+ Bf2 Rfxf2+ Kg1 Qh2# 0-1 e4 e5 Nf3 N",
          "positional_tokens": {
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "d5",
            "Bd3",
            "dxe4",
            "Bxe4",
            "Bd6",
            "O-O",
            "O-O",
            "d3",
            "Bg4",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Bxg6",
            "hxg6",
            "Ng5",
            "Be7",
            "Ne4",
            "f5",
            "Ng3",
            "Qd7",
            "f3",
            "Bh4",
            "Kg2",
            "Bxg3",
            "Kxg3",
            "Nd4",
            "c3",
            "Ne6",
            "Qe2",
            "Rad8",
            "Rd1",
            "Qd6",
            "Kg2",
            "e4",
            "fxe4",
            "fxe4",
            "Qxe4",
            "Rde8",
            "Qc4",
            "b5",
            "Qb3",
            "Kh7",
            "Be3",
            "Nc5",
            "Bxc5",
            "Re2+",
            "Bf2",
            "Rfxf2+",
            "Kg1",
            "Qh2#",
            "e4",
            "e5",
            "Nf3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Nxe4",
          "neutral_uci": "f6e4",
          "neutral_response": "4... Nxe4 5. Nxe4 d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. Re1 Bd6 11. d3 O-O 12. Ne4 Nd4 13. g4 Nxf3+ 14. Qxf3 Bg6 15. Nxd6 cxd6 16. Qxb7 Qh4 17. Qg2 f5 18. gxf5 Bxf5 19. Re3 Rac8 20. c3 Rf6 21. Rg3 Rg6 22. Rxg6 Bxg6 23. Qg3 Qh5 24. Be3 Bxd3 25. Bxa7 Bf5 26. Kh2 Rc4 27. Rg1 Rg4 28. Qf3 Rxg1 29. Qxh5 g6 30. Qh4 Rb1 31. Qd8+ Kg7 32. Qe7+ Kh6 33. Be3+ Kh5 34. Qg5# 1-0\n34... Kg4 35. Qg5+ Kf3 36. Qg2+ Ke2",
          "neutral_tokens": {
            "prompt_tokens": 139,
            "completion_tokens": 285,
            "total_tokens": 424,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "d5",
            "Bxd5",
            "Qxd5",
            "Nc3",
            "Qd8",
            "O-O",
            "Bg4",
            "h3",
            "Bh5",
            "Re1",
            "Bd6",
            "d3",
            "O-O",
            "Ne4",
            "Nd4",
            "g4",
            "Nxf3+",
            "Qxf3",
            "Bg6",
            "Nxd6",
            "cxd6",
            "Qxb7",
            "Qh4",
            "Qg2",
            "f5",
            "gxf5",
            "Bxf5",
            "Re3",
            "Rac8",
            "c3",
            "Rf6",
            "Rg3",
            "Rg6",
            "Rxg6",
            "Bxg6",
            "Qg3",
            "Qh5",
            "Be3",
            "Bxd3",
            "Bxa7",
            "Bf5",
            "Kh2",
            "Rc4",
            "Rg1",
            "Rg4",
            "Qf3",
            "Rxg1",
            "Qxh5",
            "g6",
            "Qh4",
            "Rb1",
            "Qd8+",
            "Kg7",
            "Qe7+",
            "Kh6",
            "Be3+",
            "Kh5",
            "Qg5#",
            "Kg4",
            "Qg5+",
            "Kf3",
            "Qg2+",
            "Ke2"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f6e4",
          "positional_uci": "f6e4",
          "neutral_uci": "f6e4",
          "consensus_move": "f6e4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "d5",
              "Bxd5",
              "Qxd5",
              "Nc3",
              "Qd8",
              "O-O",
              "Bg4",
              "h3",
              "Bh5",
              "d3",
              "Nd4",
              "g4",
              "Bg6",
              "Nxe5",
              "Bd6",
              "Re1",
              "O-O",
              "Nxg6",
              "fxg6",
              "Be3",
              "Nf3+",
              "Kg2",
              "Nxe1+",
              "Qxe1",
              "Qf6",
              "Ne4",
              "Qf3+",
              "Kg1",
              "Qxh3",
              "Nxd6",
              "Qxg4+",
              "Kf1",
              "cxd6",
              "Qc3",
              "Rae8",
              "Qb3+",
              "Kh8",
              "Re1",
              "Qh3+",
              "Ke2",
              "Rxf2+",
              "Kxf2",
              "Qh2+",
              "Kf1",
              "Rf8+",
              "Bf4",
              "Rxf4#",
              "Ke1",
              "Qf2+",
              "Kd1",
              "Qf3+",
              "Kc1",
              "Rf8",
              "Qe6",
              "Qf4+",
              "Kb1",
              "Qf1",
              "a3",
              "Qf6",
              "Qe4",
              "b6",
              "Qb7",
              "Qf7",
              "Re7",
              "Qf1+",
              "Ka2",
              "Qf6",
              "Qxa7",
              "h5",
              "Re4",
              "d5",
              "Rb4",
              "h4",
              "Rxb6",
              "Qf7",
              "Rb7",
              "Qf6",
              "Rb6",
              "Qf7",
              "Rb7",
              "Qf6",
              "Rb6",
              "Qf7",
              "Rb7",
              "Qf6",
              "Rb6",
              "Qf7",
              "Rb7",
              "Qf6",
              "Rb6",
              "Qf7",
              "Rb7",
              "Qf6",
              "Rb6",
              "Qf7",
              "Rb7",
              "Qf6",
              "Rb6",
              "Qf7",
              "Rb7",
              "Qf6",
              "Rb6",
              "Qf7",
              "Rb7",
              "Qf6",
              "Rb6",
              "Qf7",
              "Rb7",
              "Qf6",
              "Rb6",
              "Qf7",
              "Rb7",
              "Qf6",
              "Rb6",
              "Qf7",
              "Rb7",
              "Qf6",
              "Rb6",
              "Qf7",
              "Rb7",
              "Qf6",
              "Rb6",
              "Qf7",
              "Rb7",
              "Qf6",
              "Rb6",
              "Qf7",
              "Rb7",
              "Qf6",
              "Rb6",
              "Qf7",
              "Rb7",
              "Qf6",
              "Rb6",
              "Qf7",
              "Rb7",
              "Qf6",
              "Rb6",
              "Qf7",
              "Rb7",
              "Qf6",
              "Rb6"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "d5",
              "Bd3",
              "dxe4",
              "Bxe4",
              "Bd6",
              "O-O",
              "O-O",
              "d3",
              "Bg4",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Bxg6",
              "hxg6",
              "Ng5",
              "Be7",
              "Ne4",
              "f5",
              "Ng3",
              "Qd7",
              "f3",
              "Bh4",
              "Kg2",
              "Bxg3",
              "Kxg3",
              "Nd4",
              "c3",
              "Ne6",
              "Qe2",
              "Rad8",
              "Rd1",
              "Qd6",
              "Kg2",
              "e4",
              "fxe4",
              "fxe4",
              "Qxe4",
              "Rde8",
              "Qc4",
              "b5",
              "Qb3",
              "Kh7",
              "Be3",
              "Nc5",
              "Bxc5",
              "Re2+",
              "Bf2",
              "Rfxf2+",
              "Kg1",
              "Qh2#",
              "e4",
              "e5",
              "Nf3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "d5",
              "Bxd5",
              "Qxd5",
              "Nc3",
              "Qd8",
              "O-O",
              "Bg4",
              "h3",
              "Bh5",
              "Re1",
              "Bd6",
              "d3",
              "O-O",
              "Ne4",
              "Nd4",
              "g4",
              "Nxf3+",
              "Qxf3",
              "Bg6",
              "Nxd6",
              "cxd6",
              "Qxb7",
              "Qh4",
              "Qg2",
              "f5",
              "gxf5",
              "Bxf5",
              "Re3",
              "Rac8",
              "c3",
              "Rf6",
              "Rg3",
              "Rg6",
              "Rxg6",
              "Bxg6",
              "Qg3",
              "Qh5",
              "Be3",
              "Bxd3",
              "Bxa7",
              "Bf5",
              "Kh2",
              "Rc4",
              "Rg1",
              "Rg4",
              "Qf3",
              "Rxg1",
              "Qxh5",
              "g6",
              "Qh4",
              "Rb1",
              "Qd8+",
              "Kg7",
              "Qe7+",
              "Kh6",
              "Be3+",
              "Kh5",
              "Qg5#",
              "Kg4",
              "Qg5+",
              "Kf3",
              "Qg2+",
              "Ke2"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 139,
            "completion_tokens": 285,
            "total_tokens": 424,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 419,
          "total_completion_tokens": 1565,
          "total_tokens": 1984
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 140,
            "completion_tokens": 640,
            "total_tokens": 780,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 7,
            "turn_number": 4,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 139,
            "completion_tokens": 285,
            "total_tokens": 424,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/pppp1ppp/2n2n2/4p3/2B1P3/2N2N2/PPPP1PPP/R1BQK2R b KQkq - 5 4"
    },
    {
      "turn": 10,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "d5",
          "aggressive_uci": "d7d5",
          "aggressive_response": "5... d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. g4 Bg6 11. d3 Bc5 12. Be3 Bxe3 13. fxe3 O-O 14. e4 Nd4 15. Nxd4 Qxd4+ 16. Kh1 Rad8 17. Qf3 c6 18. Rab1 b5 19. Ne2 Qa4 20. Nc3 Qxc2 21. Rfc1 Qxd3 22. Qxd3 Rxd3 23. Rd1 Rxh3+ 24. Kg2 Re3 25. Re1 Rxe1 26. Rxe1 Rd8 27. Rd1 Rxd1 28. Nxd1 Bxe4+ 29. Kg3 Bd5 30. b3 a5 31. Ne3 a4 32. bxa4 bxa4 33. a3 g6 34. g5 Kg7 35. Kg4 f5+ 36. gxf6+ Kxf6 37. Nxd5+ cxd5 38. Kf3 Kf5 39. Ke3 h5 40. Kf3 g5 41. Kg3 h4+ 42. Kh3 Kf4 43. Kg2 g4 44. Kh2 h3 45. Kg1 g3 46. Kh1 Kf3 47. Kg1 h2+ 48. Kh1 g2+ 49. Kxh2 Kf2 50. Kh3 g1=Q 51. Kh4 Kf3 52. Kh5 Kf4 53. Kh6 Kf5 54. Kh7 Kf6 55. Kh8 Qg7# d4 e3 dxe3",
          "aggressive_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 418,
            "total_tokens": 567,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bxd5",
            "Qxd5",
            "Nc3",
            "Qd8",
            "O-O",
            "Bg4",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "d3",
            "Bc5",
            "Be3",
            "Bxe3",
            "fxe3",
            "O-O",
            "e4",
            "Nd4",
            "Nxd4",
            "Qxd4+",
            "Kh1",
            "Rad8",
            "Qf3",
            "c6",
            "Rab1",
            "b5",
            "Ne2",
            "Qa4",
            "Nc3",
            "Qxc2",
            "Rfc1",
            "Qxd3",
            "Qxd3",
            "Rxd3",
            "Rd1",
            "Rxh3+",
            "Kg2",
            "Re3",
            "Re1",
            "Rxe1",
            "Rxe1",
            "Rd8",
            "Rd1",
            "Rxd1",
            "Nxd1",
            "Bxe4+",
            "Kg3",
            "Bd5",
            "b3",
            "a5",
            "Ne3",
            "a4",
            "bxa4",
            "bxa4",
            "a3",
            "g6",
            "g5",
            "Kg7",
            "Kg4",
            "f5+",
            "gxf6+",
            "Kxf6",
            "Nxd5+",
            "cxd5",
            "Kf3",
            "Kf5",
            "Ke3",
            "h5",
            "Kf3",
            "g5",
            "Kg3",
            "h4+",
            "Kh3",
            "Kf4",
            "Kg2",
            "g4",
            "Kh2",
            "h3",
            "Kg1",
            "g3",
            "Kh1",
            "Kf3",
            "Kg1",
            "h2+",
            "Kh1",
            "g2+",
            "Kxh2",
            "Kf2",
            "Kh3",
            "g1=Q",
            "Kh4",
            "Kf3",
            "Kh5",
            "Kf4",
            "Kh6",
            "Kf5",
            "Kh7",
            "Kf6",
            "Kh8",
            "Qg7#",
            "d4",
            "e3",
            "dxe3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "d5",
          "positional_uci": "d7d5",
          "positional_response": "5... d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. g4 Bg6 11. Re1 Bd6 12. d4 O-O 13. dxe5 Bc5 14. Qxd8 Raxd8 15. Be3 Bxe3 16. Rxe3 Bxc2 17. Rc1 Bg6 18. Nb5 Rd7 19. e6 fxe6 20. Rxe6 Rxf3 21. Kg2 Rf8 22. Rce1 a6 23. Nc3 Nd4 24. R6e5 Nf3 25. Re7 Nxe1+ 26. Rxe1 Rd2 27. Re2 Rxe2 28. Nxe2 Be4+ 29. Kg3 Rf3+ 30. Kh4 Rxf2 31. Ng3 Bg6 32. Kg5 Rxb2 33. h4 Rxa2 34. h5 Ra5+ 35. Kh4 Bf7 36. Nf5 Rxf5 37. gxf5 a5 38. Kg5 a4 39. h6 gxh6+ 40. Kxh6 a3 41. f6 a2 42. Kg5 a1=Q 43. Kf5 Qd4 44. Kg5 Be6 45. Kh6 Qh4# e4 d5 Nxd5 dxe4 Nc3 Qd8 O-O Bg4 h3 Bh5 g4 Bg6 Re1 Bd6 d4 O-O dxe5 Bc5 Qxd8 Raxd8 Be3 Bxe3 Rxe3 Bxc2 Rc1 Bg6 Rxc7 Rb8 e6 fxe6 Rxe6 Rxf2 Kg3 Rf7 Ree7 Rxe7 Rxe7 Kf8 Rc7 a5 h4 b5 h5 Bf7 a3 b4 axb4 axb4 Kf4 b3 Ke5 Rb5+ Kd6 Rb8 Kc5 b2 Kc6 Rc8+ Kd7 b1=Q Rc7+ Ke6 Qb6+ Ke5 Rc5+ Bd5 Qxb2+ Ke4 Qb1+ Kd3 Qd6+ Ke2 Qe3+ Kf1 Qf3+ Ke1 Qe3+ Kf1 Qf3+ Ke1 Qe3+ Kf1 Qf3+ Ke1 Qe3+ Kf1 Qf3+ Ke1 Qe3+ Kf1 Qf3+ Ke1 Qe3+ Kf1 Qf3+ Ke1 Qe3+ Kf1 Qf3+ Ke1 Qe3+ Kf1 Qf3+ Ke1 Qe3+ Kf1",
          "positional_tokens": {
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Nxd5",
            "dxe4",
            "Nc3",
            "Qd8",
            "O-O",
            "Bg4",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Re1",
            "Bd6",
            "d4",
            "O-O",
            "dxe5",
            "Bc5",
            "Qxd8",
            "Raxd8",
            "Be3",
            "Bxe3",
            "Rxe3",
            "Bxc2",
            "Rc1",
            "Bg6",
            "Rxc7",
            "Rb8",
            "e6",
            "fxe6",
            "Rxe6",
            "Rxf2",
            "Kg3",
            "Rf7",
            "Ree7",
            "Rxe7",
            "Rxe7",
            "Kf8",
            "Rc7",
            "a5",
            "h4",
            "b5",
            "h5",
            "Bf7",
            "a3",
            "b4",
            "axb4",
            "axb4",
            "Kf4",
            "b3",
            "Ke5",
            "Rb5+",
            "Kd6",
            "Rb8",
            "Kc5",
            "b2",
            "Kc6",
            "Rc8+",
            "Kd7",
            "b1=Q",
            "Rc7+",
            "Ke6",
            "Qb6+",
            "Ke5",
            "Rc5+",
            "Bd5",
            "Qxb2+",
            "Ke4",
            "Qb1+",
            "Kd3",
            "Qd6+",
            "Ke2",
            "Qe3+",
            "Kf1",
            "Qf3+",
            "Ke1",
            "Qe3+",
            "Kf1",
            "Qf3+",
            "Ke1",
            "Qe3+",
            "Kf1",
            "Qf3+",
            "Ke1",
            "Qe3+",
            "Kf1",
            "Qf3+",
            "Ke1",
            "Qe3+",
            "Kf1",
            "Qf3+",
            "Ke1",
            "Qe3+",
            "Kf1",
            "Qf3+",
            "Ke1",
            "Qe3+",
            "Kf1",
            "Qf3+",
            "Ke1",
            "Qe3+",
            "Kf1",
            "Qf3+",
            "Ke1",
            "Qe3+",
            "Kf1"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "d5",
          "neutral_uci": "d7d5",
          "neutral_response": "5... d5 6. Bxd5 Qxd5 7. Nc3 Qd8 8. O-O Bg4 9. h3 Bh5 10. g4 Bg6 11. d3 Bc5 12. Be3 Bxe3 13. fxe3 O-O 14. e4 Nd4 15. Nxd4 Qxd4+ 16. Kh1 Rad8 17. Qf3 c5 18. Nd5 Qxb2 19. Ne7+ Kh8 20. Nxg6+ hxg6 21. Qf2 b6 22. Qh4+ Kg8 23. Qf2 Rd6 24. Rfb1 Qd4 25. Qxd4 cxd4 26. Rb5 f6 27. Rd5 Rxd5 28. exd5 Rd8 29. Rb1 Rxd5 30. Rb4 Rc5 31. Rc4 Rxc4 32. dxc4 Kf7 33. Kg2 Ke6 34. Kf3 f5 35. gxf5+ gxf5 36. h4 Kd6 37. Ke2 Kc5 38. Kd3 e4+ 39. Kd2 Kxc4 40. h5 f4 41. Ke2 Kc3 42. Kd1 f3 43. Ke1 Kxc2 44. Kf2 d3 45. Ke3 d2 46. Kxe4 d1=Q 47. Kf4 f2 48. Kg5 f1=Q 49. Kg6 Qg4+ 50. Kh7 Qff5+ 51. Kg8 Qc8+ 52. Kh7 Qgf5+ 53. Kxg7 Qcd7+ 54. Kh6 Qdh7# d5 e5 dxe5",
          "neutral_tokens": {
            "prompt_tokens": 148,
            "completion_tokens": 424,
            "total_tokens": 572,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "e5",
            "dxe5"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d7d5",
          "positional_uci": "d7d5",
          "neutral_uci": "d7d5",
          "consensus_move": "d7d5",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bxd5",
              "Qxd5",
              "Nc3",
              "Qd8",
              "O-O",
              "Bg4",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "d3",
              "Bc5",
              "Be3",
              "Bxe3",
              "fxe3",
              "O-O",
              "e4",
              "Nd4",
              "Nxd4",
              "Qxd4+",
              "Kh1",
              "Rad8",
              "Qf3",
              "c6",
              "Rab1",
              "b5",
              "Ne2",
              "Qa4",
              "Nc3",
              "Qxc2",
              "Rfc1",
              "Qxd3",
              "Qxd3",
              "Rxd3",
              "Rd1",
              "Rxh3+",
              "Kg2",
              "Re3",
              "Re1",
              "Rxe1",
              "Rxe1",
              "Rd8",
              "Rd1",
              "Rxd1",
              "Nxd1",
              "Bxe4+",
              "Kg3",
              "Bd5",
              "b3",
              "a5",
              "Ne3",
              "a4",
              "bxa4",
              "bxa4",
              "a3",
              "g6",
              "g5",
              "Kg7",
              "Kg4",
              "f5+",
              "gxf6+",
              "Kxf6",
              "Nxd5+",
              "cxd5",
              "Kf3",
              "Kf5",
              "Ke3",
              "h5",
              "Kf3",
              "g5",
              "Kg3",
              "h4+",
              "Kh3",
              "Kf4",
              "Kg2",
              "g4",
              "Kh2",
              "h3",
              "Kg1",
              "g3",
              "Kh1",
              "Kf3",
              "Kg1",
              "h2+",
              "Kh1",
              "g2+",
              "Kxh2",
              "Kf2",
              "Kh3",
              "g1=Q",
              "Kh4",
              "Kf3",
              "Kh5",
              "Kf4",
              "Kh6",
              "Kf5",
              "Kh7",
              "Kf6",
              "Kh8",
              "Qg7#",
              "d4",
              "e3",
              "dxe3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Nxd5",
              "dxe4",
              "Nc3",
              "Qd8",
              "O-O",
              "Bg4",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Re1",
              "Bd6",
              "d4",
              "O-O",
              "dxe5",
              "Bc5",
              "Qxd8",
              "Raxd8",
              "Be3",
              "Bxe3",
              "Rxe3",
              "Bxc2",
              "Rc1",
              "Bg6",
              "Rxc7",
              "Rb8",
              "e6",
              "fxe6",
              "Rxe6",
              "Rxf2",
              "Kg3",
              "Rf7",
              "Ree7",
              "Rxe7",
              "Rxe7",
              "Kf8",
              "Rc7",
              "a5",
              "h4",
              "b5",
              "h5",
              "Bf7",
              "a3",
              "b4",
              "axb4",
              "axb4",
              "Kf4",
              "b3",
              "Ke5",
              "Rb5+",
              "Kd6",
              "Rb8",
              "Kc5",
              "b2",
              "Kc6",
              "Rc8+",
              "Kd7",
              "b1=Q",
              "Rc7+",
              "Ke6",
              "Qb6+",
              "Ke5",
              "Rc5+",
              "Bd5",
              "Qxb2+",
              "Ke4",
              "Qb1+",
              "Kd3",
              "Qd6+",
              "Ke2",
              "Qe3+",
              "Kf1",
              "Qf3+",
              "Ke1",
              "Qe3+",
              "Kf1",
              "Qf3+",
              "Ke1",
              "Qe3+",
              "Kf1",
              "Qf3+",
              "Ke1",
              "Qe3+",
              "Kf1",
              "Qf3+",
              "Ke1",
              "Qe3+",
              "Kf1",
              "Qf3+",
              "Ke1",
              "Qe3+",
              "Kf1",
              "Qf3+",
              "Ke1",
              "Qe3+",
              "Kf1",
              "Qf3+",
              "Ke1",
              "Qe3+",
              "Kf1",
              "Qf3+",
              "Ke1",
              "Qe3+",
              "Kf1"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "e5",
              "dxe5"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 149,
            "completion_tokens": 418,
            "total_tokens": 567,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 148,
            "completion_tokens": 424,
            "total_tokens": 572,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 446,
          "total_completion_tokens": 1482,
          "total_tokens": 1928
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 149,
            "completion_tokens": 418,
            "total_tokens": 567,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 149,
            "completion_tokens": 640,
            "total_tokens": 789,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 9,
            "turn_number": 5,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 148,
            "completion_tokens": 424,
            "total_tokens": 572,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/pppp1ppp/2n5/4p3/2B1N3/5N2/PPPP1PPP/R1BQK2R b KQkq - 0 5"
    },
    {
      "turn": 12,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "dxe4",
          "aggressive_uci": "d5e4",
          "aggressive_response": "6... dxe4 7. Bxe4 Bd6 8. O-O O-O 9. d3 Bg4 10. h3 Bh5 11. g4 Bg6 12. Bxg6 hxg6 13. Be3 f5 14. gxf5 gxf5 15. Ng5 Qe8 16. Kh1 f4 17. Bd2 Rf5 18. Rg1 Be7 19. Ne6 Bf6 20. Nxc7 Qc8 21. Nxa8 Rh5 22. Qg4 Rxh3+ 23. Kg2 Qxg4+ 24. Kf1 Qf3 25. Nc7 Rh2 26. Be1 Nd4 27. Ne6 Qe2# 0-1\n28. Nxd4 exd4 29. Rc1 f3",
          "aggressive_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 199,
            "total_tokens": 355,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bxe4",
            "Bd6",
            "O-O",
            "O-O",
            "d3",
            "Bg4",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Bxg6",
            "hxg6",
            "Be3",
            "f5",
            "gxf5",
            "gxf5",
            "Ng5",
            "Qe8",
            "Kh1",
            "f4",
            "Bd2",
            "Rf5",
            "Rg1",
            "Be7",
            "Ne6",
            "Bf6",
            "Nxc7",
            "Qc8",
            "Nxa8",
            "Rh5",
            "Qg4",
            "Rxh3+",
            "Kg2",
            "Qxg4+",
            "Kf1",
            "Qf3",
            "Nc7",
            "Rh2",
            "Be1",
            "Nd4",
            "Ne6",
            "Qe2#",
            "Nxd4",
            "exd4",
            "Rc1",
            "f3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "dxe4",
          "positional_uci": "d5e4",
          "positional_response": "6... dxe4 7. Bxe4 Bd6 8. O-O O-O 9. d3 Bg4 10. h3 Bh5 11. g4 Bg6 12. Bxg6 hxg6 13. Be3 f5 14. gxf5 gxf5 15. Bg5 Qe8 16. Nh4 Nd4 17. c3 Ne6 18. Bd2 g5 19. Ng2 Qg6 20. Qb3 Kg7 21. Qxb7 Rab8 22. Qxa7 Rxb2 23. Bc1 Re2 24. Ba3 Bxa3 25. Qxa3 Rh8 26. Qe7+ Qf7 27. Qxf7+ Kxf7 28. Kh2 g4 29. h4 f4 30. Kg1 f3 31. Ne3 Rxh4 32. Nf5 Rh3 33. Ng3 Nf4 34. Nxe2 Nxe2# 0-1\n35. Kh1 Rxh2+ 36. Kxh2 Rh3#",
          "positional_tokens": {
            "prompt_tokens": 156,
            "completion_tokens": 255,
            "total_tokens": 411,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bxe4",
            "Bd6",
            "O-O",
            "O-O",
            "d3",
            "Bg4",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Bxg6",
            "hxg6",
            "Be3",
            "f5",
            "gxf5",
            "gxf5",
            "Bg5",
            "Qe8",
            "Nh4",
            "Nd4",
            "c3",
            "Ne6",
            "Bd2",
            "g5",
            "Ng2",
            "Qg6",
            "Qb3",
            "Kg7",
            "Qxb7",
            "Rab8",
            "Qxa7",
            "Rxb2",
            "Bc1",
            "Re2",
            "Ba3",
            "Bxa3",
            "Qxa3",
            "Rh8",
            "Qe7+",
            "Qf7",
            "Qxf7+",
            "Kxf7",
            "Kh2",
            "g4",
            "h4",
            "f4",
            "Kg1",
            "f3",
            "Ne3",
            "Rxh4",
            "Nf5",
            "Rh3",
            "Ng3",
            "Nf4",
            "Nxe2",
            "Nxe2#",
            "Kh1",
            "Rxh2+",
            "Kxh2",
            "Rh3#"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "dxe4",
          "neutral_uci": "d5e4",
          "neutral_response": "6... dxe4 7. Bxe4 Bd6 8. O-O O-O 9. d3 Bg4 10. h3 Bh5 11. Be3 f5 12. Bd5+ Kh8 13. Qe1 Nb4 14. Bb3 Bxf3 15. gxf3 f4 16. Bd2 Nc6 17. Kh2 Nd4 18. Qe4 Qh4 19. c3 Rf5 20. cxd4 Rh5 21. Be6 Re8 22. Bg4 Rh6 23. Bxf4 Qxg4 24. fxg4 *",
          "neutral_tokens": {
            "prompt_tokens": 155,
            "completion_tokens": 148,
            "total_tokens": 303,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bxe4",
            "Bd6",
            "O-O",
            "O-O",
            "d3",
            "Bg4",
            "h3",
            "Bh5",
            "Be3",
            "f5",
            "Bd5+",
            "Kh8",
            "Qe1",
            "Nb4",
            "Bb3",
            "Bxf3",
            "gxf3",
            "f4",
            "Bd2",
            "Nc6",
            "Kh2",
            "Nd4",
            "Qe4",
            "Qh4",
            "c3",
            "Rf5",
            "cxd4",
            "Rh5",
            "Be6",
            "Re8",
            "Bg4",
            "Rh6",
            "Bxf4",
            "Qxg4",
            "fxg4"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "d5e4",
          "positional_uci": "d5e4",
          "neutral_uci": "d5e4",
          "consensus_move": "d5e4",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "Bxe4",
              "Bd6",
              "O-O",
              "O-O",
              "d3",
              "Bg4",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Bxg6",
              "hxg6",
              "Be3",
              "f5",
              "gxf5",
              "gxf5",
              "Ng5",
              "Qe8",
              "Kh1",
              "f4",
              "Bd2",
              "Rf5",
              "Rg1",
              "Be7",
              "Ne6",
              "Bf6",
              "Nxc7",
              "Qc8",
              "Nxa8",
              "Rh5",
              "Qg4",
              "Rxh3+",
              "Kg2",
              "Qxg4+",
              "Kf1",
              "Qf3",
              "Nc7",
              "Rh2",
              "Be1",
              "Nd4",
              "Ne6",
              "Qe2#",
              "Nxd4",
              "exd4",
              "Rc1",
              "f3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Bxe4",
              "Bd6",
              "O-O",
              "O-O",
              "d3",
              "Bg4",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Bxg6",
              "hxg6",
              "Be3",
              "f5",
              "gxf5",
              "gxf5",
              "Bg5",
              "Qe8",
              "Nh4",
              "Nd4",
              "c3",
              "Ne6",
              "Bd2",
              "g5",
              "Ng2",
              "Qg6",
              "Qb3",
              "Kg7",
              "Qxb7",
              "Rab8",
              "Qxa7",
              "Rxb2",
              "Bc1",
              "Re2",
              "Ba3",
              "Bxa3",
              "Qxa3",
              "Rh8",
              "Qe7+",
              "Qf7",
              "Qxf7+",
              "Kxf7",
              "Kh2",
              "g4",
              "h4",
              "f4",
              "Kg1",
              "f3",
              "Ne3",
              "Rxh4",
              "Nf5",
              "Rh3",
              "Ng3",
              "Nf4",
              "Nxe2",
              "Nxe2#",
              "Kh1",
              "Rxh2+",
              "Kxh2",
              "Rh3#"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bxe4",
              "Bd6",
              "O-O",
              "O-O",
              "d3",
              "Bg4",
              "h3",
              "Bh5",
              "Be3",
              "f5",
              "Bd5+",
              "Kh8",
              "Qe1",
              "Nb4",
              "Bb3",
              "Bxf3",
              "gxf3",
              "f4",
              "Bd2",
              "Nc6",
              "Kh2",
              "Nd4",
              "Qe4",
              "Qh4",
              "c3",
              "Rf5",
              "cxd4",
              "Rh5",
              "Be6",
              "Re8",
              "Bg4",
              "Rh6",
              "Bxf4",
              "Qxg4",
              "fxg4"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 156,
            "completion_tokens": 199,
            "total_tokens": 355,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "positional": {
            "prompt_tokens": 156,
            "completion_tokens": 255,
            "total_tokens": 411,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 155,
            "completion_tokens": 148,
            "total_tokens": 303,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 467,
          "total_completion_tokens": 602,
          "total_tokens": 1069
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 156,
            "completion_tokens": 199,
            "total_tokens": 355,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 156,
            "completion_tokens": 255,
            "total_tokens": 411,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 11,
            "turn_number": 6,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 155,
            "completion_tokens": 148,
            "total_tokens": 303,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/3pp3/4N3/3B1N2/PPPP1PPP/R1BQK2R b KQkq - 1 6"
    },
    {
      "turn": 14,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "Bd6",
          "aggressive_uci": "f8d6",
          "aggressive_response": "7... Bd6 8. O-O O-O 9. d3 Bg4 10. h3 Bh5 11. g4 Bg6 12. Bxg6 hxg6 13. Be3 f5 14. gxf5 gxf5 15. Ng5 Qe8 16. Qf3 Qg6 17. Qd5+ Kh8 18. Qg2 f4 19. Bd2 Nd4 20. Qg4 Rf5 21. Ne4 Qh6 22. c3 Rh5 23. cxd4 Rxh3 24. Qg2 f3 25. Bxh6 fxg2 26. Kxg2 Rxh6 27. Rh1 exd4 28. Rxh6+ gxh6 29. Rh1 Kg7 30. Rh5 Rf8 31. Rd5 Rd8 32. Nxd6 Rxd6 33. Rxd6 cxd6 34. Kf3 Kf6 35. Ke4 Kg5 36. Kxd4 Kf4 37. Kd5 h5 38. Kxd6 h4 39. Kc7 h3 40. d4 h2 41. d5 h1=Q 42. d6 Qc6+ 43. Kb8 Qxd6+ 44. Kxb7 Qb6+ 45. Ka8 Qxb2 46. Kxa7 Ke5 47. a4 Kd6 48. a5 Kc7 49. a6 Qb8# e4 d5 Nf3 Nc6 Bc4 Nf6 Nxe4 dxe4 Bxe4 Bd6 O-O Bg4 h3 Bh5 g4 Bg6 Bxg6 hxg6 Be3 f5 gxf5 gxf5 Ng5 Qe8 Qf3 Qg6 Qd5+ Kh8 Qg2 f4 Bd2 Nd4 Qg4 Rf5 Ne4 Qh6 c3 Rh5 cxd4 Rxh3 Qg2 f3 Bxh6 fxg2 Kxg2 Rxh6 Rh1 exd4 Rxh6+ gxh6 Nxd6 cxd6 Kf3 Kg7 Ke4 Kf6 Kxd4 Ke6 Ke4 d5+ Kd4 Kd6 f4 h5 f5 h4 f6 h3 f7 Ke7 Kxd5 h2 f8=Q+ Kxf8 Kc5 h1=Q Kb6 Qc6+ Ka7 Qc5+ Kb8 Qd6+ Ka7 Qc5+ Kb8 Qd6+ Ka7 Qc5+ Kb8 Qd6+ Ka7 Qc5+ Kb8 Qd6+ Ka7 Qc5+ Kb8 Qd6+ Ka7 Qc5+ Kb8 Qd6+ Ka7 Q",
          "aggressive_tokens": {
            "prompt_tokens": 165,
            "completion_tokens": 640,
            "total_tokens": 805,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "O-O",
            "Bg4",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Bxg6",
            "hxg6",
            "Be3",
            "f5",
            "gxf5",
            "gxf5",
            "Ng5",
            "Qe8",
            "Qf3",
            "Qg6",
            "Qd5+",
            "Kh8",
            "Qg2",
            "f4",
            "Bd2",
            "Nd4",
            "Qg4",
            "Rf5",
            "Ne4",
            "Qh6",
            "c3",
            "Rh5",
            "cxd4",
            "Rxh3",
            "Qg2",
            "f3",
            "Bxh6",
            "fxg2",
            "Kxg2",
            "Rxh6",
            "Rh1",
            "exd4",
            "Rxh6+",
            "gxh6",
            "Nxd6",
            "cxd6",
            "Kf3",
            "Kg7",
            "Ke4",
            "Kf6",
            "Kxd4",
            "Ke6",
            "Ke4",
            "d5+",
            "Kd4",
            "Kd6",
            "f4",
            "h5",
            "f5",
            "h4",
            "f6",
            "h3",
            "f7",
            "Ke7",
            "Kxd5",
            "h2",
            "f8=Q+",
            "Kxf8",
            "Kc5",
            "h1=Q",
            "Kb6",
            "Qc6+",
            "Ka7",
            "Qc5+",
            "Kb8",
            "Qd6+",
            "Ka7",
            "Qc5+",
            "Kb8",
            "Qd6+",
            "Ka7",
            "Qc5+",
            "Kb8",
            "Qd6+",
            "Ka7",
            "Qc5+",
            "Kb8",
            "Qd6+",
            "Ka7",
            "Qc5+",
            "Kb8",
            "Qd6+",
            "Ka7",
            "Qc5+",
            "Kb8",
            "Qd6+",
            "Ka7"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "Bd6",
          "positional_uci": "f8d6",
          "positional_response": "7... Bd6 8. O-O O-O 9. d3 Ne7 10. d4 f5 11. Bd3 e4 12. Bc4+ Kh8 13. Ng5 Qe8 14. f3 h6 15. Nh3 b5 16. Bb3 Bb7 17. fxe4 Bxe4 18. Bf4 Qg6 19. Qd2 Rad8 20. Bxd6 Qxd6 21. c3 c5 22. Rad1 cxd4 23. Qxd4 Qxd4+ 24. Rxd4 Rxd4 25. cxd4 Rd8 26. Rd1 g5 27. Nf2 Bd5 28. Bxd5 Rxd5 29. Kf1 Nc6 30. Rc1 Nxd4 31. Rc8+ Kg7 32. Rc7+ Kf6 33. Rxa7 Rc5 34. Nd3 Rc2 35. Ra6+ Kg7 36. Rd6 Nc6 37. Rd5 f4 38. Rxb5 Nd4 39. Rc5 Rd2 40. Rc3 Nc2 41. Ne1 Ne3+ 42. Kg1 Rxb2 43. a4 Ra2 44. Rc7+ Kg6 45. Rc6+ Kh5 46. Ra6 Re2 47. Nf3 Rxg2+ 48. Kh1 Rf2 49. Ng1 Ng4 50. Nf3 Rxf3 51. Kg2 Rf2+ 52. Kg1 Rxh2 53. a5 Ra2 54. Ra8 f3 55. a6 Ra1# f3 gxf3",
          "positional_tokens": {
            "prompt_tokens": 165,
            "completion_tokens": 393,
            "total_tokens": 558,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "O-O",
            "O-O",
            "d3",
            "Ne7",
            "d4",
            "f5",
            "Bd3",
            "e4",
            "Bc4+",
            "Kh8",
            "Ng5",
            "Qe8",
            "f3",
            "h6",
            "Nh3",
            "b5",
            "Bb3",
            "Bb7",
            "fxe4",
            "Bxe4",
            "Bf4",
            "Qg6",
            "Qd2",
            "Rad8",
            "Bxd6",
            "Qxd6",
            "c3",
            "c5",
            "Rad1",
            "cxd4",
            "Qxd4",
            "Qxd4+",
            "Rxd4",
            "Rxd4",
            "cxd4",
            "Rd8",
            "Rd1",
            "g5",
            "Nf2",
            "Bd5",
            "Bxd5",
            "Rxd5",
            "Kf1",
            "Nc6",
            "Rc1",
            "Nxd4",
            "Rc8+",
            "Kg7",
            "Rc7+",
            "Kf6",
            "Rxa7",
            "Rc5",
            "Nd3",
            "Rc2",
            "Ra6+",
            "Kg7",
            "Rd6",
            "Nc6",
            "Rd5",
            "f4",
            "Rxb5",
            "Nd4",
            "Rc5",
            "Rd2",
            "Rc3",
            "Nc2",
            "Ne1",
            "Ne3+",
            "Kg1",
            "Rxb2",
            "a4",
            "Ra2",
            "Rc7+",
            "Kg6",
            "Rc6+",
            "Kh5",
            "Ra6",
            "Re2",
            "Nf3",
            "Rxg2+",
            "Kh1",
            "Rf2",
            "Ng1",
            "Ng4",
            "Nf3",
            "Rxf3",
            "Kg2",
            "Rf2+",
            "Kg1",
            "Rxh2",
            "a5",
            "Ra2",
            "Ra8",
            "f3",
            "a6",
            "Ra1#",
            "f3",
            "gxf3"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Bd6",
          "neutral_uci": "f8d6",
          "neutral_response": "7... Bd6 8. O-O O-O 9. d3 Bg4 10. h3 Bh5 11. g4 Bg6 12. Bxg6 hxg6 13. Be3 f5 14. gxf5 gxf5 15. Ng5 Qe8 16. f4 exf4 17. Bxf4 Bxf4 18. Rxf4 Qe3+ 19. Rf2 Qxg5+ 20. Rg2 Qe3+ 21. Kh2 Rae8 22. Qh5 Qf4+ 23. Kh1 Re3 24. Rag1 Rf7 25. c3 Ne5 26. d4 Nf3 27. Rf1 Re1 28. Rxe1 Nxe1 29. Re2 Qf1+ 30. Kh2 Nf3+ 31. Kg3 Qxe2 32. Qxf3 Qxf3+ 33. Kxf3 g5 34. b4 Kg7 35. a4 Kg6 36. c4 Kh5 37. d5 Kh4 38. Kg2 f4 39. c5 f3+ 40. Kf2 Kxh3 41. d6 cxd6 42. cxd6 g4 43. b5 g3+ 44. Kg1 f2+ 45. Kf1 g2+ 46. Ke2 f1=Q+ 47. Ke3 g1=Q+ 48. Ke4 Qf5# e4 d5 Nc3 d4 Nxd4 dxe3 Bxe3 Be6 O-O O-O d3 Bg4 h3 Bxf3 gxf3 Qe8 f4 exf4 Ng5 Qe3+ Rf2 Qxg5+ Rg2 Qe3+ Kh2 Rae8 Qh5 Qf4+ Kh1 Re1+ Rg1 Qf3+ Kh2 Qxh5+ Kg3 Qf3+ Kh4 Rf4+ Kg5 Qg4#",
          "neutral_tokens": {
            "prompt_tokens": 164,
            "completion_tokens": 462,
            "total_tokens": 626,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "O-O",
            "O-O",
            "d3",
            "Bg4",
            "h3",
            "Bh5",
            "g4",
            "Bg6",
            "Bxg6",
            "hxg6",
            "Be3",
            "f5",
            "gxf5",
            "gxf5",
            "Ng5",
            "Qe8",
            "f4",
            "exf4",
            "Bxf4",
            "Bxf4",
            "Rxf4",
            "Qe3+",
            "Rf2",
            "Qxg5+",
            "Rg2",
            "Qe3+",
            "Kh2",
            "Rae8",
            "Qh5",
            "Qf4+",
            "Kh1",
            "Re3",
            "Rag1",
            "Rf7",
            "c3",
            "Ne5",
            "d4",
            "Nf3",
            "Rf1",
            "Re1",
            "Rxe1",
            "Nxe1",
            "Re2",
            "Qf1+",
            "Kh2",
            "Nf3+",
            "Kg3",
            "Qxe2",
            "Qxf3",
            "Qxf3+",
            "Kxf3",
            "g5",
            "b4",
            "Kg7",
            "a4",
            "Kg6",
            "c4",
            "Kh5",
            "d5",
            "Kh4",
            "Kg2",
            "f4",
            "c5",
            "f3+",
            "Kf2",
            "Kxh3",
            "d6",
            "cxd6",
            "cxd6",
            "g4",
            "b5",
            "g3+",
            "Kg1",
            "f2+",
            "Kf1",
            "g2+",
            "Ke2",
            "f1=Q+",
            "Ke3",
            "g1=Q+",
            "Ke4",
            "Qf5#",
            "e4",
            "d5",
            "Nc3",
            "d4",
            "Nxd4",
            "dxe3",
            "Bxe3",
            "Be6",
            "O-O",
            "O-O",
            "d3",
            "Bg4",
            "h3",
            "Bxf3",
            "gxf3",
            "Qe8",
            "f4",
            "exf4",
            "Ng5",
            "Qe3+",
            "Rf2",
            "Qxg5+",
            "Rg2",
            "Qe3+",
            "Kh2",
            "Rae8",
            "Qh5",
            "Qf4+",
            "Kh1",
            "Re1+",
            "Rg1",
            "Qf3+",
            "Kh2",
            "Qxh5+",
            "Kg3",
            "Qf3+",
            "Kh4",
            "Rf4+",
            "Kg5",
            "Qg4#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "f8d6",
          "positional_uci": "f8d6",
          "neutral_uci": "f8d6",
          "consensus_move": "f8d6",
          "source_agents": [
            "aggressive",
            "positional",
            "neutral"
          ],
          "selected_agent": "neutral"
        },
        "plans": {
          "aggressive": {
            "full": [
              "O-O",
              "Bg4",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Bxg6",
              "hxg6",
              "Be3",
              "f5",
              "gxf5",
              "gxf5",
              "Ng5",
              "Qe8",
              "Qf3",
              "Qg6",
              "Qd5+",
              "Kh8",
              "Qg2",
              "f4",
              "Bd2",
              "Nd4",
              "Qg4",
              "Rf5",
              "Ne4",
              "Qh6",
              "c3",
              "Rh5",
              "cxd4",
              "Rxh3",
              "Qg2",
              "f3",
              "Bxh6",
              "fxg2",
              "Kxg2",
              "Rxh6",
              "Rh1",
              "exd4",
              "Rxh6+",
              "gxh6",
              "Nxd6",
              "cxd6",
              "Kf3",
              "Kg7",
              "Ke4",
              "Kf6",
              "Kxd4",
              "Ke6",
              "Ke4",
              "d5+",
              "Kd4",
              "Kd6",
              "f4",
              "h5",
              "f5",
              "h4",
              "f6",
              "h3",
              "f7",
              "Ke7",
              "Kxd5",
              "h2",
              "f8=Q+",
              "Kxf8",
              "Kc5",
              "h1=Q",
              "Kb6",
              "Qc6+",
              "Ka7",
              "Qc5+",
              "Kb8",
              "Qd6+",
              "Ka7",
              "Qc5+",
              "Kb8",
              "Qd6+",
              "Ka7",
              "Qc5+",
              "Kb8",
              "Qd6+",
              "Ka7",
              "Qc5+",
              "Kb8",
              "Qd6+",
              "Ka7",
              "Qc5+",
              "Kb8",
              "Qd6+",
              "Ka7",
              "Qc5+",
              "Kb8",
              "Qd6+",
              "Ka7"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "O-O",
              "O-O",
              "d3",
              "Ne7",
              "d4",
              "f5",
              "Bd3",
              "e4",
              "Bc4+",
              "Kh8",
              "Ng5",
              "Qe8",
              "f3",
              "h6",
              "Nh3",
              "b5",
              "Bb3",
              "Bb7",
              "fxe4",
              "Bxe4",
              "Bf4",
              "Qg6",
              "Qd2",
              "Rad8",
              "Bxd6",
              "Qxd6",
              "c3",
              "c5",
              "Rad1",
              "cxd4",
              "Qxd4",
              "Qxd4+",
              "Rxd4",
              "Rxd4",
              "cxd4",
              "Rd8",
              "Rd1",
              "g5",
              "Nf2",
              "Bd5",
              "Bxd5",
              "Rxd5",
              "Kf1",
              "Nc6",
              "Rc1",
              "Nxd4",
              "Rc8+",
              "Kg7",
              "Rc7+",
              "Kf6",
              "Rxa7",
              "Rc5",
              "Nd3",
              "Rc2",
              "Ra6+",
              "Kg7",
              "Rd6",
              "Nc6",
              "Rd5",
              "f4",
              "Rxb5",
              "Nd4",
              "Rc5",
              "Rd2",
              "Rc3",
              "Nc2",
              "Ne1",
              "Ne3+",
              "Kg1",
              "Rxb2",
              "a4",
              "Ra2",
              "Rc7+",
              "Kg6",
              "Rc6+",
              "Kh5",
              "Ra6",
              "Re2",
              "Nf3",
              "Rxg2+",
              "Kh1",
              "Rf2",
              "Ng1",
              "Ng4",
              "Nf3",
              "Rxf3",
              "Kg2",
              "Rf2+",
              "Kg1",
              "Rxh2",
              "a5",
              "Ra2",
              "Ra8",
              "f3",
              "a6",
              "Ra1#",
              "f3",
              "gxf3"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "O-O",
              "O-O",
              "d3",
              "Bg4",
              "h3",
              "Bh5",
              "g4",
              "Bg6",
              "Bxg6",
              "hxg6",
              "Be3",
              "f5",
              "gxf5",
              "gxf5",
              "Ng5",
              "Qe8",
              "f4",
              "exf4",
              "Bxf4",
              "Bxf4",
              "Rxf4",
              "Qe3+",
              "Rf2",
              "Qxg5+",
              "Rg2",
              "Qe3+",
              "Kh2",
              "Rae8",
              "Qh5",
              "Qf4+",
              "Kh1",
              "Re3",
              "Rag1",
              "Rf7",
              "c3",
              "Ne5",
              "d4",
              "Nf3",
              "Rf1",
              "Re1",
              "Rxe1",
              "Nxe1",
              "Re2",
              "Qf1+",
              "Kh2",
              "Nf3+",
              "Kg3",
              "Qxe2",
              "Qxf3",
              "Qxf3+",
              "Kxf3",
              "g5",
              "b4",
              "Kg7",
              "a4",
              "Kg6",
              "c4",
              "Kh5",
              "d5",
              "Kh4",
              "Kg2",
              "f4",
              "c5",
              "f3+",
              "Kf2",
              "Kxh3",
              "d6",
              "cxd6",
              "cxd6",
              "g4",
              "b5",
              "g3+",
              "Kg1",
              "f2+",
              "Kf1",
              "g2+",
              "Ke2",
              "f1=Q+",
              "Ke3",
              "g1=Q+",
              "Ke4",
              "Qf5#",
              "e4",
              "d5",
              "Nc3",
              "d4",
              "Nxd4",
              "dxe3",
              "Bxe3",
              "Be6",
              "O-O",
              "O-O",
              "d3",
              "Bg4",
              "h3",
              "Bxf3",
              "gxf3",
              "Qe8",
              "f4",
              "exf4",
              "Ng5",
              "Qe3+",
              "Rf2",
              "Qxg5+",
              "Rg2",
              "Qe3+",
              "Kh2",
              "Rae8",
              "Qh5",
              "Qf4+",
              "Kh1",
              "Re1+",
              "Rg1",
              "Qf3+",
              "Kh2",
              "Qxh5+",
              "Kg3",
              "Qf3+",
              "Kh4",
              "Rf4+",
              "Kg5",
              "Qg4#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 165,
            "completion_tokens": 640,
            "total_tokens": 805,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 165,
            "completion_tokens": 393,
            "total_tokens": 558,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "neutral": {
            "prompt_tokens": 164,
            "completion_tokens": 462,
            "total_tokens": 626,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 494,
          "total_completion_tokens": 1495,
          "total_tokens": 1989
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 165,
            "completion_tokens": 640,
            "total_tokens": 805,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 165,
            "completion_tokens": 393,
            "total_tokens": 558,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 13,
            "turn_number": 7,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 164,
            "completion_tokens": 462,
            "total_tokens": 626,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqkb1r/ppp2ppp/2n5/4p3/4B3/5N2/PPPP1PPP/R1BQK2R b KQkq - 0 7"
    },
    {
      "turn": 16,
      "attempt": 1,
      "debate_history": {
        "query1": {
          "aggressive_move": "O-O",
          "aggressive_uci": "e8g8",
          "aggressive_response": "8... O-O 9. O-O f5 10. Bc4+ Kh8 11. d3 Na5 12. Bb3 Nxb3 13. axb3 b6 14. Re1 Qf6\n15. Bg5 Qg6 16. Qd2 Bb7 17. Nh4 Qh5 18. g3 f4 19. Bxf4 exf4 20. Ng2 Qh3 21. f3 fxg3\n22. hxg3 Bxg3 23. Re3 Qh2+ 24. Kf1 Qh1+ 25. Ke2 Qxg2+ 26. Kd1 Qf1+ 27. Re1 Bxf3+ 28. Kc1 Qxe1+ 29. Qxe1 Bxe1 30. Kb1 a5 31. Ka2 Bb4 32. c3 Bd6 33. d4 a4 34. b4 Bd5+ 35. Ka3 Bb3 36. Re1 Rae8 37. Rxe8 Rxe8 38. d5 Re1 39. c4 Ra1# e4 e5 Nf3 Nc6 Bc4 Nf6 Nc3 Nxe4 Nxe4 d5 Bd3 dxe4 Bxe4 Bd6 Bd3 O-O O-O f5 Bc4+ Kh8 d3 Na5 Bb3 Nxb3 axb3 b6 Re1 Qf6 Bg5 Qg6 Qd2 Bb7 Nh4 Qh5 g3 f4 Bxf4 exf4 Ng2 Qh3 f3 fxg3 hxg3 Bxg3 Re3 Qh2+ Kf1 Qh1+ Ke2 Qxg2+ Kd1 Qf1+ Re1 Bxf3+ Kc1 Qxe1+ Qxe1 Bxe1 Kb1 a5 Ka2 Bb4 c3 Bd6 d4 a4 b4 Bd5+ Ka3 Bb3 c4 Bd6 d5 a5 bxa5 Rxa5# e4 e5 Nf3 Nc6 Bc4 Nf6 Nc3 Nxe4 Nxe4 d5 Bd3 dxe4 Bxe4 Bd6 Bd3 O-O O-O f5 Bc4+ Kh8 d3 Na5 Bb3 Nxb3 axb3 b6 Re1 Qf6 Bg5 Qg6 Qd2 Bb7 Nh4 Qh5 g3 f4 Bxf4 exf4 Ng2 Qh3 f3 fxg3 hxg3 Bxg3 Re3 Qh2+ Kf1 Qh1+ Ke2 Qxg2+ Kd1 Qf1+ Re1 Bxf3+ Kc1 Qxe1+ Qxe1 Bxe1 Kb1 a5 Ka2 Bb4 c3 Bd",
          "aggressive_tokens": {
            "prompt_tokens": 172,
            "completion_tokens": 640,
            "total_tokens": 812,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "f5",
            "Bc4+",
            "Kh8",
            "d3",
            "Na5",
            "Bb3",
            "Nxb3",
            "axb3",
            "b6",
            "Re1",
            "Qf6",
            "Bg5",
            "Qg6",
            "Qd2",
            "Bb7",
            "Nh4",
            "Qh5",
            "g3",
            "f4",
            "Bxf4",
            "exf4",
            "Ng2",
            "Qh3",
            "f3",
            "fxg3",
            "hxg3",
            "Bxg3",
            "Re3",
            "Qh2+",
            "Kf1",
            "Qh1+",
            "Ke2",
            "Qxg2+",
            "Kd1",
            "Qf1+",
            "Re1",
            "Bxf3+",
            "Kc1",
            "Qxe1+",
            "Qxe1",
            "Bxe1",
            "Kb1",
            "a5",
            "Ka2",
            "Bb4",
            "c3"
          ],
          "plan_execute": []
        },
        "query2": {
          "positional_move": "O-O",
          "positional_uci": "e8g8",
          "positional_response": "8... O-O 9. O-O Be6 10. Re1 f5 11. Bxc6 bxc6 12. Nxe5 Bd5 13. d4 f4 14. c4 Bxe5\n15. cxd5 Bd6 16. dxc6 f3 17. g3 Qc8 18. Qd3 Qh3 19. Qf1 Qg4 20. Be3 Rf5 21. h3 Qg6\n22. Kh2 Rh5 23. Qc4+ Kh8 24. d5 Qf5 25. h4 g5 26. Bd4+ Kg8 27. Be5 Bxe5 28. d6+ Kh8\n29. Rxe5 Qxe5 30. d7 Rxh4+ 31. Kg1 Rxc4 32. Rd1 Rd4 33. Rxd4 Qxd4 34. Kh2 Qxf2+ 35. Kh3 h5\n36. g4 Qg2# 8... O-O 9. O-O Be6 10. Re1 f5 11. Bxc6 bxc6 12. Nxe5 Bd5 13. d4 f4 14. c4 Bxe5\n15. cxd5 Bd6 16. dxc6 f3 17. g3 Qc8 18. Qd3 Qh3 19. Qf1 Qg4 20. Be3 Rf5 21. h3 Qg6\n22. Kh2 Rh5 23. Qc4+ Kh8 24. d5 Qf5 25. h4 g5 26. Bd4+ Kg8 27. Be5 Bxe5 28. d6+ Kh8\n29. Rxe5 Qxe5 30. d7 Rxh4+ 31. Kg1 Rxc4 32. Rd1 Rd4 33. Rxd4 Qxd4 34. Kh2 Qxf2+ 35. Kh3 h5\n36. g4 Qg2# 8... O-O 9. O-O Be6 10. Re1 f5 11. Bxc6 bxc6 12. Nxe5 Bd5 13. d4 f4 14. c4 Bxe5\n15. cxd5 Bd6 16. dxc6 f3 17. g3 Qc8 18. Qd3 Qh3 19. Qf1 Qg4 20. Be3 Rf5 21. h3 Qg6\n22. Kh2 Rh5 23. Qc4+ Kh8 24. d5 Qf5 25. h4 g5 26. Bd4+ Kg8 27. Be5 Bxe5 28. d6+ Kh8\n29. Rxe5 Qxe5 30. d7",
          "positional_tokens": {
            "prompt_tokens": 172,
            "completion_tokens": 640,
            "total_tokens": 812,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "plan_full": [
            "Be6",
            "Re1",
            "f5",
            "Bxc6",
            "bxc6",
            "Nxe5",
            "Bd5",
            "d4",
            "f4",
            "c4",
            "Bxe5",
            "cxd5",
            "Bd6",
            "dxc6",
            "f3",
            "g3",
            "Qc8",
            "Qd3",
            "Qh3",
            "Qf1",
            "Qg4",
            "Be3",
            "Rf5",
            "h3",
            "Qg6",
            "Kh2",
            "Rh5",
            "Qc4+",
            "Kh8",
            "d5",
            "Qf5",
            "h4",
            "g5",
            "Bd4+",
            "Kg8",
            "Be5",
            "Bxe5",
            "d6+",
            "Kh8",
            "Rxe5",
            "Qxe5",
            "d7"
          ],
          "plan_execute": []
        },
        "query3": {
          "neutral_move": "Re8",
          "neutral_uci": null,
          "neutral_response": "Re8 9. Bc4 * e4 10. Ng1 * Qg5 11. Bf1 * Bg4 12. Ne2 * Nd4 13. c3 * Nf3+ 14. gxf3 * exf3 15. d4 * Qh4 16. Be3 * Rxe3 17. Qd2 * Rae8 18. O-O-O * fxe2 19. Bxe2 * Rxe2 20. Qd3 * Qxf2 21. Rhf1 * Qxh2 22. Rh1 * Rc2+ 23. Qxc2 * Qxc2+ 24. Kxc2 * Bxd1+ 25. Rxd1 * Re2+ 26. Rd2 * Rxd2+ 27. Kxd2 * h5 28. Ke2 * g5 29. Kf3 * f5 30. Kg2 * h4 31. Kh3 * Bg3 32. Kg2 * g4 33. Kh1 * h3 34. Kg1 * f4 35. Kh1 * f3 36. Kg1 * f2+ 37. Kf1 * h2 38. Kg2 * h1=Q+ 39. Kxh1 * f1=Q# 0-1\n8. Bd3 * Re8 9. Bc4 * e4 10. Ng1 * Qg5 11. Bf1 * Bg4 12. Ne2 * Nd4 13. c3 * Nf3+ 14. gxf3 * exf3 15. d4 * Qh4 16. Be3 * Rxe3 17. Qd2 * Rae8 18. O-O-O * fxe2 19. Bxe2 * Rxe2 20. Qd3 * Qxf2 21. Rhf1 * Qxh2 22. Rh1 * Rc2+ 23. Qxc2 * Qxc2+ 24. Kxc2 * Bxd1+ 25. Rxd1 * Re2+ 26. Rd2 * Rxd2+ 27. Kxd2 * h5 28. Ke2 * g5 29. Kf3 * f5 30. Kg2 * h4 31. Kh3 * Bg3 32. Kg2 * g4 33. Kh1 * h3 34. Kg1 * f4 35. Kh1 * f3 36. Kg1 * f2+ 37. Kf1 * h2 38. Kg2 * h1=Q+ 39. Kxh1 * f1=Q# 0-1",
          "neutral_tokens": {
            "prompt_tokens": 171,
            "completion_tokens": 602,
            "total_tokens": 773,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "plan_full": [
            "Bc4",
            "e4",
            "Ng1",
            "Qg5",
            "Bf1",
            "Bg4",
            "Ne2",
            "Nd4",
            "c3",
            "Nf3+",
            "gxf3",
            "exf3",
            "d4",
            "Qh4",
            "Be3",
            "Rxe3",
            "Qd2",
            "Rae8",
            "O-O-O",
            "fxe2",
            "Bxe2",
            "Rxe2",
            "Qd3",
            "Qxf2",
            "Rhf1",
            "Qxh2",
            "Rh1",
            "Rc2+",
            "Qxc2",
            "Qxc2+",
            "Kxc2",
            "Bxd1+",
            "Rxd1",
            "Re2+",
            "Rd2",
            "Rxd2+",
            "Kxd2",
            "h5",
            "Ke2",
            "g5",
            "Kf3",
            "f5",
            "Kg2",
            "h4",
            "Kh3",
            "Bg3",
            "Kg2",
            "g4",
            "Kh1",
            "h3",
            "Kg1",
            "f4",
            "Kh1",
            "f3",
            "Kg1",
            "f2+",
            "Kf1",
            "h2",
            "Kg2",
            "h1=Q+",
            "Kxh1",
            "f1=Q#"
          ],
          "plan_execute": []
        },
        "final_moves": {
          "aggressive_uci": "e8g8",
          "positional_uci": "e8g8",
          "neutral_uci": null,
          "consensus_move": "e8g8",
          "source_agents": [
            "aggressive",
            "positional"
          ],
          "selected_agent": "aggressive"
        },
        "plans": {
          "aggressive": {
            "full": [
              "f5",
              "Bc4+",
              "Kh8",
              "d3",
              "Na5",
              "Bb3",
              "Nxb3",
              "axb3",
              "b6",
              "Re1",
              "Qf6",
              "Bg5",
              "Qg6",
              "Qd2",
              "Bb7",
              "Nh4",
              "Qh5",
              "g3",
              "f4",
              "Bxf4",
              "exf4",
              "Ng2",
              "Qh3",
              "f3",
              "fxg3",
              "hxg3",
              "Bxg3",
              "Re3",
              "Qh2+",
              "Kf1",
              "Qh1+",
              "Ke2",
              "Qxg2+",
              "Kd1",
              "Qf1+",
              "Re1",
              "Bxf3+",
              "Kc1",
              "Qxe1+",
              "Qxe1",
              "Bxe1",
              "Kb1",
              "a5",
              "Ka2",
              "Bb4",
              "c3"
            ],
            "execute": []
          },
          "positional": {
            "full": [
              "Be6",
              "Re1",
              "f5",
              "Bxc6",
              "bxc6",
              "Nxe5",
              "Bd5",
              "d4",
              "f4",
              "c4",
              "Bxe5",
              "cxd5",
              "Bd6",
              "dxc6",
              "f3",
              "g3",
              "Qc8",
              "Qd3",
              "Qh3",
              "Qf1",
              "Qg4",
              "Be3",
              "Rf5",
              "h3",
              "Qg6",
              "Kh2",
              "Rh5",
              "Qc4+",
              "Kh8",
              "d5",
              "Qf5",
              "h4",
              "g5",
              "Bd4+",
              "Kg8",
              "Be5",
              "Bxe5",
              "d6+",
              "Kh8",
              "Rxe5",
              "Qxe5",
              "d7"
            ],
            "execute": []
          },
          "neutral": {
            "full": [
              "Bc4",
              "e4",
              "Ng1",
              "Qg5",
              "Bf1",
              "Bg4",
              "Ne2",
              "Nd4",
              "c3",
              "Nf3+",
              "gxf3",
              "exf3",
              "d4",
              "Qh4",
              "Be3",
              "Rxe3",
              "Qd2",
              "Rae8",
              "O-O-O",
              "fxe2",
              "Bxe2",
              "Rxe2",
              "Qd3",
              "Qxf2",
              "Rhf1",
              "Qxh2",
              "Rh1",
              "Rc2+",
              "Qxc2",
              "Qxc2+",
              "Kxc2",
              "Bxd1+",
              "Rxd1",
              "Re2+",
              "Rd2",
              "Rxd2+",
              "Kxd2",
              "h5",
              "Ke2",
              "g5",
              "Kf3",
              "f5",
              "Kg2",
              "h4",
              "Kh3",
              "Bg3",
              "Kg2",
              "g4",
              "Kh1",
              "h3",
              "Kg1",
              "f4",
              "Kh1",
              "f3",
              "Kg1",
              "f2+",
              "Kf1",
              "h2",
              "Kg2",
              "h1=Q+",
              "Kxh1",
              "f1=Q#"
            ],
            "execute": []
          }
        },
        "final_plan": {
          "moves_for_execution": [],
          "full_moves": [],
          "source_agent": null
        },
        "total_tokens": {
          "aggressive": {
            "prompt_tokens": 172,
            "completion_tokens": 640,
            "total_tokens": 812,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "positional": {
            "prompt_tokens": 172,
            "completion_tokens": 640,
            "total_tokens": 812,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          "neutral": {
            "prompt_tokens": 171,
            "completion_tokens": 602,
            "total_tokens": 773,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          },
          "total_prompt_tokens": 515,
          "total_completion_tokens": 1882,
          "total_tokens": 2397
        },
        "token_events": [
          {
            "paradigm": "self_consistency",
            "agent": "aggressive",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "aggressive_gm",
            "prompt_tokens": 172,
            "completion_tokens": 640,
            "total_tokens": 812,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "positional",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "positional_gm",
            "prompt_tokens": 172,
            "completion_tokens": 640,
            "total_tokens": 812,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "length"
          },
          {
            "paradigm": "self_consistency",
            "agent": "neutral",
            "ply_index": 15,
            "turn_number": 8,
            "round": 1,
            "response_source": "neutral_gm",
            "prompt_tokens": 171,
            "completion_tokens": 602,
            "total_tokens": 773,
            "model": "gpt-3.5-turbo-instruct",
            "finish_reason": "stop"
          }
        ]
      },
      "board_fen": "r1bqk2r/ppp2ppp/2nb4/4p3/8/3B1N2/PPPP1PPP/R1BQK2R b KQkq - 2 8"
    }
  ],
  "white_errors": [],
  "black_errors": [],
  "white_plan_log": [],
  "black_plan_log": [],
  "timestamp": "2025-11-30T00:16:30.261937",
  "game_id": "mistralai_mistral-small-24b-instruct-2501_vs_gpt-3.5-turbo-instruct_SC_6",
  "configuration": "SC"
}